{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Loading... <p>An extensive Reinforcement Learning (RL) for Combinatorial Optimization (CO) benchmark. Our goal is to provide a unified framework for RL-based CO algorithms, and to facilitate reproducible research in this field, decoupling the science from the engineering.</p> <p>RL4CO is built upon:</p> <ul> <li>TorchRL: official PyTorch framework for RL algorithms and vectorized environments on GPUs</li> <li>TensorDict: a library to easily handle heterogeneous data such as states, actions and rewards</li> <li>PyTorch Lightning: a lightweight PyTorch wrapper for high-performance AI research</li> <li>Hydra: a framework for elegantly configuring complex applications</li> </ul> <p>We offer flexible and efficient implementations of the following policies:</p> <ul> <li>Constructive: learn to construct a solution from scratch<ul> <li>Autoregressive (AR): construct solutions one step at a time via a decoder</li> <li>NonAutoregressive (NAR): learn to predict a heuristic, such as a heatmap, to then construct a solution</li> </ul> </li> <li>Improvement: learn to improve an pre-existing solution</li> </ul> <p>We provide several utilities and modularization. For example, we modularize reusable components such as environment embeddings that can easily be swapped to solve new problems.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>RL4CO is now available for installation on <code>pip</code>! <pre><code>pip install rl4co\n</code></pre></p> <p>To get started, we recommend checking out our quickstart notebook or the minimalistic example below.</p>"},{"location":"#install-from-source","title":"Install from source","text":"<p>This command installs the bleeding edge <code>main</code> version, useful for staying up-to-date with the latest developments - for instance, if a bug has been fixed since the last official release but a new release hasn\u2019t been rolled out yet:</p> <pre><code>pip install -U git+https://github.com/ai4co/rl4co.git\n</code></pre>"},{"location":"#local-install-and-development","title":"Local install and development","text":"<p>If you want to develop RL4CO we recommend you to install it locally with <code>pip</code> in editable mode:</p> <pre><code>git clone https://github.com/ai4co/rl4co &amp;&amp; cd rl4co\npip install -e .\n</code></pre> <p>We recommend using a virtual environment such as <code>conda</code> to install <code>rl4co</code> locally.</p>"},{"location":"#usage","title":"Usage","text":"<p>Train model with default configuration (AM on TSP environment): <pre><code>python run.py\n</code></pre></p> <p>Tip</p> <p>You may check out this notebook to get started with Hydra!</p> Change experiment settings  Train model with chosen experiment configuration from [configs/experiment/](configs/experiment/) <pre><code>python run.py experiment=routing/am env=tsp env.num_loc=50 model.optimizer_kwargs.lr=2e-4\n</code></pre> Here you may change the environment, e.g. with `env=cvrp` by command line or by modifying the corresponding experiment e.g. [configs/experiment/routing/am.yaml](configs/experiment/routing/am.yaml).   Disable logging <pre><code>python run.py experiment=routing/am logger=none '~callbacks.learning_rate_monitor'\n</code></pre> Note that `~` is used to disable a callback that would need a logger.   Create a sweep over hyperparameters (-m for multirun) <pre><code>python run.py -m experiment=routing/am  model.optimizer.lr=1e-3,1e-4,1e-5\n</code></pre>"},{"location":"#minimalistic-example","title":"Minimalistic Example","text":"<p>Here is a minimalistic example training the Attention Model with greedy rollout baseline on TSP in less than 30 lines of code:</p> <pre><code>from rl4co.envs.routing import TSPEnv, TSPGenerator\nfrom rl4co.models import AttentionModelPolicy, POMO\nfrom rl4co.utils import RL4COTrainer\n\n# Instantiate generator and environment\ngenerator = TSPGenerator(num_loc=50, loc_distribution=\"uniform\")\nenv = TSPEnv(generator)\n\n# Create policy and RL model\npolicy = AttentionModelPolicy(env_name=env.name, num_encoder_layers=6)\nmodel = POMO(env, policy, batch_size=64, optimizer_kwargs={\"lr\": 1e-4})\n\n# Instantiate Trainer and fit\ntrainer = RL4COTrainer(max_epochs=10, accelerator=\"gpu\", precision=\"16-mixed\")\ntrainer.fit(model)\n</code></pre> <p>Other examples can be found on the documentation!</p>"},{"location":"#testing","title":"Testing","text":"<p>Run tests with <code>pytest</code> from the root directory:</p> <pre><code>pytest tests\n</code></pre>"},{"location":"#known-bugs","title":"Known Bugs","text":""},{"location":"#bugs-installing-pytorch-geometric-pyg","title":"Bugs installing PyTorch Geometric (PyG)","text":"<p>Installing <code>PyG</code> via <code>Conda</code> seems to update Torch itself. We have found that this update introduces some bugs with <code>torchrl</code>. At this moment, we recommend installing <code>PyG</code> with <code>Pip</code>: <pre><code>pip install torch_geometric\n</code></pre></p>"},{"location":"#contributing","title":"Contributing","text":"<p>Have a suggestion, request, or found a bug? Feel free to open an issue or submit a pull request. If you would like to contribute, please check out our contribution guidelines   here. We welcome and look forward to all contributions to RL4CO!</p> <p>We are also on Slack if you have any questions or would like to discuss RL4CO with us. We are open to collaborations and would love to hear from you \ud83d\ude80</p>"},{"location":"#contributors","title":"Contributors","text":""},{"location":"#citation","title":"Citation","text":"<p>If you find RL4CO valuable for your research or applied projects:</p> <pre><code>@article{berto2024rl4co,\n    title={{RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark}},\n    author={Federico Berto and Chuanbo Hua and Junyoung Park and Laurin Luttmann and Yining Ma and Fanchen Bu and Jiarui Wang and Haoran Ye and Minsu Kim and Sanghyeok Choi and Nayeli Gast Zepeda and Andr\\'e Hottung and Jianan Zhou and Jieyi Bi and Yu Hu and Fei Liu and Hyeonah Kim and Jiwoo Son and Haeyeon Kim and Davide Angioni and Wouter Kool and Zhiguang Cao and Jie Zhang and Kijung Shin and Cathy Wu and Sungsoo Ahn and Guojie Song and Changhyun Kwon and Lin Xie and Jinkyoo Park},\n    year={2024},\n    journal={arXiv preprint arXiv:2306.17100},\n    note={\\url{https://github.com/ai4co/rl4co}}\n}\n</code></pre> <p>Note that a previous version of RL4CO has been accepted as an oral presentation at the NeurIPS 2023 GLFrontiers Workshop. Since then, the library has greatly evolved and improved!</p>"},{"location":"#join-us","title":"Join us","text":"<p>We invite you to join our AI4CO community, an open research group in Artificial Intelligence (AI) for Combinatorial Optimization (CO)!</p>"},{"location":"README_backup/","title":"README backup","text":"<p> Documentation |   Getting Started |   Usage |   Contributing |   Paper |   Join Us </p> <p>An extensive Reinforcement Learning (RL) for Combinatorial Optimization (CO) benchmark. Our goal is to provide a unified framework for RL-based CO algorithms, and to facilitate reproducible research in this field, decoupling the science from the engineering.</p> <p>RL4CO is built upon:</p> <ul> <li>TorchRL: official PyTorch framework for RL algorithms and vectorized environments on GPUs</li> <li>TensorDict: a library to easily handle heterogeneous data such as states, actions and rewards</li> <li>PyTorch Lightning: a lightweight PyTorch wrapper for high-performance AI research</li> <li>Hydra: a framework for elegantly configuring complex applications</li> </ul> <p>We offer flexible and efficient implementations of the following policies:</p> <ul> <li>Constructive: learn to construct a solution from scratch<ul> <li>Autoregressive (AR): construct solutions one step at a time via a decoder</li> <li>NonAutoregressive (NAR): learn to predict a heuristic, such as a heatmap, to then construct a solution</li> </ul> </li> <li>Improvement: learn to improve an pre-existing solution</li> </ul> <p>We provide several utilities and modularization. For example, we modularize reusable components such as environment embeddings that can easily be swapped to solve new problems.</p>"},{"location":"README_backup/#getting-started","title":"Getting started","text":"<p>RL4CO is now available for installation on <code>pip</code>! <pre><code>pip install rl4co\n</code></pre></p> <p>To get started, we recommend checking out our quickstart notebook or the minimalistic example below.</p>"},{"location":"README_backup/#install-from-source","title":"Install from source","text":"<p>This command installs the bleeding edge <code>main</code> version, useful for staying up-to-date with the latest developments - for instance, if a bug has been fixed since the last official release but a new release hasn\u2019t been rolled out yet:</p> <pre><code>pip install -U git+https://github.com/ai4co/rl4co.git\n</code></pre>"},{"location":"README_backup/#local-install-and-development","title":"Local install and development","text":"<p>If you want to develop RL4CO we recommend you to install it locally with <code>pip</code> in editable mode:</p> <pre><code>git clone https://github.com/ai4co/rl4co &amp;&amp; cd rl4co\npip install -e .\n</code></pre> <p>We recommend using a virtual environment such as <code>conda</code> to install <code>rl4co</code> locally.</p>"},{"location":"README_backup/#usage","title":"Usage","text":"<p>Train model with default configuration (AM on TSP environment): <pre><code>python run.py\n</code></pre></p> <p>Tip</p> <p>You may check out this notebook to get started with Hydra!</p> Change experiment settings  Train model with chosen experiment configuration from [configs/experiment/](configs/experiment/) <pre><code>python run.py experiment=routing/am env=tsp env.num_loc=50 model.optimizer_kwargs.lr=2e-4\n</code></pre> Here you may change the environment, e.g. with `env=cvrp` by command line or by modifying the corresponding experiment e.g. [configs/experiment/routing/am.yaml](configs/experiment/routing/am.yaml).   Disable logging <pre><code>python run.py experiment=routing/am logger=none '~callbacks.learning_rate_monitor'\n</code></pre> Note that `~` is used to disable a callback that would need a logger.   Create a sweep over hyperparameters (-m for multirun) <pre><code>python run.py -m experiment=routing/am  model.optimizer.lr=1e-3,1e-4,1e-5\n</code></pre>"},{"location":"README_backup/#minimalistic-example","title":"Minimalistic Example","text":"<p>Here is a minimalistic example training the Attention Model with greedy rollout baseline on TSP in less than 30 lines of code:</p> <pre><code>from rl4co.envs.routing import TSPEnv, TSPGenerator\nfrom rl4co.models import AttentionModelPolicy, POMO\nfrom rl4co.utils import RL4COTrainer\n\n# Instantiate generator and environment\ngenerator = TSPGenerator(num_loc=50, loc_distribution=\"uniform\")\nenv = TSPEnv(generator)\n\n# Create policy and RL model\npolicy = AttentionModelPolicy(env_name=env.name, num_encoder_layers=6)\nmodel = POMO(env, policy, batch_size=64, optimizer_kwargs={\"lr\": 1e-4})\n\n# Instantiate Trainer and fit\ntrainer = RL4COTrainer(max_epochs=10, accelerator=\"gpu\", precision=\"16-mixed\")\ntrainer.fit(model)\n</code></pre> <p>Other examples can be found on the documentation!</p>"},{"location":"README_backup/#testing","title":"Testing","text":"<p>Run tests with <code>pytest</code> from the root directory:</p> <pre><code>pytest tests\n</code></pre>"},{"location":"README_backup/#known-bugs","title":"Known Bugs","text":""},{"location":"README_backup/#bugs-installing-pytorch-geometric-pyg","title":"Bugs installing PyTorch Geometric (PyG)","text":"<p>Installing <code>PyG</code> via <code>Conda</code> seems to update Torch itself. We have found that this update introduces some bugs with <code>torchrl</code>. At this moment, we recommend installing <code>PyG</code> with <code>Pip</code>: <pre><code>pip install torch_geometric\n</code></pre></p>"},{"location":"README_backup/#contributing","title":"Contributing","text":"<p>Have a suggestion, request, or found a bug? Feel free to open an issue or submit a pull request. If you would like to contribute, please check out our contribution guidelines   here. We welcome and look forward to all contributions to RL4CO!</p> <p>We are also on Slack if you have any questions or would like to discuss RL4CO with us. We are open to collaborations and would love to hear from you \ud83d\ude80</p>"},{"location":"README_backup/#contributors","title":"Contributors","text":""},{"location":"README_backup/#citation","title":"Citation","text":"<p>If you find RL4CO valuable for your research or applied projects:</p> <pre><code>@article{berto2024rl4co,\n    title={{RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark}},\n    author={Federico Berto and Chuanbo Hua and Junyoung Park and Laurin Luttmann and Yining Ma and Fanchen Bu and Jiarui Wang and Haoran Ye and Minsu Kim and Sanghyeok Choi and Nayeli Gast Zepeda and Andr\\'e Hottung and Jianan Zhou and Jieyi Bi and Yu Hu and Fei Liu and Hyeonah Kim and Jiwoo Son and Haeyeon Kim and Davide Angioni and Wouter Kool and Zhiguang Cao and Jie Zhang and Kijung Shin and Cathy Wu and Sungsoo Ahn and Guojie Song and Changhyun Kwon and Lin Xie and Jinkyoo Park},\n    year={2024},\n    journal={arXiv preprint arXiv:2306.17100},\n    note={\\url{https://github.com/ai4co/rl4co}}\n}\n</code></pre> <p>Note that a previous version of RL4CO has been accepted as an oral presentation at the NeurIPS 2023 GLFrontiers Workshop. Since then, the library has greatly evolved and improved!</p>"},{"location":"README_backup/#join-us","title":"Join us","text":"<p>We invite you to join our AI4CO community, an open research group in Artificial Intelligence (AI) for Combinatorial Optimization (CO)!</p>"},{"location":"configs/experiment/","title":"Experiments","text":"<p>[!NOTE]  To reproduce results in the latest version of the paper, please use the most recent version as described below. </p> <p>In the latest experiments version (as of the NeurIPS GLFrontiers WS paper) we used experiments in the routing folder, which  is more modular then before.</p> <p>To change the experiment task, you can simply call the model and change the <code>env</code> to the target such as:</p> <pre><code>python run.py experiment=routing/pomo env=op \n</code></pre> <p>[!TIP]  Stay tuned for the upcoming NeurIPS 2024 released, several new experiments are coming soon!</p>"},{"location":"configs/experiment/#older-experiments","title":"Older experiments","text":"<p>Older version (around <code>v0.0.3</code>) experiments are under the older experiments. Note that there can be rough edges and you may need to install an older version of the library to run these experiments.</p>"},{"location":"configs/experiment/archive/","title":"Older experiment versions","text":"<p>These experiments are the ones we ran in the first version of our paper. The only difference is that, from version <code>0.1.0</code>, we added several new features and made a major refactoring that simplifies our codebase!</p> <p>We will update the experiments with the refactored versions. To use these, you may use RL4CO no greater than version <code>0.0.6</code>:</p> <pre><code>pip install rl4co&lt;=0.0.6\n</code></pre>"},{"location":"docs/","title":"RL4CO Documentation","text":"<p>We use MkDocs to generate the documentation with the MkDocs Material theme.</p>"},{"location":"docs/#development","title":"Development","text":"<p>From the root directory:</p> <ol> <li>Install RL4CO locally</li> </ol> <pre><code>pip install -e \".[dev,graph,routing,docs]\"\n</code></pre> <p>note that <code>docs</code> is the extra requirement for the documentation.</p> <ol> <li>To build the documentation, run:</li> </ol> <pre><code>mkdocs serve\n</code></pre>"},{"location":"docs/#hooks","title":"Hooks","text":"<p>We are using the hooks.py for additional modifications. MkDocs for instance cannot detect files that are not in the same directory as an <code>__init__.py</code> (as described here) so we are automatically creating and deleting such files with our script</p>"},{"location":"docs/content/api/data/","title":"Data","text":""},{"location":"docs/content/api/data/#datasets","title":"Datasets","text":""},{"location":"docs/content/api/data/#data.dataset.FastTdDataset","title":"FastTdDataset","text":"<pre><code>FastTdDataset(td: TensorDict)\n</code></pre> <p>               Bases: <code>Dataset</code></p> Note <p>Check out the issue on tensordict for more details: https://github.com/pytorch-labs/tensordict/issues/374.</p> Source code in <code>rl4co/data/dataset.py</code> <pre><code>def __init__(self, td: TensorDict):\n    self.data_len = td.batch_size[0]\n    self.data = td\n</code></pre>"},{"location":"docs/content/api/data/#data.dataset.FastTdDataset.collate_fn","title":"collate_fn  <code>staticmethod</code>","text":"<pre><code>collate_fn(batch: Union[dict, TensorDict])\n</code></pre> <p>Collate function compatible with TensorDicts that reassembles a list of dicts.</p> Source code in <code>rl4co/data/dataset.py</code> <pre><code>@staticmethod\ndef collate_fn(batch: Union[dict, TensorDict]):\n    \"\"\"Collate function compatible with TensorDicts that reassembles a list of dicts.\"\"\"\n    return batch\n</code></pre>"},{"location":"docs/content/api/data/#data.dataset.TensorDictDataset","title":"TensorDictDataset","text":"<pre><code>TensorDictDataset(td: TensorDict)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dataset compatible with TensorDicts with low CPU usage. Fast loading but somewhat slow instantiation due to list comprehension since we \"disassemble\" the TensorDict into a list of dicts.</p> Note <p>Check out the issue on tensordict for more details: https://github.com/pytorch-labs/tensordict/issues/374.</p> Source code in <code>rl4co/data/dataset.py</code> <pre><code>def __init__(self, td: TensorDict):\n    self.data_len = td.batch_size[0]\n    self.data = [\n        {key: value[i] for key, value in td.items()} for i in range(self.data_len)\n    ]\n</code></pre>"},{"location":"docs/content/api/data/#data.dataset.TensorDictDataset.collate_fn","title":"collate_fn  <code>staticmethod</code>","text":"<pre><code>collate_fn(batch: Union[dict, TensorDict])\n</code></pre> <p>Collate function compatible with TensorDicts that reassembles a list of dicts.</p> Source code in <code>rl4co/data/dataset.py</code> <pre><code>@staticmethod\ndef collate_fn(batch: Union[dict, TensorDict]):\n    \"\"\"Collate function compatible with TensorDicts that reassembles a list of dicts.\"\"\"\n    return TensorDict(\n        {key: torch.stack([b[key] for b in batch]) for key in batch[0].keys()},\n        batch_size=torch.Size([len(batch)]),\n        **td_kwargs,\n    )\n</code></pre>"},{"location":"docs/content/api/data/#data.dataset.ExtraKeyDataset","title":"ExtraKeyDataset","text":"<pre><code>ExtraKeyDataset(\n    dataset: TensorDictDataset,\n    extra: Tensor,\n    key_name=\"extra\",\n)\n</code></pre> <p>               Bases: <code>TensorDictDataset</code></p> <p>Dataset that includes an extra key to add to the data dict. This is useful for adding a REINFORCE baseline reward to the data dict. Note that this is faster to instantiate than using list comprehension.</p> Source code in <code>rl4co/data/dataset.py</code> <pre><code>def __init__(self, dataset: TensorDictDataset, extra: torch.Tensor, key_name=\"extra\"):\n    self.data_len = len(dataset)\n    assert self.data_len == len(extra), \"Data and extra must be same length\"\n    self.data = dataset.data\n    self.extra = extra\n    self.key_name = key_name\n</code></pre>"},{"location":"docs/content/api/data/#data.dataset.TensorDictDatasetFastGeneration","title":"TensorDictDatasetFastGeneration","text":"<pre><code>TensorDictDatasetFastGeneration(td: TensorDict)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dataset compatible with TensorDicts. Similar performance in loading to list comprehension, but is faster in instantiation than :class:<code>TensorDictDatasetList</code> (more than 10x faster).</p> Warning <p>Note that directly indexing TensorDicts may be faster in creating the dataset but uses &gt; 3x more CPU. We may generally recommend using the :class:<code>TensorDictDatasetList</code></p> Note <p>Check out the issue on tensordict for more details: https://github.com/pytorch-labs/tensordict/issues/374.</p> Source code in <code>rl4co/data/dataset.py</code> <pre><code>def __init__(self, td: TensorDict):\n    self.data = td\n</code></pre>"},{"location":"docs/content/api/data/#data.dataset.TensorDictDatasetFastGeneration.collate_fn","title":"collate_fn  <code>staticmethod</code>","text":"<pre><code>collate_fn(batch: Union[dict, TensorDict])\n</code></pre> <p>Equivalent to collating with <code>lambda x: x</code></p> Source code in <code>rl4co/data/dataset.py</code> <pre><code>@staticmethod\ndef collate_fn(batch: Union[dict, TensorDict]):\n    \"\"\"Equivalent to collating with `lambda x: x`\"\"\"\n    return batch\n</code></pre>"},{"location":"docs/content/api/data/#data-generation","title":"Data Generation","text":""},{"location":"docs/content/api/data/#data.generate_data.generate_env_data","title":"generate_env_data","text":"<pre><code>generate_env_data(env_type, *args, **kwargs)\n</code></pre> <p>Generate data for a given environment type in the form of a dictionary</p> Source code in <code>rl4co/data/generate_data.py</code> <pre><code>def generate_env_data(env_type, *args, **kwargs):\n    \"\"\"Generate data for a given environment type in the form of a dictionary\"\"\"\n    try:\n        # breakpoint()\n        # remove all None values from args\n        args = [arg for arg in args if arg is not None]\n\n        return getattr(sys.modules[__name__], f\"generate_{env_type}_data\")(\n            *args, **kwargs\n        )\n    except AttributeError:\n        raise NotImplementedError(f\"Environment type {env_type} not implemented\")\n</code></pre>"},{"location":"docs/content/api/data/#data.generate_data.generate_mdpp_data","title":"generate_mdpp_data","text":"<pre><code>generate_mdpp_data(\n    dataset_size,\n    size=10,\n    num_probes_min=2,\n    num_probes_max=5,\n    num_keepout_min=1,\n    num_keepout_max=50,\n    lock_size=True,\n)\n</code></pre> <p>Generate data for the nDPP problem. If <code>lock_size</code> is True, then the size if fixed and we skip the <code>size</code> argument if it is not 10. This is because the RL environment is based on a real-world PCB (parametrized with data)</p> Source code in <code>rl4co/data/generate_data.py</code> <pre><code>def generate_mdpp_data(\n    dataset_size,\n    size=10,\n    num_probes_min=2,\n    num_probes_max=5,\n    num_keepout_min=1,\n    num_keepout_max=50,\n    lock_size=True,\n):\n    \"\"\"Generate data for the nDPP problem.\n    If `lock_size` is True, then the size if fixed and we skip the `size` argument if it is not 10.\n    This is because the RL environment is based on a real-world PCB (parametrized with data)\n    \"\"\"\n    if lock_size and size != 10:\n        # log.info(\"Locking size to 10, skipping generate_mdpp_data with size {}\".format(size))\n        return None\n\n    bs = dataset_size  # bs = batch_size to generate data in batch\n    m = n = size\n    if isinstance(bs, int):\n        bs = [bs]\n\n    locs = np.stack(np.meshgrid(np.arange(m), np.arange(n)), axis=-1).reshape(-1, 2)\n    locs = locs / np.array([m, n], dtype=np.float32)\n    locs = np.expand_dims(locs, axis=0)\n    locs = np.repeat(locs, bs[0], axis=0)\n\n    available = np.ones((bs[0], m * n), dtype=bool)\n\n    probe = np.random.randint(0, high=m * n, size=(bs[0], 1))\n    np.put_along_axis(available, probe, False, axis=1)\n\n    num_probe = np.random.randint(num_probes_min, num_probes_max + 1, size=(bs[0], 1))\n    probes = np.zeros((bs[0], m * n), dtype=bool)\n    for i in range(bs[0]):\n        p = np.random.choice(m * n, num_probe[i], replace=False)\n        np.put_along_axis(available[i], p, False, axis=0)\n        np.put_along_axis(probes[i], p, True, axis=0)\n\n    num_keepout = np.random.randint(num_keepout_min, num_keepout_max + 1, size=(bs[0], 1))\n    for i in range(bs[0]):\n        k = np.random.choice(m * n, num_keepout[i], replace=False)\n        np.put_along_axis(available[i], k, False, axis=0)\n\n    return {\n        \"locs\": locs.astype(np.float32),\n        \"probe\": probes.astype(bool),\n        \"action_mask\": available.astype(bool),\n    }\n</code></pre>"},{"location":"docs/content/api/data/#data.generate_data.generate_dataset","title":"generate_dataset","text":"<pre><code>generate_dataset(\n    filename: Union[str, List[str]] = None,\n    data_dir: str = \"data\",\n    name: str = None,\n    problem: Union[str, List[str]] = \"all\",\n    data_distribution: str = \"all\",\n    dataset_size: int = 10000,\n    graph_sizes: Union[int, List[int]] = [20, 50, 100],\n    overwrite: bool = False,\n    seed: int = 1234,\n    disable_warning: bool = True,\n    distributions_per_problem: Union[int, dict] = None,\n)\n</code></pre> <p>We keep a similar structure as in Kool et al. 2019 but save and load the data as npz This is way faster and more memory efficient than pickle and also allows for easy transfer to TensorDict</p> <p>Parameters:</p> <ul> <li> <code>filename</code>               (<code>Union[str, List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Filename to save the data to. If None, the data is saved to data_dir/problem/problem_graph_size_seed.npz. Defaults to None.</p> </li> <li> <code>data_dir</code>               (<code>str</code>, default:                   <code>'data'</code> )           \u2013            <p>Directory to save the data to. Defaults to \"data\".</p> </li> <li> <code>name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Name of the dataset. Defaults to None.</p> </li> <li> <code>problem</code>               (<code>Union[str, List[str]]</code>, default:                   <code>'all'</code> )           \u2013            <p>Problem to generate data for. Defaults to \"all\".</p> </li> <li> <code>data_distribution</code>               (<code>str</code>, default:                   <code>'all'</code> )           \u2013            <p>Data distribution to generate data for. Defaults to \"all\".</p> </li> <li> <code>dataset_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>Number of datasets to generate. Defaults to 10000.</p> </li> <li> <code>graph_sizes</code>               (<code>Union[int, List[int]]</code>, default:                   <code>[20, 50, 100]</code> )           \u2013            <p>Graph size to generate data for. Defaults to [20, 50, 100].</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>1234</code> )           \u2013            <p>Random seed. Defaults to 1234.</p> </li> <li> <code>disable_warning</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to disable warnings. Defaults to True.</p> </li> <li> <code>distributions_per_problem</code>               (<code>Union[int, dict]</code>, default:                   <code>None</code> )           \u2013            <p>Number of distributions to generate per problem. Defaults to None.</p> </li> </ul> Source code in <code>rl4co/data/generate_data.py</code> <pre><code>def generate_dataset(\n    filename: Union[str, List[str]] = None,\n    data_dir: str = \"data\",\n    name: str = None,\n    problem: Union[str, List[str]] = \"all\",\n    data_distribution: str = \"all\",\n    dataset_size: int = 10000,\n    graph_sizes: Union[int, List[int]] = [20, 50, 100],\n    overwrite: bool = False,\n    seed: int = 1234,\n    disable_warning: bool = True,\n    distributions_per_problem: Union[int, dict] = None,\n):\n    \"\"\"We keep a similar structure as in Kool et al. 2019 but save and load the data as npz\n    This is way faster and more memory efficient than pickle and also allows for easy transfer to TensorDict\n\n    Args:\n        filename: Filename to save the data to. If None, the data is saved to data_dir/problem/problem_graph_size_seed.npz. Defaults to None.\n        data_dir: Directory to save the data to. Defaults to \"data\".\n        name: Name of the dataset. Defaults to None.\n        problem: Problem to generate data for. Defaults to \"all\".\n        data_distribution: Data distribution to generate data for. Defaults to \"all\".\n        dataset_size: Number of datasets to generate. Defaults to 10000.\n        graph_sizes: Graph size to generate data for. Defaults to [20, 50, 100].\n        overwrite: Whether to overwrite existing files. Defaults to False.\n        seed: Random seed. Defaults to 1234.\n        disable_warning: Whether to disable warnings. Defaults to True.\n        distributions_per_problem: Number of distributions to generate per problem. Defaults to None.\n    \"\"\"\n\n    if isinstance(problem, list) and len(problem) == 1:\n        problem = problem[0]\n\n    graph_sizes = [graph_sizes] if isinstance(graph_sizes, int) else graph_sizes\n\n    if distributions_per_problem is None:\n        distributions_per_problem = DISTRIBUTIONS_PER_PROBLEM\n\n    if problem == \"all\":\n        problems = distributions_per_problem\n    else:\n        problems = {\n            problem: distributions_per_problem[problem]\n            if data_distribution == \"all\"\n            else [data_distribution]\n        }\n\n    # Support multiple filenames if necessary\n    filenames = [filename] if isinstance(filename, str) else filename\n    iter = 0\n\n    # Main loop for data generation. We loop over all problems, distributions and sizes\n    for problem, distributions in problems.items():\n        for distribution in distributions or [None]:\n            for graph_size in graph_sizes:\n                if filename is None:\n                    datadir = os.path.join(data_dir, problem)\n                    os.makedirs(datadir, exist_ok=True)\n                    fname = os.path.join(\n                        datadir,\n                        \"{}{}{}_{}_seed{}.npz\".format(\n                            problem,\n                            \"_{}\".format(distribution)\n                            if distribution is not None\n                            else \"\",\n                            graph_size,\n                            name,\n                            seed,\n                        ),\n                    )\n                else:\n                    try:\n                        fname = filenames[iter]\n                        # make directory if necessary\n                        os.makedirs(os.path.dirname(fname), exist_ok=True)\n                        iter += 1\n                    except Exception:\n                        raise ValueError(\n                            \"Number of filenames does not match number of problems\"\n                        )\n                    fname = check_extension(filename, extension=\".npz\")\n\n                if not overwrite and os.path.isfile(\n                    check_extension(fname, extension=\".npz\")\n                ):\n                    if not disable_warning:\n                        log.info(\n                            \"File {} already exists! Run with -f option to overwrite. Skipping...\".format(\n                                fname\n                            )\n                        )\n                    continue\n\n                # Set seed\n                np.random.seed(seed)\n\n                # Automatically generate dataset\n                dataset = generate_env_data(\n                    problem, dataset_size, graph_size, distribution\n                )\n\n                # A function can return None in case of an error or a skip\n                if dataset is not None:\n                    # Save to disk as dict\n                    log.info(\"Saving {} dataset to {}\".format(problem, fname))\n                    np.savez(fname, **dataset)\n</code></pre>"},{"location":"docs/content/api/data/#data.generate_data.generate_default_datasets","title":"generate_default_datasets","text":"<pre><code>generate_default_datasets(data_dir, generate_eda=False)\n</code></pre> <p>Generate the default datasets used in the paper and save them to data_dir/problem</p> Source code in <code>rl4co/data/generate_data.py</code> <pre><code>def generate_default_datasets(data_dir, generate_eda=False):\n    \"\"\"Generate the default datasets used in the paper and save them to data_dir/problem\"\"\"\n    generate_dataset(data_dir=data_dir, name=\"val\", problem=\"all\", seed=4321)\n    generate_dataset(data_dir=data_dir, name=\"test\", problem=\"all\", seed=1234)\n\n    # By default, we skip the EDA datasets since they can easily be generated on the fly when needed\n    if generate_eda:\n        generate_dataset(\n            data_dir=data_dir,\n            name=\"test\",\n            problem=\"mdpp\",\n            seed=1234,\n            graph_sizes=[10],\n            dataset_size=100,\n        )  # EDA (mDPP)\n</code></pre>"},{"location":"docs/content/api/data/#transforms","title":"Transforms","text":""},{"location":"docs/content/api/data/#data.transforms.StateAugmentation","title":"StateAugmentation","text":"<pre><code>StateAugmentation(\n    num_augment: int = 8,\n    augment_fn: Union[str, callable] = \"symmetric\",\n    first_aug_identity: bool = True,\n    normalize: bool = False,\n    feats: list = None,\n)\n</code></pre> <p>               Bases: <code>object</code></p> <p>Augment state by N times via symmetric rotation/reflection transform</p> <p>Parameters:</p> <ul> <li> <code>num_augment</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>number of augmentations</p> </li> <li> <code>augment_fn</code>               (<code>Union[str, callable]</code>, default:                   <code>'symmetric'</code> )           \u2013            <p>augmentation function to use, e.g. 'symmetric' (default) or 'dihedral8', if callable,  then use the function directly. If 'dihedral8', then num_augment must be 8</p> </li> <li> <code>first_aug_identity</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to augment the first data point too</p> </li> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to normalize the augmented data</p> </li> <li> <code>feats</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>list of features to augment</p> </li> </ul> Source code in <code>rl4co/data/transforms.py</code> <pre><code>def __init__(\n    self,\n    num_augment: int = 8,\n    augment_fn: Union[str, callable] = 'symmetric', \n    first_aug_identity: bool = True,\n    normalize: bool = False,\n    feats: list = None,\n):\n    self.augmentation = get_augment_function(augment_fn)\n    assert not (\n        self.augmentation == dihedral_8_augmentation_wrapper and num_augment != 8\n    ), \"When using the `dihedral8` augmentation function, then num_augment must be 8\"\n\n    if feats is None:\n        log.info(\"Features not passed, defaulting to 'locs'\")\n        self.feats = [\"locs\"]\n    else:\n        self.feats = feats\n    self.num_augment = num_augment\n    self.normalize = normalize\n    self.first_aug_identity = first_aug_identity\n</code></pre>"},{"location":"docs/content/api/data/#data.transforms.dihedral_8_augmentation","title":"dihedral_8_augmentation","text":"<pre><code>dihedral_8_augmentation(xy: Tensor) -&gt; Tensor\n</code></pre> <p>Augmentation (x8) for grid-based data (x, y) as done in POMO. This is a Dihedral group of order 8 (rotations and reflections) https://en.wikipedia.org/wiki/Examples_of_groups#dihedral_group_of_order_8</p> <p>Parameters:</p> <ul> <li> <code>xy</code>               (<code>Tensor</code>)           \u2013            <p>[batch, graph, 2] tensor of x and y coordinates</p> </li> </ul> Source code in <code>rl4co/data/transforms.py</code> <pre><code>def dihedral_8_augmentation(xy: Tensor) -&gt; Tensor:\n    \"\"\"\n    Augmentation (x8) for grid-based data (x, y) as done in POMO.\n    This is a Dihedral group of order 8 (rotations and reflections)\n    https://en.wikipedia.org/wiki/Examples_of_groups#dihedral_group_of_order_8\n\n    Args:\n        xy: [batch, graph, 2] tensor of x and y coordinates\n    \"\"\"\n    # [batch, graph, 2]\n    x, y = xy.split(1, dim=2)\n    # augmnetations [batch, graph, 2]\n    z0 = torch.cat((x, y), dim=2)\n    z1 = torch.cat((1 - x, y), dim=2)\n    z2 = torch.cat((x, 1 - y), dim=2)\n    z3 = torch.cat((1 - x, 1 - y), dim=2)\n    z4 = torch.cat((y, x), dim=2)\n    z5 = torch.cat((1 - y, x), dim=2)\n    z6 = torch.cat((y, 1 - x), dim=2)\n    z7 = torch.cat((1 - y, 1 - x), dim=2)\n    # [batch*8, graph, 2]\n    aug_xy = torch.cat((z0, z1, z2, z3, z4, z5, z6, z7), dim=0)\n    return aug_xy\n</code></pre>"},{"location":"docs/content/api/data/#data.transforms.dihedral_8_augmentation_wrapper","title":"dihedral_8_augmentation_wrapper","text":"<pre><code>dihedral_8_augmentation_wrapper(\n    xy: Tensor, reduce: bool = True, *args, **kw\n) -&gt; Tensor\n</code></pre> <p>Wrapper for dihedral_8_augmentation. If reduce, only return the first 1/8 of the augmented data since the augmentation augments the data 8 times.</p> Source code in <code>rl4co/data/transforms.py</code> <pre><code>def dihedral_8_augmentation_wrapper(\n    xy: Tensor, reduce: bool = True, *args, **kw\n) -&gt; Tensor:\n    \"\"\"Wrapper for dihedral_8_augmentation. If reduce, only return the first 1/8 of the augmented data\n    since the augmentation augments the data 8 times.\n    \"\"\"\n    xy = xy[: xy.shape[0] // 8, ...] if reduce else xy\n    return dihedral_8_augmentation(xy)\n</code></pre>"},{"location":"docs/content/api/data/#data.transforms.symmetric_transform","title":"symmetric_transform","text":"<pre><code>symmetric_transform(\n    x: Tensor, y: Tensor, phi: Tensor, offset: float = 0.5\n)\n</code></pre> <p>SR group transform with rotation and reflection Like the one in SymNCO, but a vectorized version</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>[batch, graph, 1] tensor of x coordinates</p> </li> <li> <code>y</code>               (<code>Tensor</code>)           \u2013            <p>[batch, graph, 1] tensor of y coordinates</p> </li> <li> <code>phi</code>               (<code>Tensor</code>)           \u2013            <p>[batch, 1] tensor of random rotation angles</p> </li> <li> <code>offset</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>offset for x and y coordinates</p> </li> </ul> Source code in <code>rl4co/data/transforms.py</code> <pre><code>def symmetric_transform(x: Tensor, y: Tensor, phi: Tensor, offset: float = 0.5):\n    \"\"\"SR group transform with rotation and reflection\n    Like the one in SymNCO, but a vectorized version\n\n    Args:\n        x: [batch, graph, 1] tensor of x coordinates\n        y: [batch, graph, 1] tensor of y coordinates\n        phi: [batch, 1] tensor of random rotation angles\n        offset: offset for x and y coordinates\n    \"\"\"\n    x, y = x - offset, y - offset\n    # random rotation\n    x_prime = torch.cos(phi) * x - torch.sin(phi) * y\n    y_prime = torch.sin(phi) * x + torch.cos(phi) * y\n    # make random reflection if phi &gt; 2*pi (i.e. 50% of the time)\n    mask = phi &gt; 2 * math.pi\n    # vectorized random reflection: swap axes x and y if mask\n    xy = torch.cat((x_prime, y_prime), dim=-1)\n    xy = torch.where(mask, xy.flip(-1), xy)\n    return xy + offset\n</code></pre>"},{"location":"docs/content/api/data/#data.transforms.symmetric_augmentation","title":"symmetric_augmentation","text":"<pre><code>symmetric_augmentation(\n    xy: Tensor,\n    num_augment: int = 8,\n    first_augment: bool = False,\n)\n</code></pre> <p>Augment xy data by <code>num_augment</code> times via symmetric rotation transform and concatenate to original data</p> <p>Parameters:</p> <ul> <li> <code>xy</code>               (<code>Tensor</code>)           \u2013            <p>[batch, graph, 2] tensor of x and y coordinates</p> </li> <li> <code>num_augment</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>number of augmentations</p> </li> <li> <code>first_augment</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to augment the first data point</p> </li> </ul> Source code in <code>rl4co/data/transforms.py</code> <pre><code>def symmetric_augmentation(xy: Tensor, num_augment: int = 8, first_augment: bool = False):\n    \"\"\"Augment xy data by `num_augment` times via symmetric rotation transform and concatenate to original data\n\n    Args:\n        xy: [batch, graph, 2] tensor of x and y coordinates\n        num_augment: number of augmentations\n        first_augment: whether to augment the first data point\n    \"\"\"\n    # create random rotation angles (4*pi for reflection, 2*pi for rotation)\n    phi = torch.rand(xy.shape[0], device=xy.device) * 4 * math.pi\n\n    # set phi to 0 for first , i.e. no augmentation as in SymNCO\n    if not first_augment:\n        phi[: xy.shape[0] // num_augment] = 0.0\n    x, y = xy[..., [0]], xy[..., [1]]\n    return symmetric_transform(x, y, phi[:, None, None])\n</code></pre>"},{"location":"docs/content/api/data/#utils","title":"Utils","text":""},{"location":"docs/content/api/data/#data.utils.load_npz_to_tensordict","title":"load_npz_to_tensordict","text":"<pre><code>load_npz_to_tensordict(filename)\n</code></pre> <p>Load a npz file directly into a TensorDict We assume that the npz file contains a dictionary of numpy arrays This is at least an order of magnitude faster than pickle</p> Source code in <code>rl4co/data/utils.py</code> <pre><code>def load_npz_to_tensordict(filename):\n    \"\"\"Load a npz file directly into a TensorDict\n    We assume that the npz file contains a dictionary of numpy arrays\n    This is at least an order of magnitude faster than pickle\n    \"\"\"\n    x = np.load(filename)\n    x_dict = dict(x)\n    batch_size = x_dict[list(x_dict.keys())[0]].shape[0]\n    return TensorDict(x_dict, batch_size=batch_size)\n</code></pre>"},{"location":"docs/content/api/data/#data.utils.save_tensordict_to_npz","title":"save_tensordict_to_npz","text":"<pre><code>save_tensordict_to_npz(\n    tensordict, filename, compress: bool = False\n)\n</code></pre> <p>Save a TensorDict to a npz file We assume that the TensorDict contains a dictionary of tensors</p> Source code in <code>rl4co/data/utils.py</code> <pre><code>def save_tensordict_to_npz(tensordict, filename, compress: bool = False):\n    \"\"\"Save a TensorDict to a npz file\n    We assume that the TensorDict contains a dictionary of tensors\n    \"\"\"\n    x_dict = {k: v.numpy() for k, v in tensordict.items()}\n    if compress:\n        np.savez_compressed(filename, **x_dict)\n    else:\n        np.savez(filename, **x_dict)\n</code></pre>"},{"location":"docs/content/api/data/#data.utils.check_extension","title":"check_extension","text":"<pre><code>check_extension(filename, extension='.npz')\n</code></pre> <p>Check that filename has extension, otherwise add it</p> Source code in <code>rl4co/data/utils.py</code> <pre><code>def check_extension(filename, extension=\".npz\"):\n    \"\"\"Check that filename has extension, otherwise add it\"\"\"\n    if os.path.splitext(filename)[1] != extension:\n        return filename + extension\n    return filename\n</code></pre>"},{"location":"docs/content/api/data/#data.utils.load_solomon_instance","title":"load_solomon_instance","text":"<pre><code>load_solomon_instance(name, path=None, edge_weights=False)\n</code></pre> <p>Load solomon instance from a file</p> Source code in <code>rl4co/data/utils.py</code> <pre><code>def load_solomon_instance(name, path=None, edge_weights=False):\n    \"\"\"Load solomon instance from a file\"\"\"\n    import vrplib\n\n    if not path:\n        path = \"data/solomon/instances/\"\n        path = os.path.join(ROOT_PATH, path)\n    if not os.path.isdir(path):\n        os.makedirs(path)\n    file_path = f\"{path}{name}.txt\"\n    if not os.path.isfile(file_path):\n        vrplib.download_instance(name=name, path=path)\n    return vrplib.read_instance(\n        path=file_path,\n        instance_format=\"solomon\",\n        compute_edge_weights=edge_weights,\n    )\n</code></pre>"},{"location":"docs/content/api/data/#data.utils.load_solomon_solution","title":"load_solomon_solution","text":"<pre><code>load_solomon_solution(name, path=None)\n</code></pre> <p>Load solomon solution from a file</p> Source code in <code>rl4co/data/utils.py</code> <pre><code>def load_solomon_solution(name, path=None):\n    \"\"\"Load solomon solution from a file\"\"\"\n    import vrplib\n\n    if not path:\n        path = \"data/solomon/solutions/\"\n        path = os.path.join(ROOT_PATH, path)\n    if not os.path.isdir(path):\n        os.makedirs(path)\n    file_path = f\"{path}{name}.sol\"\n    if not os.path.isfile(file_path):\n        vrplib.download_solution(name=name, path=path)\n    return vrplib.read_solution(path=file_path)\n</code></pre>"},{"location":"docs/content/api/decoding/","title":"Decoding Strategies","text":""},{"location":"docs/content/api/decoding/#utils.decoding.DecodingStrategy","title":"DecodingStrategy","text":"<pre><code>DecodingStrategy(\n    temperature: float = 1.0,\n    top_p: float = 0.0,\n    top_k: int = 0,\n    mask_logits: bool = True,\n    tanh_clipping: float = 0,\n    multistart: bool = False,\n    multisample: bool = False,\n    num_starts: Optional[int] = None,\n    select_start_nodes_fn: Optional[callable] = None,\n    improvement_method_mode: bool = False,\n    select_best: bool = False,\n    store_all_logp: bool = False,\n    **kwargs\n)\n</code></pre> <p>Base class for decoding strategies. Subclasses should implement the :meth:<code>_step</code> method. Includes hooks for pre and post main decoding operations.</p> <p>Parameters:</p> <ul> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Temperature scaling. Higher values make the distribution more uniform (exploration), lower values make it more peaky (exploitation). Defaults to 1.0.</p> </li> <li> <code>top_p</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Top-p sampling, a.k.a. Nucleus Sampling (https://arxiv.org/abs/1904.09751). Defaults to 0.0.</p> </li> <li> <code>top_k</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Top-k sampling, i.e. restrict sampling to the top k logits. If 0, do not perform. Defaults to 0.</p> </li> <li> <code>mask_logits</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to mask logits of infeasible actions. Defaults to True.</p> </li> <li> <code>tanh_clipping</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Tanh clipping (https://arxiv.org/abs/1611.09940). Defaults to 0.</p> </li> <li> <code>multistart</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use multistart decoding. Defaults to False.</p> </li> <li> <code>multisample</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use sampling decoding. Defaults to False.</p> </li> <li> <code>num_starts</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Number of starts for multistart decoding. Defaults to None.</p> </li> </ul> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>def __init__(\n    self,\n    temperature: float = 1.0,\n    top_p: float = 0.0,\n    top_k: int = 0,\n    mask_logits: bool = True,\n    tanh_clipping: float = 0,\n    multistart: bool = False,\n    multisample: bool = False,\n    num_starts: Optional[int] = None,\n    select_start_nodes_fn: Optional[callable] = None,\n    improvement_method_mode: bool = False,\n    select_best: bool = False,\n    store_all_logp: bool = False,\n    **kwargs,\n) -&gt; None:\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.mask_logits = mask_logits\n    self.tanh_clipping = tanh_clipping\n    self.multistart = multistart\n    self.multisample = multisample\n    self.num_starts = num_starts\n    self.select_start_nodes_fn = select_start_nodes_fn\n    self.improvement_method_mode = improvement_method_mode\n    self.select_best = select_best\n    self.store_all_logp = store_all_logp\n    # initialize buffers\n    self.actions = []\n    self.logprobs = []\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.DecodingStrategy.pre_decoder_hook","title":"pre_decoder_hook","text":"<pre><code>pre_decoder_hook(\n    td: TensorDict, env: RL4COEnvBase, action: Tensor = None\n)\n</code></pre> <p>Pre decoding hook. This method is called before the main decoding operation.</p> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>def pre_decoder_hook(\n    self, td: TensorDict, env: RL4COEnvBase, action: torch.Tensor = None\n):\n    \"\"\"Pre decoding hook. This method is called before the main decoding operation.\"\"\"\n\n    # Multi-start decoding. If num_starts is None, we use the number of actions in the action mask\n    if self.multistart or self.multisample:\n        if self.num_starts is None:\n            self.num_starts = env.get_num_starts(td)\n            if self.multisample:\n                log.warn(\n                    f\"num_starts is not provided for sampling, using num_starts={self.num_starts}\"\n                )\n    else:\n        if self.num_starts is not None:\n            if self.num_starts &gt;= 1:\n                log.warn(\n                    f\"num_starts={self.num_starts} is ignored for decode_type={self.name}\"\n                )\n\n        self.num_starts = 0\n\n    # Multi-start decoding: first action is chosen by ad-hoc node selection\n    if self.num_starts &gt;= 1:\n        if self.multistart:\n            if action is None:  # if action is provided, we use it as the first action\n                if self.select_start_nodes_fn is not None:\n                    action = self.select_start_nodes_fn(td, env, self.num_starts)\n                else:\n                    action = env.select_start_nodes(td, num_starts=self.num_starts)\n\n            # Expand td to batch_size * num_starts\n            td = batchify(td, self.num_starts)\n\n            td.set(\"action\", action)\n            td = env.step(td)[\"next\"]\n            # first logprobs is 0, so p = logprobs.exp() = 1\n            if self.store_all_logp:\n                logprobs = torch.zeros_like(td[\"action_mask\"])  # [B, N]\n            else:\n                logprobs = torch.zeros_like(action, device=td.device)  # [B]\n\n            self.logprobs.append(logprobs)\n            self.actions.append(action)\n        else:\n            # Expand td to batch_size * num_samplestarts\n            td = batchify(td, self.num_starts)\n\n    return td, env, self.num_starts\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.DecodingStrategy.step","title":"step","text":"<pre><code>step(\n    logits: Tensor,\n    mask: Tensor,\n    td: TensorDict = None,\n    action: Tensor = None,\n    **kwargs\n) -&gt; TensorDict\n</code></pre> <p>Main decoding operation. This method should be called in a loop until all sequences are done.</p> <p>Parameters:</p> <ul> <li> <code>logits</code>               (<code>Tensor</code>)           \u2013            <p>Logits from the model.</p> </li> <li> <code>mask</code>               (<code>Tensor</code>)           \u2013            <p>Action mask. 1 if feasible, 0 otherwise (so we keep if 1 as done in PyTorch).</p> </li> <li> <code>td</code>               (<code>TensorDict</code>, default:                   <code>None</code> )           \u2013            <p>TensorDict containing the current state of the environment.</p> </li> <li> <code>action</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Optional action to use, e.g. for evaluating log probabilities.</p> </li> </ul> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>def step(\n    self,\n    logits: torch.Tensor,\n    mask: torch.Tensor,\n    td: TensorDict = None,\n    action: torch.Tensor = None,\n    **kwargs,\n) -&gt; TensorDict:\n    \"\"\"Main decoding operation. This method should be called in a loop until all sequences are done.\n\n    Args:\n        logits: Logits from the model.\n        mask: Action mask. 1 if feasible, 0 otherwise (so we keep if 1 as done in PyTorch).\n        td: TensorDict containing the current state of the environment.\n        action: Optional action to use, e.g. for evaluating log probabilities.\n    \"\"\"\n    if not self.mask_logits:  # set mask_logit to None if mask_logits is False\n        mask = None\n\n    logprobs = process_logits(\n        logits,\n        mask,\n        temperature=self.temperature,\n        top_p=self.top_p,\n        top_k=self.top_k,\n        tanh_clipping=self.tanh_clipping,\n        mask_logits=self.mask_logits,\n    )\n    logprobs, selected_action, td = self._step(\n        logprobs, mask, td, action=action, **kwargs\n    )\n\n    # directly return for improvement methods, since the action for improvement methods is finalized in its own policy\n    if self.improvement_method_mode:\n        return logprobs, selected_action\n    # for others\n    if not self.store_all_logp:\n        logprobs = gather_by_index(logprobs, selected_action, dim=1)\n    td.set(\"action\", selected_action)\n    self.actions.append(selected_action)\n    self.logprobs.append(logprobs)\n    return td\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.DecodingStrategy.greedy","title":"greedy  <code>staticmethod</code>","text":"<pre><code>greedy(logprobs, mask=None)\n</code></pre> <p>Select the action with the highest probability.</p> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>@staticmethod\ndef greedy(logprobs, mask=None):\n    \"\"\"Select the action with the highest probability.\"\"\"\n    # [BS], [BS]\n    selected = logprobs.argmax(dim=-1)\n    if mask is not None:\n        assert (\n            not (~mask).gather(1, selected.unsqueeze(-1)).data.any()\n        ), \"infeasible action selected\"\n\n    return selected\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.DecodingStrategy.sampling","title":"sampling  <code>staticmethod</code>","text":"<pre><code>sampling(logprobs, mask=None)\n</code></pre> <p>Sample an action with a multinomial distribution given by the log probabilities.</p> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>@staticmethod\ndef sampling(logprobs, mask=None):\n    \"\"\"Sample an action with a multinomial distribution given by the log probabilities.\"\"\"\n    probs = logprobs.exp()\n    selected = torch.multinomial(probs, 1).squeeze(1)\n\n    if mask is not None:\n        while (~mask).gather(1, selected.unsqueeze(-1)).data.any():\n            log.info(\"Sampled bad values, resampling!\")\n            selected = probs.multinomial(1).squeeze(1)\n        assert (\n            not (~mask).gather(1, selected.unsqueeze(-1)).data.any()\n        ), \"infeasible action selected\"\n\n    return selected\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.Greedy","title":"Greedy","text":"<pre><code>Greedy(\n    temperature: float = 1.0,\n    top_p: float = 0.0,\n    top_k: int = 0,\n    mask_logits: bool = True,\n    tanh_clipping: float = 0,\n    multistart: bool = False,\n    multisample: bool = False,\n    num_starts: Optional[int] = None,\n    select_start_nodes_fn: Optional[callable] = None,\n    improvement_method_mode: bool = False,\n    select_best: bool = False,\n    store_all_logp: bool = False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>DecodingStrategy</code></p> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>def __init__(\n    self,\n    temperature: float = 1.0,\n    top_p: float = 0.0,\n    top_k: int = 0,\n    mask_logits: bool = True,\n    tanh_clipping: float = 0,\n    multistart: bool = False,\n    multisample: bool = False,\n    num_starts: Optional[int] = None,\n    select_start_nodes_fn: Optional[callable] = None,\n    improvement_method_mode: bool = False,\n    select_best: bool = False,\n    store_all_logp: bool = False,\n    **kwargs,\n) -&gt; None:\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.mask_logits = mask_logits\n    self.tanh_clipping = tanh_clipping\n    self.multistart = multistart\n    self.multisample = multisample\n    self.num_starts = num_starts\n    self.select_start_nodes_fn = select_start_nodes_fn\n    self.improvement_method_mode = improvement_method_mode\n    self.select_best = select_best\n    self.store_all_logp = store_all_logp\n    # initialize buffers\n    self.actions = []\n    self.logprobs = []\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.Sampling","title":"Sampling","text":"<pre><code>Sampling(\n    temperature: float = 1.0,\n    top_p: float = 0.0,\n    top_k: int = 0,\n    mask_logits: bool = True,\n    tanh_clipping: float = 0,\n    multistart: bool = False,\n    multisample: bool = False,\n    num_starts: Optional[int] = None,\n    select_start_nodes_fn: Optional[callable] = None,\n    improvement_method_mode: bool = False,\n    select_best: bool = False,\n    store_all_logp: bool = False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>DecodingStrategy</code></p> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>def __init__(\n    self,\n    temperature: float = 1.0,\n    top_p: float = 0.0,\n    top_k: int = 0,\n    mask_logits: bool = True,\n    tanh_clipping: float = 0,\n    multistart: bool = False,\n    multisample: bool = False,\n    num_starts: Optional[int] = None,\n    select_start_nodes_fn: Optional[callable] = None,\n    improvement_method_mode: bool = False,\n    select_best: bool = False,\n    store_all_logp: bool = False,\n    **kwargs,\n) -&gt; None:\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.mask_logits = mask_logits\n    self.tanh_clipping = tanh_clipping\n    self.multistart = multistart\n    self.multisample = multisample\n    self.num_starts = num_starts\n    self.select_start_nodes_fn = select_start_nodes_fn\n    self.improvement_method_mode = improvement_method_mode\n    self.select_best = select_best\n    self.store_all_logp = store_all_logp\n    # initialize buffers\n    self.actions = []\n    self.logprobs = []\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.Evaluate","title":"Evaluate","text":"<pre><code>Evaluate(\n    temperature: float = 1.0,\n    top_p: float = 0.0,\n    top_k: int = 0,\n    mask_logits: bool = True,\n    tanh_clipping: float = 0,\n    multistart: bool = False,\n    multisample: bool = False,\n    num_starts: Optional[int] = None,\n    select_start_nodes_fn: Optional[callable] = None,\n    improvement_method_mode: bool = False,\n    select_best: bool = False,\n    store_all_logp: bool = False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>DecodingStrategy</code></p> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>def __init__(\n    self,\n    temperature: float = 1.0,\n    top_p: float = 0.0,\n    top_k: int = 0,\n    mask_logits: bool = True,\n    tanh_clipping: float = 0,\n    multistart: bool = False,\n    multisample: bool = False,\n    num_starts: Optional[int] = None,\n    select_start_nodes_fn: Optional[callable] = None,\n    improvement_method_mode: bool = False,\n    select_best: bool = False,\n    store_all_logp: bool = False,\n    **kwargs,\n) -&gt; None:\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.mask_logits = mask_logits\n    self.tanh_clipping = tanh_clipping\n    self.multistart = multistart\n    self.multisample = multisample\n    self.num_starts = num_starts\n    self.select_start_nodes_fn = select_start_nodes_fn\n    self.improvement_method_mode = improvement_method_mode\n    self.select_best = select_best\n    self.store_all_logp = store_all_logp\n    # initialize buffers\n    self.actions = []\n    self.logprobs = []\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.BeamSearch","title":"BeamSearch","text":"<pre><code>BeamSearch(beam_width=None, select_best=True, **kwargs)\n</code></pre> <p>               Bases: <code>DecodingStrategy</code></p> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>def __init__(self, beam_width=None, select_best=True, **kwargs) -&gt; None:\n    # TODO do we really need all logp in beam search?\n    kwargs[\"store_all_logp\"] = True\n    super().__init__(**kwargs)\n    self.beam_width = beam_width\n    self.select_best = select_best\n    self.parent_beam_logprobs = None\n    self.beam_path = []\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.BeamSearch.pre_decoder_hook","title":"pre_decoder_hook","text":"<pre><code>pre_decoder_hook(\n    td: TensorDict, env: RL4COEnvBase, **kwargs\n)\n</code></pre> <p>Pre decoding hook. This method is called before the main decoding operation.</p> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>def pre_decoder_hook(self, td: TensorDict, env: RL4COEnvBase, **kwargs):\n    if self.beam_width is None:\n        self.beam_width = env.get_num_starts(td)\n    assert self.beam_width &gt; 1, \"beam width must be larger than 1\"\n\n    # select start nodes. TODO: include first step in beam search as well\n    if self.select_start_nodes_fn is not None:\n        action = self.select_start_nodes_fn(td, env, self.beam_width)\n    else:\n        action = env.select_start_nodes(td, num_starts=self.beam_width)\n\n    # Expand td to batch_size * beam_width\n    td = batchify(td, self.beam_width)\n\n    td.set(\"action\", action)\n    td = env.step(td)[\"next\"]\n\n    logprobs = torch.zeros_like(td[\"action_mask\"], device=td.device)\n    beam_parent = torch.zeros(logprobs.size(0), device=td.device, dtype=torch.int32)\n\n    self.logprobs.append(logprobs)\n    self.actions.append(action)\n    self.parent_beam_logprobs = logprobs.gather(1, action[..., None])\n    self.beam_path.append(beam_parent)\n\n    return td, env, self.beam_width\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.get_log_likelihood","title":"get_log_likelihood","text":"<pre><code>get_log_likelihood(\n    logprobs,\n    actions=None,\n    mask=None,\n    return_sum: bool = True,\n)\n</code></pre> <p>Get log likelihood of selected actions. Note that mask is a boolean tensor where True means the value should be kept.</p> <p>Parameters:</p> <ul> <li> <code>logprobs</code>           \u2013            <p>Log probabilities of actions from the model (batch_size, seq_len, action_dim).</p> </li> <li> <code>actions</code>           \u2013            <p>Selected actions (batch_size, seq_len).</p> </li> <li> <code>mask</code>           \u2013            <p>Action mask. 1 if feasible, 0 otherwise (so we keep if 1 as done in PyTorch).</p> </li> <li> <code>return_sum</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to return the sum of log probabilities or not. Defaults to True.</p> </li> </ul> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>def get_log_likelihood(logprobs, actions=None, mask=None, return_sum: bool = True):\n    \"\"\"Get log likelihood of selected actions.\n    Note that mask is a boolean tensor where True means the value should be kept.\n\n    Args:\n        logprobs: Log probabilities of actions from the model (batch_size, seq_len, action_dim).\n        actions: Selected actions (batch_size, seq_len).\n        mask: Action mask. 1 if feasible, 0 otherwise (so we keep if 1 as done in PyTorch).\n        return_sum: Whether to return the sum of log probabilities or not. Defaults to True.\n    \"\"\"\n    # Optional: select logp when logp.shape = (bs, dec_steps, N)\n    if actions is not None and logprobs.dim() == 3:\n        logprobs = logprobs.gather(-1, actions.unsqueeze(-1)).squeeze(-1)\n\n    # Optional: mask out actions irrelevant to objective so they do not get reinforced\n    if mask is not None:\n        logprobs[~mask] = 0\n\n    assert (\n        logprobs &gt; -1000\n    ).data.all(), \"Logprobs should not be -inf, check sampling procedure!\"\n\n    # Calculate log_likelihood\n    if return_sum:\n        return logprobs.sum(1)  # [batch]\n    else:\n        return logprobs  # [batch, decode_len]\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.decode_logprobs","title":"decode_logprobs","text":"<pre><code>decode_logprobs(logprobs, mask, decode_type='sampling')\n</code></pre> <p>Decode log probabilities to select actions with mask. Note that mask is a boolean tensor where True means the value should be kept.</p> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>def decode_logprobs(logprobs, mask, decode_type=\"sampling\"):\n    \"\"\"Decode log probabilities to select actions with mask.\n    Note that mask is a boolean tensor where True means the value should be kept.\n    \"\"\"\n    if \"greedy\" in decode_type:\n        selected = DecodingStrategy.greedy(logprobs, mask)\n    elif \"sampling\" in decode_type:\n        selected = DecodingStrategy.sampling(logprobs, mask)\n    else:\n        assert False, \"Unknown decode type: {}\".format(decode_type)\n    return selected\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.random_policy","title":"random_policy","text":"<pre><code>random_policy(td)\n</code></pre> <p>Helper function to select a random action from available actions</p> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>def random_policy(td):\n    \"\"\"Helper function to select a random action from available actions\"\"\"\n    action = torch.multinomial(td[\"action_mask\"].float(), 1).squeeze(-1)\n    td.set(\"action\", action)\n    return td\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.rollout","title":"rollout","text":"<pre><code>rollout(env, td, policy, max_steps: int = None)\n</code></pre> <p>Helper function to rollout a policy. Currently, TorchRL does not allow to step over envs when done with <code>env.rollout()</code>. We need this because for environments that complete at different steps.</p> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>def rollout(env, td, policy, max_steps: int = None):\n    \"\"\"Helper function to rollout a policy. Currently, TorchRL does not allow to step\n    over envs when done with `env.rollout()`. We need this because for environments that complete at different steps.\n    \"\"\"\n\n    max_steps = float(\"inf\") if max_steps is None else max_steps\n    actions = []\n    steps = 0\n\n    while not td[\"done\"].all():\n        td = policy(td)\n        actions.append(td[\"action\"])\n        td = env.step(td)[\"next\"]\n        steps += 1\n        if steps &gt; max_steps:\n            log.info(\"Max steps reached\")\n            break\n    return (\n        env.get_reward(td, torch.stack(actions, dim=1)),\n        td,\n        torch.stack(actions, dim=1),\n    )\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.modify_logits_for_top_k_filtering","title":"modify_logits_for_top_k_filtering","text":"<pre><code>modify_logits_for_top_k_filtering(logits, top_k)\n</code></pre> <p>Set the logits for none top-k values to -inf. Done out-of-place. Ref: https://github.com/togethercomputer/stripedhyena/blob/7e13f618027fea9625be1f2d2d94f9a361f6bd02/stripedhyena/sample.py#L6</p> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>def modify_logits_for_top_k_filtering(logits, top_k):\n    \"\"\"Set the logits for none top-k values to -inf. Done out-of-place.\n    Ref: https://github.com/togethercomputer/stripedhyena/blob/7e13f618027fea9625be1f2d2d94f9a361f6bd02/stripedhyena/sample.py#L6\n    \"\"\"\n    indices_to_remove = logits &lt; torch.topk(logits, top_k)[0][..., -1, None]\n    return logits.masked_fill(indices_to_remove, float(\"-inf\"))\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.modify_logits_for_top_p_filtering","title":"modify_logits_for_top_p_filtering","text":"<pre><code>modify_logits_for_top_p_filtering(logits, top_p)\n</code></pre> <p>Set the logits for none top-p values to -inf. Done out-of-place. Ref: https://github.com/togethercomputer/stripedhyena/blob/7e13f618027fea9625be1f2d2d94f9a361f6bd02/stripedhyena/sample.py#L14</p> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>def modify_logits_for_top_p_filtering(logits, top_p):\n    \"\"\"Set the logits for none top-p values to -inf. Done out-of-place.\n    Ref: https://github.com/togethercomputer/stripedhyena/blob/7e13f618027fea9625be1f2d2d94f9a361f6bd02/stripedhyena/sample.py#L14\n    \"\"\"\n    if top_p &lt;= 0.0 or top_p &gt;= 1.0:\n        return logits\n\n    # First sort and calculate cumulative sum of probabilities.\n    sorted_logits, sorted_indices = torch.sort(logits, descending=False)\n    cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n\n    # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\n    sorted_indices_to_remove = cumulative_probs &lt;= (1 - top_p)\n\n    # Scatter sorted tensors to original indexing\n    indices_to_remove = sorted_indices_to_remove.scatter(\n        -1, sorted_indices, sorted_indices_to_remove\n    )\n    return logits.masked_fill(indices_to_remove, float(\"-inf\"))\n</code></pre>"},{"location":"docs/content/api/decoding/#utils.decoding.process_logits","title":"process_logits","text":"<pre><code>process_logits(\n    logits: Tensor,\n    mask: Tensor = None,\n    temperature: float = 1.0,\n    top_p: float = 0.0,\n    top_k: int = 0,\n    tanh_clipping: float = 0,\n    mask_logits: bool = True,\n)\n</code></pre> <p>Convert logits to log probabilities with additional features like temperature scaling, top-k and top-p sampling.</p> Note <p>We convert to log probabilities instead of probabilities to avoid numerical instability. This is because, roughly, softmax = exp(logits) / sum(exp(logits)) and log(softmax) = logits - log(sum(exp(logits))), and avoiding the division by the sum of exponentials can help with numerical stability. You may check the official PyTorch documentation.</p> <p>Parameters:</p> <ul> <li> <code>logits</code>               (<code>Tensor</code>)           \u2013            <p>Logits from the model (batch_size, num_actions).</p> </li> <li> <code>mask</code>               (<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>Action mask. 1 if feasible, 0 otherwise (so we keep if 1 as done in PyTorch).</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Temperature scaling. Higher values make the distribution more uniform (exploration), lower values make it more peaky (exploitation).</p> </li> <li> <code>top_p</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Top-p sampling, a.k.a. Nucleus Sampling (https://arxiv.org/abs/1904.09751). Remove tokens that have a cumulative probability less than the threshold 1 - top_p (lower tail of the distribution). If 0, do not perform.</p> </li> <li> <code>top_k</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Top-k sampling, i.e. restrict sampling to the top k logits. If 0, do not perform. Note that we only do filtering and do not return all the top-k logits here.</p> </li> <li> <code>tanh_clipping</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Tanh clipping (https://arxiv.org/abs/1611.09940).</p> </li> <li> <code>mask_logits</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to mask logits of infeasible actions.</p> </li> </ul> Source code in <code>rl4co/utils/decoding.py</code> <pre><code>def process_logits(\n    logits: torch.Tensor,\n    mask: torch.Tensor = None,\n    temperature: float = 1.0,\n    top_p: float = 0.0,\n    top_k: int = 0,\n    tanh_clipping: float = 0,\n    mask_logits: bool = True,\n):\n    \"\"\"Convert logits to log probabilities with additional features like temperature scaling, top-k and top-p sampling.\n\n    Note:\n        We convert to log probabilities instead of probabilities to avoid numerical instability.\n        This is because, roughly, softmax = exp(logits) / sum(exp(logits)) and log(softmax) = logits - log(sum(exp(logits))),\n        and avoiding the division by the sum of exponentials can help with numerical stability.\n        You may check the [official PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html).\n\n    Args:\n        logits: Logits from the model (batch_size, num_actions).\n        mask: Action mask. 1 if feasible, 0 otherwise (so we keep if 1 as done in PyTorch).\n        temperature: Temperature scaling. Higher values make the distribution more uniform (exploration),\n            lower values make it more peaky (exploitation).\n        top_p: Top-p sampling, a.k.a. Nucleus Sampling (https://arxiv.org/abs/1904.09751). Remove tokens that have a cumulative probability\n            less than the threshold 1 - top_p (lower tail of the distribution). If 0, do not perform.\n        top_k: Top-k sampling, i.e. restrict sampling to the top k logits. If 0, do not perform. Note that we only do filtering and\n            do not return all the top-k logits here.\n        tanh_clipping: Tanh clipping (https://arxiv.org/abs/1611.09940).\n        mask_logits: Whether to mask logits of infeasible actions.\n    \"\"\"\n\n    # Tanh clipping from Bello et al. 2016\n    if tanh_clipping &gt; 0:\n        logits = torch.tanh(logits) * tanh_clipping\n\n    # In RL, we want to mask the logits to prevent the agent from selecting infeasible actions\n    if mask_logits:\n        assert mask is not None, \"mask must be provided if mask_logits is True\"\n        logits[~mask] = float(\"-inf\")\n\n    logits = logits / temperature  # temperature scaling\n\n    if top_k &gt; 0:\n        top_k = min(top_k, logits.size(-1))  # safety check\n        logits = modify_logits_for_top_k_filtering(logits, top_k)\n\n    if top_p &gt; 0:\n        assert top_p &lt;= 1.0, \"top-p should be in (0, 1].\"\n        logits = modify_logits_for_top_p_filtering(logits, top_p)\n\n    # Compute log probabilities\n    return F.log_softmax(logits, dim=-1)\n</code></pre>"},{"location":"docs/content/api/tasks/","title":"Train and Evaluation","text":""},{"location":"docs/content/api/tasks/#train","title":"Train","text":""},{"location":"docs/content/api/tasks/#tasks.train.run","title":"run","text":"<pre><code>run(cfg: DictConfig) -&gt; Tuple[dict, dict]\n</code></pre> <p>Trains the model. Can additionally evaluate on a testset, using best weights obtained during training. This method is wrapped in optional @task_wrapper decorator, that controls the behavior during failure. Useful for multiruns, saving info about the crash, etc.</p> <p>Parameters:</p> <ul> <li> <code>cfg</code>               (<code>DictConfig</code>)           \u2013            <p>Configuration composed by Hydra.</p> </li> </ul> <p>Returns:     Tuple[dict, dict]: Dict with metrics and dict with all instantiated objects.</p> Source code in <code>rl4co/tasks/train.py</code> <pre><code>@utils.task_wrapper\ndef run(cfg: DictConfig) -&gt; Tuple[dict, dict]:\n    \"\"\"Trains the model. Can additionally evaluate on a testset, using best weights obtained during\n    training.\n    This method is wrapped in optional @task_wrapper decorator, that controls the behavior during\n    failure. Useful for multiruns, saving info about the crash, etc.\n\n    Args:\n        cfg (DictConfig): Configuration composed by Hydra.\n    Returns:\n        Tuple[dict, dict]: Dict with metrics and dict with all instantiated objects.\n    \"\"\"\n\n    # set seed for random number generators in pytorch, numpy and python.random\n    if cfg.get(\"seed\"):\n        L.seed_everything(cfg.seed, workers=True)\n\n    # We instantiate the environment separately and then pass it to the model\n    log.info(f\"Instantiating environment &lt;{cfg.env._target_}&gt;\")\n    env = hydra.utils.instantiate(cfg.env)\n\n    # Note that the RL environment is instantiated inside the model\n    log.info(f\"Instantiating model &lt;{cfg.model._target_}&gt;\")\n    model: LightningModule = hydra.utils.instantiate(cfg.model, env)\n\n    log.info(\"Instantiating callbacks...\")\n    callbacks: List[Callback] = utils.instantiate_callbacks(cfg.get(\"callbacks\"))\n\n    log.info(\"Instantiating loggers...\")\n    logger: List[Logger] = utils.instantiate_loggers(cfg.get(\"logger\"), model)\n\n    log.info(\"Instantiating trainer...\")\n    trainer: RL4COTrainer = hydra.utils.instantiate(\n        cfg.trainer,\n        callbacks=callbacks,\n        logger=logger,\n    )\n\n    object_dict = {\n        \"cfg\": cfg,\n        \"model\": model,\n        \"callbacks\": callbacks,\n        \"logger\": logger,\n        \"trainer\": trainer,\n    }\n\n    if logger:\n        log.info(\"Logging hyperparameters!\")\n        utils.log_hyperparameters(object_dict)\n\n    if cfg.get(\"compile\", False):\n        log.info(\"Compiling model!\")\n        model = torch.compile(model)\n\n    if cfg.get(\"train\"):\n        log.info(\"Starting training!\")\n        trainer.fit(model=model, ckpt_path=cfg.get(\"ckpt_path\"))\n\n        train_metrics = trainer.callback_metrics\n\n    if cfg.get(\"test\"):\n        log.info(\"Starting testing!\")\n        ckpt_path = trainer.checkpoint_callback.best_model_path\n        if ckpt_path == \"\":\n            log.warning(\"Best ckpt not found! Using current weights for testing...\")\n            ckpt_path = None\n        trainer.test(model=model, ckpt_path=ckpt_path)\n        log.info(f\"Best ckpt path: {ckpt_path}\")\n\n    test_metrics = trainer.callback_metrics\n\n    # merge train and test metrics\n    metric_dict = {**train_metrics, **test_metrics}\n\n    return metric_dict, object_dict\n</code></pre>"},{"location":"docs/content/api/tasks/#evaluate","title":"Evaluate","text":""},{"location":"docs/content/api/tasks/#tasks.eval.EvalBase","title":"EvalBase","text":"<pre><code>EvalBase(env, progress=True, **kwargs)\n</code></pre> <p>Base class for evaluation</p> <p>Parameters:</p> <ul> <li> <code>env</code>           \u2013            <p>Environment</p> </li> <li> <code>progress</code>           \u2013            <p>Whether to show progress bar</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments (to be implemented in subclasses)</p> </li> </ul> Source code in <code>rl4co/tasks/eval.py</code> <pre><code>def __init__(self, env, progress=True, **kwargs):\n    check_unused_kwargs(self, kwargs)\n    self.env = env\n    self.progress = progress\n</code></pre>"},{"location":"docs/content/api/tasks/#tasks.eval.GreedyEval","title":"GreedyEval","text":"<pre><code>GreedyEval(env, **kwargs)\n</code></pre> <p>               Bases: <code>EvalBase</code></p> <p>Evaluates the policy using greedy decoding and single trajectory</p> Source code in <code>rl4co/tasks/eval.py</code> <pre><code>def __init__(self, env, **kwargs):\n    check_unused_kwargs(self, kwargs)\n    super().__init__(env, kwargs.get(\"progress\", True))\n</code></pre>"},{"location":"docs/content/api/tasks/#tasks.eval.AugmentationEval","title":"AugmentationEval","text":"<pre><code>AugmentationEval(\n    env,\n    num_augment=8,\n    force_dihedral_8=False,\n    feats=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>EvalBase</code></p> <p>Evaluates the policy via N state augmentations <code>force_dihedral_8</code> forces the use of 8 augmentations (rotations and flips) as in POMO https://en.wikipedia.org/wiki/Examples_of_groups#dihedral_group_of_order_8</p> <p>Parameters:</p> <ul> <li> <code>num_augment</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of state augmentations</p> </li> <li> <code>force_dihedral_8</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the use of 8 augmentations</p> </li> </ul> Source code in <code>rl4co/tasks/eval.py</code> <pre><code>def __init__(self, env, num_augment=8, force_dihedral_8=False, feats=None, **kwargs):\n    check_unused_kwargs(self, kwargs)\n    super().__init__(env, kwargs.get(\"progress\", True))\n    self.augmentation = StateAugmentation(\n        num_augment=num_augment,\n        augment_fn=\"dihedral8\" if force_dihedral_8 else \"symmetric\",\n        feats=feats,\n    )\n</code></pre>"},{"location":"docs/content/api/tasks/#tasks.eval.SamplingEval","title":"SamplingEval","text":"<pre><code>SamplingEval(\n    env,\n    samples,\n    softmax_temp=None,\n    select_best=True,\n    temperature=1.0,\n    top_p=0.0,\n    top_k=0,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>EvalBase</code></p> <p>Evaluates the policy via N samples from the policy</p> <p>Parameters:</p> <ul> <li> <code>samples</code>               (<code>int</code>)           \u2013            <p>Number of samples to take</p> </li> <li> <code>softmax_temp</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Temperature for softmax sampling. The higher the temperature, the more random the sampling</p> </li> </ul> Source code in <code>rl4co/tasks/eval.py</code> <pre><code>def __init__(\n    self,\n    env,\n    samples,\n    softmax_temp=None,\n    select_best=True,\n    temperature=1.0,\n    top_p=0.0,\n    top_k=0,\n    **kwargs,\n):\n    check_unused_kwargs(self, kwargs)\n    super().__init__(env, kwargs.get(\"progress\", True))\n\n    self.samples = samples\n    self.softmax_temp = softmax_temp\n    self.temperature = temperature\n    self.select_best = select_best\n    self.top_p = top_p\n    self.top_k = top_k\n</code></pre>"},{"location":"docs/content/api/tasks/#tasks.eval.GreedyMultiStartEval","title":"GreedyMultiStartEval","text":"<pre><code>GreedyMultiStartEval(env, num_starts=None, **kwargs)\n</code></pre> <p>               Bases: <code>EvalBase</code></p> <p>Evaluates the policy via <code>num_starts</code> greedy multistarts samples from the policy</p> <p>Parameters:</p> <ul> <li> <code>num_starts</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of greedy multistarts to use</p> </li> </ul> Source code in <code>rl4co/tasks/eval.py</code> <pre><code>def __init__(self, env, num_starts=None, **kwargs):\n    check_unused_kwargs(self, kwargs)\n    super().__init__(env, kwargs.get(\"progress\", True))\n\n    assert num_starts is not None, \"Must specify num_starts\"\n    self.num_starts = num_starts\n</code></pre>"},{"location":"docs/content/api/tasks/#tasks.eval.GreedyMultiStartAugmentEval","title":"GreedyMultiStartAugmentEval","text":"<pre><code>GreedyMultiStartAugmentEval(\n    env,\n    num_starts=None,\n    num_augment=8,\n    force_dihedral_8=False,\n    feats=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>EvalBase</code></p> <p>Evaluates the policy via <code>num_starts</code> samples from the policy and <code>num_augment</code> augmentations of each sample.<code></code>force_dihedral_8` forces the use of 8 augmentations (rotations and flips) as in POMO https://en.wikipedia.org/wiki/Examples_of_groups#dihedral_group_of_order_8</p> <p>Parameters:</p> <ul> <li> <code>num_starts</code>           \u2013            <p>Number of greedy multistart samples</p> </li> <li> <code>num_augment</code>           \u2013            <p>Number of augmentations per sample</p> </li> <li> <code>force_dihedral_8</code>           \u2013            <p>If True, force the use of 8 augmentations (rotations and flips) as in POMO</p> </li> </ul> Source code in <code>rl4co/tasks/eval.py</code> <pre><code>def __init__(\n    self,\n    env,\n    num_starts=None,\n    num_augment=8,\n    force_dihedral_8=False,\n    feats=None,\n    **kwargs,\n):\n    check_unused_kwargs(self, kwargs)\n    super().__init__(env, kwargs.get(\"progress\", True))\n\n    assert num_starts is not None, \"Must specify num_starts\"\n    self.num_starts = num_starts\n    assert not (\n        num_augment != 8 and force_dihedral_8\n    ), \"Cannot force dihedral 8 when num_augment != 8\"\n    self.augmentation = StateAugmentation(\n        num_augment=num_augment,\n        augment_fn=\"dihedral8\" if force_dihedral_8 else \"symmetric\",\n        feats=feats,\n    )\n</code></pre>"},{"location":"docs/content/api/tasks/#tasks.eval.get_automatic_batch_size","title":"get_automatic_batch_size","text":"<pre><code>get_automatic_batch_size(\n    eval_fn, start_batch_size=8192, max_batch_size=4096\n)\n</code></pre> <p>Automatically reduces the batch size based on the eval function</p> <p>Parameters:</p> <ul> <li> <code>eval_fn</code>           \u2013            <p>The eval function</p> </li> <li> <code>start_batch_size</code>           \u2013            <p>The starting batch size. This should be the theoretical maximum batch size</p> </li> <li> <code>max_batch_size</code>           \u2013            <p>The maximum batch size. This is the practical maximum batch size</p> </li> </ul> Source code in <code>rl4co/tasks/eval.py</code> <pre><code>def get_automatic_batch_size(eval_fn, start_batch_size=8192, max_batch_size=4096):\n    \"\"\"Automatically reduces the batch size based on the eval function\n\n    Args:\n        eval_fn: The eval function\n        start_batch_size: The starting batch size. This should be the theoretical maximum batch size\n        max_batch_size: The maximum batch size. This is the practical maximum batch size\n    \"\"\"\n    batch_size = start_batch_size\n\n    effective_ratio = 1\n\n    if hasattr(eval_fn, \"num_starts\"):\n        batch_size = batch_size // (eval_fn.num_starts // 10)\n        effective_ratio *= eval_fn.num_starts // 10\n    if hasattr(eval_fn, \"num_augment\"):\n        batch_size = batch_size // eval_fn.num_augment\n        effective_ratio *= eval_fn.num_augment\n    if hasattr(eval_fn, \"samples\"):\n        batch_size = batch_size // eval_fn.samples\n        effective_ratio *= eval_fn.samples\n\n    batch_size = min(batch_size, max_batch_size)\n    # get closest integer power of 2\n    batch_size = 2 ** int(np.log2(batch_size))\n\n    print(f\"Effective batch size: {batch_size} (ratio: {effective_ratio})\")\n\n    return batch_size\n</code></pre>"},{"location":"docs/content/api/envs/base/","title":"Base Environment","text":"<p>This is the base wrapper around TorchRL's <code>EnvBase</code>, with additional functionality.</p>"},{"location":"docs/content/api/envs/base/#envs.common.base.RL4COEnvBase","title":"RL4COEnvBase","text":"<pre><code>RL4COEnvBase(\n    *,\n    data_dir: str = \"data/\",\n    train_file: str = None,\n    val_file: str = None,\n    test_file: str = None,\n    val_dataloader_names: list = None,\n    test_dataloader_names: list = None,\n    check_solution: bool = True,\n    dataset_cls: callable = TensorDictDataset,\n    seed: int = None,\n    device: str = \"cpu\",\n    batch_size: Size = None,\n    run_type_checks: bool = False,\n    allow_done_after_reset: bool = False,\n    _torchrl_mode: bool = False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>EnvBase</code></p> <p>Base class for RL4CO environments based on TorchRL EnvBase. The environment has the usual methods for stepping, resetting, and getting the specifications of the environment that shoud be implemented by the subclasses of this class. It also has methods for getting the reward, action mask, and checking the validity of the solution, and for generating and loading the datasets (supporting multiple dataloaders as well for validation and testing).</p> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>str</code>, default:                   <code>'data/'</code> )           \u2013            <p>Root directory for the dataset</p> </li> <li> <code>train_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Name of the training file</p> </li> <li> <code>val_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Name of the validation file</p> </li> <li> <code>test_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Name of the test file</p> </li> <li> <code>val_dataloader_names</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>Names of the dataloaders to use for validation</p> </li> <li> <code>test_dataloader_names</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>Names of the dataloaders to use for testing</p> </li> <li> <code>check_solution</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to check the validity of the solution at the end of the episode</p> </li> <li> <code>dataset_cls</code>               (<code>callable</code>, default:                   <code>TensorDictDataset</code> )           \u2013            <p>Dataset class to use for the environment (which can influence performance)</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Seed for the environment</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Device to use. Generally, no need to set as tensors are updated on the fly</p> </li> <li> <code>batch_size</code>               (<code>Size</code>, default:                   <code>None</code> )           \u2013            <p>Batch size to use for the environment. Generally, no need to set as tensors are updated on the fly</p> </li> <li> <code>run_type_checks</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, run type checks on the TensorDicts at each step</p> </li> <li> <code>allow_done_after_reset</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, an environment can be done after a reset</p> </li> <li> <code>_torchrl_mode</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use the TorchRL mode (see :meth:<code>step</code> for more details)</p> </li> </ul> Source code in <code>rl4co/envs/common/base.py</code> <pre><code>def __init__(\n    self,\n    *,\n    data_dir: str = \"data/\",\n    train_file: str = None,\n    val_file: str = None,\n    test_file: str = None,\n    val_dataloader_names: list = None,\n    test_dataloader_names: list = None,\n    check_solution: bool = True,\n    dataset_cls: callable = TensorDictDataset,\n    seed: int = None,\n    device: str = \"cpu\",\n    batch_size: torch.Size = None,\n    run_type_checks: bool = False,\n    allow_done_after_reset: bool = False,\n    _torchrl_mode: bool = False,\n    **kwargs,\n):\n    super().__init__(\n        device=device,\n        batch_size=batch_size,\n        run_type_checks=run_type_checks,\n        allow_done_after_reset=allow_done_after_reset,\n    )\n    # if any kwargs are left, we want to warn the user\n    kwargs.pop(\"name\", None)  # we remove the name for checking\n    if kwargs:\n        log.error(\n            f\"Unused keyword arguments: {', '.join(kwargs.keys())}. \"\n            \"Please check the base class documentation at https://rl4co.readthedocs.io/en/latest/_content/api/envs/base.html. \"\n            \"In case you would like to pass data generation arguments, please pass a `generator` method instead \"\n            \"or for example: `generator_kwargs=dict(num_loc=50)` to the constructor.\"\n        )\n    self.data_dir = data_dir\n    self.train_file = pjoin(data_dir, train_file) if train_file is not None else None\n    self._torchrl_mode = _torchrl_mode\n    self.dataset_cls = dataset_cls\n\n    def get_files(f):\n        if f is not None:\n            if isinstance(f, Iterable) and not isinstance(f, str):\n                return [pjoin(data_dir, _f) for _f in f]\n            else:\n                return pjoin(data_dir, f)\n        return None\n\n    def get_multiple_dataloader_names(f, names):\n        if f is not None:\n            if isinstance(f, Iterable) and not isinstance(f, str):\n                if names is None:\n                    names = [f\"{i}\" for i in range(len(f))]\n                else:\n                    assert len(names) == len(\n                        f\n                    ), \"Number of dataloader names must match number of files\"\n            else:\n                if names is not None:\n                    log.warning(\n                        \"Ignoring dataloader names since only one dataloader is provided\"\n                    )\n        return names\n\n    self.val_file = get_files(val_file)\n    self.test_file = get_files(test_file)\n    self.val_dataloader_names = get_multiple_dataloader_names(\n        self.val_file, val_dataloader_names\n    )\n    self.test_dataloader_names = get_multiple_dataloader_names(\n        self.test_file, test_dataloader_names\n    )\n    self.check_solution = check_solution\n    if seed is None:\n        seed = torch.empty((), dtype=torch.int64).random_().item()\n    self.set_seed(seed)\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.base.RL4COEnvBase.step","title":"step","text":"<pre><code>step(td: TensorDict) -&gt; TensorDict\n</code></pre> <p>Step function to call at each step of the episode containing an action. If <code>_torchrl_mode</code> is True, we call <code>_torchrl_step</code> instead which set the <code>next</code> key of the TensorDict to the next state - this is the usual way to do it in TorchRL, but inefficient in our case</p> Source code in <code>rl4co/envs/common/base.py</code> <pre><code>def step(self, td: TensorDict) -&gt; TensorDict:\n    \"\"\"Step function to call at each step of the episode containing an action.\n    If `_torchrl_mode` is True, we call `_torchrl_step` instead which set the\n    `next` key of the TensorDict to the next state - this is the usual way to do it in TorchRL,\n    but inefficient in our case\n    \"\"\"\n    if not self._torchrl_mode:\n        # Default: just return the TensorDict without farther checks etc is faster\n        td = self._step(td)\n        return {\"next\": td}\n    else:\n        # Since we simplify the syntax\n        return self._torchrl_step(td)\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.base.RL4COEnvBase.reset","title":"reset","text":"<pre><code>reset(\n    td: Optional[TensorDict] = None, batch_size=None\n) -&gt; TensorDict\n</code></pre> <p>Reset function to call at the beginning of each episode</p> Source code in <code>rl4co/envs/common/base.py</code> <pre><code>def reset(self, td: Optional[TensorDict] = None, batch_size=None) -&gt; TensorDict:\n    \"\"\"Reset function to call at the beginning of each episode\"\"\"\n    if batch_size is None:\n        batch_size = self.batch_size if td is None else td.batch_size\n    if td is None or td.is_empty():\n        td = self.generator(batch_size=batch_size)\n    batch_size = [batch_size] if isinstance(batch_size, int) else batch_size\n    self.to(td.device)\n    return super().reset(td, batch_size=batch_size)\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.base.RL4COEnvBase.get_reward","title":"get_reward","text":"<pre><code>get_reward(td: TensorDict, actions: Tensor) -&gt; Tensor\n</code></pre> <p>Function to compute the reward. Can be called by the agent to compute the reward of the current state This is faster than calling step() and getting the reward from the returned TensorDict at each time for CO tasks</p> Source code in <code>rl4co/envs/common/base.py</code> <pre><code>def get_reward(self, td: TensorDict, actions: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Function to compute the reward. Can be called by the agent to compute the reward of the current state\n    This is faster than calling step() and getting the reward from the returned TensorDict at each time for CO tasks\n    \"\"\"\n    if self.check_solution:\n        self.check_solution_validity(td, actions)\n    return self._get_reward(td, actions)\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.base.RL4COEnvBase.get_action_mask","title":"get_action_mask","text":"<pre><code>get_action_mask(td: TensorDict) -&gt; Tensor\n</code></pre> <p>Function to compute the action mask (feasible actions) for the current state Action mask is 1 if the action is feasible, 0 otherwise</p> Source code in <code>rl4co/envs/common/base.py</code> <pre><code>def get_action_mask(self, td: TensorDict) -&gt; torch.Tensor:\n    \"\"\"Function to compute the action mask (feasible actions) for the current state\n    Action mask is 1 if the action is feasible, 0 otherwise\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.base.RL4COEnvBase.check_solution_validity","title":"check_solution_validity","text":"<pre><code>check_solution_validity(\n    td: TensorDict, actions: Tensor\n) -&gt; None\n</code></pre> <p>Function to check whether the solution is valid. Can be called by the agent to check the validity of the current state This is called with the full solution (i.e. all actions) at the end of the episode</p> Source code in <code>rl4co/envs/common/base.py</code> <pre><code>def check_solution_validity(self, td: TensorDict, actions: torch.Tensor) -&gt; None:\n    \"\"\"Function to check whether the solution is valid. Can be called by the agent to check the validity of the current state\n    This is called with the full solution (i.e. all actions) at the end of the episode\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.base.RL4COEnvBase.replace_selected_actions","title":"replace_selected_actions","text":"<pre><code>replace_selected_actions(\n    cur_actions: Tensor,\n    new_actions: Tensor,\n    selection_mask: Tensor,\n) -&gt; Tensor\n</code></pre> <p>Replace selected current actions with updated actions based on <code>selection_mask</code>.</p> Source code in <code>rl4co/envs/common/base.py</code> <pre><code>def replace_selected_actions(\n    self,\n    cur_actions: torch.Tensor,\n    new_actions: torch.Tensor,\n    selection_mask: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Replace selected current actions with updated actions based on `selection_mask`.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.base.RL4COEnvBase.local_search","title":"local_search","text":"<pre><code>local_search(\n    td: TensorDict, actions: Tensor, **kwargs\n) -&gt; Tensor\n</code></pre> <p>Function to improve the solution. Can be called by the agent to improve the current state This is called with the full solution (i.e. all actions) at the end of the episode</p> Source code in <code>rl4co/envs/common/base.py</code> <pre><code>def local_search(\n    self, td: TensorDict, actions: torch.Tensor, **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"Function to improve the solution. Can be called by the agent to improve the current state\n    This is called with the full solution (i.e. all actions) at the end of the episode\n    \"\"\"\n    raise NotImplementedError(\n        f\"Local is not implemented yet for {self.name} environment\"\n    )\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.base.RL4COEnvBase.dataset","title":"dataset","text":"<pre><code>dataset(batch_size=[], phase='train', filename=None)\n</code></pre> <p>Return a dataset of observations Generates the dataset if it does not exist, otherwise loads it from file</p> Source code in <code>rl4co/envs/common/base.py</code> <pre><code>def dataset(self, batch_size=[], phase=\"train\", filename=None):\n    \"\"\"Return a dataset of observations\n    Generates the dataset if it does not exist, otherwise loads it from file\n    \"\"\"\n    if filename is not None:\n        log.info(f\"Overriding dataset filename from {filename}\")\n    f = getattr(self, f\"{phase}_file\") if filename is None else filename\n    if f is None:\n        if phase != \"train\":\n            log.warning(f\"{phase}_file not set. Generating dataset instead\")\n        td = self.generator(batch_size)\n    else:\n        log.info(f\"Loading {phase} dataset from {f}\")\n        if phase == \"train\":\n            log.warning(\n                \"Loading training dataset from file. This may not be desired in RL since \"\n                \"the dataset is fixed and the agent will not be able to explore new states\"\n            )\n        try:\n            if isinstance(f, Iterable) and not isinstance(f, str):\n                names = getattr(self, f\"{phase}_dataloader_names\")\n                return {\n                    name: self.dataset_cls(self.load_data(_f, batch_size))\n                    for name, _f in zip(names, f)\n                }\n            else:\n                td = self.load_data(f, batch_size)\n        except FileNotFoundError:\n            log.error(\n                f\"Provided file name {f} not found. Make sure to provide a file in the right path first or \"\n                f\"unset {phase}_file to generate data automatically instead\"\n            )\n            td = self.generator(batch_size)\n\n    return self.dataset_cls(td)\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.base.RL4COEnvBase.transform","title":"transform","text":"<pre><code>transform()\n</code></pre> <p>Used for converting TensorDict variables (such as with torch.cat) efficiently https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.Transform.html By default, we do not need to transform the environment since we use specific embeddings</p> Source code in <code>rl4co/envs/common/base.py</code> <pre><code>def transform(self):\n    \"\"\"Used for converting TensorDict variables (such as with torch.cat) efficiently\n    https://pytorch.org/rl/reference/generated/torchrl.envs.transforms.Transform.html\n    By default, we do not need to transform the environment since we use specific embeddings\n    \"\"\"\n    return self\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.base.RL4COEnvBase.render","title":"render","text":"<pre><code>render(*args, **kwargs)\n</code></pre> <p>Render the environment</p> Source code in <code>rl4co/envs/common/base.py</code> <pre><code>def render(self, *args, **kwargs):\n    \"\"\"Render the environment\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.base.RL4COEnvBase.load_data","title":"load_data  <code>staticmethod</code>","text":"<pre><code>load_data(fpath, batch_size=[])\n</code></pre> <p>Dataset loading from file</p> Source code in <code>rl4co/envs/common/base.py</code> <pre><code>@staticmethod\ndef load_data(fpath, batch_size=[]):\n    \"\"\"Dataset loading from file\"\"\"\n    return load_npz_to_tensordict(fpath)\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.base.RL4COEnvBase.to","title":"to","text":"<pre><code>to(device)\n</code></pre> <p>Override <code>to</code> device method for safety against <code>None</code> device (may be found in <code>TensorDict</code>)</p> Source code in <code>rl4co/envs/common/base.py</code> <pre><code>def to(self, device):\n    \"\"\"Override `to` device method for safety against `None` device (may be found in `TensorDict`)\"\"\"\n    if device is None:\n        return self\n    else:\n        return super().to(device)\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.base.RL4COEnvBase.solve","title":"solve  <code>staticmethod</code>","text":"<pre><code>solve(\n    instances: TensorDict,\n    max_runtime: float,\n    num_procs: int = 1,\n    **kwargs\n) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Classical solver for the environment. This is a wrapper for the baselines solver.</p> <p>Parameters:</p> <ul> <li> <code>instances</code>               (<code>TensorDict</code>)           \u2013            <p>The instances to solve</p> </li> <li> <code>max_runtime</code>               (<code>float</code>)           \u2013            <p>The maximum runtime for the solver</p> </li> <li> <code>num_procs</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of processes to use</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Tensor, Tensor]</code>           \u2013            <p>A tuple containing the action and the cost, respectively</p> </li> </ul> Source code in <code>rl4co/envs/common/base.py</code> <pre><code>@staticmethod\ndef solve(\n    instances: TensorDict,\n    max_runtime: float,\n    num_procs: int = 1,\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Classical solver for the environment. This is a wrapper for the baselines solver.\n\n    Args:\n        instances: The instances to solve\n        max_runtime: The maximum runtime for the solver\n        num_procs: The number of processes to use\n\n    Returns:\n        A tuple containing the action and the cost, respectively\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.base.ImprovementEnvBase","title":"ImprovementEnvBase","text":"<pre><code>ImprovementEnvBase(**kwargs)\n</code></pre> <p>               Bases: <code>RL4COEnvBase</code></p> <p>Base class for Improvement environments based on RL4CO EnvBase. Note that this class assumes that the solution is stored in a linked list format. Here, if <code>rec[i] = j</code>, it means the node <code>i</code> is connected to node <code>j</code>, i.e., edge <code>i-j</code> is in the solution. For example, if edge <code>0-1</code>, edge <code>1-5</code>, edge <code>2-10</code> are in the solution, so we have <code>rec[0]=1</code>, <code>rec[1]=5</code> and <code>rec[2]=10</code>. Kindly see https://github.com/yining043/VRP-DACT/blob/new_version/Play_with_DACT.ipynb for an example at the end for TSP.</p> Source code in <code>rl4co/envs/common/base.py</code> <pre><code>def __init__(\n    self,\n    **kwargs,\n):\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"docs/content/api/envs/base/#utilities","title":"Utilities","text":"<p>These contain utilities such as the base <code>Generator</code> class and <code>get_sampler</code>.</p>"},{"location":"docs/content/api/envs/base/#envs.common.utils.Generator","title":"Generator","text":"<pre><code>Generator(**kwargs)\n</code></pre> <p>Base data generator class, to be called with <code>env.generator(batch_size)</code></p> Source code in <code>rl4co/envs/common/utils.py</code> <pre><code>def __init__(self, **kwargs):\n    self.kwargs = kwargs\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.utils.get_sampler","title":"get_sampler","text":"<pre><code>get_sampler(\n    val_name: str,\n    distribution: Union[int, float, str, type, Callable],\n    low: float = 0,\n    high: float = 1.0,\n    **kwargs\n)\n</code></pre> <p>Get the sampler for the variable with the given distribution. If kwargs are passed, they will be parsed e.g. with <code>val_name</code> + <code>_dist_arg</code> (e.g. <code>loc_std</code> for Normal distribution).</p> <p>Parameters:</p> <ul> <li> <code>val_name</code>               (<code>str</code>)           \u2013            <p>Name of the variable</p> </li> <li> <code>distribution</code>               (<code>Union[int, float, str, type, Callable]</code>)           \u2013            <p>int/float value (as constant distribution), or string with the distribution name (supporting uniform, normal, exponential, and poisson) or PyTorch Distribution type or a callable function that returns a PyTorch Distribution</p> </li> <li> <code>low</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Minimum value for the variable, used for Uniform distribution</p> </li> <li> <code>high</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Maximum value for the variable, used for Uniform distribution</p> </li> <li> <code>kwargs</code>           \u2013            <p>Additional arguments for the distribution</p> </li> </ul> Example <pre><code>sampler_uniform = get_sampler(\"loc\", \"uniform\", 0, 1)\nsampler_normal = get_sampler(\"loc\", \"normal\", loc_mean=0.5, loc_std=.2)\n</code></pre> Source code in <code>rl4co/envs/common/utils.py</code> <pre><code>def get_sampler(\n    val_name: str,\n    distribution: Union[int, float, str, type, Callable],\n    low: float = 0,\n    high: float = 1.0,\n    **kwargs,\n):\n    \"\"\"Get the sampler for the variable with the given distribution.\n    If kwargs are passed, they will be parsed e.g. with `val_name` + `_dist_arg` (e.g. `loc_std` for Normal distribution).\n\n    Args:\n        val_name: Name of the variable\n        distribution: int/float value (as constant distribution), or string with the distribution name (supporting\n            uniform, normal, exponential, and poisson) or PyTorch Distribution type or a callable function that\n            returns a PyTorch Distribution\n        low: Minimum value for the variable, used for Uniform distribution\n        high: Maximum value for the variable, used for Uniform distribution\n        kwargs: Additional arguments for the distribution\n\n    Example:\n        ```python\n        sampler_uniform = get_sampler(\"loc\", \"uniform\", 0, 1)\n        sampler_normal = get_sampler(\"loc\", \"normal\", loc_mean=0.5, loc_std=.2)\n        ```\n    \"\"\"\n    if isinstance(distribution, (int, float)):\n        return Uniform(low=distribution, high=distribution)\n    elif distribution == Uniform or distribution == \"uniform\":\n        return Uniform(low=low, high=high)\n    elif distribution == Normal or distribution == \"normal\" or distribution == \"gaussian\":\n        assert (\n            kwargs.get(val_name + \"_mean\", None) is not None\n        ), \"mean is required for Normal distribution\"\n        assert (\n            kwargs.get(val_name + \"_std\", None) is not None\n        ), \"std is required for Normal distribution\"\n        return Normal(loc=kwargs[val_name + \"_mean\"], scale=kwargs[val_name + \"_std\"])\n    elif distribution == Exponential or distribution == \"exponential\":\n        assert (\n            kwargs.get(val_name + \"_rate\", None) is not None\n        ), \"rate is required for Exponential/Poisson distribution\"\n        return Exponential(rate=kwargs[val_name + \"_rate\"])\n    elif distribution == Poisson or distribution == \"poisson\":\n        assert (\n            kwargs.get(val_name + \"_rate\", None) is not None\n        ), \"rate is required for Exponential/Poisson distribution\"\n        return Poisson(rate=kwargs[val_name + \"_rate\"])\n    elif distribution == \"center\":\n        return Uniform(low=(high - low) / 2, high=(high - low) / 2)\n    elif distribution == \"corner\":\n        return Uniform(\n            low=low, high=low\n        )  # todo: should be also `low, high` and any other corner\n    elif isinstance(distribution, Callable):\n        return distribution(**kwargs)\n    elif distribution == \"gaussian_mixture\":\n        return Gaussian_Mixture(num_modes=kwargs[\"num_modes\"], cdist=kwargs[\"cdist\"])\n    elif distribution == \"cluster\":\n        return Cluster(kwargs[\"n_cluster\"])\n    elif distribution == \"mixed\":\n        return Mixed(kwargs[\"n_cluster_mix\"])\n    elif distribution == \"mix_distribution\":\n        return Mix_Distribution(kwargs[\"n_cluster\"], kwargs[\"n_cluster_mix\"])\n    elif distribution == \"mix_multi_distributions\":\n        return Mix_Multi_Distributions()\n    else:\n        raise ValueError(f\"Invalid distribution type of {distribution}\")\n</code></pre>"},{"location":"docs/content/api/envs/base/#envs.common.utils.batch_to_scalar","title":"batch_to_scalar","text":"<pre><code>batch_to_scalar(param)\n</code></pre> <p>Return first element if in batch. Used for batched parameters that are the same for all elements in the batch.</p> Source code in <code>rl4co/envs/common/utils.py</code> <pre><code>def batch_to_scalar(param):\n    \"\"\"Return first element if in batch. Used for batched parameters that are the same for all elements in the batch.\"\"\"\n    if len(param.shape) &gt; 0:\n        return param[0].item()\n    if isinstance(param, torch.Tensor):\n        return param.item()\n    return param\n</code></pre>"},{"location":"docs/content/api/envs/eda/","title":"EDA Problems","text":"<p>Environment for Electronic Design Automation (EDA) problems</p>"},{"location":"docs/content/api/envs/eda/#decap-placement-problem-dpp","title":"Decap Placement Problem (DPP)","text":""},{"location":"docs/content/api/envs/eda/#envs.eda.dpp.env.DPPEnv","title":"DPPEnv","text":"<pre><code>DPPEnv(\n    generator: DPPGenerator = None,\n    generator_params: dict = {},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RL4COEnvBase</code></p> <p>Decap Placement Problem (DPP) as done in DevFormer paper: https://arxiv.org/abs/2205.13225</p> <p>The environment is a 10x10 grid with 100 locations containing either a probing port or a keepout region. The goal is to place decaps (decoupling capacitors) to maximize the impedance suppression at the probing port. Decaps cannot be placed in keepout regions or at the probing port and the number of decaps is limited.</p> Observations <ul> <li>locations of the probing port and keepout regions</li> <li>current decap placement</li> <li>remaining decaps</li> </ul> Constraints <ul> <li>decaps cannot be placed at the probing port or keepout regions</li> <li>the number of decaps is limited</li> </ul> Finish Condition <ul> <li>the number of decaps exceeds the limit</li> </ul> Reward <ul> <li>the impedance suppression at the probing port</li> </ul> <p>Parameters:</p> <ul> <li> <code>generator</code>               (<code>DPPGenerator</code>, default:                   <code>None</code> )           \u2013            <p>DPPGenerator instance as the data generator</p> </li> <li> <code>generator_params</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>parameters for the generator</p> </li> </ul> Source code in <code>rl4co/envs/eda/dpp/env.py</code> <pre><code>def __init__(\n    self,\n    generator: DPPGenerator = None,\n    generator_params: dict = {},\n    **kwargs,\n):\n    super().__init__(**kwargs)\n    if generator is None:\n        generator = DPPGenerator(**generator_params)\n    self.generator = generator\n\n    self.max_decaps = self.generator.max_decaps\n    self.size = self.generator.size\n    self.raw_pdn = self.generator.raw_pdn\n    self.decap = self.generator.decap\n    self.freq = self.generator.freq\n    self.num_freq = self.generator.num_freq\n    self.data_dir = self.generator.data_dir\n\n    self._make_spec(self.generator)\n</code></pre>"},{"location":"docs/content/api/envs/eda/#envs.eda.dpp.generator.DPPGenerator","title":"DPPGenerator","text":"<pre><code>DPPGenerator(\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    num_keepout_min: int = 1,\n    num_keepout_max: int = 50,\n    max_decaps: int = 20,\n    data_dir: str = \"data/dpp/\",\n    chip_file: str = \"10x10_pkg_chip.npy\",\n    decap_file: str = \"01nF_decap.npy\",\n    freq_file: str = \"freq_201.npy\",\n    url: str = None,\n    **unused_kwargs\n)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>Data generator for the Decap Placement Problem (DPP).</p> <p>Parameters:</p> <ul> <li> <code>min_loc</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Minimum location value. Defaults to 0.</p> </li> <li> <code>max_loc</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Maximum location value. Defaults to 1.</p> </li> <li> <code>num_keepout_min</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Minimum number of keepout regions. Defaults to 1.</p> </li> <li> <code>num_keepout_max</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>Maximum number of keepout regions. Defaults to 50.</p> </li> <li> <code>max_decaps</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>Maximum number of decaps. Defaults to 20.</p> </li> <li> <code>data_dir</code>               (<code>str</code>, default:                   <code>'data/dpp/'</code> )           \u2013            <p>Directory to store data. Defaults to \"data/dpp/\". This can be downloaded from this url.</p> </li> <li> <code>chip_file</code>               (<code>str</code>, default:                   <code>'10x10_pkg_chip.npy'</code> )           \u2013            <p>Name of the chip file. Defaults to \"10x10_pkg_chip.npy\".</p> </li> <li> <code>decap_file</code>               (<code>str</code>, default:                   <code>'01nF_decap.npy'</code> )           \u2013            <p>Name of the decap file. Defaults to \"01nF_decap.npy\".</p> </li> <li> <code>freq_file</code>               (<code>str</code>, default:                   <code>'freq_201.npy'</code> )           \u2013            <p>Name of the frequency file. Defaults to \"freq_201.npy\".</p> </li> <li> <code>url</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>URL to download data from. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>A TensorDict with the following keys: locs [batch_size, num_loc, 2]: locations of each customer depot [batch_size, 2]: location of the depot demand [batch_size, num_loc]: demand of each customer capacity [batch_size]: capacity of the vehicle</p> </li> </ul> Source code in <code>rl4co/envs/eda/dpp/generator.py</code> <pre><code>def __init__(\n    self,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    num_keepout_min: int = 1,\n    num_keepout_max: int = 50,\n    max_decaps: int = 20,\n    data_dir: str = \"data/dpp/\",\n    chip_file: str = \"10x10_pkg_chip.npy\",\n    decap_file: str = \"01nF_decap.npy\",\n    freq_file: str = \"freq_201.npy\",\n    url: str = None,\n    **unused_kwargs\n):\n    self.min_loc = min_loc\n    self.max_loc = max_loc\n    self.num_keepout_min = num_keepout_min\n    self.num_keepout_max = num_keepout_max\n    self.max_decaps = max_decaps\n    self.data_dir = data_dir\n\n    # DPP environment doen't have any other kwargs\n    if len(unused_kwargs) &gt; 0:\n        log.error(f\"Found {len(unused_kwargs)} unused kwargs: {unused_kwargs}\")\n\n\n    # Download and load the data from online dataset\n    self.url = (\n        \"https://github.com/kaist-silab/devformer/raw/main/data/data.zip\"\n        if url is None\n        else url\n    )\n    self.backup_url = (\n        \"https://drive.google.com/uc?id=1IEuR2v8Le-mtHWHxwTAbTOPIkkQszI95\"\n    )\n    self._load_dpp_data(chip_file, decap_file, freq_file)\n\n    # Check the validity of the keepout parameters\n    assert (\n        num_keepout_min &lt;= num_keepout_max\n    ), \"num_keepout_min must be &lt;= num_keepout_max\"\n    assert (\n        num_keepout_max &lt;= self.size**2\n    ), \"num_keepout_max must be &lt;= size * size (total number of locations)\"\n</code></pre>"},{"location":"docs/content/api/envs/eda/#multi-port-decap-placement-problem-mdpp","title":"Multi-port Decap Placement Problem (mDPP)","text":""},{"location":"docs/content/api/envs/eda/#envs.eda.mdpp.env.MDPPEnv","title":"MDPPEnv","text":"<pre><code>MDPPEnv(\n    generator: MDPPGenerator = None,\n    generator_params: dict = {},\n    reward_type: str = \"minmax\",\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>DPPEnv</code></p> <p>Multiple decap placement problem (mDPP) environment This is a modified version of the DPP environment where we allow multiple probing ports</p> Observations <ul> <li>locations of the probing ports and keepout regions</li> <li>current decap placement</li> <li>remaining decaps</li> </ul> Constraints <ul> <li>decaps cannot be placed at the probing ports or keepout regions</li> <li>the number of decaps is limited</li> </ul> Finish Condition <ul> <li>the number of decaps exceeds the limit</li> </ul> Reward <ul> <li>the impedance suppression at the probing ports</li> </ul> <p>Parameters:</p> <ul> <li> <code>generator</code>               (<code>MDPPGenerator</code>, default:                   <code>None</code> )           \u2013            <p>DPPGenerator instance as the data generator</p> </li> <li> <code>generator_params</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>parameters for the generator</p> </li> <li> <code>reward_type</code>               (<code>str</code>, default:                   <code>'minmax'</code> )           \u2013            <p>reward type, either minmax or meansum</p> <ul> <li>minmax: min of the max of the decap scores</li> <li>meansum: mean of the sum of the decap scores</li> </ul> </li> </ul> Note <p>The minmax is more challenging as it requires to find the best decap location  for the worst case</p> Source code in <code>rl4co/envs/eda/mdpp/env.py</code> <pre><code>def __init__(\n    self,\n    generator: MDPPGenerator = None,\n    generator_params: dict = {},\n    reward_type: str = \"minmax\",\n    **kwargs,\n):\n    super().__init__(**kwargs)\n    if generator is None:\n        generator = MDPPGenerator(**generator_params)\n    self.generator = generator\n\n    assert reward_type in [\n        \"minmax\",\n        \"meansum\",\n    ], \"reward_type must be minmax or meansum\"\n    self.reward_type = reward_type\n\n    self._make_spec(self.generator)\n</code></pre>"},{"location":"docs/content/api/envs/eda/#envs.eda.mdpp.generator.MDPPGenerator","title":"MDPPGenerator","text":"<pre><code>MDPPGenerator(\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    num_keepout_min: int = 1,\n    num_keepout_max: int = 50,\n    num_probes_min: int = 2,\n    num_probes_max: int = 5,\n    max_decaps: int = 20,\n    data_dir: str = \"data/dpp/\",\n    chip_file: str = \"10x10_pkg_chip.npy\",\n    decap_file: str = \"01nF_decap.npy\",\n    freq_file: str = \"freq_201.npy\",\n    url: str = None,\n    **unused_kwargs\n)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>Data generator for the Multi Decap Placement Problem (MDPP).</p> <p>Parameters:</p> <ul> <li> <code>min_loc</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Minimum location value. Defaults to 0.</p> </li> <li> <code>max_loc</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Maximum location value. Defaults to 1.</p> </li> <li> <code>num_keepout_min</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Minimum number of keepout regions. Defaults to 1.</p> </li> <li> <code>num_keepout_max</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>Maximum number of keepout regions. Defaults to 50.</p> </li> <li> <code>max_decaps</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>Maximum number of decaps. Defaults to 20.</p> </li> <li> <code>data_dir</code>               (<code>str</code>, default:                   <code>'data/dpp/'</code> )           \u2013            <p>Directory to store data. Defaults to \"data/dpp/\". This can be downloaded from this url.</p> </li> <li> <code>chip_file</code>               (<code>str</code>, default:                   <code>'10x10_pkg_chip.npy'</code> )           \u2013            <p>Name of the chip file. Defaults to \"10x10_pkg_chip.npy\".</p> </li> <li> <code>decap_file</code>               (<code>str</code>, default:                   <code>'01nF_decap.npy'</code> )           \u2013            <p>Name of the decap file. Defaults to \"01nF_decap.npy\".</p> </li> <li> <code>freq_file</code>               (<code>str</code>, default:                   <code>'freq_201.npy'</code> )           \u2013            <p>Name of the frequency file. Defaults to \"freq_201.npy\".</p> </li> <li> <code>url</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>URL to download data from. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>A TensorDict with the following keys: locs [batch_size, num_loc, 2]: locations of each customer depot [batch_size, 2]: location of the depot demand [batch_size, num_loc]: demand of each customer capacity [batch_size]: capacity of the vehicle</p> </li> </ul> Source code in <code>rl4co/envs/eda/mdpp/generator.py</code> <pre><code>def __init__(\n    self,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    num_keepout_min: int = 1,\n    num_keepout_max: int = 50,\n    num_probes_min: int = 2,\n    num_probes_max: int = 5,\n    max_decaps: int = 20,\n    data_dir: str = \"data/dpp/\",\n    chip_file: str = \"10x10_pkg_chip.npy\",\n    decap_file: str = \"01nF_decap.npy\",\n    freq_file: str = \"freq_201.npy\",\n    url: str = None,\n    **unused_kwargs\n):\n    self.min_loc = min_loc\n    self.max_loc = max_loc\n    self.num_keepout_min = num_keepout_min\n    self.num_keepout_max = num_keepout_max\n    self.num_probes_min = num_probes_min\n    self.num_probes_max = num_probes_max\n    self.max_decaps = max_decaps\n    self.data_dir = data_dir\n\n    # DPP environment doen't have any other kwargs\n    if len(unused_kwargs) &gt; 0:\n        log.error(f\"Found {len(unused_kwargs)} unused kwargs: {unused_kwargs}\")\n\n\n    # Download and load the data from online dataset\n    self.url = (\n        \"https://github.com/kaist-silab/devformer/raw/main/data/data.zip\"\n        if url is None\n        else url\n    )\n    self.backup_url = (\n        \"https://drive.google.com/uc?id=1IEuR2v8Le-mtHWHxwTAbTOPIkkQszI95\"\n    )\n    self._load_dpp_data(chip_file, decap_file, freq_file)\n\n    # Check the validity of the keepout parameters\n    assert (\n        num_keepout_min &lt;= num_keepout_max\n    ), \"num_keepout_min must be &lt;= num_keepout_max\"\n    assert (\n        num_keepout_max &lt;= self.size**2\n    ), \"num_keepout_max must be &lt;= size * size (total number of locations)\"\n</code></pre>"},{"location":"docs/content/api/envs/routing/","title":"Routing Problems","text":"<p>See also the Multi-Task VRP at the bottom of this page, that includes 16 variants!</p>"},{"location":"docs/content/api/envs/routing/#asymmetric-traveling-salesman-problem-atsp","title":"Asymmetric Traveling Salesman Problem (ATSP)","text":""},{"location":"docs/content/api/envs/routing/#envs.routing.atsp.env.ATSPEnv","title":"ATSPEnv","text":"<pre><code>ATSPEnv(\n    generator: ATSPGenerator = None,\n    generator_params: dict = {},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RL4COEnvBase</code></p> <p>Asymmetric Traveling Salesman Problem (ATSP) environment At each step, the agent chooses a customer to visit. The reward is 0 unless the agent visits all the customers. In that case, the reward is (-)length of the path: maximizing the reward is equivalent to minimizing the path length. Unlike the TSP, the distance matrix is asymmetric, i.e., the distance from A to B is not necessarily the same as the distance from B to A.</p> Observations <ul> <li>distance matrix between customers</li> <li>the current customer</li> <li>the first customer (for calculating the reward)</li> <li>the remaining unvisited customers</li> </ul> Constraints <ul> <li>the tour starts and ends at the same customer.</li> <li>each customer must be visited exactly once.</li> </ul> Finish Condition <ul> <li>the agent has visited all customers.</li> </ul> Reward <ul> <li>(minus) the negative length of the path.</li> </ul> <p>Parameters:</p> <ul> <li> <code>generator</code>               (<code>ATSPGenerator</code>, default:                   <code>None</code> )           \u2013            <p>ATSPGenerator instance as the data generator</p> </li> <li> <code>generator_params</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>parameters for the generator</p> </li> </ul> Source code in <code>rl4co/envs/routing/atsp/env.py</code> <pre><code>def __init__(\n    self,\n    generator: ATSPGenerator = None,\n    generator_params: dict = {},\n    **kwargs,\n):\n    super().__init__(**kwargs)\n    if generator is None:\n        generator = ATSPGenerator(**generator_params)\n    self.generator = generator\n    self._make_spec(self.generator)\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.atsp.generator.ATSPGenerator","title":"ATSPGenerator","text":"<pre><code>ATSPGenerator(\n    num_loc: int = 10,\n    min_dist: float = 0.0,\n    max_dist: float = 1.0,\n    dist_distribution: Union[\n        int, float, str, type, Callable\n    ] = Uniform,\n    tmat_class: bool = True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>Data generator for the Asymmetric Travelling Salesman Problem (ATSP) Generate distance matrices inspired by the reference MatNet (Kwon et al., 2021) We satifsy the triangle inequality (TMAT class) in a batch</p> <p>Parameters:</p> <ul> <li> <code>num_loc</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>number of locations (customers) in the TSP</p> </li> <li> <code>min_dist</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>minimum value for the distance between nodes</p> </li> <li> <code>max_dist</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>maximum value for the distance between nodes</p> </li> <li> <code>dist_distribution</code>               (<code>Union[int, float, str, type, Callable]</code>, default:                   <code>Uniform</code> )           \u2013            <p>distribution for the distance between nodes</p> </li> <li> <code>tmat_class</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to generate a class of distance matrix</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>A TensorDict with the following keys: locs [batch_size, num_loc, 2]: locations of each customer</p> </li> </ul> Source code in <code>rl4co/envs/routing/atsp/generator.py</code> <pre><code>def __init__(\n    self,\n    num_loc: int = 10,\n    min_dist: float = 0.0,\n    max_dist: float = 1.0,\n    dist_distribution: Union[\n        int, float, str, type, Callable\n    ] = Uniform,\n    tmat_class: bool = True,\n    **kwargs\n):\n    self.num_loc = num_loc\n    self.min_dist = min_dist\n    self.max_dist = max_dist\n    self.tmat_class = tmat_class\n\n    # Distance distribution\n    if kwargs.get(\"dist_sampler\", None) is not None:\n        self.dist_sampler = kwargs[\"dist_sampler\"]\n    else:\n        self.dist_sampler = get_sampler(\"dist\", dist_distribution, 0.0, 1.0, **kwargs)\n</code></pre>"},{"location":"docs/content/api/envs/routing/#capacitated-vehicle-routing-problem-cvrp","title":"Capacitated Vehicle Routing Problem (CVRP)","text":""},{"location":"docs/content/api/envs/routing/#envs.routing.cvrp.env.CVRPEnv","title":"CVRPEnv","text":"<pre><code>CVRPEnv(\n    generator: CVRPGenerator = None,\n    generator_params: dict = {},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RL4COEnvBase</code></p> <p>Capacitated Vehicle Routing Problem (CVRP) environment. At each step, the agent chooses a customer to visit depending on the current location and the remaining capacity. When the agent visits a customer, the remaining capacity is updated. If the remaining capacity is not enough to visit any customer, the agent must go back to the depot. The reward is 0 unless the agent visits all the cities. In that case, the reward is (-)length of the path: maximizing the reward is equivalent to minimizing the path length.</p> Observations <ul> <li>location of the depot.</li> <li>locations and demand of each customer.</li> <li>current location of the vehicle.</li> <li>the remaining customer of the vehicle,</li> </ul> Constraints <ul> <li>the tour starts and ends at the depot.</li> <li>each customer must be visited exactly once.</li> <li>the vehicle cannot visit customers exceed the remaining capacity.</li> <li>the vehicle can return to the depot to refill the capacity.</li> </ul> Finish Condition <ul> <li>the vehicle has visited all customers and returned to the depot.</li> </ul> Reward <ul> <li>(minus) the negative length of the path.</li> </ul> <p>Parameters:</p> <ul> <li> <code>generator</code>               (<code>CVRPGenerator</code>, default:                   <code>None</code> )           \u2013            <p>CVRPGenerator instance as the data generator</p> </li> <li> <code>generator_params</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>parameters for the generator</p> </li> </ul> Source code in <code>rl4co/envs/routing/cvrp/env.py</code> <pre><code>def __init__(\n    self,\n    generator: CVRPGenerator = None,\n    generator_params: dict = {},\n    **kwargs,\n):\n    super().__init__(**kwargs)\n    if generator is None:\n        generator = CVRPGenerator(**generator_params)\n    self.generator = generator\n    self._make_spec(self.generator)\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.cvrp.env.CVRPEnv.check_solution_validity","title":"check_solution_validity  <code>staticmethod</code>","text":"<pre><code>check_solution_validity(td: TensorDict, actions: Tensor)\n</code></pre> <p>Check that solution is valid: nodes are not visited twice except depot and capacity is not exceeded</p> Source code in <code>rl4co/envs/routing/cvrp/env.py</code> <pre><code>@staticmethod\ndef check_solution_validity(td: TensorDict, actions: torch.Tensor):\n    \"\"\"Check that solution is valid: nodes are not visited twice except depot and capacity is not exceeded\"\"\"\n    # Check if tour is valid, i.e. contain 0 to n-1\n    batch_size, graph_size = td[\"demand\"].size()\n    sorted_pi = actions.data.sort(1)[0]\n\n    # Sorting it should give all zeros at front and then 1...n\n    assert (\n        torch.arange(1, graph_size + 1, out=sorted_pi.data.new())\n        .view(1, -1)\n        .expand(batch_size, graph_size)\n        == sorted_pi[:, -graph_size:]\n    ).all() and (sorted_pi[:, :-graph_size] == 0).all(), \"Invalid tour\"\n\n    # Visiting depot resets capacity so we add demand = -capacity (we make sure it does not become negative)\n    demand_with_depot = torch.cat((-td[\"vehicle_capacity\"], td[\"demand\"]), 1)\n    d = demand_with_depot.gather(1, actions)\n\n    used_cap = torch.zeros_like(td[\"demand\"][:, 0])\n    for i in range(actions.size(1)):\n        used_cap += d[\n            :, i\n        ]  # This will reset/make capacity negative if i == 0, e.g. depot visited\n        # Cannot use less than 0\n        used_cap[used_cap &lt; 0] = 0\n        assert (\n            used_cap &lt;= td[\"vehicle_capacity\"] + 1e-5\n        ).all(), \"Used more than capacity\"\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.cvrp.env.CVRPEnv.load_data","title":"load_data  <code>staticmethod</code>","text":"<pre><code>load_data(fpath, batch_size=[])\n</code></pre> <p>Dataset loading from file Normalize demand by capacity to be in [0, 1]</p> Source code in <code>rl4co/envs/routing/cvrp/env.py</code> <pre><code>@staticmethod\ndef load_data(fpath, batch_size=[]):\n    \"\"\"Dataset loading from file\n    Normalize demand by capacity to be in [0, 1]\n    \"\"\"\n    td_load = load_npz_to_tensordict(fpath)\n    td_load.set(\"demand\", td_load[\"demand\"] / td_load[\"capacity\"][:, None])\n    return td_load\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.cvrp.env.CVRPEnv.replace_selected_actions","title":"replace_selected_actions","text":"<pre><code>replace_selected_actions(\n    cur_actions: Tensor,\n    new_actions: Tensor,\n    selection_mask: Tensor,\n) -&gt; Tensor\n</code></pre> <p>Replace selected current actions with updated actions based on <code>selection_mask</code>.</p> Source code in <code>rl4co/envs/routing/cvrp/env.py</code> <pre><code>def replace_selected_actions(self, cur_actions: torch.Tensor, new_actions: torch.Tensor, selection_mask: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Replace selected current actions with updated actions based on `selection_mask`.\n\n    Args:\n        cur_actions [batch_size, num_loc]\n        new_actions [batch_size, num_loc]\n        selection_mask [batch_size,]\n    \"\"\"\n    diff_length = cur_actions.size(-1) - new_actions.size(-1)\n    if diff_length &gt; 0:\n        new_actions = torch.nn.functional.pad(new_actions, (0, diff_length, 0, 0), mode=\"constant\", value=0)\n    elif diff_length &lt; 0:\n        cur_actions = torch.nn.functional.pad(cur_actions, (0, -diff_length, 0, 0), mode=\"constant\", value=0)\n    cur_actions[selection_mask] = new_actions[selection_mask]\n    return cur_actions\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.cvrp.generator.CVRPGenerator","title":"CVRPGenerator","text":"<pre><code>CVRPGenerator(\n    num_loc: int = 20,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    loc_distribution: Union[\n        int, float, str, type, Callable\n    ] = Uniform,\n    depot_distribution: Union[\n        int, float, str, type, Callable\n    ] = None,\n    min_demand: int = 1,\n    max_demand: int = 10,\n    demand_distribution: Union[\n        int, float, type, Callable\n    ] = Uniform,\n    vehicle_capacity: float = 1.0,\n    capacity: float = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>Data generator for the Capacitated Vehicle Routing Problem (CVRP).</p> <p>Parameters:</p> <ul> <li> <code>num_loc</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>number of locations (cities) in the VRP, without the depot. (e.g. 10 means 10 locs + 1 depot)</p> </li> <li> <code>min_loc</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>minimum value for the location coordinates</p> </li> <li> <code>max_loc</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>maximum value for the location coordinates</p> </li> <li> <code>loc_distribution</code>               (<code>Union[int, float, str, type, Callable]</code>, default:                   <code>Uniform</code> )           \u2013            <p>distribution for the location coordinates</p> </li> <li> <code>depot_distribution</code>               (<code>Union[int, float, str, type, Callable]</code>, default:                   <code>None</code> )           \u2013            <p>distribution for the depot location. If None, sample the depot from the locations</p> </li> <li> <code>min_demand</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>minimum value for the demand of each customer</p> </li> <li> <code>max_demand</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>maximum value for the demand of each customer</p> </li> <li> <code>demand_distribution</code>               (<code>Union[int, float, type, Callable]</code>, default:                   <code>Uniform</code> )           \u2013            <p>distribution for the demand of each customer</p> </li> <li> <code>capacity</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>capacity of the vehicle</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>A TensorDict with the following keys: locs [batch_size, num_loc, 2]: locations of each customer depot [batch_size, 2]: location of the depot demand [batch_size, num_loc]: demand of each customer capacity [batch_size]: capacity of the vehicle</p> </li> </ul> Source code in <code>rl4co/envs/routing/cvrp/generator.py</code> <pre><code>def __init__(\n    self,\n    num_loc: int = 20,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    loc_distribution: Union[int, float, str, type, Callable] = Uniform,\n    depot_distribution: Union[int, float, str, type, Callable] = None,\n    min_demand: int = 1,\n    max_demand: int = 10,\n    demand_distribution: Union[int, float, type, Callable] = Uniform,\n    vehicle_capacity: float = 1.0,\n    capacity: float = None,\n    **kwargs,\n):\n    self.num_loc = num_loc\n    self.min_loc = min_loc\n    self.max_loc = max_loc\n    self.min_demand = min_demand\n    self.max_demand = max_demand\n    self.vehicle_capacity = vehicle_capacity\n\n    # Location distribution\n    if kwargs.get(\"loc_sampler\", None) is not None:\n        self.loc_sampler = kwargs[\"loc_sampler\"]\n    else:\n        self.loc_sampler = get_sampler(\n            \"loc\", loc_distribution, min_loc, max_loc, **kwargs\n        )\n\n    # Depot distribution\n    if kwargs.get(\"depot_sampler\", None) is not None:\n        self.depot_sampler = kwargs[\"depot_sampler\"]\n    else:\n        self.depot_sampler = get_sampler(\n            \"depot\", depot_distribution, min_loc, max_loc, **kwargs\n        ) if depot_distribution is not None else None\n\n    # Demand distribution\n    if kwargs.get(\"demand_sampler\", None) is not None:\n        self.demand_sampler = kwargs[\"demand_sampler\"]\n    else:\n        self.demand_sampler = get_sampler(\n            \"demand\", demand_distribution, min_demand - 1, max_demand - 1, **kwargs\n        )\n\n    # Capacity\n    if (\n        capacity is None\n    ):  # If not provided, use the default capacity from Kool et al. 2019\n        capacity = CAPACITIES.get(num_loc, None)\n    if (\n        capacity is None\n    ):  # If not in the table keys, find the closest number of nodes as the key\n        closest_num_loc = min(CAPACITIES.keys(), key=lambda x: abs(x - num_loc))\n        capacity = CAPACITIES[closest_num_loc]\n        log.warning(\n            f\"The capacity capacity for {num_loc} locations is not defined. Using the closest capacity: {capacity}\\\n                with {closest_num_loc} locations.\"\n        )\n    self.capacity = capacity\n</code></pre>"},{"location":"docs/content/api/envs/routing/#multiple-traveling-salesman-problem-mtsp","title":"Multiple Traveling Salesman Problem (mTSP)","text":""},{"location":"docs/content/api/envs/routing/#envs.routing.mtsp.env.MTSPEnv","title":"MTSPEnv","text":"<pre><code>MTSPEnv(\n    generator: MTSPGenerator = None,\n    generator_params: dict = {},\n    cost_type: str = \"minmax\",\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RL4COEnvBase</code></p> <p>Multiple Traveling Salesman Problem environment At each step, an agent chooses to visit a city. A maximum of <code>num_agents</code> agents can be employed to visit the cities. The cost can be defined in two ways:</p> <pre><code>- `minmax`: (default) the reward is the maximum of the path lengths of all the agents\n- `sum`: the cost is the sum of the path lengths of all the agents\n</code></pre> <p>Reward is - cost, so the goal is to maximize the reward (minimize the cost).</p> Observations <ul> <li>locations of the depot and each customer.</li> <li>number of agents.</li> <li>the current agent index.</li> <li>the current location of the vehicle.</li> </ul> Constrains <ul> <li>each agent's tour starts and ends at the depot.</li> <li>each customer must be visited exactly once.</li> </ul> Finish condition <ul> <li>all customers are visited and all agents back to the depot.</li> </ul> Reward <p>There are two ways to calculate the cost (-reward):</p> <ul> <li><code>minmax</code>: (default) the cost is the maximum of the path lengths of all the agents.</li> <li><code>sum</code>: the cost is the sum of the path lengths of all the agents.</li> </ul> <p>Parameters:</p> <ul> <li> <code>cost_type</code>               (<code>str</code>, default:                   <code>'minmax'</code> )           \u2013            <p>type of cost to use, either <code>minmax</code> or <code>sum</code></p> </li> <li> <code>generator</code>               (<code>MTSPGenerator</code>, default:                   <code>None</code> )           \u2013            <p>MTSPGenerator instance as the data generator</p> </li> <li> <code>generator_params</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>parameters for the generator</p> </li> </ul> Source code in <code>rl4co/envs/routing/mtsp/env.py</code> <pre><code>def __init__(\n    self,\n    generator: MTSPGenerator = None,\n    generator_params: dict = {},\n    cost_type: str = \"minmax\",\n    **kwargs,\n):\n    super().__init__(**kwargs)\n    if generator is None:\n        generator = MTSPGenerator(**generator_params)\n    self.generator = generator\n    self.cost_type = cost_type\n    self._make_spec(self.generator)\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.mtsp.generator.MTSPGenerator","title":"MTSPGenerator","text":"<pre><code>MTSPGenerator(\n    num_loc: int = 20,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    loc_distribution: Union[\n        int, float, str, type, Callable\n    ] = Uniform,\n    min_num_agents: int = 5,\n    max_num_agents: int = 5,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>Data generator for the Multiple Travelling Salesman Problem (mTSP).</p> <p>Parameters:</p> <ul> <li> <code>num_loc</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>number of locations (customers) in the TSP</p> </li> <li> <code>min_loc</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>minimum value for the location coordinates</p> </li> <li> <code>max_loc</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>maximum value for the location coordinates</p> </li> <li> <code>loc_distribution</code>               (<code>Union[int, float, str, type, Callable]</code>, default:                   <code>Uniform</code> )           \u2013            <p>distribution for the location coordinates</p> </li> <li> <code>min_num_agents</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>minimum number of agents (vehicles), include</p> </li> <li> <code>max_num_agents</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>maximum number of agents (vehicles), include</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>A TensorDict with the following keys: locs [batch_size, num_loc, 2]: locations of each customer num_agents [batch_size]: number of agents (vehicles)</p> </li> </ul> Source code in <code>rl4co/envs/routing/mtsp/generator.py</code> <pre><code>def __init__(\n    self,\n    num_loc: int = 20,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    loc_distribution: Union[int, float, str, type, Callable] = Uniform,\n    min_num_agents: int = 5,\n    max_num_agents: int = 5,\n    **kwargs,\n):\n    self.num_loc = num_loc\n    self.min_loc = min_loc\n    self.max_loc = max_loc\n    self.min_num_agents = min_num_agents\n    self.max_num_agents = max_num_agents\n\n    # Location distribution\n    if kwargs.get(\"loc_sampler\", None) is not None:\n        self.loc_sampler = kwargs[\"loc_sampler\"]\n    else:\n        self.loc_sampler = get_sampler(\n            \"loc\", loc_distribution, min_loc, max_loc, **kwargs\n        )\n</code></pre>"},{"location":"docs/content/api/envs/routing/#orienteering-problem-op","title":"Orienteering Problem (OP)","text":""},{"location":"docs/content/api/envs/routing/#envs.routing.op.env.OPEnv","title":"OPEnv","text":"<pre><code>OPEnv(\n    generator: OPGenerator = None,\n    generator_params: dict = {},\n    prize_type: str = \"dist\",\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RL4COEnvBase</code></p> <p>Orienteering Problem (OP) environment. At each step, the agent chooses a location to visit in order to maximize the collected prize. The total length of the path must not exceed a given threshold.</p> Observations <ul> <li>location of the depot</li> <li>locations and prize of each customer</li> <li>current location of the vehicle</li> <li>current tour length</li> <li>current total prize</li> <li>the remaining length of the path</li> </ul> Constraints <ul> <li>the tour starts and ends at the depot</li> <li>not all customers need to be visited</li> <li>the vehicle cannot visit customers exceed the remaining length of the path</li> </ul> Finish Condition <ul> <li>the vehicle back to the depot</li> </ul> Reward <ul> <li>the sum of the prizes of visited nodes</li> </ul> <p>Parameters:</p> <ul> <li> <code>generator</code>               (<code>OPGenerator</code>, default:                   <code>None</code> )           \u2013            <p>OPGenerator instance as the data generator</p> </li> <li> <code>generator_params</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>parameters for the generator</p> </li> </ul> Source code in <code>rl4co/envs/routing/op/env.py</code> <pre><code>def __init__(\n    self,\n    generator: OPGenerator = None,\n    generator_params: dict = {},\n    prize_type: str = \"dist\",\n    **kwargs,\n):\n    super().__init__(**kwargs)\n    if generator is None:\n        generator = OPGenerator(**generator_params)\n    self.generator = generator\n    self.prize_type = prize_type\n    assert self.prize_type in [\n        \"dist\",\n        \"unif\",\n        \"const\",\n    ], f\"Invalid prize_type: {self.prize_type}\"\n    self._make_spec(self.generator)\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.op.env.OPEnv.get_action_mask","title":"get_action_mask  <code>staticmethod</code>","text":"<pre><code>get_action_mask(td: TensorDict) -&gt; Tensor\n</code></pre> <p>Get action mask with 1 = feasible action, 0 = infeasible action. Cannot visit if already visited, if depot has been visited, or if the length exceeds the maximum length.</p> Source code in <code>rl4co/envs/routing/op/env.py</code> <pre><code>@staticmethod\ndef get_action_mask(td: TensorDict) -&gt; torch.Tensor:\n    \"\"\"Get action mask with 1 = feasible action, 0 = infeasible action.\n    Cannot visit if already visited, if depot has been visited, or if the length exceeds the maximum length.\n    \"\"\"\n    current_loc = gather_by_index(td[\"locs\"], td[\"current_node\"])[..., None, :]\n    exceeds_length = (\n        td[\"tour_length\"][..., None] + (td[\"locs\"] - current_loc).norm(p=2, dim=-1)\n        &gt; td[\"max_length\"]\n    )\n    mask = td[\"visited\"] | td[\"visited\"][..., 0:1] | exceeds_length\n\n    action_mask = ~mask  # 1 = feasible action, 0 = infeasible action\n\n    # Depot can always be visited: we do not hardcode knowledge that this is strictly suboptimal if other options are available\n    action_mask[..., 0] = 1\n    return action_mask\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.op.env.OPEnv.check_solution_validity","title":"check_solution_validity  <code>staticmethod</code>","text":"<pre><code>check_solution_validity(\n    td: TensorDict,\n    actions: Tensor,\n    add_distance_to_depot: bool = True,\n) -&gt; None\n</code></pre> <p>Check that solution is valid: nodes are not visited twice except depot and capacity is not exceeded. If <code>add_distance_to_depot</code> if True, then the distance to the depot is added to max length since by default, the max length is modified in the reset function to account for the distance to the depot.</p> Source code in <code>rl4co/envs/routing/op/env.py</code> <pre><code>@staticmethod\ndef check_solution_validity(\n    td: TensorDict, actions: torch.Tensor, add_distance_to_depot: bool = True\n) -&gt; None:\n    \"\"\"Check that solution is valid: nodes are not visited twice except depot and capacity is not exceeded.\n    If `add_distance_to_depot` if True, then the distance to the depot is added to max length since by default, the max length is\n    modified in the reset function to account for the distance to the depot.\n    \"\"\"\n\n    # Check that tours are valid, i.e. contain 0 to n -1\n    sorted_actions = actions.data.sort(1)[0]\n    # Make sure each node visited once at most (except for depot)\n    assert (\n        (sorted_actions[:, 1:] == 0)\n        | (sorted_actions[:, 1:] &gt; sorted_actions[:, :-1])\n    ).all(), \"Duplicates\"\n\n    # Gather locations in order of tour and get the length of tours\n    locs_ordered = gather_by_index(td[\"locs\"], actions)\n    length = get_tour_length(locs_ordered)\n\n    max_length = td[\"max_length\"]\n    if add_distance_to_depot:\n        max_length = (\n            max_length\n            + (td[\"locs\"][..., 0:1, :] - td[\"locs\"]).norm(p=2, dim=-1)\n            + 1e-6\n        )\n    assert (\n        length[..., None] &lt;= max_length + 1e-5\n    ).all(), \"Max length exceeded by {}\".format(\n        (length[..., None] - max_length).max()\n    )\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.op.generator.OPGenerator","title":"OPGenerator","text":"<pre><code>OPGenerator(\n    num_loc: int = 20,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    loc_distribution: Union[\n        int, float, str, type, Callable\n    ] = Uniform,\n    depot_distribution: Union[\n        int, float, str, type, Callable\n    ] = None,\n    min_prize: float = 1.0,\n    max_prize: float = 1.0,\n    prize_distribution: Union[\n        int, float, type, Callable\n    ] = Uniform,\n    prize_type: str = \"dist\",\n    max_length: Union[float, Tensor] = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>Data generator for the Orienteering Problem (OP).</p> <p>Parameters:</p> <ul> <li> <code>num_loc</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>number of locations (customers) in the OP, without the depot. (e.g. 10 means 10 locs + 1 depot)</p> </li> <li> <code>min_loc</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>minimum value for the location coordinates</p> </li> <li> <code>max_loc</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>maximum value for the location coordinates</p> </li> <li> <code>loc_distribution</code>               (<code>Union[int, float, str, type, Callable]</code>, default:                   <code>Uniform</code> )           \u2013            <p>distribution for the location coordinates</p> </li> <li> <code>depot_distribution</code>               (<code>Union[int, float, str, type, Callable]</code>, default:                   <code>None</code> )           \u2013            <p>distribution for the depot location. If None, sample the depot from the locations</p> </li> <li> <code>min_prize</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>minimum value for the prize of each customer</p> </li> <li> <code>max_prize</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>maximum value for the prize of each customer</p> </li> <li> <code>prize_distribution</code>               (<code>Union[int, float, type, Callable]</code>, default:                   <code>Uniform</code> )           \u2013            <p>distribution for the prize of each customer</p> </li> <li> <code>max_length</code>               (<code>Union[float, Tensor]</code>, default:                   <code>None</code> )           \u2013            <p>maximum length of the path</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>A TensorDict with the following keys: locs [batch_size, num_loc, 2]: locations of each customer depot [batch_size, 2]: location of the depot prize [batch_size, num_loc]: prize of each customer max_length [batch_size, 1]: maximum length of the path for each customer</p> </li> </ul> Source code in <code>rl4co/envs/routing/op/generator.py</code> <pre><code>def __init__(\n    self,\n    num_loc: int = 20,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    loc_distribution: Union[int, float, str, type, Callable] = Uniform,\n    depot_distribution: Union[int, float, str, type, Callable] = None,\n    min_prize: float = 1.0,\n    max_prize: float = 1.0,\n    prize_distribution: Union[int, float, type, Callable] = Uniform,\n    prize_type: str = \"dist\",\n    max_length: Union[float, torch.Tensor] = None,\n    **kwargs,\n):\n    self.num_loc = num_loc\n    self.min_loc = min_loc\n    self.max_loc = max_loc\n    self.min_prize = min_prize\n    self.max_prize = max_prize\n    self.prize_type = prize_type\n    self.max_length = max_length\n\n    # Location distribution\n    if kwargs.get(\"loc_sampler\", None) is not None:\n        self.loc_sampler = kwargs[\"loc_sampler\"]\n    else:\n        self.loc_sampler = get_sampler(\n            \"loc\", loc_distribution, min_loc, max_loc, **kwargs\n        )\n\n    # Depot distribution\n    if kwargs.get(\"depot_sampler\", None) is not None:\n        self.depot_sampler = kwargs[\"depot_sampler\"]\n    else:\n        self.depot_sampler = get_sampler(\n            \"depot\", depot_distribution, min_loc, max_loc, **kwargs\n        ) if depot_distribution is not None else None\n\n    # Prize distribution\n    if kwargs.get(\"prize_sampler\", None) is not None:\n        self.prize_sampler = kwargs[\"prize_sampler\"]\n    elif (\n        prize_distribution == \"dist\"\n    ):  # If prize_distribution is 'dist', then the prize is the distance from the depot\n        self.prize_sampler = None\n    else:\n        self.prize_sampler = get_sampler(\n            \"prize\", prize_distribution, min_prize, max_prize, **kwargs\n        )\n\n    # Max length\n    if max_length is not None:\n        self.max_length = max_length\n    else:\n        self.max_length = MAX_LENGTHS.get(num_loc, None)\n    if self.max_length is None:\n        closest_num_loc = min(MAX_LENGTHS.keys(), key=lambda x: abs(x - num_loc))\n        self.max_length = MAX_LENGTHS[closest_num_loc]\n        log.warning(\n            f\"The max length for {num_loc} locations is not defined. Using the closest max length: {self.max_length}\\\n                with {closest_num_loc} locations.\"\n        )\n</code></pre>"},{"location":"docs/content/api/envs/routing/#pickup-and-delivery-problem-pdp","title":"Pickup and Delivery Problem (PDP)","text":""},{"location":"docs/content/api/envs/routing/#envs.routing.pdp.env.PDPEnv","title":"PDPEnv","text":"<pre><code>PDPEnv(\n    generator: PDPGenerator = None,\n    generator_params: dict = {},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RL4COEnvBase</code></p> <p>Pickup and Delivery Problem (PDP) environment. The environment is made of num_loc + 1 locations (cities):</p> <pre><code>- 1 depot\n- `num_loc` / 2 pickup locations\n- `num_loc` / 2 delivery locations\n</code></pre> <p>The goal is to visit all the pickup and delivery locations in the shortest path possible starting from the depot The conditions is that the agent must visit a pickup location before visiting its corresponding delivery location</p> Observations <ul> <li>locations of the depot, pickup, and delivery locations</li> <li>current location of the vehicle</li> <li>the remaining locations to deliver</li> <li>the visited locations</li> <li>the current step</li> </ul> Constraints <ul> <li>the tour starts and ends at the depot</li> <li>each pickup location must be visited before its corresponding delivery location</li> <li>the vehicle cannot visit the same location twice</li> </ul> Finish Condition <ul> <li>the vehicle has visited all locations</li> </ul> Reward <ul> <li>(minus) the negative length of the path</li> </ul> <p>Parameters:</p> <ul> <li> <code>generator</code>               (<code>PDPGenerator</code>, default:                   <code>None</code> )           \u2013            <p>PDPGenerator instance as the data generator</p> </li> <li> <code>generator_params</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>parameters for the generator</p> </li> </ul> Source code in <code>rl4co/envs/routing/pdp/env.py</code> <pre><code>def __init__(\n    self,\n    generator: PDPGenerator = None,\n    generator_params: dict = {},\n    **kwargs,\n):\n    super().__init__(**kwargs)\n    if generator is None:\n        generator = PDPGenerator(**generator_params)\n    self.generator = generator\n    self._make_spec(self.generator)\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.pdp.env.PDPEnv.get_num_starts","title":"get_num_starts","text":"<pre><code>get_num_starts(td)\n</code></pre> <p>Only half of the nodes (i.e. pickup nodes) can be start nodes</p> Source code in <code>rl4co/envs/routing/pdp/env.py</code> <pre><code>def get_num_starts(self, td):\n    \"\"\"Only half of the nodes (i.e. pickup nodes) can be start nodes\"\"\"\n    return (td[\"locs\"].shape[-2] - 1) // 2\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.pdp.env.PDPEnv.select_start_nodes","title":"select_start_nodes","text":"<pre><code>select_start_nodes(td, num_starts)\n</code></pre> <p>Only nodes from [1 : num_loc // 2 +1] (i.e. pickups) can be selected</p> Source code in <code>rl4co/envs/routing/pdp/env.py</code> <pre><code>def select_start_nodes(self, td, num_starts):\n    \"\"\"Only nodes from [1 : num_loc // 2 +1] (i.e. pickups) can be selected\"\"\"\n    num_possible_starts = (td[\"locs\"].shape[-2] - 1) // 2\n    selected = (\n        torch.arange(num_starts, device=td.device).repeat_interleave(td.shape[0])\n        % num_possible_starts\n        + 1\n    )\n    return selected\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.pdp.generator.PDPGenerator","title":"PDPGenerator","text":"<pre><code>PDPGenerator(\n    num_loc: int = 20,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    init_sol_type: str = \"random\",\n    loc_distribution: Union[\n        int, float, str, type, Callable\n    ] = Uniform,\n    depot_distribution: Union[\n        int, float, str, type, Callable\n    ] = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>Data generator for the Pickup and Delivery Problem (PDP). Args:     num_loc: number of locations (customers) in the PDP, without the depot. (e.g. 10 means 10 locs + 1 depot)</p> <pre><code>    - 1 depot\n    - `num_loc` / 2 pickup locations\n    - `num_loc` / 2 delivery locations\nmin_loc: minimum value for the location coordinates\nmax_loc: maximum value for the location coordinates\ninit_sol_type: the method type used for generating initial solutions (random or greedy)\nloc_distribution: distribution for the location coordinates\ndepot_distribution: distribution for the depot location. If None, sample the depot from the locations\n</code></pre> <p>Returns:</p> <ul> <li>           \u2013            <p>A TensorDict with the following keys: locs [batch_size, num_loc, 2]: locations of each customer depot [batch_size, 2]: location of the depot</p> </li> </ul> Source code in <code>rl4co/envs/routing/pdp/generator.py</code> <pre><code>def __init__(\n    self,\n    num_loc: int = 20,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    init_sol_type: str = \"random\",\n    loc_distribution: Union[int, float, str, type, Callable] = Uniform,\n    depot_distribution: Union[int, float, str, type, Callable] = None,\n    **kwargs,\n):\n    self.num_loc = num_loc\n    self.min_loc = min_loc\n    self.max_loc = max_loc\n    self.init_sol_type = init_sol_type\n\n    # Number of locations must be even\n    if num_loc % 2 != 0:\n        log.warn(\n            \"Number of locations must be even. Adding 1 to the number of locations.\"\n        )\n        self.num_loc += 1\n\n    # Location distribution\n    if kwargs.get(\"loc_sampler\", None) is not None:\n        self.loc_sampler = kwargs[\"loc_sampler\"]\n    else:\n        self.loc_sampler = get_sampler(\n            \"loc\", loc_distribution, min_loc, max_loc, **kwargs\n        )\n\n    # Depot distribution\n    if kwargs.get(\"depot_sampler\", None) is not None:\n        self.depot_sampler = kwargs[\"depot_sampler\"]\n    else:\n        self.depot_sampler = get_sampler(\n            \"depot\", depot_distribution, min_loc, max_loc, **kwargs\n        ) if depot_distribution is not None else None\n</code></pre>"},{"location":"docs/content/api/envs/routing/#prize-collecting-traveling-salesman-problem-pctsp","title":"Prize Collecting Traveling Salesman Problem (PCTSP)","text":""},{"location":"docs/content/api/envs/routing/#envs.routing.pctsp.env.PCTSPEnv","title":"PCTSPEnv","text":"<pre><code>PCTSPEnv(\n    generator: PCTSPGenerator = None,\n    generator_params: dict = {},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RL4COEnvBase</code></p> <p>Prize-collecting TSP (PCTSP) environment. The goal is to collect as much prize as possible while minimizing the total travel cost. The environment is stochastic, the prize is only revealed when the node is visited.</p> Observations <ul> <li>locations of the nodes</li> <li>prize and penalty of each node</li> <li>current location of the vehicle</li> <li>current total prize</li> <li>current total penalty</li> <li>visited nodes</li> <li>prize required to visit a node</li> <li>the current step</li> </ul> Constraints <ul> <li>the tour starts and ends at the depot</li> <li>the vehicle cannot visit nodes exceed the remaining prize</li> </ul> Finish Condition <ul> <li>the vehicle back to the depot</li> </ul> Reward <ul> <li>the sum of the saved penalties</li> </ul> <p>Parameters:</p> <ul> <li> <code>generator</code>               (<code>PCTSPGenerator</code>, default:                   <code>None</code> )           \u2013            <p>OPGenerator instance as the data generator</p> </li> <li> <code>generator_params</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>parameters for the generator</p> </li> </ul> Source code in <code>rl4co/envs/routing/pctsp/env.py</code> <pre><code>def __init__(\n    self,\n    generator: PCTSPGenerator = None,\n    generator_params: dict = {},\n    **kwargs,\n):\n    super().__init__(**kwargs)\n    if generator is None:\n        generator = PCTSPGenerator(**generator_params)\n    self.generator = generator\n    self._make_spec(self.generator)\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.pctsp.env.PCTSPEnv.get_action_mask","title":"get_action_mask  <code>staticmethod</code>","text":"<pre><code>get_action_mask(td: TensorDict) -&gt; Tensor\n</code></pre> <p>Cannot visit depot if not yet collected 1 total prize and there are unvisited nodes</p> Source code in <code>rl4co/envs/routing/pctsp/env.py</code> <pre><code>@staticmethod\ndef get_action_mask(td: TensorDict) -&gt; torch.Tensor:\n    \"\"\"Cannot visit depot if not yet collected 1 total prize and there are unvisited nodes\"\"\"\n    mask = td[\"visited\"] | td[\"visited\"][..., 0:1]\n    mask[..., 0] = (td[\"cur_total_prize\"] &lt; 1.0) &amp; (\n        td[\"visited\"][..., 1:].int().sum(-1) &lt; td[\"visited\"][..., 1:].size(-1)\n    )\n    return ~(mask &gt; 0)  # Invert mask, since 1 means feasible action\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.pctsp.env.PCTSPEnv.check_solution_validity","title":"check_solution_validity  <code>staticmethod</code>","text":"<pre><code>check_solution_validity(\n    td: TensorDict, actions: Tensor\n) -&gt; None\n</code></pre> <p>Check that the solution is valid, i.e. contains all nodes once at most, and either prize constraint is met or all nodes are visited</p> Source code in <code>rl4co/envs/routing/pctsp/env.py</code> <pre><code>@staticmethod\ndef check_solution_validity(td: TensorDict, actions: torch.Tensor) -&gt; None:\n    \"\"\"Check that the solution is valid, i.e. contains all nodes once at most, and either prize constraint is met or all nodes are visited\"\"\"\n\n    # Check that tours are valid, i.e. contain 0 to n -1\n    sorted_actions = actions.data.sort(1)[0]\n\n    # Make sure each node visited once at most (except for depot)\n    assert (\n        (sorted_actions[..., 1:] == 0)\n        | (sorted_actions[..., 1:] &gt; sorted_actions[..., :-1])\n    ).all(), \"Duplicates\"\n\n    prize = td[\"real_prize\"][..., 1:]  # Remove depot\n    prize_with_depot = torch.cat((torch.zeros_like(prize[:, :1]), prize), 1)\n    p = prize_with_depot.gather(1, actions)\n\n    # Either prize constraint should be satisfied or all prizes should be visited\n    assert (\n        (p.sum(-1) &gt;= 1 - 1e-5)\n        | (\n            sorted_actions.size(-1) - (sorted_actions == 0).int().sum(-1)\n            == (td[\"locs\"].size(-2) - 1)\n        )  # no depot\n    ).all(), \"Total prize does not satisfy min total prize\"\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.pctsp.generator.PCTSPGenerator","title":"PCTSPGenerator","text":"<pre><code>PCTSPGenerator(\n    num_loc: int = 20,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    loc_distribution: Union[\n        int, float, str, type, Callable\n    ] = Uniform,\n    depot_distribution: Union[\n        int, float, str, type, Callable\n    ] = None,\n    penalty_factor: float = 3.0,\n    prize_required: float = 1.0,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>Data generator for the Prize-collecting Traveling Salesman Problem (PCTSP).</p> <p>Parameters:</p> <ul> <li> <code>num_loc</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>number of locations (customers) in the VRP, without the depot. (e.g. 10 means 10 locs + 1 depot)</p> </li> <li> <code>min_loc</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>minimum value for the location coordinates</p> </li> <li> <code>max_loc</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>maximum value for the location coordinates</p> </li> <li> <code>loc_distribution</code>               (<code>Union[int, float, str, type, Callable]</code>, default:                   <code>Uniform</code> )           \u2013            <p>distribution for the location coordinates</p> </li> <li> <code>depot_distribution</code>               (<code>Union[int, float, str, type, Callable]</code>, default:                   <code>None</code> )           \u2013            <p>distribution for the depot location. If None, sample the depot from the locations</p> </li> <li> <code>min_demand</code>           \u2013            <p>minimum value for the demand of each customer</p> </li> <li> <code>max_demand</code>           \u2013            <p>maximum value for the demand of each customer</p> </li> <li> <code>demand_distribution</code>           \u2013            <p>distribution for the demand of each customer</p> </li> <li> <code>capacity</code>           \u2013            <p>capacity of the vehicle</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>A TensorDict with the following keys: locs [batch_size, num_loc, 2]: locations of each city depot [batch_size, 2]: location of the depot demand [batch_size, num_loc]: demand of each customer capacity [batch_size, 1]: capacity of the vehicle</p> </li> </ul> Source code in <code>rl4co/envs/routing/pctsp/generator.py</code> <pre><code>def __init__(\n    self,\n    num_loc: int = 20,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    loc_distribution: Union[int, float, str, type, Callable] = Uniform,\n    depot_distribution: Union[int, float, str, type, Callable] = None,\n    penalty_factor: float = 3.0,\n    prize_required: float = 1.0,\n    **kwargs,\n):\n    self.num_loc = num_loc\n    self.min_loc = min_loc\n    self.max_loc = max_loc\n    self.penalty_fctor = penalty_factor\n    self.prize_required = prize_required\n\n    # Location distribution\n    if kwargs.get(\"loc_sampler\", None) is not None:\n        self.loc_sampler = kwargs[\"loc_sampler\"]\n    else:\n        self.loc_sampler = get_sampler(\n            \"loc\", loc_distribution, min_loc, max_loc, **kwargs\n        )\n\n    # Depot distribution\n    if kwargs.get(\"depot_sampler\", None) is not None:\n        self.depot_sampler = kwargs[\"depot_sampler\"]\n    else:\n        self.depot_sampler = get_sampler(\n            \"depot\", depot_distribution, min_loc, max_loc, **kwargs\n        ) if depot_distribution is not None else None\n\n    # Prize distribution\n    self.deterministic_prize_sampler = get_sampler(\n        \"deterministric_prize\", \"uniform\", 0.0, 4.0 / self.num_loc, **kwargs\n    )\n    self.stochastic_prize_sampler = get_sampler(\n        \"stochastic_prize\", \"uniform\", 0.0, 8.0 / self.num_loc, **kwargs\n    )\n\n    # For the penalty to make sense it should be not too large (in which case all nodes will be visited) nor too small\n    # so we want the objective term to be approximately equal to the length of the tour, which we estimate with half\n    # of the nodes by half of the tour length (which is very rough but similar to op)\n    # This means that the sum of penalties for all nodes will be approximately equal to the tour length (on average)\n    # The expected total (uniform) penalty of half of the nodes (since approx half will be visited by the constraint)\n    # is (n / 2) / 2 = n / 4 so divide by this means multiply by 4 / n,\n    # However instead of 4 we use penalty_factor (3 works well) so we can make them larger or smaller        \n    self.max_penalty = kwargs.get(\"max_penalty\", None)\n    if self.max_penalty is None:  # If not provided, use the default max penalty\n        self.max_penalty = MAX_LENGTHS.get(num_loc, None)\n    if (\n        self.max_penalty is None\n    ):  # If not in the table keys, find the closest number of nodes as the key\n        closest_num_loc = min(MAX_LENGTHS.keys(), key=lambda x: abs(x - num_loc))\n        self.max_penalty = MAX_LENGTHS[closest_num_loc]\n        log.warning(\n            f\"The max penalty for {num_loc} locations is not defined. Using the closest max penalty: {self.max_penalty}\\\n                with {closest_num_loc} locations.\"\n        )\n\n    # Adjust as in Kool et al. (2019)\n    self.max_penalty *= penalty_factor / self.num_loc\n    self.penalty_sampler = get_sampler(\n        \"penalty\", \"uniform\", 0.0, self.max_penalty, **kwargs\n    )\n</code></pre>"},{"location":"docs/content/api/envs/routing/#split-delivery-vehicle-routing-problem-sdvrp","title":"Split Delivery Vehicle Routing Problem (SDVRP)","text":""},{"location":"docs/content/api/envs/routing/#envs.routing.sdvrp.env.SDVRPEnv","title":"SDVRPEnv","text":"<pre><code>SDVRPEnv(\n    generator: CVRPGenerator = None,\n    generator_params: dict = {},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>CVRPEnv</code></p> <p>Split Delivery Vehicle Routing Problem (SDVRP) environment. SDVRP is a generalization of CVRP, where nodes can be visited multiple times and a fraction of the demand can be met. At each step, the agent chooses a customer to visit depending on the current location and the remaining capacity. When the agent visits a customer, the remaining capacity is updated. If the remaining capacity is not enough to visit any customer, the agent must go back to the depot. The reward is the -infinite unless the agent visits all the customers. In that case, the reward is (-)length of the path: maximizing the reward is equivalent to minimizing the path length.</p> Observations <ul> <li>location of the depot.</li> <li>locations and demand/remaining demand of each customer </li> <li>current location of the vehicle.</li> <li>the remaining capacity of the vehicle.</li> </ul> Constraints <ul> <li>the tour starts and ends at the depot.</li> <li>each customer can be visited multiple times.</li> <li>the vehicle cannot visit customers exceed the remaining capacity.</li> <li>the vehicle can return to the depot to refill the capacity.</li> </ul> Finish Condition <ul> <li>the vehicle has finished all customers demand and returned to the depot.</li> </ul> Reward <ul> <li>(minus) the negative length of the path.</li> </ul> <p>Parameters:</p> <ul> <li> <code>generator</code>               (<code>CVRPGenerator</code>, default:                   <code>None</code> )           \u2013            <p>CVRPGenerator instance as the data generator</p> </li> <li> <code>generator_params</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>parameters for the generator</p> </li> </ul> Source code in <code>rl4co/envs/routing/sdvrp/env.py</code> <pre><code>def __init__(\n    self,\n    generator: CVRPGenerator = None,\n    generator_params: dict = {},\n    **kwargs,\n):\n    super().__init__(generator, generator_params, **kwargs)\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.sdvrp.env.SDVRPEnv.check_solution_validity","title":"check_solution_validity  <code>staticmethod</code>","text":"<pre><code>check_solution_validity(\n    td: TensorDict, actions: Tensor\n) -&gt; None\n</code></pre> <p>Check that the solution is valid (all demand is satisfied)</p> Source code in <code>rl4co/envs/routing/sdvrp/env.py</code> <pre><code>@staticmethod\ndef check_solution_validity(td: TensorDict, actions: torch.Tensor) -&gt; None:\n    \"\"\"Check that the solution is valid (all demand is satisfied)\"\"\"\n\n    batch_size, graph_size = td[\"demand\"].size()\n\n    # Each node can be visited multiple times, but we always deliver as much demand as possible\n    # We check that at the end all demand has been satisfied\n    demands = torch.cat((-td[\"vehicle_capacity\"], td[\"demand\"]), 1)\n\n    rng = torch.arange(batch_size, out=demands.data.new().long())\n    used_cap = torch.zeros_like(td[\"demand\"][..., 0])\n    a_prev = None\n    for a in actions.transpose(0, 1):\n        assert (\n            a_prev is None or (demands[((a_prev == 0) &amp; (a == 0)), :] == 0).all()\n        ), \"Cannot visit depot twice if any nonzero demand\"\n        d = torch.min(demands[rng, a], td[\"vehicle_capacity\"].squeeze(-1) - used_cap)\n        demands[rng, a] -= d\n        used_cap += d\n        used_cap[a == 0] = 0\n        a_prev = a\n    assert (demands == 0).all(), \"All demand must be satisfied\"\n</code></pre>"},{"location":"docs/content/api/envs/routing/#stochastic-prize-collecting-traveling-salesman-problem-spctsp","title":"Stochastic Prize Collecting Traveling Salesman Problem (SPCTSP)","text":""},{"location":"docs/content/api/envs/routing/#envs.routing.spctsp.env.SPCTSPEnv","title":"SPCTSPEnv","text":"<pre><code>SPCTSPEnv(**kwargs)\n</code></pre> <p>               Bases: <code>PCTSPEnv</code></p> <p>Stochastic Prize Collecting Traveling Salesman Problem (SPCTSP) environment.</p> Note <p>The only difference with deterministic PCTSP is that the prizes are stochastic (i.e. the expected prize is not the same as the real prize).</p> Source code in <code>rl4co/envs/routing/spctsp/env.py</code> <pre><code>def __init__(self, **kwargs):\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"docs/content/api/envs/routing/#traveling-salesman-problem-tsp","title":"Traveling Salesman Problem (TSP)","text":""},{"location":"docs/content/api/envs/routing/#envs.routing.tsp.env.TSPEnv","title":"TSPEnv","text":"<pre><code>TSPEnv(\n    generator: TSPGenerator = None,\n    generator_params: dict = {},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RL4COEnvBase</code></p> <p>Traveling Salesman Problem (TSP) environment At each step, the agent chooses a city to visit. The reward is 0 unless the agent visits all the cities. In that case, the reward is (-)length of the path: maximizing the reward is equivalent to minimizing the path length.</p> Observations <ul> <li>locations of each customer.</li> <li>the current location of the vehicle.</li> </ul> Constraints <ul> <li>the tour must return to the starting customer.</li> <li>each customer must be visited exactly once.</li> </ul> Finish condition <ul> <li>the agent has visited all customers and returned to the starting customer.</li> </ul> Reward <ul> <li>(minus) the negative length of the path.</li> </ul> <p>Parameters:</p> <ul> <li> <code>generator</code>               (<code>TSPGenerator</code>, default:                   <code>None</code> )           \u2013            <p>TSPGenerator instance as the data generator</p> </li> <li> <code>generator_params</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>parameters for the generator</p> </li> </ul> Source code in <code>rl4co/envs/routing/tsp/env.py</code> <pre><code>def __init__(\n    self,\n    generator: TSPGenerator = None,\n    generator_params: dict = {},\n    **kwargs,\n):\n    super().__init__(**kwargs)\n    if generator is None:\n        generator = TSPGenerator(**generator_params)\n    self.generator = generator\n    self._make_spec(self.generator)\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.tsp.env.TSPEnv.check_solution_validity","title":"check_solution_validity  <code>staticmethod</code>","text":"<pre><code>check_solution_validity(\n    td: TensorDict, actions: Tensor\n) -&gt; None\n</code></pre> <p>Check that solution is valid: nodes are visited exactly once</p> Source code in <code>rl4co/envs/routing/tsp/env.py</code> <pre><code>@staticmethod\ndef check_solution_validity(td: TensorDict, actions: torch.Tensor) -&gt; None:\n    \"\"\"Check that solution is valid: nodes are visited exactly once\"\"\"\n    assert (\n        torch.arange(actions.size(1), out=actions.data.new())\n        .view(1, -1)\n        .expand_as(actions)\n        == actions.data.sort(1)[0]\n    ).all(), \"Invalid tour\"\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.tsp.env.TSPEnv.replace_selected_actions","title":"replace_selected_actions","text":"<pre><code>replace_selected_actions(\n    cur_actions: Tensor,\n    new_actions: Tensor,\n    selection_mask: Tensor,\n) -&gt; Tensor\n</code></pre> <p>Replace selected current actions with updated actions based on <code>selection_mask</code>.</p> Source code in <code>rl4co/envs/routing/tsp/env.py</code> <pre><code>def replace_selected_actions(\n    self,\n    cur_actions: torch.Tensor,\n    new_actions: torch.Tensor,\n    selection_mask: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Replace selected current actions with updated actions based on `selection_mask`.\n\n    Args:\n        cur_actions [batch_size, num_loc]\n        new_actions [batch_size, num_loc]\n        selection_mask [batch_size,]\n    \"\"\"\n    cur_actions[selection_mask] = new_actions[selection_mask]\n    return cur_actions\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.tsp.generator.TSPGenerator","title":"TSPGenerator","text":"<pre><code>TSPGenerator(\n    num_loc: int = 20,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    init_sol_type: str = \"random\",\n    loc_distribution: Union[\n        int, float, str, type, Callable\n    ] = Uniform,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>Data generator for the Travelling Salesman Problem (TSP).</p> <p>Parameters:</p> <ul> <li> <code>num_loc</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>number of locations (customers) in the TSP</p> </li> <li> <code>min_loc</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>minimum value for the location coordinates</p> </li> <li> <code>max_loc</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>maximum value for the location coordinates</p> </li> <li> <code>init_sol_type</code>               (<code>str</code>, default:                   <code>'random'</code> )           \u2013            <p>the method type used for generating initial solutions (random or greedy)</p> </li> <li> <code>loc_distribution</code>               (<code>Union[int, float, str, type, Callable]</code>, default:                   <code>Uniform</code> )           \u2013            <p>distribution for the location coordinates</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>A TensorDict with the following keys: locs [batch_size, num_loc, 2]: locations of each customer</p> </li> </ul> Source code in <code>rl4co/envs/routing/tsp/generator.py</code> <pre><code>def __init__(\n    self,\n    num_loc: int = 20,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    init_sol_type: str = \"random\",\n    loc_distribution: Union[int, float, str, type, Callable] = Uniform,\n    **kwargs,\n):\n    self.num_loc = num_loc\n    self.min_loc = min_loc\n    self.max_loc = max_loc\n    self.init_sol_type = init_sol_type\n\n    # Location distribution\n    if kwargs.get(\"loc_sampler\", None) is not None:\n        self.loc_sampler = kwargs[\"loc_sampler\"]\n    else:\n        self.loc_sampler = get_sampler(\n            \"loc\", loc_distribution, min_loc, max_loc, **kwargs\n        )\n</code></pre>"},{"location":"docs/content/api/envs/routing/#multi-task-vehicle-routing-problem-mtvrp","title":"Multi-Task Vehicle Routing Problem (MTVRP)","text":""},{"location":"docs/content/api/envs/routing/#envs.routing.mtvrp.env.MTVRPEnv","title":"MTVRPEnv","text":"<pre><code>MTVRPEnv(\n    generator: MTVRPGenerator = None,\n    generator_params: dict = {},\n    check_solution: bool = False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RL4COEnvBase</code></p> <p>MTVRPEnv is a Multi-Task VRP environment which can take any combination of the following constraints:</p> <p>Features:</p> <ul> <li>Capacity (C)     - Each vehicle has a maximum capacity \\(Q\\), restricting the total load that can be in the vehicle at any point of the route.     - The route must be planned such that the sum of demands and pickups for all customers visited does not exceed this capacity.</li> <li>Time Windows (TW)     - Every node \\(i\\) has an associated time window \\([e_i, l_i]\\) during which service must commence.     - Additionally, each node has a service time \\(s_i\\). Vehicles must reach node \\(i\\) within its time window; early arrivals must wait at the node location until time \\(e_i\\).</li> <li>Open Routes (O)     - Vehicles are not required to return to the depot after serving all customers.     - Note that this does not need to be counted as a constraint since it can be modelled by setting zero costs on arcs returning to the depot \\(c_{i0} = 0\\) from any customer \\(i \\in C\\), and not counting the return arc as part of the route.</li> <li>Backhauls (B)     - Backhauls generalize demand to also account for return shipments. Customers are either linehaul or backhaul customers.     - Linehaul customers require delivery of a demand \\(q_i &gt; 0\\) that needs to be transported from the depot to the customer, whereas backhaul customers need a pickup of an amount \\(p_i &gt; 0\\) that is transported from the client back to the depot.     - It is possible for vehicles to serve a combination of linehaul and backhaul customers in a single route, but then any linehaul customers must precede the backhaul customers in the route.</li> <li>Duration Limits (L)     - Imposes a limit on the total travel duration (or length) of each route, ensuring a balanced workload across vehicles.</li> </ul> <p>The environment covers the following 16 variants depending on the data generation:</p> VRP Variant Capacity (C) Open Route (O) Backhaul (B) Duration Limit (L) Time Window (TW) CVRP \u2714 OVRP \u2714 \u2714 VRPB \u2714 \u2714 VRPL \u2714 \u2714 VRPTW \u2714 \u2714 OVRPTW \u2714 \u2714 \u2714 OVRPB \u2714 \u2714 \u2714 OVRPL \u2714 \u2714 \u2714 VRPBL \u2714 \u2714 \u2714 VRPBTW \u2714 \u2714 \u2714 VRPLTW \u2714 \u2714 \u2714 OVRPBL \u2714 \u2714 \u2714 \u2714 OVRPBTW \u2714 \u2714 \u2714 \u2714 OVRPLTW \u2714 \u2714 \u2714 \u2714 VRPBLTW \u2714 \u2714 \u2714 \u2714 OVRPBLTW \u2714 \u2714 \u2714 \u2714 \u2714 <p>You may also check out the following papers as reference:</p> <ul> <li>\"Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot Generalization\" (Liu et al, 2024)</li> <li>\"MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts\" (Zhou et al, 2024)</li> <li>\"RouteFinder: Towards Foundation Models for Vehicle Routing Problems\" (Berto et al, 2024)</li> </ul> Tip <p>Have a look at https://pyvrp.org/ for more information about VRP and its variants and their solutions. Kudos to their help and great job!</p> <p>Parameters:</p> <ul> <li> <code>generator</code>               (<code>MTVRPGenerator</code>, default:                   <code>None</code> )           \u2013            <p>Generator for the environment, see :class:<code>MTVRPGenerator</code>.</p> </li> <li> <code>generator_params</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Parameters for the generator.</p> </li> </ul> Source code in <code>rl4co/envs/routing/mtvrp/env.py</code> <pre><code>def __init__(\n    self,\n    generator: MTVRPGenerator = None,\n    generator_params: dict = {},\n    check_solution: bool = False,\n    **kwargs,\n):\n    if check_solution:\n        log.warning(\n            \"Solution checking is enabled. This may slow down the environment.\"\n            \" We recommend disabling this for training by passing `check_solution=False`.\"\n        )\n\n    super().__init__(check_solution=check_solution, **kwargs)\n\n    if generator is None:\n        generator = MTVRPGenerator(**generator_params)\n    self.generator = generator\n    self._make_spec(self.generator)\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.mtvrp.env.MTVRPEnv.load_data","title":"load_data","text":"<pre><code>load_data(fpath, batch_size=[], scale=False)\n</code></pre> <p>Dataset loading from file Normalize demand by capacity to be in [0, 1]</p> Source code in <code>rl4co/envs/routing/mtvrp/env.py</code> <pre><code>def load_data(self, fpath, batch_size=[], scale=False):\n    \"\"\"Dataset loading from file\n    Normalize demand by capacity to be in [0, 1]\n    \"\"\"\n    td_load = load_npz_to_tensordict(fpath)\n    if scale:\n        td_load.set(\n            \"demand_linehaul\",\n            td_load[\"demand_linehaul\"] / td_load[\"capacity_original\"],\n        )\n        td_load.set(\n            \"demand_backhaul\",\n            td_load[\"demand_backhaul\"] / td_load[\"capacity_original\"],\n        )\n    return td_load\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.mtvrp.env.MTVRPEnv.render","title":"render  <code>staticmethod</code>","text":"<pre><code>render(*args, **kwargs)\n</code></pre> <p>Simple wrapper for render function</p> Source code in <code>rl4co/envs/routing/mtvrp/env.py</code> <pre><code>@staticmethod\ndef render(*args, **kwargs):\n    \"\"\"Simple wrapper for render function\"\"\"\n    from .render import render\n\n    return render(*args, **kwargs)\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.mtvrp.env.MTVRPEnv.select_start_nodes","title":"select_start_nodes","text":"<pre><code>select_start_nodes(td, num_starts)\n</code></pre> <p>Select available start nodes for the environment (e.g. for POMO-based training)</p> Source code in <code>rl4co/envs/routing/mtvrp/env.py</code> <pre><code>def select_start_nodes(self, td, num_starts):\n    \"\"\"Select available start nodes for the environment (e.g. for POMO-based training)\"\"\"\n    num_loc = td[\"locs\"].shape[-2] - 1\n    selected = (\n        torch.arange(num_starts, device=td.device).repeat_interleave(td.shape[0])\n        % num_loc\n        + 1\n    )\n    return selected\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.mtvrp.env.MTVRPEnv.solve","title":"solve  <code>staticmethod</code>","text":"<pre><code>solve(\n    instances: TensorDict,\n    max_runtime: float,\n    num_procs: int = 1,\n    solver: str = \"pyvrp\",\n    **kwargs\n) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Classical solver for the environment. This is a wrapper for the baselines solver. Available solvers are: <code>pyvrp</code>, <code>ortools</code>, <code>lkh</code>. Returns the actions and costs.</p> Source code in <code>rl4co/envs/routing/mtvrp/env.py</code> <pre><code>@staticmethod\ndef solve(\n    instances: TensorDict,\n    max_runtime: float,\n    num_procs: int = 1,\n    solver: str = \"pyvrp\",\n    **kwargs,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Classical solver for the environment. This is a wrapper for the baselines solver.\n    Available solvers are: `pyvrp`, `ortools`, `lkh`. Returns the actions and costs.\n    \"\"\"\n    from .baselines.solve import solve\n\n    return solve(instances, max_runtime, num_procs, solver, **kwargs)\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.mtvrp.env.MTVRPEnv.check_variants","title":"check_variants  <code>staticmethod</code>","text":"<pre><code>check_variants(td)\n</code></pre> <p>Check if the problem has the variants</p> Source code in <code>rl4co/envs/routing/mtvrp/env.py</code> <pre><code>@staticmethod\ndef check_variants(td):\n    \"\"\"Check if the problem has the variants\"\"\"\n    has_open = td[\"open_route\"].squeeze(-1)\n    has_tw = (td[\"time_windows\"][:, :, 1] != float(\"inf\")).any(-1)\n    has_limit = (td[\"distance_limit\"] != float(\"inf\")).squeeze(-1)\n    has_backhaul = (td[\"demand_backhaul\"] != 0).any(-1)\n    return has_open, has_tw, has_limit, has_backhaul\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.mtvrp.generator.MTVRPGenerator","title":"MTVRPGenerator","text":"<pre><code>MTVRPGenerator(\n    num_loc: int = 20,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    loc_distribution: Union[\n        int, float, str, type, Callable\n    ] = Uniform,\n    capacity: float = None,\n    min_demand: int = 1,\n    max_demand: int = 10,\n    min_backhaul: int = 1,\n    max_backhaul: int = 10,\n    scale_demand: bool = True,\n    max_time: float = 4.6,\n    backhaul_ratio: float = 0.2,\n    distance_limit: float = 3.0,\n    speed: float = 1.0,\n    prob_open: float = 0.5,\n    prob_time_window: float = 0.5,\n    prob_limit: float = 0.5,\n    prob_backhaul: float = 0.5,\n    variant_preset=None,\n    use_combinations=True,\n    subsample=True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>MTVRP Generator. Class to generate instances of the MTVRP problem. If a variant is declared and Subsample is True, the generator will sample the problem based on the variant probabilities. By default, we use Mixed-Batch Training as in Berto et al. 2024 (RouteFinder), i.e. one batch can contain multiple variants.</p> <p>Example presets:</p> <ul> <li>\"all\": Sample uniformly from 16 variants</li> <li>\"single_feat\": Sample uniformly between CVRP, OVRP, VRPB, VRPL, VRPTW (as done in Liu et al. 2024 (MTPOMO))</li> <li>\"single_feat_otw\": Sample uniformly between CVRP, OVRP, VRPB, VRPL, VRPTW, OVRPTW (as done in Zhou et al. 2024 (MVMoE))</li> <li>\"cvrp\": Only CVRP (similarly for other variants)</li> </ul> <p>Parameters:</p> <ul> <li> <code>num_loc</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>Number of locations to generate</p> </li> <li> <code>min_loc</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Minimum location value</p> </li> <li> <code>max_loc</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Maximum location value</p> </li> <li> <code>loc_distribution</code>               (<code>Union[int, float, str, type, Callable]</code>, default:                   <code>Uniform</code> )           \u2013            <p>Distribution to sample locations from</p> </li> <li> <code>capacity</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Vehicle capacity. If None, get value based on <code>get_vehicle_capacity</code></p> </li> <li> <code>min_demand</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Minimum demand value</p> </li> <li> <code>max_demand</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum demand value</p> </li> <li> <code>min_backhaul</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Minimum backhaul value</p> </li> <li> <code>max_backhaul</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum backhaul value</p> </li> <li> <code>scale_demand</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Scale demand values (by default, generate between 1 and 10)</p> </li> <li> <code>max_time</code>               (<code>float</code>, default:                   <code>4.6</code> )           \u2013            <p>Maximum time window value (at depot)</p> </li> <li> <code>backhaul_ratio</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>Fraction of backhauls (e.g. 0.2 means 20% of nodes are backhaul)</p> </li> <li> <code>distance_limit</code>               (<code>float</code>, default:                   <code>3.0</code> )           \u2013            <p>Distance limit</p> </li> <li> <code>speed</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Speed of vehicle. Defaults to 1</p> </li> <li> <code>subsample</code>           \u2013            <p>If False, we always sample all attributes (i.e., OVRPBLTW) If true, we use the</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments</p> </li> </ul> Source code in <code>rl4co/envs/routing/mtvrp/generator.py</code> <pre><code>def __init__(\n    self,\n    num_loc: int = 20,\n    min_loc: float = 0.0,\n    max_loc: float = 1.0,\n    loc_distribution: Union[int, float, str, type, Callable] = Uniform,\n    capacity: float = None,\n    min_demand: int = 1,\n    max_demand: int = 10,\n    min_backhaul: int = 1,\n    max_backhaul: int = 10,\n    scale_demand: bool = True,\n    max_time: float = 4.6,\n    backhaul_ratio: float = 0.2,\n    distance_limit: float = 3.0,\n    speed: float = 1.0,\n    prob_open: float = 0.5,\n    prob_time_window: float = 0.5,\n    prob_limit: float = 0.5,\n    prob_backhaul: float = 0.5,\n    variant_preset=None,\n    use_combinations=True,\n    subsample=True,\n    **kwargs,\n) -&gt; None:\n    # Location distribution\n    self.num_loc = num_loc\n    self.min_loc = min_loc\n    self.max_loc = max_loc\n    if kwargs.get(\"loc_sampler\", None) is not None:\n        self.loc_sampler = kwargs[\"loc_sampler\"]\n    else:\n        self.loc_sampler = get_sampler(\n            \"loc\", loc_distribution, min_loc, max_loc, **kwargs\n        )\n\n    if capacity is None:\n        capacity = get_vehicle_capacity(num_loc)\n    self.capacity = capacity\n    self.min_demand = min_demand\n    self.max_demand = max_demand\n    self.min_backhaul = min_backhaul\n    self.max_backhaul = max_backhaul\n    self.scale_demand = scale_demand\n    self.backhaul_ratio = backhaul_ratio\n\n    self.max_time = max_time\n    self.distance_limit = distance_limit\n    self.speed = speed\n\n    assert not (subsample and (variant_preset is None)), (\n        \"Cannot use subsample if variant_preset is not specified. \"\n    )\n    if variant_preset is not None:\n        log.info(f\"Using variant generation preset {variant_preset}\")\n        variant_probs = VARIANT_GENERATION_PRESETS.get(variant_preset)\n        assert (\n            variant_probs is not None\n        ), f\"Variant generation preset {variant_preset} not found. \\\n            Available presets are {VARIANT_GENERATION_PRESETS.keys()} with probabilities {VARIANT_GENERATION_PRESETS.values()}\"\n    else:\n        variant_probs = {\n            \"O\": prob_open,\n            \"TW\": prob_time_window,\n            \"L\": prob_limit,\n            \"B\": prob_backhaul,\n        }\n    # check probabilities\n    for key, prob in variant_probs.items():\n        assert 0 &lt;= prob &lt;= 1, f\"Probability {key} must be between 0 and 1\"\n    self.variant_probs = variant_probs\n    self.variant_preset = variant_preset\n    if isinstance(variant_preset, str) and variant_preset != \"all\":\n        log.warning(f\"{variant_preset} selected. Will not use feature combination!\")\n        use_combinations = False\n    self.use_combinations = use_combinations\n    self.subsample = subsample\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.mtvrp.generator.MTVRPGenerator.subsample_problems","title":"subsample_problems","text":"<pre><code>subsample_problems(td)\n</code></pre> <p>Create subproblems starting from seed probabilities depending on their variant. If random seed sampled in [0, 1] in batch is greater than prob, remove the constraint thus, if prob high, it is less likely to remove the constraint (i.e. prob=0.9, 90% chance to keep constraint)</p> Source code in <code>rl4co/envs/routing/mtvrp/generator.py</code> <pre><code>def subsample_problems(self, td):\n    \"\"\"Create subproblems starting from seed probabilities depending on their variant.\n    If random seed sampled in [0, 1] in batch is greater than prob, remove the constraint\n    thus, if prob high, it is less likely to remove the constraint (i.e. prob=0.9, 90% chance to keep constraint)\n    \"\"\"\n    batch_size = td.batch_size[0]\n\n    variant_probs = torch.tensor(list(self.variant_probs.values()))\n\n    if self.use_combinations:\n        # in a batch, multiple variants combinations can be picked\n        keep_mask = torch.rand(batch_size, 4) &gt;= variant_probs  # O, TW, L, B\n    else:\n        # in a batch, only a variant can be picked.\n        # we assign a 0.5 prob to the last variant (which is normal cvrp)\n        if self.variant_preset in list(\n            VARIANT_GENERATION_PRESETS.keys()\n        ) and self.variant_preset not in (\n            \"all\",\n            \"cvrp\",\n            \"single_feat\",\n            \"single_feat_otw\",\n        ):\n            cvrp_prob = 0\n        else:\n            cvrp_prob = 0.5\n        if self.variant_preset in (\"all\", \"cvrp\", \"single_feat\", \"single_feat_otw\"):\n            indices = torch.distributions.Categorical(\n                torch.Tensor(list(self.variant_probs.values()) + [cvrp_prob])[\n                    None\n                ].repeat(batch_size, 1)\n            ).sample()\n            if self.variant_preset == \"single_feat_otw\":\n                keep_mask = torch.zeros((batch_size, 6), dtype=torch.bool)\n                keep_mask[torch.arange(batch_size), indices] = True\n\n                # If keep_mask[:, 4] is True, make both keep_mask[:, 0] and keep_mask[:, 1] True\n                keep_mask[:, :2] |= keep_mask[:, 4:5]\n            else:\n                keep_mask = torch.zeros((batch_size, 5), dtype=torch.bool)\n                keep_mask[torch.arange(batch_size), indices] = True\n        else:\n            # if the variant is specified, we keep the attributes with probability &gt; 0\n            keep_mask = torch.zeros((batch_size, 4), dtype=torch.bool)\n            indices = torch.nonzero(variant_probs).squeeze()\n            keep_mask[:, indices] = True\n\n    td = self._default_open(td, ~keep_mask[:, 0])\n    td = self._default_time_window(td, ~keep_mask[:, 1])\n    td = self._default_distance_limit(td, ~keep_mask[:, 2])\n    td = self._default_backhaul(td, ~keep_mask[:, 3])\n\n    return td\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.mtvrp.generator.MTVRPGenerator.generate_locations","title":"generate_locations","text":"<pre><code>generate_locations(batch_size, num_loc) -&gt; Tensor\n</code></pre> <p>Generate seed locations.</p> <p>Returns:</p> <ul> <li> <code>locs</code> (              <code>Tensor</code> )          \u2013            <p>[B, N+1, 2] where the first location is the depot.</p> </li> </ul> Source code in <code>rl4co/envs/routing/mtvrp/generator.py</code> <pre><code>def generate_locations(self, batch_size, num_loc) -&gt; torch.Tensor:\n    \"\"\"Generate seed locations.\n\n    Returns:\n        locs: [B, N+1, 2] where the first location is the depot.\n    \"\"\"\n    locs = torch.FloatTensor(*batch_size, num_loc + 1, 2).uniform_(\n        self.min_loc, self.max_loc\n    )\n    return locs\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.mtvrp.generator.MTVRPGenerator.generate_demands","title":"generate_demands","text":"<pre><code>generate_demands(batch_size: int, num_loc: int) -&gt; Tensor\n</code></pre> <p>Classical lineahul demand / delivery from depot (C) and backhaul demand / pickup to depot (B) generation. Initialize the demand for nodes except the depot, which are added during reset. Demand sampling Following Kool et al. (2019), demands as integers between 1 and 10. Generates a slightly different distribution than using torch.randint.</p> <p>Returns:</p> <ul> <li> <code>linehaul_demand</code> (              <code>Tensor</code> )          \u2013            <p>[B, N]</p> </li> <li> <code>backhaul_demand</code> (              <code>Tensor</code> )          \u2013            <p>[B, N]</p> </li> </ul> Source code in <code>rl4co/envs/routing/mtvrp/generator.py</code> <pre><code>def generate_demands(self, batch_size: int, num_loc: int) -&gt; torch.Tensor:\n    \"\"\"Classical lineahul demand / delivery from depot (C) and backhaul demand / pickup to depot (B) generation.\n    Initialize the demand for nodes except the depot, which are added during reset.\n    Demand sampling Following Kool et al. (2019), demands as integers between 1 and 10.\n    Generates a slightly different distribution than using torch.randint.\n\n    Returns:\n        linehaul_demand: [B, N]\n        backhaul_demand: [B, N]\n    \"\"\"\n    linehaul_demand = (\n        torch.FloatTensor(*batch_size, num_loc)\n        .uniform_(self.min_demand - 1, self.max_demand - 1)\n        .int()\n        + 1\n    ).float()\n    # Backhaul demand sampling\n    backhaul_demand = (\n        torch.FloatTensor(*batch_size, num_loc)\n        .uniform_(self.min_backhaul - 1, self.max_backhaul - 1)\n        .int()\n        + 1\n    ).float()\n    is_linehaul = torch.rand(*batch_size, num_loc) &gt; self.backhaul_ratio\n    backhaul_demand = (\n        backhaul_demand * ~is_linehaul\n    )  # keep only values where they are not linehauls\n    linehaul_demand = (\n        linehaul_demand * is_linehaul\n    )\n    return linehaul_demand, backhaul_demand\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.mtvrp.generator.MTVRPGenerator.generate_time_windows","title":"generate_time_windows","text":"<pre><code>generate_time_windows(\n    locs: Tensor, speed: Tensor\n) -&gt; Tensor\n</code></pre> <p>Generate time windows (TW) and service times for each location including depot. We refer to the generation process in \"Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot Generalization\" (Liu et al., 2024). Note that another way to generate is from \"Learning to Delegate for Large-scale Vehicle Routing\" (Li et al, 2021) which is used in \"MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts\" (Zhou et al, 2024). Note that however, in that case the distance limit would have no influence when time windows are present, since the tw for depot is the same as distance with speed=1. This function can be overridden for that implementation. See also https://github.com/RoyalSkye/Routing-MVMoE</p> <p>Parameters:</p> <ul> <li> <code>locs</code>               (<code>Tensor</code>)           \u2013            <p>[B, N+1, 2] (depot, locs)</p> </li> <li> <code>speed</code>               (<code>Tensor</code>)           \u2013            <p>[B]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>time_windows</code> (              <code>Tensor</code> )          \u2013            <p>[B, N+1, 2]</p> </li> <li> <code>service_time</code> (              <code>Tensor</code> )          \u2013            <p>[B, N+1]</p> </li> </ul> Source code in <code>rl4co/envs/routing/mtvrp/generator.py</code> <pre><code>def generate_time_windows(\n    self,\n    locs: torch.Tensor,\n    speed: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Generate time windows (TW) and service times for each location including depot.\n    We refer to the generation process in \"Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot Generalization\"\n    (Liu et al., 2024). Note that another way to generate is from \"Learning to Delegate for Large-scale Vehicle Routing\" (Li et al, 2021) which\n    is used in \"MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts\" (Zhou et al, 2024). Note that however, in that case\n    the distance limit would have no influence when time windows are present, since the tw for depot is the same as distance with speed=1.\n    This function can be overridden for that implementation.\n    See also https://github.com/RoyalSkye/Routing-MVMoE\n\n    Args:\n        locs: [B, N+1, 2] (depot, locs)\n        speed: [B]\n\n    Returns:\n        time_windows: [B, N+1, 2]\n        service_time: [B, N+1]\n    \"\"\"\n\n    batch_size, n_loc = locs.shape[0], locs.shape[1] - 1  # no depot\n\n    a, b, c = 0.15, 0.18, 0.2\n    service_time = a + (b - a) * torch.rand(batch_size, n_loc)\n    tw_length = b + (c - b) * torch.rand(batch_size, n_loc)\n    d_0i = get_distance(locs[:, 0:1], locs[:, 1:])\n    h_max = (self.max_time - service_time - tw_length) / d_0i * speed - 1\n    tw_start = (1 + (h_max - 1) * torch.rand(batch_size, n_loc)) * d_0i / speed\n    tw_end = tw_start + tw_length\n\n    # Depot tw is 0, max_time\n    time_windows = torch.stack(\n        (\n            torch.cat((torch.zeros(batch_size, 1), tw_start), -1),  # start\n            torch.cat((torch.full((batch_size, 1), self.max_time), tw_end), -1),\n        ),  # en\n        dim=-1,\n    )\n    # depot service time is 0\n    service_time = torch.cat((torch.zeros(batch_size, 1), service_time), dim=-1)\n    return time_windows, service_time  # [B, N+1, 2], [B, N+1]\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.mtvrp.generator.MTVRPGenerator.generate_distance_limit","title":"generate_distance_limit","text":"<pre><code>generate_distance_limit(\n    shape: Tuple[int, int], locs: Tensor\n) -&gt; Tensor\n</code></pre> <p>Generates distance limits (L) and checks their feasibilities.</p> <p>Returns:</p> <ul> <li> <code>distance_limit</code> (              <code>Tensor</code> )          \u2013            <p>[B, 1]</p> </li> </ul> Source code in <code>rl4co/envs/routing/mtvrp/generator.py</code> <pre><code>def generate_distance_limit(\n    self, shape: Tuple[int, int], locs: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Generates distance limits (L) and checks their feasibilities.\n\n    Returns:\n        distance_limit: [B, 1]\n    \"\"\"\n    # calculate distance of all locations to depot\n    dist_to_depot = torch.cdist(locs, locs[:, 0:1, :], p=2)\n    assert (\n        dist_to_depot * 2 &lt; self.distance_limit  # go back and forth\n    ).all(), \"Distance limit too low, not all nodes can be reached from the depot.\"\n    return torch.full(shape, self.distance_limit, dtype=torch.float32)\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.mtvrp.generator.MTVRPGenerator.generate_open_route","title":"generate_open_route","text":"<pre><code>generate_open_route(shape: Tuple[int, int])\n</code></pre> <p>Generate open route flags (O). Here we could have a sampler but we simply return True here so all routes are open. Afterwards, we subsample the problems.</p> Source code in <code>rl4co/envs/routing/mtvrp/generator.py</code> <pre><code>def generate_open_route(self, shape: Tuple[int, int]):\n    \"\"\"Generate open route flags (O). Here we could have a sampler but we simply return True here so all\n    routes are open. Afterwards, we subsample the problems.\n    \"\"\"\n    return torch.ones(shape, dtype=torch.bool)\n</code></pre>"},{"location":"docs/content/api/envs/routing/#envs.routing.mtvrp.generator.MTVRPGenerator.generate_speed","title":"generate_speed","text":"<pre><code>generate_speed(shape: Tuple[int, int])\n</code></pre> <p>We simply generate the speed as constant here</p> Source code in <code>rl4co/envs/routing/mtvrp/generator.py</code> <pre><code>def generate_speed(self, shape: Tuple[int, int]):\n    \"\"\"We simply generate the speed as constant here\"\"\"\n    # in this version, the speed is constant but this class may be overridden\n    return torch.full(shape, self.speed, dtype=torch.float32)\n</code></pre>"},{"location":"docs/content/api/envs/scheduling/","title":"Scheduling Problems","text":""},{"location":"docs/content/api/envs/scheduling/#flexible-flow-shop-problem-ffsp","title":"Flexible Flow Shop Problem (FFSP)","text":""},{"location":"docs/content/api/envs/scheduling/#envs.scheduling.ffsp.env.FFSPEnv","title":"FFSPEnv","text":"<pre><code>FFSPEnv(\n    generator: FFSPGenerator = None,\n    generator_params: dict = {},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RL4COEnvBase</code></p> <p>Flexible Flow Shop Problem (FFSP) environment. The goal is to schedule a set of jobs on a set of machines such that the makespan is minimized.</p> Observations <ul> <li>time index</li> <li>sub time index</li> <li>batch index</li> <li>machine index</li> <li>schedule</li> <li>machine wait step</li> <li>job location</li> <li>job wait step</li> <li>job duration</li> </ul> Constraints <ul> <li>each job has to be processed on each machine in a specific order</li> <li>the machine has to be available to process the job</li> <li>the job has to be available to be processed</li> </ul> Finish Condition <ul> <li>all jobs are scheduled</li> </ul> Reward <ul> <li>(minus) the makespan of the schedule</li> </ul> <p>Parameters:</p> <ul> <li> <code>generator</code>               (<code>FFSPGenerator</code>, default:                   <code>None</code> )           \u2013            <p>FFSPGenerator instance as the data generator</p> </li> <li> <code>generator_params</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>parameters for the generator</p> </li> </ul> Source code in <code>rl4co/envs/scheduling/ffsp/env.py</code> <pre><code>def __init__(\n    self,\n    generator: FFSPGenerator = None,\n    generator_params: dict = {},\n    **kwargs,\n):\n    super().__init__(check_solution=False, dataset_cls=FastTdDataset, **kwargs)\n    if generator is None:\n        generator = FFSPGenerator(**generator_params)\n    self.generator = generator\n\n    self.num_stage = generator.num_stage\n    self.num_machine = generator.num_machine\n    self.num_job = generator.num_job\n    self.num_machine_total = generator.num_machine_total\n    self.tables = None\n    self.step_cnt = None\n    self.flatten_stages = generator.flatten_stages\n\n    self._make_spec(generator)\n</code></pre>"},{"location":"docs/content/api/envs/scheduling/#envs.scheduling.ffsp.generator.FFSPGenerator","title":"FFSPGenerator","text":"<pre><code>FFSPGenerator(\n    num_stage: int = 2,\n    num_machine: int = 3,\n    num_job: int = 4,\n    min_time: int = 2,\n    max_time: int = 10,\n    flatten_stages: bool = True,\n    **unused_kwargs\n)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>Data generator for the Flow Shop Scheduling Problem (FFSP).</p> <p>Parameters:</p> <ul> <li> <code>num_stage</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>number of stages</p> </li> <li> <code>num_machine</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>number of machines</p> </li> <li> <code>num_job</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>number of jobs</p> </li> <li> <code>min_time</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>minimum running time of each job on each machine</p> </li> <li> <code>max_time</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>maximum running time of each job on each machine</p> </li> <li> <code>flatten_stages</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to flatten the stages</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>A TensorDict with the following key: run_time [batch_size, num_job, num_machine, num_stage]: running time of each job on each machine</p> </li> </ul> Note <ul> <li>[IMPORTANT] This version of ffsp requires the number of machines in each stage to be the same</li> </ul> Source code in <code>rl4co/envs/scheduling/ffsp/generator.py</code> <pre><code>def __init__(\n    self,\n    num_stage: int = 2,\n    num_machine: int = 3,\n    num_job: int = 4,\n    min_time: int = 2,\n    max_time: int = 10,\n    flatten_stages: bool = True,\n    **unused_kwargs,\n):\n    self.num_stage = num_stage\n    self.num_machine = num_machine\n    self.num_machine_total = num_machine * num_stage\n    self.num_job = num_job\n    self.min_time = min_time\n    self.max_time = max_time\n    self.flatten_stages = flatten_stages\n\n    # FFSP environment doen't have any other kwargs\n    if len(unused_kwargs) &gt; 0:\n        log.error(f\"Found {len(unused_kwargs)} unused kwargs: {unused_kwargs}\")\n</code></pre>"},{"location":"docs/content/api/envs/scheduling/#single-machine-total-weighted-tardiness-problem-smtwtp","title":"Single Machine Total Weighted Tardiness Problem (SMTWTP)","text":""},{"location":"docs/content/api/envs/scheduling/#envs.scheduling.smtwtp.env.SMTWTPEnv","title":"SMTWTPEnv","text":"<pre><code>SMTWTPEnv(\n    generator: SMTWTPGenerator = None,\n    generator_params: dict = {},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RL4COEnvBase</code></p> <p>Single Machine Total Weighted Tardiness Problem environment as described in DeepACO (https://arxiv.org/pdf/2309.14032.pdf) SMTWTP is a scheduling problem in which a set of jobs must be processed on a single machine. Each job i has a processing time, a weight, and a due date. The objective is to minimize the sum of the weighted tardiness of all jobs, where the weighted tardiness of a job is defined as the product of its weight and the duration by which its completion time exceeds its due date. At each step, the agent chooses a job to process. The reward is 0 unless the agent processes all the jobs. In that case, the reward is (-)objective value of the processing order: maximizing the reward is equivalent to minimizing the objective.</p> Observation <ul> <li>job_due_time: the due time of each job</li> <li>job_weight: the weight of each job</li> <li>job_process_time: the process time of each job</li> <li>current_node: the current node</li> <li>action_mask: a mask of available actions</li> <li>current_time: the current time</li> </ul> Constants <ul> <li>num_job: number of jobs</li> <li>min_time_span: lower bound of jobs' due time. By default, jobs' due time is uniformly sampled from (min_time_span, max_time_span)</li> <li>max_time_span: upper bound of jobs' due time. By default, it will be set to num_job / 2</li> <li>min_job_weight: lower bound of jobs' weights. By default, jobs' weights are uniformly sampled from (min_job_weight, max_job_weight)</li> <li>max_job_weight: upper bound of jobs' weights</li> <li>min_process_time: lower bound of jobs' process time. By default, jobs' process time is uniformly sampled from (min_process_time, max_process_time)</li> <li>max_process_time: upper bound of jobs' process time</li> </ul> Finishing condition <ul> <li>All jobs are processed</li> </ul> Reward <ul> <li>The reward is 0 unless the agent processes all the jobs.</li> <li>In that case, the reward is (-)objective value of the processing order: maximizing the reward is equivalent to minimizing the objective.</li> </ul> <p>Parameters:</p> <ul> <li> <code>generator</code>               (<code>SMTWTPGenerator</code>, default:                   <code>None</code> )           \u2013            <p>FFSPGenerator instance as the data generator</p> </li> <li> <code>generator_params</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>parameters for the generator</p> </li> </ul> Source code in <code>rl4co/envs/scheduling/smtwtp/env.py</code> <pre><code>def __init__(\n    self,\n    generator: SMTWTPGenerator = None,\n    generator_params: dict = {},\n    **kwargs,\n):\n    super().__init__(**kwargs)\n    if generator is None:\n        generator = SMTWTPGenerator(**generator_params)\n    self.generator = generator\n    self._make_spec(self.generator)\n</code></pre>"},{"location":"docs/content/api/envs/scheduling/#envs.scheduling.smtwtp.generator.SMTWTPGenerator","title":"SMTWTPGenerator","text":"<pre><code>SMTWTPGenerator(\n    num_job: int = 10,\n    min_time_span: float = 0,\n    max_time_span: float = None,\n    min_job_weight: float = 0,\n    max_job_weight: float = 1,\n    min_process_time: float = 0,\n    max_process_time: float = 1,\n    **unused_kwargs\n)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>Data generator for the Single Machine Total Weighted Tardiness Problem (SMTWTP) environment</p> <p>Parameters:</p> <ul> <li> <code>num_job</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>number of jobs</p> </li> <li> <code>min_time_span</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>lower bound of jobs' due time. By default, jobs' due time is uniformly sampled from (min_time_span, max_time_span)</p> </li> <li> <code>max_time_span</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>upper bound of jobs' due time. By default, it will be set to num_job / 2</p> </li> <li> <code>min_job_weight</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>lower bound of jobs' weights. By default, jobs' weights are uniformly sampled from (min_job_weight, max_job_weight)</p> </li> <li> <code>max_job_weight</code>               (<code>float</code>, default:                   <code>1</code> )           \u2013            <p>upper bound of jobs' weights</p> </li> <li> <code>min_process_time</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>lower bound of jobs' process time. By default, jobs' process time is uniformly sampled from (min_process_time, max_process_time)</p> </li> <li> <code>max_process_time</code>               (<code>float</code>, default:                   <code>1</code> )           \u2013            <p>upper bound of jobs' process time</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>A TensorDict with the following key: job_due_time [batch_size, num_job + 1]: the due time of each job job_weight [batch_size, num_job + 1]: the weight of each job job_process_time [batch_size, num_job + 1]: the process time of each job</p> </li> </ul> Source code in <code>rl4co/envs/scheduling/smtwtp/generator.py</code> <pre><code>def __init__(\n    self,\n    num_job: int = 10,\n    min_time_span: float = 0,\n    max_time_span: float = None, # will be set to num_job / 2 by default. In DeepACO, it is set to num_job, which would be too simple\n    min_job_weight: float = 0,\n    max_job_weight: float = 1,\n    min_process_time: float = 0,\n    max_process_time: float = 1,\n    **unused_kwargs\n):\n    self.num_job = num_job\n    self.min_time_span = min_time_span\n    self.max_time_span = num_job / 2 if max_time_span is None else max_time_span\n    self.min_job_weight = min_job_weight\n    self.max_job_weight = max_job_weight\n    self.min_process_time = min_process_time\n    self.max_process_time = max_process_time\n\n    # SMTWTP environment doen't have any other kwargs\n    if len(unused_kwargs) &gt; 0:\n        log.error(f\"Found {len(unused_kwargs)} unused kwargs: {unused_kwargs}\")\n</code></pre>"},{"location":"docs/content/api/networks/base_policies/","title":"Constructive Policies Base Classes","text":""},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.base.ConstructiveEncoder","title":"ConstructiveEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Base class for the encoder of constructive models</p>"},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.base.ConstructiveEncoder.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(td: TensorDict) -&gt; Tuple[Any, Tensor]\n</code></pre> <p>Forward pass for the encoder</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>TensorDict containing the input data</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tuple[Any, Tensor]</code>           \u2013            <p>Tuple containing:</p> <ul> <li>latent representation (any type)</li> <li>initial embeddings (from feature space to embedding space)</li> </ul> </li> </ul> Source code in <code>rl4co/models/common/constructive/base.py</code> <pre><code>@abc.abstractmethod\ndef forward(self, td: TensorDict) -&gt; Tuple[Any, Tensor]:\n    \"\"\"Forward pass for the encoder\n\n    Args:\n        td: TensorDict containing the input data\n\n    Returns:\n        Tuple containing:\n          - latent representation (any type)\n          - initial embeddings (from feature space to embedding space)\n    \"\"\"\n    raise NotImplementedError(\"Implement me in subclass!\")\n</code></pre>"},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.base.ConstructiveDecoder","title":"ConstructiveDecoder","text":"<p>               Bases: <code>Module</code></p> <p>Base decoder model for constructive models. The decoder is responsible for generating the logits for the action</p>"},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.base.ConstructiveDecoder.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(\n    td: TensorDict, hidden: Any = None, num_starts: int = 0\n) -&gt; Tuple[Tensor, Tensor]\n</code></pre> <p>Obtain logits for current action to the next ones</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>TensorDict containing the input data</p> </li> <li> <code>hidden</code>               (<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>Hidden state from the encoder. Can be any type</p> </li> <li> <code>num_starts</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of starts for multistart decoding</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tuple[Tensor, Tensor]</code>           \u2013            <p>Tuple containing the logits and the action mask</p> </li> </ul> Source code in <code>rl4co/models/common/constructive/base.py</code> <pre><code>@abc.abstractmethod\ndef forward(\n    self, td: TensorDict, hidden: Any = None, num_starts: int = 0\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Obtain logits for current action to the next ones\n\n    Args:\n        td: TensorDict containing the input data\n        hidden: Hidden state from the encoder. Can be any type\n        num_starts: Number of starts for multistart decoding\n\n    Returns:\n        Tuple containing the logits and the action mask\n    \"\"\"\n    raise NotImplementedError(\"Implement me in subclass!\")\n</code></pre>"},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.base.ConstructiveDecoder.pre_decoder_hook","title":"pre_decoder_hook","text":"<pre><code>pre_decoder_hook(\n    td: TensorDict,\n    env: RL4COEnvBase,\n    hidden: Any = None,\n    num_starts: int = 0,\n) -&gt; Tuple[TensorDict, Any, RL4COEnvBase]\n</code></pre> <p>By default, we don't need to do anything here.</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>TensorDict containing the input data</p> </li> <li> <code>hidden</code>               (<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>Hidden state from the encoder</p> </li> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>Environment for decoding</p> </li> <li> <code>num_starts</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of starts for multistart decoding</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tuple[TensorDict, Any, RL4COEnvBase]</code>           \u2013            <p>Tuple containing the updated hidden state, TensorDict, and environment</p> </li> </ul> Source code in <code>rl4co/models/common/constructive/base.py</code> <pre><code>def pre_decoder_hook(\n    self, td: TensorDict, env: RL4COEnvBase, hidden: Any = None, num_starts: int = 0\n) -&gt; Tuple[TensorDict, Any, RL4COEnvBase]:\n    \"\"\"By default, we don't need to do anything here.\n\n    Args:\n        td: TensorDict containing the input data\n        hidden: Hidden state from the encoder\n        env: Environment for decoding\n        num_starts: Number of starts for multistart decoding\n\n    Returns:\n        Tuple containing the updated hidden state, TensorDict, and environment\n    \"\"\"\n    return td, env, hidden\n</code></pre>"},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.base.NoEncoder","title":"NoEncoder","text":"<p>               Bases: <code>ConstructiveEncoder</code></p> <p>Default encoder decoder-only models, i.e. autoregressive models that re-encode all the state at each decoding step.</p>"},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.base.NoEncoder.forward","title":"forward","text":"<pre><code>forward(td: TensorDict) -&gt; Tuple[Tensor, Tensor]\n</code></pre> <p>Return Nones for the hidden state and initial embeddings</p> Source code in <code>rl4co/models/common/constructive/base.py</code> <pre><code>def forward(self, td: TensorDict) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Return Nones for the hidden state and initial embeddings\"\"\"\n    return None, None\n</code></pre>"},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.base.ConstructivePolicy","title":"ConstructivePolicy","text":"<pre><code>ConstructivePolicy(\n    encoder: Union[ConstructiveEncoder, Callable],\n    decoder: Union[ConstructiveDecoder, Callable],\n    env_name: str = \"tsp\",\n    temperature: float = 1.0,\n    tanh_clipping: float = 0,\n    mask_logits: bool = True,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"greedy\",\n    test_decode_type: str = \"greedy\",\n    **unused_kw\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base class for constructive policies. Constructive policies take as input and instance and output a solution (sequence of actions). \"Constructive\" means that a solution is created from scratch by the model.</p> The structure follows roughly the following steps <ol> <li>Create a hidden state from the encoder</li> <li>Initialize decoding strategy (such as greedy, sampling, etc.)</li> <li>Decode the action given the hidden state and the environment state at the current step</li> <li>Update the environment state with the action. Repeat 3-4 until all sequences are done</li> <li>Obtain log likelihood, rewards etc.</li> </ol> <p>Note that an encoder is not strictly needed (see :class:<code>NoEncoder</code>).). A decoder however is always needed either in the form of a network or a function.</p> Note <p>There are major differences between this decoding and most RL problems. The most important one is that reward may not defined for partial solutions, hence we have to wait for the environment to reach a terminal state before we can compute the reward with <code>env.get_reward()</code>.</p> Warning <p>We suppose environments in the <code>done</code> state are still available for sampling. This is because in NCO we need to wait for all the environments to reach a terminal state before we can stop the decoding process. This is in contrast with the TorchRL framework (at the moment) where the <code>env.rollout</code> function automatically resets. You may follow tighter integration with TorchRL here: https://github.com/ai4co/rl4co/issues/72.</p> <p>Parameters:</p> <ul> <li> <code>encoder</code>               (<code>Union[ConstructiveEncoder, Callable]</code>)           \u2013            <p>Encoder to use</p> </li> <li> <code>decoder</code>               (<code>Union[ConstructiveDecoder, Callable]</code>)           \u2013            <p>Decoder to use</p> </li> <li> <code>env_name</code>               (<code>str</code>, default:                   <code>'tsp'</code> )           \u2013            <p>Environment name to solve (used for automatically instantiating networks)</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Temperature for the softmax during decoding</p> </li> <li> <code>tanh_clipping</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Clipping value for the tanh activation (see Bello et al. 2016) during decoding</p> </li> <li> <code>mask_logits</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to mask the logits or not during decoding</p> </li> <li> <code>train_decode_type</code>               (<code>str</code>, default:                   <code>'sampling'</code> )           \u2013            <p>Decoding strategy for training</p> </li> <li> <code>val_decode_type</code>               (<code>str</code>, default:                   <code>'greedy'</code> )           \u2013            <p>Decoding strategy for validation</p> </li> <li> <code>test_decode_type</code>               (<code>str</code>, default:                   <code>'greedy'</code> )           \u2013            <p>Decoding strategy for testing</p> </li> </ul> Source code in <code>rl4co/models/common/constructive/base.py</code> <pre><code>def __init__(\n    self,\n    encoder: Union[ConstructiveEncoder, Callable],\n    decoder: Union[ConstructiveDecoder, Callable],\n    env_name: str = \"tsp\",\n    temperature: float = 1.0,\n    tanh_clipping: float = 0,\n    mask_logits: bool = True,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"greedy\",\n    test_decode_type: str = \"greedy\",\n    **unused_kw,\n):\n    super(ConstructivePolicy, self).__init__()\n\n    if len(unused_kw) &gt; 0:\n        log.error(f\"Found {len(unused_kw)} unused kwargs: {unused_kw}\")\n\n    self.env_name = env_name\n\n    # Encoder and decoder\n    if encoder is None:\n        log.warning(\"`None` was provided as encoder. Using `NoEncoder`.\")\n        encoder = NoEncoder()\n    self.encoder = encoder\n    self.decoder = decoder\n\n    # Decoding strategies\n    self.temperature = temperature\n    self.tanh_clipping = tanh_clipping\n    self.mask_logits = mask_logits\n    self.train_decode_type = train_decode_type\n    self.val_decode_type = val_decode_type\n    self.test_decode_type = test_decode_type\n</code></pre>"},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.base.ConstructivePolicy.forward","title":"forward","text":"<pre><code>forward(\n    td: TensorDict,\n    env: Optional[Union[str, RL4COEnvBase]] = None,\n    phase: str = \"train\",\n    calc_reward: bool = True,\n    return_actions: bool = False,\n    return_entropy: bool = False,\n    return_hidden: bool = False,\n    return_init_embeds: bool = False,\n    return_sum_log_likelihood: bool = True,\n    actions=None,\n    max_steps=1000000,\n    **decoding_kwargs\n) -&gt; dict\n</code></pre> <p>Forward pass of the policy.</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>TensorDict containing the environment state</p> </li> <li> <code>env</code>               (<code>Optional[Union[str, RL4COEnvBase]]</code>, default:                   <code>None</code> )           \u2013            <p>Environment to use for decoding. If None, the environment is instantiated from <code>env_name</code>. Note that it is more efficient to pass an already instantiated environment each time for fine-grained control</p> </li> <li> <code>phase</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>Phase of the algorithm (train, val, test)</p> </li> <li> <code>calc_reward</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to calculate the reward</p> </li> <li> <code>return_actions</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the actions</p> </li> <li> <code>return_entropy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the entropy</p> </li> <li> <code>return_hidden</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the hidden state</p> </li> <li> <code>return_init_embeds</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the initial embeddings</p> </li> <li> <code>return_sum_log_likelihood</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to return the sum of the log likelihood</p> </li> <li> <code>actions</code>           \u2013            <p>Actions to use for evaluating the policy. If passed, use these actions instead of sampling from the policy to calculate log likelihood</p> </li> <li> <code>max_steps</code>           \u2013            <p>Maximum number of decoding steps for sanity check to avoid infinite loops if envs are buggy (i.e. do not reach <code>done</code>)</p> </li> <li> <code>decoding_kwargs</code>           \u2013            <p>Keyword arguments for the decoding strategy. See :class:<code>rl4co.utils.decoding.DecodingStrategy</code> for more information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the reward, log likelihood, and optionally the actions and entropy</p> </li> </ul> Source code in <code>rl4co/models/common/constructive/base.py</code> <pre><code>def forward(\n    self,\n    td: TensorDict,\n    env: Optional[Union[str, RL4COEnvBase]] = None,\n    phase: str = \"train\",\n    calc_reward: bool = True,\n    return_actions: bool = False,\n    return_entropy: bool = False,\n    return_hidden: bool = False,\n    return_init_embeds: bool = False,\n    return_sum_log_likelihood: bool = True,\n    actions=None,\n    max_steps=1_000_000,\n    **decoding_kwargs,\n) -&gt; dict:\n    \"\"\"Forward pass of the policy.\n\n    Args:\n        td: TensorDict containing the environment state\n        env: Environment to use for decoding. If None, the environment is instantiated from `env_name`. Note that\n            it is more efficient to pass an already instantiated environment each time for fine-grained control\n        phase: Phase of the algorithm (train, val, test)\n        calc_reward: Whether to calculate the reward\n        return_actions: Whether to return the actions\n        return_entropy: Whether to return the entropy\n        return_hidden: Whether to return the hidden state\n        return_init_embeds: Whether to return the initial embeddings\n        return_sum_log_likelihood: Whether to return the sum of the log likelihood\n        actions: Actions to use for evaluating the policy.\n            If passed, use these actions instead of sampling from the policy to calculate log likelihood\n        max_steps: Maximum number of decoding steps for sanity check to avoid infinite loops if envs are buggy (i.e. do not reach `done`)\n        decoding_kwargs: Keyword arguments for the decoding strategy. See :class:`rl4co.utils.decoding.DecodingStrategy` for more information.\n\n    Returns:\n        out: Dictionary containing the reward, log likelihood, and optionally the actions and entropy\n    \"\"\"\n\n    # Encoder: get encoder output and initial embeddings from initial state\n    hidden, init_embeds = self.encoder(td)\n\n    # Instantiate environment if needed\n    if isinstance(env, str) or env is None:\n        env_name = self.env_name if env is None else env\n        env_name = 'lop'\n        log.info(f\"Instantiated environment not provided; instantiating {env_name}\")\n        env = get_env(env_name)\n\n    # Get decode type depending on phase and whether actions are passed for evaluation\n    decode_type = decoding_kwargs.pop(\"decode_type\", None)\n    if actions is not None:\n        decode_type = \"evaluate\"\n    elif decode_type is None:\n        decode_type = getattr(self, f\"{phase}_decode_type\")\n\n    # Setup decoding strategy\n    # we pop arguments that are not part of the decoding strategy\n    decode_strategy: DecodingStrategy = get_decoding_strategy(\n        decode_type,\n        temperature=decoding_kwargs.pop(\"temperature\", self.temperature),\n        tanh_clipping=decoding_kwargs.pop(\"tanh_clipping\", self.tanh_clipping),\n        mask_logits=decoding_kwargs.pop(\"mask_logits\", self.mask_logits),\n        store_all_logp=decoding_kwargs.pop(\"store_all_logp\", return_entropy),\n        **decoding_kwargs,\n    )\n\n    # Pre-decoding hook: used for the initial step(s) of the decoding strategy\n    td, env, num_starts = decode_strategy.pre_decoder_hook(td, env)\n\n    # Additionally call a decoder hook if needed before main decoding\n    td, env, hidden = self.decoder.pre_decoder_hook(td, env, hidden, num_starts)\n\n    # Main decoding: loop until all sequences are done\n    step = 0\n    while not td[\"done\"].all():\n        logits, mask = self.decoder(td, hidden, num_starts)\n        td = decode_strategy.step(\n            logits,\n            mask,\n            td,\n            action=actions[..., step] if actions is not None else None,\n        )\n        td = env.step(td)[\"next\"]\n        step += 1\n        if step &gt; max_steps:\n            log.error(\n                f\"Exceeded maximum number of steps ({max_steps}) duing decoding\"\n            )\n            break\n\n    # Post-decoding hook: used for the final step(s) of the decoding strategy\n    logprobs, actions, td, env = decode_strategy.post_decoder_hook(td, env)\n\n    # Output dictionary construction\n    if calc_reward:\n        td.set(\"reward\", env.get_reward(td, actions))\n\n    outdict = {\n        \"reward\": td[\"reward\"],\n        \"log_likelihood\": get_log_likelihood(\n            logprobs, actions, td.get(\"mask\", None), return_sum_log_likelihood\n        ),\n    }\n\n    if return_actions:\n        outdict[\"actions\"] = actions\n    if return_entropy:\n        outdict[\"entropy\"] = calculate_entropy(logprobs)\n    if return_hidden:\n        outdict[\"hidden\"] = hidden\n    if return_init_embeds:\n        outdict[\"init_embeds\"] = init_embeds\n\n    return outdict\n</code></pre>"},{"location":"docs/content/api/networks/base_policies/#autoregressive-policies","title":"Autoregressive Policies","text":""},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.autoregressive.encoder.AutoregressiveEncoder","title":"AutoregressiveEncoder","text":"<p>               Bases: <code>ConstructiveEncoder</code></p> <p>Template class for an autoregressive encoder, simple wrapper around :class:<code>rl4co.models.common.constructive.base.ConstructiveEncoder</code>.</p> Tip <p>This class will not work as it is and is just a template. An example for autoregressive encoder can be found as :class:<code>rl4co.models.zoo.am.encoder.AttentionModelEncoder</code>.</p>"},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.autoregressive.decoder.AutoregressiveDecoder","title":"AutoregressiveDecoder","text":"<p>               Bases: <code>ConstructiveDecoder</code></p> <p>Template class for an autoregressive decoder, simple wrapper around :class:<code>rl4co.models.common.constructive.base.ConstructiveDecoder</code></p> Tip <p>This class will not work as it is and is just a template. An example for autoregressive encoder can be found as :class:<code>rl4co.models.zoo.am.decoder.AttentionModelDecoder</code>.</p>"},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.autoregressive.policy.AutoregressivePolicy","title":"AutoregressivePolicy","text":"<pre><code>AutoregressivePolicy(\n    encoder: AutoregressiveEncoder,\n    decoder: AutoregressiveDecoder,\n    env_name: str = \"tsp\",\n    temperature: float = 1.0,\n    tanh_clipping: float = 0,\n    mask_logits: bool = True,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"greedy\",\n    test_decode_type: str = \"greedy\",\n    **unused_kw\n)\n</code></pre> <p>               Bases: <code>ConstructivePolicy</code></p> <p>Template class for an autoregressive policy, simple wrapper around :class:<code>rl4co.models.common.constructive.base.ConstructivePolicy</code>.</p> Note <p>While a decoder is required, an encoder is optional and will be initialized to :class:<code>rl4co.models.common.constructive.autoregressive.encoder.NoEncoder</code>. This can be used in decoder-only models in which at each step actions do not depend on previously encoded states.</p> Source code in <code>rl4co/models/common/constructive/autoregressive/policy.py</code> <pre><code>def __init__(\n    self,\n    encoder: AutoregressiveEncoder,\n    decoder: AutoregressiveDecoder,\n    env_name: str = \"tsp\",\n    temperature: float = 1.0,\n    tanh_clipping: float = 0,\n    mask_logits: bool = True,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"greedy\",\n    test_decode_type: str = \"greedy\",\n    **unused_kw,\n):\n    # We raise an error for the user if no decoder was provided\n    if decoder is None:\n        raise ValueError(\"AutoregressivePolicy requires a decoder to be provided.\")\n\n    super(AutoregressivePolicy, self).__init__(\n        encoder=encoder,\n        decoder=decoder,\n        env_name=env_name,\n        temperature=temperature,\n        tanh_clipping=tanh_clipping,\n        mask_logits=mask_logits,\n        train_decode_type=train_decode_type,\n        val_decode_type=val_decode_type,\n        test_decode_type=test_decode_type,\n        **unused_kw,\n    )\n</code></pre>"},{"location":"docs/content/api/networks/base_policies/#nonautoregressive-policies","title":"Nonautoregressive Policies","text":""},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.nonautoregressive.encoder.NonAutoregressiveEncoder","title":"NonAutoregressiveEncoder","text":"<p>               Bases: <code>ConstructiveEncoder</code></p> <p>Template class for an autoregressive encoder, simple wrapper around :class:<code>rl4co.models.common.constructive.base.ConstructiveEncoder</code>.</p> Tip <p>This class will not work as it is and is just a template. An example for autoregressive encoder can be found as :class:<code>rl4co.models.zoo.am.encoder.AttentionModelEncoder</code>.</p>"},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.nonautoregressive.decoder.NonAutoregressiveDecoder","title":"NonAutoregressiveDecoder","text":"<p>               Bases: <code>ConstructiveDecoder</code></p> <p>The nonautoregressive decoder is a simple callable class that takes the tensor dictionary and the heatmaps logits and returns the logits for the current action logits and the action mask.</p>"},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.nonautoregressive.decoder.NonAutoregressiveDecoder.heatmap_to_logits","title":"heatmap_to_logits  <code>staticmethod</code>","text":"<pre><code>heatmap_to_logits(\n    td: TensorDict, heatmaps_logits: Tensor, num_starts: int\n)\n</code></pre> <p>Obtain heatmap logits for current action to the next ones</p> Source code in <code>rl4co/models/common/constructive/nonautoregressive/decoder.py</code> <pre><code>@staticmethod\ndef heatmap_to_logits(td: TensorDict, heatmaps_logits: torch.Tensor, num_starts: int):\n    \"\"\"Obtain heatmap logits for current action to the next ones\"\"\"\n    current_action = td.get(\"action\", None)\n    if current_action is None:\n        logits = heatmaps_logits.mean(-1)\n    else:\n        batch_size = heatmaps_logits.shape[0]\n        _indexer = _multistart_batched_index(batch_size, num_starts)\n        logits = heatmaps_logits[_indexer, current_action, :]\n    return logits, td[\"action_mask\"]\n</code></pre>"},{"location":"docs/content/api/networks/base_policies/#models.common.constructive.nonautoregressive.policy.NonAutoregressivePolicy","title":"NonAutoregressivePolicy","text":"<pre><code>NonAutoregressivePolicy(\n    encoder: NonAutoregressiveEncoder,\n    decoder: NonAutoregressiveDecoder = None,\n    env_name: str = \"tsp\",\n    temperature: float = 1.0,\n    tanh_clipping: float = 0,\n    mask_logits: bool = True,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"greedy\",\n    test_decode_type: str = \"greedy\",\n    **unused_kw\n)\n</code></pre> <p>               Bases: <code>ConstructivePolicy</code></p> <p>Template class for an nonautoregressive policy, simple wrapper around :class:<code>rl4co.models.common.constructive.base.ConstructivePolicy</code>.</p> Source code in <code>rl4co/models/common/constructive/nonautoregressive/policy.py</code> <pre><code>def __init__(\n    self,\n    encoder: NonAutoregressiveEncoder,\n    decoder: NonAutoregressiveDecoder = None,\n    env_name: str = \"tsp\",\n    temperature: float = 1.0,\n    tanh_clipping: float = 0,\n    mask_logits: bool = True,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"greedy\",\n    test_decode_type: str = \"greedy\",\n    **unused_kw,\n):\n    # If decoder is not passed, we default to the non-autoregressive decoder that decodes the heatmap\n    if decoder is None:\n        decoder = NonAutoregressiveDecoder()\n\n    super(NonAutoregressivePolicy, self).__init__(\n        encoder=encoder,\n        decoder=decoder,\n        env_name=env_name,\n        temperature=temperature,\n        tanh_clipping=tanh_clipping,\n        mask_logits=mask_logits,\n        train_decode_type=train_decode_type,\n        val_decode_type=val_decode_type,\n        test_decode_type=test_decode_type,\n        **unused_kw,\n    )\n</code></pre>"},{"location":"docs/content/api/networks/base_policies/#improvement-policies-base-classes","title":"Improvement Policies (Base Classes)","text":""},{"location":"docs/content/api/networks/base_policies/#models.common.improvement.base.ImprovementEncoder","title":"ImprovementEncoder","text":"<pre><code>ImprovementEncoder(\n    embed_dim: int = 128,\n    init_embedding: Module = None,\n    pos_embedding: Module = None,\n    env_name: str = \"pdp_ruin_repair\",\n    pos_type: str = \"CPE\",\n    num_heads: int = 4,\n    num_layers: int = 3,\n    normalization: str = \"layer\",\n    feedforward_hidden: int = 128,\n    linear_bias: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base class for the encoder of improvement models</p> Source code in <code>rl4co/models/common/improvement/base.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int = 128,\n    init_embedding: nn.Module = None,\n    pos_embedding: nn.Module = None,\n    env_name: str = \"pdp_ruin_repair\",\n    pos_type: str = \"CPE\",\n    num_heads: int = 4,\n    num_layers: int = 3,\n    normalization: str = \"layer\",\n    feedforward_hidden: int = 128,\n    linear_bias: bool = False,\n):\n    super(ImprovementEncoder, self).__init__()\n\n    if isinstance(env_name, RL4COEnvBase):\n        env_name = env_name.name\n    self.env_name = env_name\n    self.init_embedding = (\n        env_init_embedding(\n            self.env_name, {\"embed_dim\": embed_dim, \"linear_bias\": linear_bias}\n        )\n        if init_embedding is None\n        else init_embedding\n    )\n\n    self.pos_type = pos_type\n    self.pos_embedding = (\n        pos_init_embedding(self.pos_type, {\"embed_dim\": embed_dim})\n        if pos_embedding is None\n        else pos_embedding\n    )\n</code></pre>"},{"location":"docs/content/api/networks/base_policies/#models.common.improvement.base.ImprovementEncoder.forward","title":"forward","text":"<pre><code>forward(td: TensorDict) -&gt; Tuple[Tensor, Tensor]\n</code></pre> <p>Forward pass of the encoder. Transform the input TensorDict into a latent representation.</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>Input TensorDict containing the environment state</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>h</code> (              <code>Tensor</code> )          \u2013            <p>Latent representation of the input</p> </li> <li> <code>init_h</code> (              <code>Tensor</code> )          \u2013            <p>Initial embedding of the input</p> </li> </ul> Source code in <code>rl4co/models/common/improvement/base.py</code> <pre><code>def forward(self, td: TensorDict) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Forward pass of the encoder.\n    Transform the input TensorDict into a latent representation.\n\n    Args:\n        td: Input TensorDict containing the environment state\n\n    Returns:\n        h: Latent representation of the input\n        init_h: Initial embedding of the input\n    \"\"\"\n    # Transfer to embedding space (node)\n    init_h = self.init_embedding(td)\n\n    # Transfer to embedding space (solution)\n    init_p = self.pos_embedding(td)\n\n    # Process embedding\n    final_h, final_p = self._encoder_forward(init_h, init_p)\n\n    # Return latent representation and initial embedding\n    return final_h, final_p\n</code></pre>"},{"location":"docs/content/api/networks/base_policies/#models.common.improvement.base.ImprovementDecoder","title":"ImprovementDecoder","text":"<p>               Bases: <code>Module</code></p> <p>Base decoder model for improvement models. The decoder is responsible for generating the logits of the action</p>"},{"location":"docs/content/api/networks/base_policies/#models.common.improvement.base.ImprovementDecoder.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(\n    td: TensorDict, final_h: Tensor, final_p: Tensor\n) -&gt; Tensor\n</code></pre> <p>Obtain logits to perform operators that improve the current solution to the next ones</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>TensorDict with the current environment state</p> </li> <li> <code>final_h</code>               (<code>Tensor</code>)           \u2013            <p>final node embeddings</p> </li> <li> <code>final_p</code>               (<code>Tensor</code>)           \u2013            <p>final positional embeddings</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Tuple containing the logits</p> </li> </ul> Source code in <code>rl4co/models/common/improvement/base.py</code> <pre><code>@abc.abstractmethod\ndef forward(self, td: TensorDict, final_h: Tensor, final_p: Tensor) -&gt; Tensor:\n    \"\"\"Obtain logits to perform operators that improve the current solution to the next ones\n\n    Args:\n        td: TensorDict with the current environment state\n        final_h: final node embeddings\n        final_p: final positional embeddings\n\n    Returns:\n        Tuple containing the logits\n    \"\"\"\n    raise NotImplementedError(\"Implement me in subclass!\")\n</code></pre>"},{"location":"docs/content/api/networks/base_policies/#models.common.improvement.base.ImprovementPolicy","title":"ImprovementPolicy","text":"<p>               Bases: <code>Module</code></p> <p>Base class for improvement policies. Improvement policies take an instance + a solution as input and output a specific operator that changes the current solution to a new one.</p> <p>\"Improvement\" means that a solution is (potentially) improved to a new one by the model.</p>"},{"location":"docs/content/api/networks/base_policies/#models.common.improvement.base.ImprovementPolicy.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(\n    td: TensorDict,\n    env: Union[str, RL4COEnvBase] = None,\n    phase: str = \"train\",\n    return_actions: bool = False,\n    return_entropy: bool = False,\n    return_init_embeds: bool = False,\n    actions=None,\n    **decoding_kwargs\n) -&gt; dict\n</code></pre> <p>Forward pass of the policy.</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>TensorDict containing the environment state</p> </li> <li> <code>env</code>               (<code>Union[str, RL4COEnvBase]</code>, default:                   <code>None</code> )           \u2013            <p>Environment to use for decoding. If None, the environment is instantiated from <code>env_name</code>. Note that it is more efficient to pass an already instantiated environment each time for fine-grained control</p> </li> <li> <code>phase</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>Phase of the algorithm (train, val, test)</p> </li> <li> <code>return_actions</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the actions</p> </li> <li> <code>return_entropy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the entropy</p> </li> <li> <code>return_init_embeds</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the initial embeddings</p> </li> <li> <code>actions</code>           \u2013            <p>Actions to use for evaluating the policy. If passed, use these actions instead of sampling from the policy to calculate log likelihood</p> </li> <li> <code>decoding_kwargs</code>           \u2013            <p>Keyword arguments for the decoding strategy. See :class:<code>rl4co.utils.decoding.DecodingStrategy</code> for more information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the reward, log likelihood, and optionally the actions and entropy</p> </li> </ul> Source code in <code>rl4co/models/common/improvement/base.py</code> <pre><code>@abc.abstractmethod\ndef forward(\n    self,\n    td: TensorDict,\n    env: Union[str, RL4COEnvBase] = None,\n    phase: str = \"train\",\n    return_actions: bool = False,\n    return_entropy: bool = False,\n    return_init_embeds: bool = False,\n    actions=None,\n    **decoding_kwargs,\n) -&gt; dict:\n    \"\"\"Forward pass of the policy.\n\n    Args:\n        td: TensorDict containing the environment state\n        env: Environment to use for decoding. If None, the environment is instantiated from `env_name`. Note that\n            it is more efficient to pass an already instantiated environment each time for fine-grained control\n        phase: Phase of the algorithm (train, val, test)\n        return_actions: Whether to return the actions\n        return_entropy: Whether to return the entropy\n        return_init_embeds: Whether to return the initial embeddings\n        actions: Actions to use for evaluating the policy.\n            If passed, use these actions instead of sampling from the policy to calculate log likelihood\n        decoding_kwargs: Keyword arguments for the decoding strategy. See :class:`rl4co.utils.decoding.DecodingStrategy` for more information.\n\n    Returns:\n        out: Dictionary containing the reward, log likelihood, and optionally the actions and entropy\n    \"\"\"\n    raise NotImplementedError(\"Implement me in subclass!\")\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/","title":"Environment Embeddings","text":"<p>In autoregressive policies, environment embeddings transfer data from feature space to hidden space:</p> <ul> <li>Initial Embeddings: encode global problem features</li> <li>Context Embeddings: modify current node embedding during decoding</li> <li>Dynamic Embeddings: modify all nodes embeddings during decoding</li> </ul> <p></p>"},{"location":"docs/content/api/networks/env_embeddings/#context-embeddings","title":"Context Embeddings","text":"<p>The context embedding is used to modify the query embedding of the problem node of the current partial solution. Usually consists of a projection of gathered node embeddings and features to the embedding space.</p>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.EnvContext","title":"EnvContext","text":"<pre><code>EnvContext(\n    embed_dim, step_context_dim=None, linear_bias=False\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base class for environment context embeddings. The context embedding is used to modify the query embedding of the problem node of the current partial solution. Consists of a linear layer that projects the node features to the embedding space.</p> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def __init__(self, embed_dim, step_context_dim=None, linear_bias=False):\n    super(EnvContext, self).__init__()\n    self.embed_dim = embed_dim\n    step_context_dim = step_context_dim if step_context_dim is not None else embed_dim\n    self.project_context = nn.Linear(step_context_dim, embed_dim, bias=linear_bias)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.FFSPContext","title":"FFSPContext","text":"<pre><code>FFSPContext(embed_dim, stage_cnt=None)\n</code></pre> <p>               Bases: <code>EnvContext</code></p> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def __init__(self, embed_dim, stage_cnt=None):\n    self.has_stage_emb = stage_cnt is not None\n    step_context_dim = (1 + int(self.has_stage_emb)) * embed_dim\n    super().__init__(embed_dim=embed_dim, step_context_dim=step_context_dim)\n    if self.has_stage_emb:\n        self.stage_emb = nn.Parameter(torch.rand(stage_cnt, embed_dim))\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.TSPContext","title":"TSPContext","text":"<pre><code>TSPContext(embed_dim)\n</code></pre> <p>               Bases: <code>EnvContext</code></p> <p>Context embedding for the Traveling Salesman Problem (TSP). Project the following to the embedding space:</p> <pre><code>- first node embedding\n- current node embedding\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def __init__(self, embed_dim):\n    super(TSPContext, self).__init__(embed_dim, 2 * embed_dim)\n    self.W_placeholder = nn.Parameter(\n        torch.Tensor(2 * self.embed_dim).uniform_(-1, 1)\n    )\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.LOPContext","title":"LOPContext","text":"<pre><code>LOPContext(embed_dim)\n</code></pre> <p>               Bases: <code>EnvContext</code></p> <p>Context embedding for the Landuse Optimization Problem (LOP). Project the following to the embedding space:</p> <pre><code>- current node embedding\n- current type\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def __init__(self, embed_dim):\n    super(LOPContext, self).__init__(\n        embed_dim=embed_dim, step_context_dim=embed_dim + 8\n    )\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.VRPContext","title":"VRPContext","text":"<pre><code>VRPContext(embed_dim)\n</code></pre> <p>               Bases: <code>EnvContext</code></p> <p>Context embedding for the Capacitated Vehicle Routing Problem (CVRP). Project the following to the embedding space:</p> <pre><code>- current node embedding\n- remaining capacity (vehicle_capacity - used_capacity)\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def __init__(self, embed_dim):\n    super(VRPContext, self).__init__(\n        embed_dim=embed_dim, step_context_dim=embed_dim + 1\n    )\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.VRPTWContext","title":"VRPTWContext","text":"<pre><code>VRPTWContext(embed_dim)\n</code></pre> <p>               Bases: <code>VRPContext</code></p> <p>Context embedding for the Capacitated Vehicle Routing Problem (CVRP). Project the following to the embedding space:</p> <pre><code>- current node embedding\n- remaining capacity (vehicle_capacity - used_capacity)\n- current time\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def __init__(self, embed_dim):\n    super(VRPContext, self).__init__(\n        embed_dim=embed_dim, step_context_dim=embed_dim + 2\n    )\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.SVRPContext","title":"SVRPContext","text":"<pre><code>SVRPContext(embed_dim)\n</code></pre> <p>               Bases: <code>EnvContext</code></p> <p>Context embedding for the Skill Vehicle Routing Problem (SVRP). Project the following to the embedding space:</p> <pre><code>- current node embedding\n- current technician\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def __init__(self, embed_dim):\n    super(SVRPContext, self).__init__(embed_dim=embed_dim, step_context_dim=embed_dim)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.PCTSPContext","title":"PCTSPContext","text":"<pre><code>PCTSPContext(embed_dim)\n</code></pre> <p>               Bases: <code>EnvContext</code></p> <p>Context embedding for the Prize Collecting TSP (PCTSP). Project the following to the embedding space:</p> <pre><code>- current node embedding\n- remaining prize (prize_required - cur_total_prize)\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def __init__(self, embed_dim):\n    super(PCTSPContext, self).__init__(embed_dim, embed_dim + 1)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.OPContext","title":"OPContext","text":"<pre><code>OPContext(embed_dim)\n</code></pre> <p>               Bases: <code>EnvContext</code></p> <p>Context embedding for the Orienteering Problem (OP). Project the following to the embedding space:</p> <pre><code>- current node embedding\n- remaining distance (max_length - tour_length)\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def __init__(self, embed_dim):\n    super(OPContext, self).__init__(embed_dim, embed_dim + 1)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.DPPContext","title":"DPPContext","text":"<pre><code>DPPContext(embed_dim)\n</code></pre> <p>               Bases: <code>EnvContext</code></p> <p>Context embedding for the Decap Placement Problem (DPP), EDA (electronic design automation). Project the following to the embedding space:</p> <pre><code>- current cell embedding\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def __init__(self, embed_dim):\n    super(DPPContext, self).__init__(embed_dim)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.DPPContext.forward","title":"forward","text":"<pre><code>forward(embeddings, td)\n</code></pre> <p>Context cannot be defined by a single node embedding for DPP, hence 0. We modify the dynamic embedding instead to capture placed items</p> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def forward(self, embeddings, td):\n    \"\"\"Context cannot be defined by a single node embedding for DPP, hence 0.\n    We modify the dynamic embedding instead to capture placed items\n    \"\"\"\n    return embeddings.new_zeros(embeddings.size(0), self.embed_dim)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.PDPContext","title":"PDPContext","text":"<pre><code>PDPContext(embed_dim)\n</code></pre> <p>               Bases: <code>EnvContext</code></p> <p>Context embedding for the Pickup and Delivery Problem (PDP). Project the following to the embedding space:</p> <pre><code>- current node embedding\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def __init__(self, embed_dim):\n    super(PDPContext, self).__init__(embed_dim, embed_dim)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.MTSPContext","title":"MTSPContext","text":"<pre><code>MTSPContext(embed_dim, linear_bias=False)\n</code></pre> <p>               Bases: <code>EnvContext</code></p> <p>Context embedding for the Multiple Traveling Salesman Problem (mTSP). Project the following to the embedding space:</p> <pre><code>- current node embedding\n- remaining_agents\n- current_length\n- max_subtour_length\n- distance_from_depot\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def __init__(self, embed_dim, linear_bias=False):\n    super(MTSPContext, self).__init__(embed_dim, 2 * embed_dim)\n    proj_in_dim = (\n        4  # remaining_agents, current_length, max_subtour_length, distance_from_depot\n    )\n    self.proj_dynamic_feats = nn.Linear(proj_in_dim, embed_dim, bias=linear_bias)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.SMTWTPContext","title":"SMTWTPContext","text":"<pre><code>SMTWTPContext(embed_dim)\n</code></pre> <p>               Bases: <code>EnvContext</code></p> <p>Context embedding for the Single Machine Total Weighted Tardiness Problem (SMTWTP). Project the following to the embedding space:</p> <pre><code>- current node embedding\n- current time\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def __init__(self, embed_dim):\n    super(SMTWTPContext, self).__init__(embed_dim, embed_dim + 1)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.MDCPDPContext","title":"MDCPDPContext","text":"<pre><code>MDCPDPContext(embed_dim)\n</code></pre> <p>               Bases: <code>EnvContext</code></p> <p>Context embedding for the MDCPDP. Project the following to the embedding space:</p> <pre><code>- current node embedding\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def __init__(self, embed_dim):\n    super(MDCPDPContext, self).__init__(embed_dim, embed_dim)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.MTVRPContext","title":"MTVRPContext","text":"<pre><code>MTVRPContext(embed_dim)\n</code></pre> <p>               Bases: <code>VRPContext</code></p> <p>Context embedding for Multi-Task VRPEnv. Project the following to the embedding space:</p> <pre><code>- current node embedding\n- remaining_linehaul_capacity (vehicle_capacity - used_capacity_linehaul)\n- remaining_backhaul_capacity (vehicle_capacity - used_capacity_backhaul)\n- current time\n- current_route_length\n- open route indicator\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def __init__(self, embed_dim):\n    super(VRPContext, self).__init__(\n        embed_dim=embed_dim, step_context_dim=embed_dim + 5\n    )\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.context.env_context_embedding","title":"env_context_embedding","text":"<pre><code>env_context_embedding(\n    env_name: str, config: dict\n) -&gt; Module\n</code></pre> <p>Get environment context embedding. The context embedding is used to modify the query embedding of the problem node of the current partial solution. Usually consists of a projection of gathered node embeddings and features to the embedding space.</p> <p>Parameters:</p> <ul> <li> <code>env</code>           \u2013            <p>Environment or its name.</p> </li> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>A dictionary of configuration options for the environment.</p> </li> </ul> Source code in <code>rl4co/models/nn/env_embeddings/context.py</code> <pre><code>def env_context_embedding(env_name: str, config: dict) -&gt; nn.Module:\n    \"\"\"Get environment context embedding. The context embedding is used to modify the\n    query embedding of the problem node of the current partial solution.\n    Usually consists of a projection of gathered node embeddings and features to the embedding space.\n\n    Args:\n        env: Environment or its name.\n        config: A dictionary of configuration options for the environment.\n    \"\"\"\n    embedding_registry = {\n        \"tsp\": TSPContext,\n        \"atsp\": TSPContext,\n        \"cvrp\": VRPContext,\n        \"cvrptw\": VRPTWContext,\n        \"ffsp\": FFSPContext,\n        \"svrp\": SVRPContext,\n        \"sdvrp\": VRPContext,\n        \"pctsp\": PCTSPContext,\n        \"spctsp\": PCTSPContext,\n        \"op\": OPContext,\n        \"dpp\": DPPContext,\n        \"mdpp\": DPPContext,\n        \"pdp\": PDPContext,\n        \"mtsp\": MTSPContext,\n        \"smtwtp\": SMTWTPContext,\n        \"mdcpdp\": MDCPDPContext,\n        \"mtvrp\": MTVRPContext,\n        \"lop\": LOPContext,\n    }\n\n    if env_name not in embedding_registry:\n        raise ValueError(\n            f\"Unknown environment name '{env_name}'. Available context embeddings: {embedding_registry.keys()}\"\n        )\n\n    return embedding_registry[env_name](**config)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#dynamic-embeddings","title":"Dynamic Embeddings","text":"<p>The dynamic embedding is used to modify query, key and value vectors of the attention mechanism  based on the current state of the environment (which is changing during the rollout). Generally consists of a linear layer that projects the node features to the embedding space.</p>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.dynamic.StaticEmbedding","title":"StaticEmbedding","text":"<pre><code>StaticEmbedding(*args, **kwargs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Static embedding for general problems. This is used for problems that do not have any dynamic information, except for the information regarding the current action (e.g. the current node in TSP). See context embedding for more details.</p> Source code in <code>rl4co/models/nn/env_embeddings/dynamic.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    super(StaticEmbedding, self).__init__()\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.dynamic.SDVRPDynamicEmbedding","title":"SDVRPDynamicEmbedding","text":"<pre><code>SDVRPDynamicEmbedding(embed_dim, linear_bias=False)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Dynamic embedding for the Split Delivery Vehicle Routing Problem (SDVRP). Embed the following node features to the embedding space:</p> <pre><code>- demand_with_depot: demand of the customers and the depot\n</code></pre> <p>The demand with depot is used to modify the query, key and value vectors of the attention mechanism based on the current state of the environment (which is changing during the rollout).</p> Source code in <code>rl4co/models/nn/env_embeddings/dynamic.py</code> <pre><code>def __init__(self, embed_dim, linear_bias=False):\n    super(SDVRPDynamicEmbedding, self).__init__()\n    self.projection = nn.Linear(1, 3 * embed_dim, bias=linear_bias)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.dynamic.env_dynamic_embedding","title":"env_dynamic_embedding","text":"<pre><code>env_dynamic_embedding(\n    env_name: str, config: dict\n) -&gt; Module\n</code></pre> <p>Get environment dynamic embedding. The dynamic embedding is used to modify query, key and value vectors of the attention mechanism based on the current state of the environment (which is changing during the rollout). Consists of a linear layer that projects the node features to the embedding space.</p> <p>Parameters:</p> <ul> <li> <code>env</code>           \u2013            <p>Environment or its name.</p> </li> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>A dictionary of configuration options for the environment.</p> </li> </ul> Source code in <code>rl4co/models/nn/env_embeddings/dynamic.py</code> <pre><code>def env_dynamic_embedding(env_name: str, config: dict) -&gt; nn.Module:\n    \"\"\"Get environment dynamic embedding. The dynamic embedding is used to modify query, key and value vectors of the attention mechanism\n    based on the current state of the environment (which is changing during the rollout).\n    Consists of a linear layer that projects the node features to the embedding space.\n\n    Args:\n        env: Environment or its name.\n        config: A dictionary of configuration options for the environment.\n    \"\"\"\n    embedding_registry = {\n        \"tsp\": StaticEmbedding,\n        \"atsp\": StaticEmbedding,\n        \"cvrp\": StaticEmbedding,\n        \"cvrptw\": StaticEmbedding,\n        \"ffsp\": StaticEmbedding,\n        \"svrp\": StaticEmbedding,\n        \"sdvrp\": SDVRPDynamicEmbedding,\n        \"pctsp\": StaticEmbedding,\n        \"spctsp\": StaticEmbedding,\n        \"op\": StaticEmbedding,\n        \"dpp\": StaticEmbedding,\n        \"mdpp\": StaticEmbedding,\n        \"pdp\": StaticEmbedding,\n        \"mtsp\": StaticEmbedding,\n        \"smtwtp\": StaticEmbedding,\n        \"jssp\": JSSPDynamicEmbedding,\n        \"fjsp\": JSSPDynamicEmbedding,\n        \"mtvrp\": StaticEmbedding,\n        \"lop\": StaticEmbedding,\n    }\n\n    if env_name not in embedding_registry:\n        log.warning(\n            f\"Unknown environment name '{env_name}'. Available dynamic embeddings: {embedding_registry.keys()}. Defaulting to StaticEmbedding.\"\n        )\n    return embedding_registry.get(env_name, StaticEmbedding)(**config)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#init-embeddings","title":"Init Embeddings","text":"<p>The init embedding is used to initialize the general embedding of the problem nodes without any solution information. Generally consists of a linear layer that projects the node features to the embedding space.</p>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.init.TSPInitEmbedding","title":"TSPInitEmbedding","text":"<pre><code>TSPInitEmbedding(embed_dim, linear_bias=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Initial embedding for the Traveling Salesman Problems (TSP). Embed the following node features to the embedding space:</p> <pre><code>- locs: x, y coordinates of the cities\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/init.py</code> <pre><code>def __init__(self, embed_dim, linear_bias=True):\n    super(TSPInitEmbedding, self).__init__()\n    node_dim = 2  # x, y\n    self.init_embed = nn.Linear(node_dim, embed_dim, linear_bias)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.init.MatNetInitEmbedding","title":"MatNetInitEmbedding","text":"<pre><code>MatNetInitEmbedding(\n    embed_dim: int, mode: str = \"RandomOneHot\"\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Preparing the initial row and column embeddings for MatNet.</p> <p>Reference: https://github.com/yd-kwon/MatNet/blob/782698b60979effe2e7b61283cca155b7cdb727f/ATSP/ATSP_MatNet/ATSPModel.py#L51</p> Source code in <code>rl4co/models/nn/env_embeddings/init.py</code> <pre><code>def __init__(self, embed_dim: int, mode: str = \"RandomOneHot\") -&gt; None:\n    super().__init__()\n\n    self.embed_dim = embed_dim\n    assert mode in {\n        \"RandomOneHot\",\n        \"Random\",\n    }, \"mode must be one of ['RandomOneHot', 'Random']\"\n    self.mode = mode\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.init.lopInitEmbedding","title":"lopInitEmbedding","text":"<pre><code>lopInitEmbedding(embed_dim, linear_bias=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Initial embedding for the Landuse Planning Problem (LUP). Embed the following node features to the embedding space:</p> <pre><code>- locs: x, y coordinates of the nodes\n- landtype: type of land\n- area: area of the land\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/init.py</code> <pre><code>def __init__(self, embed_dim, linear_bias=True):\n    super(lopInitEmbedding, self).__init__()\n    self.landtype_dim = 8  # Number of land types\n    node_dim = 2 + 1  # x, y, area\n    self.init_embed = nn.Linear(node_dim, embed_dim, linear_bias)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.init.VRPInitEmbedding","title":"VRPInitEmbedding","text":"<pre><code>VRPInitEmbedding(\n    embed_dim, linear_bias=True, node_dim: int = 3\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Initial embedding for the Vehicle Routing Problems (VRP). Embed the following node features to the embedding space:</p> <pre><code>- locs: x, y coordinates of the nodes (depot and customers separately)\n- demand: demand of the customers\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/init.py</code> <pre><code>def __init__(self, embed_dim, linear_bias=True, node_dim: int = 3):\n    super(VRPInitEmbedding, self).__init__()\n    node_dim = node_dim  # 3: x, y, demand\n    self.init_embed = nn.Linear(node_dim, embed_dim, linear_bias)\n    self.init_embed_depot = nn.Linear(2, embed_dim, linear_bias)  # depot embedding\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.init.PCTSPInitEmbedding","title":"PCTSPInitEmbedding","text":"<pre><code>PCTSPInitEmbedding(embed_dim, linear_bias=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Initial embedding for the Prize Collecting Traveling Salesman Problems (PCTSP). Embed the following node features to the embedding space:</p> <pre><code>- locs: x, y coordinates of the nodes (depot and customers separately)\n- expected_prize: expected prize for visiting the customers.\n    In PCTSP, this is the actual prize. In SPCTSP, this is the expected prize.\n- penalty: penalty for not visiting the customers\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/init.py</code> <pre><code>def __init__(self, embed_dim, linear_bias=True):\n    super(PCTSPInitEmbedding, self).__init__()\n    node_dim = 4  # x, y, prize, penalty\n    self.init_embed = nn.Linear(node_dim, embed_dim, linear_bias)\n    self.init_embed_depot = nn.Linear(2, embed_dim, linear_bias)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.init.OPInitEmbedding","title":"OPInitEmbedding","text":"<pre><code>OPInitEmbedding(embed_dim, linear_bias=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Initial embedding for the Orienteering Problems (OP). Embed the following node features to the embedding space:</p> <pre><code>- locs: x, y coordinates of the nodes (depot and customers separately)\n- prize: prize for visiting the customers\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/init.py</code> <pre><code>def __init__(self, embed_dim, linear_bias=True):\n    super(OPInitEmbedding, self).__init__()\n    node_dim = 3  # x, y, prize\n    self.init_embed = nn.Linear(node_dim, embed_dim, linear_bias)\n    self.init_embed_depot = nn.Linear(2, embed_dim, linear_bias)  # depot embedding\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.init.DPPInitEmbedding","title":"DPPInitEmbedding","text":"<pre><code>DPPInitEmbedding(embed_dim, linear_bias=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Initial embedding for the Decap Placement Problem (DPP), EDA (electronic design automation). Embed the following node features to the embedding space:</p> <pre><code>- locs: x, y coordinates of the nodes (cells)\n- probe: index of the (single) probe cell. We embed the euclidean distance from the probe to all cells.\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/init.py</code> <pre><code>def __init__(self, embed_dim, linear_bias=True):\n    super(DPPInitEmbedding, self).__init__()\n    node_dim = 2  # x, y\n    self.init_embed = nn.Linear(node_dim, embed_dim // 2, linear_bias)  # locs\n    self.init_embed_probe = nn.Linear(1, embed_dim // 2, linear_bias)  # probe\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.init.MDPPInitEmbedding","title":"MDPPInitEmbedding","text":"<pre><code>MDPPInitEmbedding(embed_dim, linear_bias=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Initial embedding for the Multi-port Placement Problem (MDPP), EDA (electronic design automation). Embed the following node features to the embedding space:</p> <pre><code>- locs: x, y coordinates of the nodes (cells)\n- probe: indexes of the probe cells (multiple). We embed the euclidean distance of each cell to the closest probe.\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/init.py</code> <pre><code>def __init__(self, embed_dim, linear_bias=True):\n    super(MDPPInitEmbedding, self).__init__()\n    node_dim = 2  # x, y\n    self.init_embed = nn.Linear(node_dim, embed_dim, linear_bias)  # locs\n    self.init_embed_probe_distance = nn.Linear(\n        1, embed_dim, linear_bias\n    )  # probe_distance\n    self.project_out = nn.Linear(embed_dim * 2, embed_dim, linear_bias)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.init.PDPInitEmbedding","title":"PDPInitEmbedding","text":"<pre><code>PDPInitEmbedding(embed_dim, linear_bias=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Initial embedding for the Pickup and Delivery Problem (PDP). Embed the following node features to the embedding space:</p> <pre><code>- locs: x, y coordinates of the nodes (depot, pickups and deliveries separately)\n   Note that pickups and deliveries are interleaved in the input.\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/init.py</code> <pre><code>def __init__(self, embed_dim, linear_bias=True):\n    super(PDPInitEmbedding, self).__init__()\n    node_dim = 2  # x, y\n    self.init_embed_depot = nn.Linear(2, embed_dim, linear_bias)\n    self.init_embed_pick = nn.Linear(node_dim * 2, embed_dim, linear_bias)\n    self.init_embed_delivery = nn.Linear(node_dim, embed_dim, linear_bias)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.init.MTSPInitEmbedding","title":"MTSPInitEmbedding","text":"<pre><code>MTSPInitEmbedding(embed_dim, linear_bias=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Initial embedding for the Multiple Traveling Salesman Problem (mTSP). Embed the following node features to the embedding space:</p> <pre><code>- locs: x, y coordinates of the nodes (depot, cities)\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/init.py</code> <pre><code>def __init__(self, embed_dim, linear_bias=True):\n    \"\"\"NOTE: new made by Fede. May need to be checked\"\"\"\n    super(MTSPInitEmbedding, self).__init__()\n    node_dim = 2  # x, y\n    self.init_embed = nn.Linear(node_dim, embed_dim, linear_bias)\n    self.init_embed_depot = nn.Linear(2, embed_dim, linear_bias)  # depot embedding\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.init.SMTWTPInitEmbedding","title":"SMTWTPInitEmbedding","text":"<pre><code>SMTWTPInitEmbedding(embed_dim, linear_bias=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Initial embedding for the Single Machine Total Weighted Tardiness Problem (SMTWTP). Embed the following node features to the embedding space:</p> <pre><code>- job_due_time: due time of the jobs\n- job_weight: weights of the jobs\n- job_process_time: the processing time of jobs\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/init.py</code> <pre><code>def __init__(self, embed_dim, linear_bias=True):\n    super(SMTWTPInitEmbedding, self).__init__()\n    node_dim = 3  # job_due_time, job_weight, job_process_time\n    self.init_embed = nn.Linear(node_dim, embed_dim, linear_bias)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.init.MDCPDPInitEmbedding","title":"MDCPDPInitEmbedding","text":"<pre><code>MDCPDPInitEmbedding(embed_dim, linear_bias=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Initial embedding for the MDCPDP environment Embed the following node features to the embedding space:</p> <pre><code>- locs: x, y coordinates of the nodes (depot, pickups and deliveries separately)\n   Note that pickups and deliveries are interleaved in the input.\n</code></pre> Source code in <code>rl4co/models/nn/env_embeddings/init.py</code> <pre><code>def __init__(self, embed_dim, linear_bias=True):\n    super(MDCPDPInitEmbedding, self).__init__()\n    node_dim = 2  # x, y\n    self.init_embed_depot = nn.Linear(2, embed_dim, linear_bias)\n    self.init_embed_pick = nn.Linear(node_dim * 2, embed_dim, linear_bias)\n    self.init_embed_delivery = nn.Linear(node_dim, embed_dim, linear_bias)\n</code></pre>"},{"location":"docs/content/api/networks/env_embeddings/#models.nn.env_embeddings.init.env_init_embedding","title":"env_init_embedding","text":"<pre><code>env_init_embedding(env_name: str, config: dict) -&gt; Module\n</code></pre> <p>Get environment initial embedding. The init embedding is used to initialize the general embedding of the problem nodes without any solution information. Consists of a linear layer that projects the node features to the embedding space.</p> <p>Parameters:</p> <ul> <li> <code>env</code>           \u2013            <p>Environment or its name.</p> </li> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>A dictionary of configuration options for the environment.</p> </li> </ul> Source code in <code>rl4co/models/nn/env_embeddings/init.py</code> <pre><code>def env_init_embedding(env_name: str, config: dict) -&gt; nn.Module:\n    \"\"\"Get environment initial embedding. The init embedding is used to initialize the\n    general embedding of the problem nodes without any solution information.\n    Consists of a linear layer that projects the node features to the embedding space.\n\n    Args:\n        env: Environment or its name.\n        config: A dictionary of configuration options for the environment.\n    \"\"\"\n    embedding_registry = {\n        \"tsp\": TSPInitEmbedding,\n        \"atsp\": TSPInitEmbedding,\n        \"matnet\": MatNetInitEmbedding,\n        \"cvrp\": VRPInitEmbedding,\n        \"cvrptw\": VRPTWInitEmbedding,\n        \"svrp\": SVRPInitEmbedding,\n        \"sdvrp\": VRPInitEmbedding,\n        \"pctsp\": PCTSPInitEmbedding,\n        \"spctsp\": PCTSPInitEmbedding,\n        \"op\": OPInitEmbedding,\n        \"dpp\": DPPInitEmbedding,\n        \"mdpp\": MDPPInitEmbedding,\n        \"pdp\": PDPInitEmbedding,\n        \"pdp_ruin_repair\": TSPInitEmbedding,\n        \"tsp_kopt\": TSPInitEmbedding,\n        \"mtsp\": MTSPInitEmbedding,\n        \"smtwtp\": SMTWTPInitEmbedding,\n        \"mdcpdp\": MDCPDPInitEmbedding,\n        \"fjsp\": FJSPInitEmbedding,\n        \"jssp\": FJSPInitEmbedding,\n        \"mtvrp\": MTVRPInitEmbedding,\n        \"lop\": lopInitEmbedding,\n    }\n\n    if env_name not in embedding_registry:\n        raise ValueError(\n            f\"Unknown environment name '{env_name}'. Available init embeddings: {embedding_registry.keys()}\"\n        )\n\n    return embedding_registry[env_name](**config)\n</code></pre>"},{"location":"docs/content/api/networks/improvement_policies/","title":"Improvement policies","text":""},{"location":"docs/content/api/networks/improvement_policies/#improvement-policies-base-classes","title":"Improvement Policies (Base Classes)","text":""},{"location":"docs/content/api/networks/improvement_policies/#models.common.improvement.base.ImprovementEncoder","title":"ImprovementEncoder","text":"<pre><code>ImprovementEncoder(\n    embed_dim: int = 128,\n    init_embedding: Module = None,\n    pos_embedding: Module = None,\n    env_name: str = \"pdp_ruin_repair\",\n    pos_type: str = \"CPE\",\n    num_heads: int = 4,\n    num_layers: int = 3,\n    normalization: str = \"layer\",\n    feedforward_hidden: int = 128,\n    linear_bias: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base class for the encoder of improvement models</p> Source code in <code>rl4co/models/common/improvement/base.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int = 128,\n    init_embedding: nn.Module = None,\n    pos_embedding: nn.Module = None,\n    env_name: str = \"pdp_ruin_repair\",\n    pos_type: str = \"CPE\",\n    num_heads: int = 4,\n    num_layers: int = 3,\n    normalization: str = \"layer\",\n    feedforward_hidden: int = 128,\n    linear_bias: bool = False,\n):\n    super(ImprovementEncoder, self).__init__()\n\n    if isinstance(env_name, RL4COEnvBase):\n        env_name = env_name.name\n    self.env_name = env_name\n    self.init_embedding = (\n        env_init_embedding(\n            self.env_name, {\"embed_dim\": embed_dim, \"linear_bias\": linear_bias}\n        )\n        if init_embedding is None\n        else init_embedding\n    )\n\n    self.pos_type = pos_type\n    self.pos_embedding = (\n        pos_init_embedding(self.pos_type, {\"embed_dim\": embed_dim})\n        if pos_embedding is None\n        else pos_embedding\n    )\n</code></pre>"},{"location":"docs/content/api/networks/improvement_policies/#models.common.improvement.base.ImprovementEncoder.forward","title":"forward","text":"<pre><code>forward(td: TensorDict) -&gt; Tuple[Tensor, Tensor]\n</code></pre> <p>Forward pass of the encoder. Transform the input TensorDict into a latent representation.</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>Input TensorDict containing the environment state</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>h</code> (              <code>Tensor</code> )          \u2013            <p>Latent representation of the input</p> </li> <li> <code>init_h</code> (              <code>Tensor</code> )          \u2013            <p>Initial embedding of the input</p> </li> </ul> Source code in <code>rl4co/models/common/improvement/base.py</code> <pre><code>def forward(self, td: TensorDict) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Forward pass of the encoder.\n    Transform the input TensorDict into a latent representation.\n\n    Args:\n        td: Input TensorDict containing the environment state\n\n    Returns:\n        h: Latent representation of the input\n        init_h: Initial embedding of the input\n    \"\"\"\n    # Transfer to embedding space (node)\n    init_h = self.init_embedding(td)\n\n    # Transfer to embedding space (solution)\n    init_p = self.pos_embedding(td)\n\n    # Process embedding\n    final_h, final_p = self._encoder_forward(init_h, init_p)\n\n    # Return latent representation and initial embedding\n    return final_h, final_p\n</code></pre>"},{"location":"docs/content/api/networks/improvement_policies/#models.common.improvement.base.ImprovementDecoder","title":"ImprovementDecoder","text":"<p>               Bases: <code>Module</code></p> <p>Base decoder model for improvement models. The decoder is responsible for generating the logits of the action</p>"},{"location":"docs/content/api/networks/improvement_policies/#models.common.improvement.base.ImprovementDecoder.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(\n    td: TensorDict, final_h: Tensor, final_p: Tensor\n) -&gt; Tensor\n</code></pre> <p>Obtain logits to perform operators that improve the current solution to the next ones</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>TensorDict with the current environment state</p> </li> <li> <code>final_h</code>               (<code>Tensor</code>)           \u2013            <p>final node embeddings</p> </li> <li> <code>final_p</code>               (<code>Tensor</code>)           \u2013            <p>final positional embeddings</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Tuple containing the logits</p> </li> </ul> Source code in <code>rl4co/models/common/improvement/base.py</code> <pre><code>@abc.abstractmethod\ndef forward(self, td: TensorDict, final_h: Tensor, final_p: Tensor) -&gt; Tensor:\n    \"\"\"Obtain logits to perform operators that improve the current solution to the next ones\n\n    Args:\n        td: TensorDict with the current environment state\n        final_h: final node embeddings\n        final_p: final positional embeddings\n\n    Returns:\n        Tuple containing the logits\n    \"\"\"\n    raise NotImplementedError(\"Implement me in subclass!\")\n</code></pre>"},{"location":"docs/content/api/networks/improvement_policies/#models.common.improvement.base.ImprovementPolicy","title":"ImprovementPolicy","text":"<p>               Bases: <code>Module</code></p> <p>Base class for improvement policies. Improvement policies take an instance + a solution as input and output a specific operator that changes the current solution to a new one.</p> <p>\"Improvement\" means that a solution is (potentially) improved to a new one by the model.</p>"},{"location":"docs/content/api/networks/improvement_policies/#models.common.improvement.base.ImprovementPolicy.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(\n    td: TensorDict,\n    env: Union[str, RL4COEnvBase] = None,\n    phase: str = \"train\",\n    return_actions: bool = False,\n    return_entropy: bool = False,\n    return_init_embeds: bool = False,\n    actions=None,\n    **decoding_kwargs\n) -&gt; dict\n</code></pre> <p>Forward pass of the policy.</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>TensorDict containing the environment state</p> </li> <li> <code>env</code>               (<code>Union[str, RL4COEnvBase]</code>, default:                   <code>None</code> )           \u2013            <p>Environment to use for decoding. If None, the environment is instantiated from <code>env_name</code>. Note that it is more efficient to pass an already instantiated environment each time for fine-grained control</p> </li> <li> <code>phase</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>Phase of the algorithm (train, val, test)</p> </li> <li> <code>return_actions</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the actions</p> </li> <li> <code>return_entropy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the entropy</p> </li> <li> <code>return_init_embeds</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the initial embeddings</p> </li> <li> <code>actions</code>           \u2013            <p>Actions to use for evaluating the policy. If passed, use these actions instead of sampling from the policy to calculate log likelihood</p> </li> <li> <code>decoding_kwargs</code>           \u2013            <p>Keyword arguments for the decoding strategy. See :class:<code>rl4co.utils.decoding.DecodingStrategy</code> for more information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the reward, log likelihood, and optionally the actions and entropy</p> </li> </ul> Source code in <code>rl4co/models/common/improvement/base.py</code> <pre><code>@abc.abstractmethod\ndef forward(\n    self,\n    td: TensorDict,\n    env: Union[str, RL4COEnvBase] = None,\n    phase: str = \"train\",\n    return_actions: bool = False,\n    return_entropy: bool = False,\n    return_init_embeds: bool = False,\n    actions=None,\n    **decoding_kwargs,\n) -&gt; dict:\n    \"\"\"Forward pass of the policy.\n\n    Args:\n        td: TensorDict containing the environment state\n        env: Environment to use for decoding. If None, the environment is instantiated from `env_name`. Note that\n            it is more efficient to pass an already instantiated environment each time for fine-grained control\n        phase: Phase of the algorithm (train, val, test)\n        return_actions: Whether to return the actions\n        return_entropy: Whether to return the entropy\n        return_init_embeds: Whether to return the initial embeddings\n        actions: Actions to use for evaluating the policy.\n            If passed, use these actions instead of sampling from the policy to calculate log likelihood\n        decoding_kwargs: Keyword arguments for the decoding strategy. See :class:`rl4co.utils.decoding.DecodingStrategy` for more information.\n\n    Returns:\n        out: Dictionary containing the reward, log likelihood, and optionally the actions and entropy\n    \"\"\"\n    raise NotImplementedError(\"Implement me in subclass!\")\n</code></pre>"},{"location":"docs/content/api/networks/nn/","title":"Neural Network Modules","text":""},{"location":"docs/content/api/networks/nn/#critic-network","title":"Critic Network","text":""},{"location":"docs/content/api/networks/nn/#models.rl.common.critic.CriticNetwork","title":"CriticNetwork","text":"<pre><code>CriticNetwork(\n    encoder: Module,\n    value_head: Optional[Module] = None,\n    embed_dim: int = 128,\n    hidden_dim: int = 512,\n    customized: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Create a critic network given an encoder (e.g. as the one in the policy network) with a value head to transform the embeddings to a scalar value.</p> <p>Parameters:</p> <ul> <li> <code>encoder</code>               (<code>Module</code>)           \u2013            <p>Encoder module to encode the input</p> </li> <li> <code>value_head</code>               (<code>Optional[Module]</code>, default:                   <code>None</code> )           \u2013            <p>Value head to transform the embeddings to a scalar value</p> </li> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Dimension of the embeddings of the value head</p> </li> <li> <code>hidden_dim</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Dimension of the hidden layer of the value head</p> </li> </ul> Source code in <code>rl4co/models/rl/common/critic.py</code> <pre><code>def __init__(\n    self,\n    encoder: nn.Module,\n    value_head: Optional[nn.Module] = None,\n    embed_dim: int = 128,\n    hidden_dim: int = 512,\n    customized: bool = False,\n):\n    super(CriticNetwork, self).__init__()\n\n    self.encoder = encoder\n    if value_head is None:\n        # check if embed dim of encoder is different, if so, use it\n        if getattr(encoder, \"embed_dim\", embed_dim) != embed_dim:\n            log.warning(\n                f\"Found encoder with different embed_dim {encoder.embed_dim} than the value head {embed_dim}. Using encoder embed_dim for value head.\"\n            )\n            embed_dim = getattr(encoder, \"embed_dim\", embed_dim)\n        value_head = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 1)\n        )\n    self.value_head = value_head\n    self.customized = customized\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.rl.common.critic.CriticNetwork.forward","title":"forward","text":"<pre><code>forward(\n    x: Union[Tensor, TensorDict], hidden=None\n) -&gt; Tensor\n</code></pre> <p>Forward pass of the critic network: encode the imput in embedding space and return the value</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Union[Tensor, TensorDict]</code>)           \u2013            <p>Input containing the environment state. Can be a Tensor or a TensorDict</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Value of the input state</p> </li> </ul> Source code in <code>rl4co/models/rl/common/critic.py</code> <pre><code>def forward(self, x: Union[Tensor, TensorDict], hidden=None) -&gt; Tensor:\n    \"\"\"Forward pass of the critic network: encode the imput in embedding space and return the value\n\n    Args:\n        x: Input containing the environment state. Can be a Tensor or a TensorDict\n\n    Returns:\n        Value of the input state\n    \"\"\"\n    if not self.customized:  # fir for most of costructive tasks\n        h, _ = self.encoder(x)  # [batch_size, N, embed_dim] -&gt; [batch_size, N]\n        return self.value_head(h).mean(1)  # [batch_size, N] -&gt; [batch_size]\n    else:  # custimized encoder and value head with hidden input\n        h = self.encoder(x)  # [batch_size, N, embed_dim] -&gt; [batch_size, N]\n        return self.value_head(h, hidden)\n</code></pre>"},{"location":"docs/content/api/networks/nn/#graph-neural-networks","title":"Graph Neural Networks","text":""},{"location":"docs/content/api/networks/nn/#models.nn.graph.attnnet.MultiHeadAttentionLayer","title":"MultiHeadAttentionLayer","text":"<pre><code>MultiHeadAttentionLayer(\n    embed_dim: int,\n    num_heads: int = 8,\n    feedforward_hidden: int = 512,\n    normalization: Optional[str] = \"batch\",\n    bias: bool = True,\n    sdpa_fn: Optional[Callable] = None,\n    moe_kwargs: Optional[dict] = None,\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Multi-Head Attention Layer with normalization and feed-forward layer</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>)           \u2013            <p>dimension of the embeddings</p> </li> <li> <code>num_heads</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>number of heads in the MHA</p> </li> <li> <code>feedforward_hidden</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>dimension of the hidden layer in the feed-forward layer</p> </li> <li> <code>normalization</code>               (<code>Optional[str]</code>, default:                   <code>'batch'</code> )           \u2013            <p>type of normalization to use (batch, layer, none)</p> </li> <li> <code>sdpa_fn</code>               (<code>Optional[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>scaled dot product attention function (SDPA)</p> </li> <li> <code>moe_kwargs</code>               (<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments for MoE</p> </li> </ul> Source code in <code>rl4co/models/nn/graph/attnnet.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int,\n    num_heads: int = 8,\n    feedforward_hidden: int = 512,\n    normalization: Optional[str] = \"batch\",\n    bias: bool = True,\n    sdpa_fn: Optional[Callable] = None,\n    moe_kwargs: Optional[dict] = None,\n):\n    num_neurons = [feedforward_hidden] if feedforward_hidden &gt; 0 else []\n    if moe_kwargs is not None:\n        ffn = MoE(embed_dim, embed_dim, num_neurons=num_neurons, **moe_kwargs)\n    else:\n        ffn = MLP(input_dim=embed_dim, output_dim=embed_dim, num_neurons=num_neurons, hidden_act=\"ReLU\")\n\n    super(MultiHeadAttentionLayer, self).__init__(\n        SkipConnection(\n            MultiHeadAttention(embed_dim, num_heads, bias=bias, sdpa_fn=sdpa_fn)\n        ),\n        Normalization(embed_dim, normalization),\n        SkipConnection(ffn),\n        Normalization(embed_dim, normalization),\n    )\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.graph.attnnet.GraphAttentionNetwork","title":"GraphAttentionNetwork","text":"<pre><code>GraphAttentionNetwork(\n    num_heads: int,\n    embed_dim: int,\n    num_layers: int,\n    normalization: str = \"batch\",\n    feedforward_hidden: int = 512,\n    sdpa_fn: Optional[Callable] = None,\n    moe_kwargs: Optional[dict] = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Graph Attention Network to encode embeddings with a series of MHA layers consisting of a MHA layer, normalization, feed-forward layer, and normalization. Similar to Transformer encoder, as used in Kool et al. (2019).</p> <p>Parameters:</p> <ul> <li> <code>num_heads</code>               (<code>int</code>)           \u2013            <p>number of heads in the MHA</p> </li> <li> <code>embed_dim</code>               (<code>int</code>)           \u2013            <p>dimension of the embeddings</p> </li> <li> <code>num_layers</code>               (<code>int</code>)           \u2013            <p>number of MHA layers</p> </li> <li> <code>normalization</code>               (<code>str</code>, default:                   <code>'batch'</code> )           \u2013            <p>type of normalization to use (batch, layer, none)</p> </li> <li> <code>feedforward_hidden</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>dimension of the hidden layer in the feed-forward layer</p> </li> <li> <code>sdpa_fn</code>               (<code>Optional[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>scaled dot product attention function (SDPA)</p> </li> <li> <code>moe_kwargs</code>               (<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments for MoE</p> </li> </ul> Source code in <code>rl4co/models/nn/graph/attnnet.py</code> <pre><code>def __init__(\n    self,\n    num_heads: int,\n    embed_dim: int,\n    num_layers: int,\n    normalization: str = \"batch\",\n    feedforward_hidden: int = 512,\n    sdpa_fn: Optional[Callable] = None,\n    moe_kwargs: Optional[dict] = None,\n):\n    super(GraphAttentionNetwork, self).__init__()\n\n    self.layers = nn.Sequential(\n        *(\n            MultiHeadAttentionLayer(\n                embed_dim,\n                num_heads,\n                feedforward_hidden=feedforward_hidden,\n                normalization=normalization,\n                sdpa_fn=sdpa_fn,\n                moe_kwargs=moe_kwargs,\n            )\n            for _ in range(num_layers)\n        )\n    )\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.graph.attnnet.GraphAttentionNetwork.forward","title":"forward","text":"<pre><code>forward(x: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor\n</code></pre> <p>Forward pass of the encoder</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>[batch_size, graph_size, embed_dim] initial embeddings to process</p> </li> <li> <code>mask</code>               (<code>Optional[Tensor]</code>, default:                   <code>None</code> )           \u2013            <p>[batch_size, graph_size, graph_size] mask for the input embeddings. Unused for now.</p> </li> </ul> Source code in <code>rl4co/models/nn/graph/attnnet.py</code> <pre><code>def forward(self, x: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n    \"\"\"Forward pass of the encoder\n\n    Args:\n        x: [batch_size, graph_size, embed_dim] initial embeddings to process\n        mask: [batch_size, graph_size, graph_size] mask for the input embeddings. Unused for now.\n    \"\"\"\n    assert mask is None, \"Mask not yet supported!\"\n    h = self.layers(x)\n    return h\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.graph.gcn.GCNEncoder","title":"GCNEncoder","text":"<pre><code>GCNEncoder(\n    env_name: str,\n    embed_dim: int,\n    num_layers: int,\n    init_embedding: Module = None,\n    residual: bool = True,\n    edge_idx_fn: EdgeIndexFnSignature = None,\n    dropout: float = 0.5,\n    bias: bool = True,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Graph Convolutional Network to encode embeddings with a series of GCN layers from the pytorch geometric package</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>)           \u2013            <p>dimension of the embeddings</p> </li> <li> <code>num_nodes</code>           \u2013            <p>number of nodes in the graph</p> </li> <li> <code>num_gcn_layer</code>           \u2013            <p>number of GCN layers</p> </li> <li> <code>self_loop</code>           \u2013            <p>whether to add self loop in the graph</p> </li> <li> <code>residual</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to use residual connection</p> </li> </ul> Source code in <code>rl4co/models/nn/graph/gcn.py</code> <pre><code>def __init__(\n    self,\n    env_name: str,\n    embed_dim: int,\n    num_layers: int,\n    init_embedding: nn.Module = None,\n    residual: bool = True,\n    edge_idx_fn: EdgeIndexFnSignature = None,\n    dropout: float = 0.5,\n    bias: bool = True,\n):\n    super().__init__()\n\n    self.env_name = env_name\n    self.embed_dim = embed_dim\n    self.residual = residual\n    self.dropout = dropout\n\n    self.init_embedding = (\n        env_init_embedding(self.env_name, {\"embed_dim\": embed_dim})\n        if init_embedding is None\n        else init_embedding\n    )\n\n    if edge_idx_fn is None:\n        log.warning(\"No edge indices passed. Assume a fully connected graph\")\n        edge_idx_fn = edge_idx_fn_wrapper\n\n    self.edge_idx_fn = edge_idx_fn\n\n    # Define the GCN layers\n    self.gcn_layers = nn.ModuleList(\n        [GCNConv(embed_dim, embed_dim, bias=bias) for _ in range(num_layers)]\n    )\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.graph.gcn.GCNEncoder.forward","title":"forward","text":"<pre><code>forward(\n    td: TensorDict, mask: Union[Tensor, None] = None\n) -&gt; Tuple[Tensor, Tensor]\n</code></pre> <p>Forward pass of the encoder. Transform the input TensorDict into a latent representation.</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>Input TensorDict containing the environment state</p> </li> <li> <code>mask</code>               (<code>Union[Tensor, None]</code>, default:                   <code>None</code> )           \u2013            <p>Mask to apply to the attention</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>h</code> (              <code>Tensor</code> )          \u2013            <p>Latent representation of the input</p> </li> <li> <code>init_h</code> (              <code>Tensor</code> )          \u2013            <p>Initial embedding of the input</p> </li> </ul> Source code in <code>rl4co/models/nn/graph/gcn.py</code> <pre><code>def forward(\n    self, td: TensorDict, mask: Union[Tensor, None] = None\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Forward pass of the encoder.\n    Transform the input TensorDict into a latent representation.\n\n    Args:\n        td: Input TensorDict containing the environment state\n        mask: Mask to apply to the attention\n\n    Returns:\n        h: Latent representation of the input\n        init_h: Initial embedding of the input\n    \"\"\"\n    # Transfer to embedding space\n    init_h = self.init_embedding(td)\n    bs, num_nodes, emb_dim = init_h.shape\n    # (bs*num_nodes, emb_dim)\n    update_node_feature = init_h.reshape(-1, emb_dim)\n    # shape=(2, num_edges)\n    edge_index = self.edge_idx_fn(td, num_nodes)\n\n    for layer in self.gcn_layers[:-1]:\n        update_node_feature = layer(update_node_feature, edge_index)\n        update_node_feature = F.relu(update_node_feature)\n        update_node_feature = F.dropout(\n            update_node_feature, training=self.training, p=self.dropout\n        )\n\n    # last layer without relu activation and dropout\n    update_node_feature = self.gcn_layers[-1](update_node_feature, edge_index)\n\n    # De-batch the graph\n    update_node_feature = update_node_feature.view(bs, num_nodes, emb_dim)\n\n    # Residual\n    if self.residual:\n        update_node_feature = update_node_feature + init_h\n\n    return update_node_feature, init_h\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.graph.mpnn.MessagePassingEncoder","title":"MessagePassingEncoder","text":"<pre><code>MessagePassingEncoder(\n    env_name: str,\n    embed_dim: int,\n    num_nodes: int,\n    num_layers: int,\n    init_embedding: Module = None,\n    aggregation: str = \"add\",\n    self_loop: bool = False,\n    residual: bool = True,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>rl4co/models/nn/graph/mpnn.py</code> <pre><code>def __init__(\n    self,\n    env_name: str,\n    embed_dim: int,\n    num_nodes: int,\n    num_layers: int,\n    init_embedding: nn.Module = None,\n    aggregation: str = \"add\",\n    self_loop: bool = False,\n    residual: bool = True,\n):\n    \"\"\"\n    Note:\n        - Support fully connected graph for now.\n    \"\"\"\n    super(MessagePassingEncoder, self).__init__()\n\n    self.env_name = env_name\n\n    self.init_embedding = (\n        env_init_embedding(self.env_name, {\"embed_dim\": embed_dim})\n        if init_embedding is None\n        else init_embedding\n    )\n\n    # Generate edge index for a fully connected graph\n    adj_matrix = torch.ones(num_nodes, num_nodes)\n    if self_loop:\n        adj_matrix.fill_diagonal_(0)  # No self-loops\n    self.edge_index = torch.permute(torch.nonzero(adj_matrix), (1, 0))\n\n    # Init message passing models\n    self.mpnn_layers = nn.ModuleList(\n        [\n            MessagePassingLayer(\n                node_indim=embed_dim,\n                node_outdim=embed_dim,\n                edge_indim=1,\n                edge_outdim=1,\n                aggregation=aggregation,\n                residual=residual,\n            )\n            for _ in range(num_layers)\n        ]\n    )\n\n    # Record parameters\n    self.self_loop = self_loop\n</code></pre>"},{"location":"docs/content/api/networks/nn/#attention-mechanisms","title":"Attention Mechanisms","text":""},{"location":"docs/content/api/networks/nn/#models.nn.attention.MultiHeadAttention","title":"MultiHeadAttention","text":"<pre><code>MultiHeadAttention(\n    embed_dim: int,\n    num_heads: int,\n    bias: bool = True,\n    attention_dropout: float = 0.0,\n    causal: bool = False,\n    device: str = None,\n    dtype: dtype = None,\n    sdpa_fn: Optional[Callable] = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>PyTorch native implementation of Flash Multi-Head Attention with automatic mixed precision support. Uses PyTorch's native <code>scaled_dot_product_attention</code> implementation, available from 2.0</p> Note <p>If <code>scaled_dot_product_attention</code> is not available, use custom implementation of <code>scaled_dot_product_attention</code> without Flash Attention.</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>)           \u2013            <p>total dimension of the model</p> </li> <li> <code>num_heads</code>               (<code>int</code>)           \u2013            <p>number of heads</p> </li> <li> <code>bias</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to use bias</p> </li> <li> <code>attention_dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>dropout rate for attention weights</p> </li> <li> <code>causal</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to apply causal mask to attention scores</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>torch device</p> </li> <li> <code>dtype</code>               (<code>dtype</code>, default:                   <code>None</code> )           \u2013            <p>torch dtype</p> </li> <li> <code>sdpa_fn</code>               (<code>Optional[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>scaled dot product attention function (SDPA) implementation</p> </li> </ul> Source code in <code>rl4co/models/nn/attention.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int,\n    num_heads: int,\n    bias: bool = True,\n    attention_dropout: float = 0.0,\n    causal: bool = False,\n    device: str = None,\n    dtype: torch.dtype = None,\n    sdpa_fn: Optional[Callable] = None,\n) -&gt; None:\n    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n    super().__init__()\n    self.embed_dim = embed_dim\n    self.causal = causal\n    self.attention_dropout = attention_dropout\n    self.sdpa_fn = sdpa_fn if sdpa_fn is not None else scaled_dot_product_attention\n\n    self.num_heads = num_heads\n    assert self.embed_dim % num_heads == 0, \"self.kdim must be divisible by num_heads\"\n    self.head_dim = self.embed_dim // num_heads\n    assert (\n        self.head_dim % 8 == 0 and self.head_dim &lt;= 128\n    ), \"Only support head_dim &lt;= 128 and divisible by 8\"\n\n    self.Wqkv = nn.Linear(embed_dim, 3 * embed_dim, bias=bias, **factory_kwargs)\n    self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.attention.MultiHeadAttention.forward","title":"forward","text":"<pre><code>forward(x, attn_mask=None)\n</code></pre> <p>x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) attn_mask: bool tensor of shape (batch, seqlen)</p> Source code in <code>rl4co/models/nn/attention.py</code> <pre><code>def forward(self, x, attn_mask=None):\n    \"\"\"x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim)\n    attn_mask: bool tensor of shape (batch, seqlen)\n    \"\"\"\n    # Project query, key, value\n    q, k, v = rearrange(\n        self.Wqkv(x), \"b s (three h d) -&gt; three b h s d\", three=3, h=self.num_heads\n    ).unbind(dim=0)\n\n    if attn_mask is not None:\n        attn_mask = (\n            attn_mask.unsqueeze(1)\n            if attn_mask.ndim == 3\n            else attn_mask.unsqueeze(1).unsqueeze(2)\n        )\n\n    # Scaled dot product attention\n    out = self.sdpa_fn(\n        q,\n        k,\n        v,\n        attn_mask=attn_mask,\n        dropout_p=self.attention_dropout,\n    )\n    return self.out_proj(rearrange(out, \"b h s d -&gt; b s (h d)\"))\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.attention.MultiHeadCrossAttention","title":"MultiHeadCrossAttention","text":"<pre><code>MultiHeadCrossAttention(\n    embed_dim: int,\n    num_heads: int,\n    bias: bool = False,\n    attention_dropout: float = 0.0,\n    device: str = None,\n    dtype: dtype = None,\n    sdpa_fn: Optional[Union[Callable, Module]] = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>PyTorch native implementation of Flash Multi-Head Cross Attention with automatic mixed precision support. Uses PyTorch's native <code>scaled_dot_product_attention</code> implementation, available from 2.0</p> Note <p>If <code>scaled_dot_product_attention</code> is not available, use custom implementation of <code>scaled_dot_product_attention</code> without Flash Attention.</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>)           \u2013            <p>total dimension of the model</p> </li> <li> <code>num_heads</code>               (<code>int</code>)           \u2013            <p>number of heads</p> </li> <li> <code>bias</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use bias</p> </li> <li> <code>attention_dropout</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>dropout rate for attention weights</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>torch device</p> </li> <li> <code>dtype</code>               (<code>dtype</code>, default:                   <code>None</code> )           \u2013            <p>torch dtype</p> </li> <li> <code>sdpa_fn</code>               (<code>Optional[Union[Callable, Module]]</code>, default:                   <code>None</code> )           \u2013            <p>scaled dot product attention function (SDPA)</p> </li> </ul> Source code in <code>rl4co/models/nn/attention.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int,\n    num_heads: int,\n    bias: bool = False,\n    attention_dropout: float = 0.0,\n    device: str = None,\n    dtype: torch.dtype = None,\n    sdpa_fn: Optional[Union[Callable, nn.Module]] = None,\n) -&gt; None:\n    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n    super().__init__()\n    self.embed_dim = embed_dim\n    self.attention_dropout = attention_dropout\n\n    # Default to `scaled_dot_product_attention` if `sdpa_fn` is not provided\n    if sdpa_fn is None:\n        sdpa_fn = sdpa_fn_wrapper\n    self.sdpa_fn = sdpa_fn\n\n    self.num_heads = num_heads\n    assert self.embed_dim % num_heads == 0, \"self.kdim must be divisible by num_heads\"\n    self.head_dim = self.embed_dim // num_heads\n    assert (\n        self.head_dim % 8 == 0 and self.head_dim &lt;= 128\n    ), \"Only support head_dim &lt;= 128 and divisible by 8\"\n\n    self.Wq = nn.Linear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n    self.Wkv = nn.Linear(embed_dim, 2 * embed_dim, bias=bias, **factory_kwargs)\n    self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.attention.PointerAttention","title":"PointerAttention","text":"<pre><code>PointerAttention(\n    embed_dim: int,\n    num_heads: int,\n    mask_inner: bool = True,\n    out_bias: bool = False,\n    check_nan: bool = True,\n    sdpa_fn: Optional[Callable] = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Calculate logits given query, key and value and logit key. This follows the pointer mechanism of Vinyals et al. (2015) (https://arxiv.org/abs/1506.03134).</p> Note <p>With Flash Attention, masking is not supported</p> Performs the following <ol> <li>Apply cross attention to get the heads</li> <li>Project heads to get glimpse</li> <li>Compute attention score between glimpse and logit key</li> </ol> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>)           \u2013            <p>total dimension of the model</p> </li> <li> <code>num_heads</code>               (<code>int</code>)           \u2013            <p>number of heads</p> </li> <li> <code>mask_inner</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to mask inner attention</p> </li> <li> <code>linear_bias</code>           \u2013            <p>whether to use bias in linear projection</p> </li> <li> <code>check_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to check for NaNs in logits</p> </li> <li> <code>sdpa_fn</code>               (<code>Optional[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>scaled dot product attention function (SDPA) implementation</p> </li> </ul> Source code in <code>rl4co/models/nn/attention.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int,\n    num_heads: int,\n    mask_inner: bool = True,\n    out_bias: bool = False,\n    check_nan: bool = True,\n    sdpa_fn: Optional[Callable] = None,\n    **kwargs,\n):\n    super(PointerAttention, self).__init__()\n    self.num_heads = num_heads\n    self.mask_inner = mask_inner\n\n    # Projection - query, key, value already include projections\n    self.project_out = nn.Linear(embed_dim, embed_dim, bias=out_bias)\n    self.sdpa_fn = sdpa_fn if sdpa_fn is not None else scaled_dot_product_attention\n    self.check_nan = check_nan\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.attention.PointerAttention.forward","title":"forward","text":"<pre><code>forward(query, key, value, logit_key, attn_mask=None)\n</code></pre> <p>Compute attention logits given query, key, value, logit key and attention mask.</p> <p>Parameters:</p> <ul> <li> <code>query</code>           \u2013            <p>query tensor of shape [B, ..., L, E]</p> </li> <li> <code>key</code>           \u2013            <p>key tensor of shape [B, ..., S, E]</p> </li> <li> <code>value</code>           \u2013            <p>value tensor of shape [B, ..., S, E]</p> </li> <li> <code>logit_key</code>           \u2013            <p>logit key tensor of shape [B, ..., S, E]</p> </li> <li> <code>attn_mask</code>           \u2013            <p>attention mask tensor of shape [B, ..., S]. Note that <code>True</code> means that the value should take part in attention as described in the PyTorch Documentation</p> </li> </ul> Source code in <code>rl4co/models/nn/attention.py</code> <pre><code>def forward(self, query, key, value, logit_key, attn_mask=None):\n    \"\"\"Compute attention logits given query, key, value, logit key and attention mask.\n\n    Args:\n        query: query tensor of shape [B, ..., L, E]\n        key: key tensor of shape [B, ..., S, E]\n        value: value tensor of shape [B, ..., S, E]\n        logit_key: logit key tensor of shape [B, ..., S, E]\n        attn_mask: attention mask tensor of shape [B, ..., S]. Note that `True` means that the value _should_ take part in attention\n            as described in the [PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n    \"\"\"\n    # Compute inner multi-head attention with no projections.\n    heads = self._inner_mha(query, key, value, attn_mask)\n    glimpse = self._project_out(heads, attn_mask)\n\n    # Batch matrix multiplication to compute logits (batch_size, num_steps, graph_size)\n    # bmm is slightly faster than einsum and matmul\n    logits = (torch.bmm(glimpse, logit_key.squeeze(-2).transpose(-2, -1))).squeeze(\n        -2\n    ) / math.sqrt(glimpse.size(-1))\n\n    if self.check_nan:\n        assert not torch.isnan(logits).any(), \"Logits contain NaNs\"\n\n    return logits\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.attention.PointerAttnMoE","title":"PointerAttnMoE","text":"<pre><code>PointerAttnMoE(\n    embed_dim: int,\n    num_heads: int,\n    mask_inner: bool = True,\n    out_bias: bool = False,\n    check_nan: bool = True,\n    sdpa_fn: Optional[Callable] = None,\n    moe_kwargs: Optional[dict] = None,\n)\n</code></pre> <p>               Bases: <code>PointerAttention</code></p> <p>Calculate logits given query, key and value and logit key. This follows the pointer mechanism of Vinyals et al. (2015) https://arxiv.org/abs/1506.03134,     and the MoE gating mechanism of Zhou et al. (2024) https://arxiv.org/abs/2405.01029.</p> Note <p>With Flash Attention, masking is not supported</p> Performs the following <ol> <li>Apply cross attention to get the heads</li> <li>Project heads to get glimpse</li> <li>Compute attention score between glimpse and logit key</li> </ol> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>)           \u2013            <p>total dimension of the model</p> </li> <li> <code>num_heads</code>               (<code>int</code>)           \u2013            <p>number of heads</p> </li> <li> <code>mask_inner</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to mask inner attention</p> </li> <li> <code>linear_bias</code>           \u2013            <p>whether to use bias in linear projection</p> </li> <li> <code>check_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to check for NaNs in logits</p> </li> <li> <code>sdpa_fn</code>               (<code>Optional[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>scaled dot product attention function (SDPA) implementation</p> </li> <li> <code>moe_kwargs</code>               (<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments for MoE</p> </li> </ul> Source code in <code>rl4co/models/nn/attention.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int,\n    num_heads: int,\n    mask_inner: bool = True,\n    out_bias: bool = False,\n    check_nan: bool = True,\n    sdpa_fn: Optional[Callable] = None,\n    moe_kwargs: Optional[dict] = None,\n):\n    super(PointerAttnMoE, self).__init__(\n        embed_dim, num_heads, mask_inner, out_bias, check_nan, sdpa_fn\n    )\n    self.moe_kwargs = moe_kwargs\n\n    self.project_out = None\n    self.project_out_moe = MoE(\n        embed_dim, embed_dim, num_neurons=[], out_bias=out_bias, **moe_kwargs\n    )\n    if self.moe_kwargs[\"light_version\"]:\n        self.dense_or_moe = nn.Linear(embed_dim, 2, bias=False)\n        self.project_out = nn.Linear(embed_dim, embed_dim, bias=out_bias)\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.attention.MultiHeadCompat","title":"MultiHeadCompat","text":"<pre><code>MultiHeadCompat(\n    n_heads,\n    input_dim,\n    embed_dim=None,\n    val_dim=None,\n    key_dim=None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>rl4co/models/nn/attention.py</code> <pre><code>def __init__(self, n_heads, input_dim, embed_dim=None, val_dim=None, key_dim=None):\n    super(MultiHeadCompat, self).__init__()\n\n    if val_dim is None:\n        # assert embed_dim is not None, \"Provide either embed_dim or val_dim\"\n        val_dim = embed_dim // n_heads\n    if key_dim is None:\n        key_dim = val_dim\n\n    self.n_heads = n_heads\n    self.input_dim = input_dim\n    self.embed_dim = embed_dim\n    self.val_dim = val_dim\n    self.key_dim = key_dim\n\n    self.W_query = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))\n    self.W_key = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))\n\n    self.init_parameters()\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.attention.MultiHeadCompat.forward","title":"forward","text":"<pre><code>forward(q, h=None, mask=None)\n</code></pre> <p>:param q: queries (batch_size, n_query, input_dim) :param h: data (batch_size, graph_size, input_dim) :param mask: mask (batch_size, n_query, graph_size) or viewable as that (i.e. can be 2 dim if n_query == 1) Mask should contain 1 if attention is not possible (i.e. mask is negative adjacency) :return:</p> Source code in <code>rl4co/models/nn/attention.py</code> <pre><code>def forward(self, q, h=None, mask=None):\n    \"\"\"\n\n    :param q: queries (batch_size, n_query, input_dim)\n    :param h: data (batch_size, graph_size, input_dim)\n    :param mask: mask (batch_size, n_query, graph_size) or viewable as that (i.e. can be 2 dim if n_query == 1)\n    Mask should contain 1 if attention is not possible (i.e. mask is negative adjacency)\n    :return:\n    \"\"\"\n\n    if h is None:\n        h = q  # compute self-attention\n\n    # h should be (batch_size, graph_size, input_dim)\n    batch_size, graph_size, input_dim = h.size()\n    n_query = q.size(1)\n\n    hflat = h.contiguous().view(-1, input_dim)  #################   reshape\n    qflat = q.contiguous().view(-1, input_dim)\n\n    # last dimension can be different for keys and values\n    shp = (self.n_heads, batch_size, graph_size, -1)\n    shp_q = (self.n_heads, batch_size, n_query, -1)\n\n    # Calculate queries, (n_heads, n_query, graph_size, key/val_size)\n    Q = torch.matmul(qflat, self.W_query).view(shp_q)\n    K = torch.matmul(hflat, self.W_key).view(shp)\n\n    # Calculate compatibility (n_heads, batch_size, n_query, graph_size)\n    compatibility_s2n = torch.matmul(Q, K.transpose(2, 3))\n\n    return compatibility_s2n\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.attention.PolyNetAttention","title":"PolyNetAttention","text":"<pre><code>PolyNetAttention(\n    k: int,\n    embed_dim: int,\n    poly_layer_dim: int,\n    num_heads: int,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>PointerAttention</code></p> <p>Calculate logits given query, key and value and logit key. This implements a modified version the pointer mechanism of Vinyals et al. (2015) (https://arxiv.org/abs/1506.03134) as described in Hottung et al. (2024) (https://arxiv.org/abs/2402.14048) PolyNetAttention conditions the attention logits on a set of k different binary vectors allowing to learn k different solution strategies.</p> Note <p>With Flash Attention, masking is not supported</p> Performs the following <ol> <li>Apply cross attention to get the heads</li> <li>Project heads to get glimpse</li> <li>Apply PolyNet layers</li> <li>Compute attention score between glimpse and logit key</li> </ol> <p>Parameters:</p> <ul> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Number unique bit vectors used to compute attention score</p> </li> <li> <code>embed_dim</code>               (<code>int</code>)           \u2013            <p>total dimension of the model</p> </li> <li> <code>poly_layer_dim</code>               (<code>int</code>)           \u2013            <p>Dimension of the PolyNet layers</p> </li> <li> <code>num_heads</code>               (<code>int</code>)           \u2013            <p>number of heads</p> </li> <li> <code>mask_inner</code>           \u2013            <p>whether to mask inner attention</p> </li> <li> <code>linear_bias</code>           \u2013            <p>whether to use bias in linear projection</p> </li> <li> <code>check_nan</code>           \u2013            <p>whether to check for NaNs in logits</p> </li> <li> <code>sdpa_fn</code>           \u2013            <p>scaled dot product attention function (SDPA) implementation</p> </li> </ul> Source code in <code>rl4co/models/nn/attention.py</code> <pre><code>def __init__(\n    self, k: int, embed_dim: int, poly_layer_dim: int, num_heads: int, **kwargs\n):\n    super(PolyNetAttention, self).__init__(embed_dim, num_heads, **kwargs)\n\n    self.k = k\n    self.binary_vector_dim = math.ceil(math.log2(k))\n    self.binary_vectors = torch.nn.Parameter(\n        torch.Tensor(\n            list(itertools.product([0, 1], repeat=self.binary_vector_dim))[:k]\n        ),\n        requires_grad=False,\n    )\n\n    self.poly_layer_1 = nn.Linear(embed_dim + self.binary_vector_dim, poly_layer_dim)\n    self.poly_layer_2 = nn.Linear(poly_layer_dim, embed_dim)\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.attention.PolyNetAttention.forward","title":"forward","text":"<pre><code>forward(query, key, value, logit_key, attn_mask=None)\n</code></pre> <p>Compute attention logits given query, key, value, logit key and attention mask.</p> <p>Parameters:</p> <ul> <li> <code>query</code>           \u2013            <p>query tensor of shape [B, ..., L, E]</p> </li> <li> <code>key</code>           \u2013            <p>key tensor of shape [B, ..., S, E]</p> </li> <li> <code>value</code>           \u2013            <p>value tensor of shape [B, ..., S, E]</p> </li> <li> <code>logit_key</code>           \u2013            <p>logit key tensor of shape [B, ..., S, E]</p> </li> <li> <code>attn_mask</code>           \u2013            <p>attention mask tensor of shape [B, ..., S]. Note that <code>True</code> means that the value should take part in attention as described in the PyTorch Documentation</p> </li> </ul> Source code in <code>rl4co/models/nn/attention.py</code> <pre><code>def forward(self, query, key, value, logit_key, attn_mask=None):\n    \"\"\"Compute attention logits given query, key, value, logit key and attention mask.\n\n    Args:\n        query: query tensor of shape [B, ..., L, E]\n        key: key tensor of shape [B, ..., S, E]\n        value: value tensor of shape [B, ..., S, E]\n        logit_key: logit key tensor of shape [B, ..., S, E]\n        attn_mask: attention mask tensor of shape [B, ..., S]. Note that `True` means that the value _should_ take part in attention\n            as described in the [PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n    \"\"\"\n    # Compute inner multi-head attention with no projections.\n    heads = self._inner_mha(query, key, value, attn_mask)\n    glimpse = self.project_out(heads)\n\n    num_solutions = glimpse.shape[1]\n    z = self.binary_vectors.repeat(math.ceil(num_solutions / self.k), 1)[\n        :num_solutions\n    ]\n    z = z[None].expand(glimpse.shape[0], num_solutions, self.binary_vector_dim)\n\n    # PolyNet layers\n    poly_out = self.poly_layer_1(torch.cat((glimpse, z), dim=2))\n    poly_out = F.relu(poly_out)\n    poly_out = self.poly_layer_2(poly_out)\n\n    glimpse += poly_out\n\n    # Batch matrix multiplication to compute logits (batch_size, num_steps, graph_size)\n    # bmm is slightly faster than einsum and matmul\n    logits = (torch.bmm(glimpse, logit_key.squeeze(-2).transpose(-2, -1))).squeeze(\n        -2\n    ) / math.sqrt(glimpse.size(-1))\n\n    if self.check_nan:\n        assert not torch.isnan(logits).any(), \"Logits contain NaNs\"\n\n    return logits\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.attention.scaled_dot_product_attention_simple","title":"scaled_dot_product_attention_simple","text":"<pre><code>scaled_dot_product_attention_simple(\n    q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False\n)\n</code></pre> <p>Simple Scaled Dot-Product Attention in PyTorch without Flash Attention</p> Source code in <code>rl4co/models/nn/attention.py</code> <pre><code>def scaled_dot_product_attention_simple(\n    q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False\n):\n    \"\"\"Simple Scaled Dot-Product Attention in PyTorch without Flash Attention\"\"\"\n    # Check for causal and attn_mask conflict\n    if is_causal and attn_mask is not None:\n        raise ValueError(\"Cannot set both is_causal and attn_mask\")\n\n    # Calculate scaled dot product\n    scores = torch.matmul(q, k.transpose(-2, -1)) / (k.size(-1) ** 0.5)\n\n    # Apply the provided attention mask\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.bool:\n            scores.masked_fill_(~attn_mask, float(\"-inf\"))\n        else:\n            scores += attn_mask\n\n    # Apply causal mask\n    if is_causal:\n        s, l_ = scores.size(-2), scores.size(-1)\n        mask = torch.triu(torch.ones((s, l_), device=scores.device), diagonal=1)\n        scores.masked_fill_(mask.bool(), float(\"-inf\"))\n\n    # Softmax to get attention weights\n    attn_weights = F.softmax(scores, dim=-1)\n\n    # Apply dropout\n    if dropout_p &gt; 0.0:\n        attn_weights = F.dropout(attn_weights, p=dropout_p)\n\n    # Compute the weighted sum of values\n    return torch.matmul(attn_weights, v)\n</code></pre>"},{"location":"docs/content/api/networks/nn/#multi-layer-perceptron","title":"Multi-Layer Perceptron","text":""},{"location":"docs/content/api/networks/nn/#models.nn.mlp.MLP","title":"MLP","text":"<pre><code>MLP(\n    input_dim: int,\n    output_dim: int,\n    num_neurons: List[int] = [64, 32],\n    dropout_probs: Union[None, List[float]] = None,\n    hidden_act: str = \"ReLU\",\n    out_act: str = \"Identity\",\n    input_norm: str = \"None\",\n    output_norm: str = \"None\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>rl4co/models/nn/mlp.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    output_dim: int,\n    num_neurons: List[int] = [64, 32],\n    dropout_probs: Union[None, List[float]] = None,\n    hidden_act: str = \"ReLU\",\n    out_act: str = \"Identity\",\n    input_norm: str = \"None\",\n    output_norm: str = \"None\",\n):\n    super(MLP, self).__init__()\n\n    assert input_norm in [\"Batch\", \"Layer\", \"None\"]\n    assert output_norm in [\"Batch\", \"Layer\", \"None\"]\n\n    if dropout_probs is None:\n        dropout_probs = [0.0] * len(num_neurons)\n    elif len(dropout_probs) != len(num_neurons):\n        log.info(\n            \"dropout_probs List length should match the num_neurons List length for MLP, dropouts set to False instead\"\n        )\n        dropout_probs = [0.0] * len(num_neurons)\n\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.num_neurons = num_neurons\n    self.hidden_act = getattr(nn, hidden_act)()\n    self.out_act = getattr(nn, out_act)()\n    self.dropouts = []\n    for i in range(len(dropout_probs)):\n        self.dropouts.append(nn.Dropout(p=dropout_probs[i]))\n\n    input_dims = [input_dim] + num_neurons\n    output_dims = num_neurons + [output_dim]\n\n    self.lins = nn.ModuleList()\n    for i, (in_dim, out_dim) in enumerate(zip(input_dims, output_dims)):\n        self.lins.append(nn.Linear(in_dim, out_dim))\n\n    self.input_norm = self._get_norm_layer(input_norm, input_dim)\n    self.output_norm = self._get_norm_layer(output_norm, output_dim)\n</code></pre>"},{"location":"docs/content/api/networks/nn/#operations","title":"Operations","text":""},{"location":"docs/content/api/networks/nn/#models.nn.ops.PositionalEncoding","title":"PositionalEncoding","text":"<pre><code>PositionalEncoding(\n    embed_dim: int,\n    dropout: float = 0.1,\n    max_len: int = 1000,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>rl4co/models/nn/ops.py</code> <pre><code>def __init__(self, embed_dim: int, dropout: float = 0.1, max_len: int = 1000):\n    super().__init__()\n    self.dropout = nn.Dropout(p=dropout)\n    self.d_model = embed_dim\n    max_len = max_len\n    position = torch.arange(max_len).unsqueeze(1)\n    div_term = torch.exp(\n        torch.arange(0, self.d_model, 2) * (-math.log(10000.0) / self.d_model)\n    )\n    pe = torch.zeros(max_len, 1, self.d_model)\n    pe[:, 0, 0::2] = torch.sin(position * div_term)\n    pe[:, 0, 1::2] = torch.cos(position * div_term)\n    pe = pe.transpose(0, 1)  # [1, max_len, d_model]\n    self.register_buffer(\"pe\", pe)\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.ops.PositionalEncoding.forward","title":"forward","text":"<pre><code>forward(hidden: Tensor, seq_pos) -&gt; Tensor\n</code></pre> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <p>Tensor, shape <code>[batch_size, seq_len, embedding_dim]</code></p> </li> <li> <code>seq_pos</code>           \u2013            <p>Tensor, shape <code>[batch_size, seq_len]</code></p> </li> </ul> Source code in <code>rl4co/models/nn/ops.py</code> <pre><code>def forward(self, hidden: torch.Tensor, seq_pos) -&gt; torch.Tensor:\n    \"\"\"\n    Arguments:\n        x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n        seq_pos: Tensor, shape ``[batch_size, seq_len]``\n    \"\"\"\n    pes = self.pe.expand(hidden.size(0), -1, -1).gather(\n        1, seq_pos.unsqueeze(-1).expand(-1, -1, self.d_model)\n    )\n    hidden = hidden + pes\n    return self.dropout(hidden)\n</code></pre>"},{"location":"docs/content/api/networks/nn/#models.nn.ops.RandomEncoding","title":"RandomEncoding","text":"<pre><code>RandomEncoding(embed_dim: int, max_classes: int = 100)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>This is like torch.nn.Embedding but with rows of embeddings are randomly permuted in each forward pass before lookup operation. This might be useful in cases where classes have no fixed meaning but rather indicate a connection between different elements in a sequence. Reference is the MatNet model.</p> Source code in <code>rl4co/models/nn/ops.py</code> <pre><code>def __init__(self, embed_dim: int, max_classes: int = 100):\n    super().__init__()\n    self.embed_dim = embed_dim\n    self.max_classes = max_classes\n    rand_emb = torch.rand(max_classes, self.embed_dim)\n    self.register_buffer(\"emb\", rand_emb)\n</code></pre>"},{"location":"docs/content/api/rl/a2c/","title":"A2C","text":""},{"location":"docs/content/api/rl/a2c/#models.rl.a2c.a2c.A2C","title":"A2C","text":"<pre><code>A2C(\n    env: RL4COEnvBase,\n    policy: Module,\n    critic: CriticNetwork = None,\n    critic_kwargs: dict = {},\n    actor_optimizer_kwargs: dict = {\"lr\": 0.0001},\n    critic_optimizer_kwargs: dict = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>REINFORCE</code></p> <p>Advantage Actor Critic (A2C) algorithm. A2C is a variant of REINFORCE where a baseline is provided by a critic network. Here we additionally support different optimizers for the actor and the critic.</p> <p>Parameters:</p> <ul> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>Environment to use for the algorithm</p> </li> <li> <code>policy</code>               (<code>Module</code>)           \u2013            <p>Policy to use for the algorithm</p> </li> <li> <code>critic</code>               (<code>CriticNetwork</code>, default:                   <code>None</code> )           \u2013            <p>Critic network to use for the algorithm</p> </li> <li> <code>critic_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to pass to the critic network</p> </li> <li> <code>actor_optimizer_kwargs</code>               (<code>dict</code>, default:                   <code>{'lr': 0.0001}</code> )           \u2013            <p>Keyword arguments for the policy (=actor) optimizer</p> </li> <li> <code>critic_optimizer_kwargs</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments for the critic optimizer. If None, use the same as actor_optimizer_kwargs</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Keyword arguments passed to the superclass</p> </li> </ul> Source code in <code>rl4co/models/rl/a2c/a2c.py</code> <pre><code>def __init__(\n    self,\n    env: RL4COEnvBase,\n    policy: nn.Module,\n    critic: CriticNetwork = None,\n    critic_kwargs: dict = {},\n    actor_optimizer_kwargs: dict = {\"lr\": 1e-4},\n    critic_optimizer_kwargs: dict = None,\n    **kwargs,\n):\n    if critic is None:\n        log.info(\"Creating critic network for {}\".format(env.name))\n        critic = create_critic_from_actor(policy, **critic_kwargs)\n\n    # The baseline is directly created here, so we eliminate the baseline argument\n    kwargs.pop(\"baseline\", None)\n\n    super().__init__(env, policy, baseline=CriticBaseline(critic), **kwargs)\n    self.actor_optimizer_kwargs = actor_optimizer_kwargs\n    self.critic_optimizer_kwargs = (\n        critic_optimizer_kwargs\n        if critic_optimizer_kwargs is not None\n        else actor_optimizer_kwargs\n    )\n</code></pre>"},{"location":"docs/content/api/rl/a2c/#models.rl.a2c.a2c.A2C.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> <p>Configure the optimizers for the policy and the critic network (=baseline)</p> Source code in <code>rl4co/models/rl/a2c/a2c.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure the optimizers for the policy and the critic network (=baseline)\"\"\"\n    parameters = [\n        {\"params\": self.policy.parameters(), **self.actor_optimizer_kwargs},\n    ] + [{\"params\": self.baseline.parameters(), **self.critic_optimizer_kwargs}]\n\n    return super().configure_optimizers(parameters)\n</code></pre>"},{"location":"docs/content/api/rl/base/","title":"RL4COLitModule","text":"<p>The <code>RL4COLitModule</code> is a wrapper around PyTorch Lightning's <code>LightningModule</code> that provides additional functionality for RL algorithms. It is the parent class for all RL algorithms in the library.</p>"},{"location":"docs/content/api/rl/base/#models.rl.common.base.RL4COLitModule","title":"RL4COLitModule","text":"<pre><code>RL4COLitModule(\n    env: RL4COEnvBase,\n    policy: Module,\n    batch_size: int = 512,\n    val_batch_size: Union[List[int], int] = None,\n    test_batch_size: Union[List[int], int] = None,\n    train_data_size: int = 100000,\n    val_data_size: int = 10000,\n    test_data_size: int = 10000,\n    optimizer: Union[str, Optimizer, partial] = \"Adam\",\n    optimizer_kwargs: dict = {\"lr\": 0.0001},\n    lr_scheduler: Union[str, LRScheduler, partial] = None,\n    lr_scheduler_kwargs: dict = {\n        \"milestones\": [80, 95],\n        \"gamma\": 0.1,\n    },\n    lr_scheduler_interval: str = \"epoch\",\n    lr_scheduler_monitor: str = \"val/reward\",\n    generate_default_data: bool = False,\n    shuffle_train_dataloader: bool = False,\n    dataloader_num_workers: int = 0,\n    data_dir: str = \"data/\",\n    log_on_step: bool = True,\n    metrics: dict = {},\n    **litmodule_kwargs\n)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>Base class for Lightning modules for RL4CO. This defines the general training loop in terms of RL algorithms. Subclasses should implement mainly the <code>shared_step</code> to define the specific loss functions and optimization routines.</p> <p>Parameters:</p> <ul> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>RL4CO environment</p> </li> <li> <code>policy</code>               (<code>Module</code>)           \u2013            <p>policy network (actor)</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>batch size (general one, default used for training)</p> </li> <li> <code>val_batch_size</code>               (<code>Union[List[int], int]</code>, default:                   <code>None</code> )           \u2013            <p>specific batch size for validation. If None, will use <code>batch_size</code>. If list, will use one for each dataset</p> </li> <li> <code>test_batch_size</code>               (<code>Union[List[int], int]</code>, default:                   <code>None</code> )           \u2013            <p>specific batch size for testing. If None, will use <code>val_batch_size</code>. If list, will use one for each dataset</p> </li> <li> <code>train_data_size</code>               (<code>int</code>, default:                   <code>100000</code> )           \u2013            <p>size of training dataset for one epoch</p> </li> <li> <code>val_data_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>size of validation dataset for one epoch</p> </li> <li> <code>test_data_size</code>               (<code>int</code>, default:                   <code>10000</code> )           \u2013            <p>size of testing dataset for one epoch</p> </li> <li> <code>optimizer</code>               (<code>Union[str, Optimizer, partial]</code>, default:                   <code>'Adam'</code> )           \u2013            <p>optimizer or optimizer name</p> </li> <li> <code>optimizer_kwargs</code>               (<code>dict</code>, default:                   <code>{'lr': 0.0001}</code> )           \u2013            <p>optimizer kwargs</p> </li> <li> <code>lr_scheduler</code>               (<code>Union[str, LRScheduler, partial]</code>, default:                   <code>None</code> )           \u2013            <p>learning rate scheduler or learning rate scheduler name</p> </li> <li> <code>lr_scheduler_kwargs</code>               (<code>dict</code>, default:                   <code>{'milestones': [80, 95], 'gamma': 0.1}</code> )           \u2013            <p>learning rate scheduler kwargs</p> </li> <li> <code>lr_scheduler_interval</code>               (<code>str</code>, default:                   <code>'epoch'</code> )           \u2013            <p>learning rate scheduler interval</p> </li> <li> <code>lr_scheduler_monitor</code>               (<code>str</code>, default:                   <code>'val/reward'</code> )           \u2013            <p>learning rate scheduler monitor</p> </li> <li> <code>generate_default_data</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to generate default datasets, filling up the data directory</p> </li> <li> <code>shuffle_train_dataloader</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to shuffle training dataloader. Default is False since we recreate dataset every epoch</p> </li> <li> <code>dataloader_num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>number of workers for dataloader</p> </li> <li> <code>data_dir</code>               (<code>str</code>, default:                   <code>'data/'</code> )           \u2013            <p>data directory</p> </li> <li> <code>metrics</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>metrics</p> </li> <li> <code>litmodule_kwargs</code>           \u2013            <p>kwargs for <code>LightningModule</code></p> </li> </ul> Source code in <code>rl4co/models/rl/common/base.py</code> <pre><code>def __init__(\n    self,\n    env: RL4COEnvBase,\n    policy: nn.Module,\n    batch_size: int = 512,\n    val_batch_size: Union[List[int], int] = None,\n    test_batch_size: Union[List[int], int] = None,\n    train_data_size: int = 100_000,\n    val_data_size: int = 10_000,\n    test_data_size: int = 10_000,\n    optimizer: Union[str, torch.optim.Optimizer, partial] = \"Adam\",\n    optimizer_kwargs: dict = {\"lr\": 1e-4},\n    lr_scheduler: Union[str, torch.optim.lr_scheduler.LRScheduler, partial] = None,\n    lr_scheduler_kwargs: dict = {\n        \"milestones\": [80, 95],\n        \"gamma\": 0.1,\n    },\n    lr_scheduler_interval: str = \"epoch\",\n    lr_scheduler_monitor: str = \"val/reward\",\n    generate_default_data: bool = False,\n    shuffle_train_dataloader: bool = False,\n    dataloader_num_workers: int = 0,\n    data_dir: str = \"data/\",\n    log_on_step: bool = True,\n    metrics: dict = {},\n    **litmodule_kwargs,\n):\n    super().__init__(**litmodule_kwargs)\n\n    # This line ensures params passed to LightningModule will be saved to ckpt\n    # it also allows to access params with 'self.hparams' attribute\n    # Note: we will send to logger with `self.logger.save_hyperparams` in `setup`\n    self.save_hyperparameters(logger=False)\n\n    self.env = env\n    self.policy = policy\n\n    self.instantiate_metrics(metrics)\n    self.log_on_step = log_on_step\n\n    self.data_cfg = {\n        \"batch_size\": batch_size,\n        \"val_batch_size\": val_batch_size,\n        \"test_batch_size\": test_batch_size,\n        \"generate_default_data\": generate_default_data,\n        \"data_dir\": data_dir,\n        \"train_data_size\": train_data_size,\n        \"val_data_size\": val_data_size,\n        \"test_data_size\": test_data_size,\n    }\n\n    self._optimizer_name_or_cls: Union[str, torch.optim.Optimizer] = optimizer\n    self.optimizer_kwargs: dict = optimizer_kwargs\n    self._lr_scheduler_name_or_cls: Union[\n        str, torch.optim.lr_scheduler.LRScheduler\n    ] = lr_scheduler\n    self.lr_scheduler_kwargs: dict = lr_scheduler_kwargs\n    self.lr_scheduler_interval: str = lr_scheduler_interval\n    self.lr_scheduler_monitor: str = lr_scheduler_monitor\n\n    self.shuffle_train_dataloader = shuffle_train_dataloader\n    self.dataloader_num_workers = dataloader_num_workers\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.rl.common.base.RL4COLitModule.instantiate_metrics","title":"instantiate_metrics","text":"<pre><code>instantiate_metrics(metrics: dict)\n</code></pre> <p>Dictionary of metrics to be logged at each phase</p> Source code in <code>rl4co/models/rl/common/base.py</code> <pre><code>def instantiate_metrics(self, metrics: dict):\n    \"\"\"Dictionary of metrics to be logged at each phase\"\"\"\n\n    if not metrics:\n        log.info(\"No metrics specified, using default\")\n    self.train_metrics = metrics.get(\"train\", [\"loss\", \"reward\"])\n    self.val_metrics = metrics.get(\"val\", [\"reward\"])\n    self.test_metrics = metrics.get(\"test\", [\"reward\"])\n    self.log_on_step = metrics.get(\"log_on_step\", True)\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.rl.common.base.RL4COLitModule.setup","title":"setup","text":"<pre><code>setup(stage='fit')\n</code></pre> <p>Base LightningModule setup method. This will setup the datasets and dataloaders</p> Note <p>We also send to the loggers all hyperparams that are not <code>nn.Module</code> (i.e. the policy). Apparently PyTorch Lightning does not do this by default.</p> Source code in <code>rl4co/models/rl/common/base.py</code> <pre><code>def setup(self, stage=\"fit\"):\n    \"\"\"Base LightningModule setup method. This will setup the datasets and dataloaders\n\n    Note:\n        We also send to the loggers all hyperparams that are not `nn.Module` (i.e. the policy).\n        Apparently PyTorch Lightning does not do this by default.\n    \"\"\"\n\n    log.info(\"Setting up batch sizes for train/val/test\")\n    train_bs, val_bs, test_bs = (\n        self.data_cfg[\"batch_size\"],\n        self.data_cfg[\"val_batch_size\"],\n        self.data_cfg[\"test_batch_size\"],\n    )\n    self.train_batch_size = train_bs\n    self.val_batch_size = train_bs if val_bs is None else val_bs\n    self.test_batch_size = self.val_batch_size if test_bs is None else test_bs\n\n    if self.data_cfg[\"generate_default_data\"]:\n        log.info(\n            \"Generating default datasets. If found, they will not be overwritten\"\n        )\n        generate_default_datasets(data_dir=self.data_cfg[\"data_dir\"])\n\n    log.info(\"Setting up datasets\")\n    self.train_dataset = self.wrap_dataset(\n        self.env.dataset(self.data_cfg[\"train_data_size\"], phase=\"train\")\n    )\n    self.val_dataset = self.env.dataset(self.data_cfg[\"val_data_size\"], phase=\"val\")\n    self.test_dataset = self.env.dataset(\n        self.data_cfg[\"test_data_size\"], phase=\"test\"\n    )\n    self.dataloader_names = None\n    self.setup_loggers()\n    self.post_setup_hook()\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.rl.common.base.RL4COLitModule.setup_loggers","title":"setup_loggers","text":"<pre><code>setup_loggers()\n</code></pre> <p>Log all hyperparameters except those in <code>nn.Module</code></p> Source code in <code>rl4co/models/rl/common/base.py</code> <pre><code>def setup_loggers(self):\n    \"\"\"Log all hyperparameters except those in `nn.Module`\"\"\"\n    if self.loggers is not None:\n        hparams_save = {\n            k: v for k, v in self.hparams.items() if not isinstance(v, nn.Module)\n        }\n        for logger in self.loggers:\n            logger.log_hyperparams(hparams_save)\n            logger.log_graph(self)\n            logger.save()\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.rl.common.base.RL4COLitModule.post_setup_hook","title":"post_setup_hook","text":"<pre><code>post_setup_hook()\n</code></pre> <p>Hook to be called after setup. Can be used to set up subclasses without overriding <code>setup</code></p> Source code in <code>rl4co/models/rl/common/base.py</code> <pre><code>def post_setup_hook(self):\n    \"\"\"Hook to be called after setup. Can be used to set up subclasses without overriding `setup`\"\"\"\n    pass\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.rl.common.base.RL4COLitModule.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers(parameters=None)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>parameters</code>           \u2013            <p>parameters to be optimized. If None, will use <code>self.parameters()</code>, i.e. all parameters</p> </li> </ul> Source code in <code>rl4co/models/rl/common/base.py</code> <pre><code>def configure_optimizers(self, parameters=None):\n    \"\"\"\n    Args:\n        parameters: parameters to be optimized. If None, will use `self.parameters()`, i.e. all parameters\n    \"\"\"\n\n    if parameters is None:\n        parameters = self.parameters()\n\n    log.info(f\"Instantiating optimizer &lt;{self._optimizer_name_or_cls}&gt;\")\n    if isinstance(self._optimizer_name_or_cls, str):\n        optimizer = create_optimizer(\n            parameters, self._optimizer_name_or_cls, **self.optimizer_kwargs\n        )\n    elif isinstance(self._optimizer_name_or_cls, partial):\n        optimizer = self._optimizer_name_or_cls(parameters, **self.optimizer_kwargs)\n    else:  # User-defined optimizer\n        opt_cls = self._optimizer_name_or_cls\n        optimizer = opt_cls(parameters, **self.optimizer_kwargs)\n        assert isinstance(optimizer, torch.optim.Optimizer)\n\n    # instantiate lr scheduler\n    if self._lr_scheduler_name_or_cls is None:\n        return optimizer\n    else:\n        log.info(f\"Instantiating LR scheduler &lt;{self._lr_scheduler_name_or_cls}&gt;\")\n        if isinstance(self._lr_scheduler_name_or_cls, str):\n            scheduler = create_scheduler(\n                optimizer, self._lr_scheduler_name_or_cls, **self.lr_scheduler_kwargs\n            )\n        elif isinstance(self._lr_scheduler_name_or_cls, partial):\n            scheduler = self._lr_scheduler_name_or_cls(\n                optimizer, **self.lr_scheduler_kwargs\n            )\n        else:  # User-defined scheduler\n            scheduler_cls = self._lr_scheduler_name_or_cls\n            scheduler = scheduler_cls(optimizer, **self.lr_scheduler_kwargs)\n            assert isinstance(scheduler, torch.optim.lr_scheduler.LRScheduler)\n        return [optimizer], {\n            \"scheduler\": scheduler,\n            \"interval\": self.lr_scheduler_interval,\n            \"monitor\": self.lr_scheduler_monitor,\n        }\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.rl.common.base.RL4COLitModule.log_metrics","title":"log_metrics","text":"<pre><code>log_metrics(\n    metric_dict: dict,\n    phase: str,\n    dataloader_idx: Union[int, None] = None,\n)\n</code></pre> <p>Log metrics to logger and progress bar</p> Source code in <code>rl4co/models/rl/common/base.py</code> <pre><code>def log_metrics(\n    self, metric_dict: dict, phase: str, dataloader_idx: Union[int, None] = None\n):\n    \"\"\"Log metrics to logger and progress bar\"\"\"\n    metrics = getattr(self, f\"{phase}_metrics\")\n    dataloader_name = \"\"\n    if dataloader_idx is not None and self.dataloader_names is not None:\n        dataloader_name = \"/\" + self.dataloader_names[dataloader_idx]\n    metrics = {\n        f\"{phase}/{k}{dataloader_name}\": v.mean()\n        if isinstance(v, torch.Tensor)\n        else v\n        for k, v in metric_dict.items()\n        if k in metrics\n    }\n    log_on_step = self.log_on_step if phase == \"train\" else False\n    on_epoch = False if phase == \"train\" else True\n    self.log_dict(\n        metrics,\n        on_step=log_on_step,\n        on_epoch=on_epoch,\n        prog_bar=True,\n        sync_dist=True,\n        add_dataloader_idx=False,  # we add manually above\n    )\n    return metrics\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.rl.common.base.RL4COLitModule.forward","title":"forward","text":"<pre><code>forward(td, **kwargs)\n</code></pre> <p>Forward pass for the model. Simple wrapper around <code>policy</code>. Uses <code>env</code> from the module if not provided.</p> Source code in <code>rl4co/models/rl/common/base.py</code> <pre><code>def forward(self, td, **kwargs):\n    \"\"\"Forward pass for the model. Simple wrapper around `policy`. Uses `env` from the module if not provided.\"\"\"\n    if kwargs.get(\"env\", None) is None:\n        env = self.env\n    else:\n        log.info(\"Using env from kwargs\")\n        env = kwargs.pop(\"env\")\n    return self.policy(td, env, **kwargs)\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.rl.common.base.RL4COLitModule.shared_step","title":"shared_step","text":"<pre><code>shared_step(\n    batch: Any, batch_idx: int, phase: str, **kwargs\n)\n</code></pre> <p>Shared step between train/val/test. To be implemented in subclass</p> Source code in <code>rl4co/models/rl/common/base.py</code> <pre><code>def shared_step(self, batch: Any, batch_idx: int, phase: str, **kwargs):\n    \"\"\"Shared step between train/val/test. To be implemented in subclass\"\"\"\n    raise NotImplementedError(\"Shared step is required to implemented in subclass\")\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.rl.common.base.RL4COLitModule.on_train_epoch_end","title":"on_train_epoch_end","text":"<pre><code>on_train_epoch_end()\n</code></pre> <p>Called at the end of the training epoch. This can be used for instance to update the train dataset with new data (which is the case in RL).</p> Source code in <code>rl4co/models/rl/common/base.py</code> <pre><code>def on_train_epoch_end(self):\n    \"\"\"Called at the end of the training epoch. This can be used for instance to update the train dataset\n    with new data (which is the case in RL).\n    \"\"\"\n    # Only update if not in the first epoch\n    # If last epoch, we don't need to update since we will not use the dataset anymore\n    if self.current_epoch &lt; self.trainer.max_epochs - 1:\n        log.info(\"Generating training dataset for next epoch...\")\n        train_dataset = self.env.dataset(self.data_cfg[\"train_data_size\"], \"train\")\n        self.train_dataset = self.wrap_dataset(train_dataset)\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.rl.common.base.RL4COLitModule.wrap_dataset","title":"wrap_dataset","text":"<pre><code>wrap_dataset(dataset)\n</code></pre> <p>Wrap dataset with policy-specific wrapper. This is useful i.e. in REINFORCE where we need to collect the greedy rollout baseline outputs.</p> Source code in <code>rl4co/models/rl/common/base.py</code> <pre><code>def wrap_dataset(self, dataset):\n    \"\"\"Wrap dataset with policy-specific wrapper. This is useful i.e. in REINFORCE where we need to\n    collect the greedy rollout baseline outputs.\n    \"\"\"\n    return dataset\n</code></pre>"},{"location":"docs/content/api/rl/base/#transductive-learning","title":"Transductive Learning","text":"<p>Transductive models are learning algorithms that optimize on a specific instance. They improve solutions by updating policy parameters \\(\\theta\\), which means that we are running optimization (backprop) at test time.  Transductive learning can be performed with different policies: for example EAS updates (a part of) AR policies parameters to obtain better solutions, but I guess there are ways (or papers out there I don't know of) that optimize at test time.</p> <p>Tip</p> <p>You may refer to the definition of inductive vs transductive RL . In inductive RL, we train to generalize to new instances. In transductive RL we train (or finetune) to solve only specific ones.</p>"},{"location":"docs/content/api/rl/base/#models.common.transductive.base.TransductiveModel","title":"TransductiveModel","text":"<pre><code>TransductiveModel(\n    env,\n    policy,\n    dataset: Union[Dataset, str],\n    batch_size: int = 1,\n    max_iters: int = 100,\n    max_runtime: Optional[int] = 86400,\n    save_path: Optional[str] = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RL4COLitModule</code></p> <p>Base class for transductive algorithms (i.e. that optimize policy parameters for specific instances, see https://en.wikipedia.org/wiki/Transduction_(machine_learning)). Transductive algorithms are used online to find better solutions for a given dataset, i.e. given a policy, improve (a part of) its parameters such that the policy performs better on the given dataset.</p> Note <p>By default, we use manual optimization to handle the search.</p> <p>Parameters:</p> <ul> <li> <code>env</code>           \u2013            <p>RL4CO environment</p> </li> <li> <code>policy</code>           \u2013            <p>policy network</p> </li> <li> <code>dataset</code>               (<code>Union[Dataset, str]</code>)           \u2013            <p>dataset to use for training</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>batch size</p> </li> <li> <code>max_iters</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>maximum number of iterations</p> </li> <li> <code>max_runtime</code>               (<code>Optional[int]</code>, default:                   <code>86400</code> )           \u2013            <p>maximum runtime in seconds</p> </li> <li> <code>save_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>path to save the model</p> </li> <li> <code>**kwargs</code>           \u2013            <p>additional arguments</p> </li> </ul> Source code in <code>rl4co/models/common/transductive/base.py</code> <pre><code>def __init__(\n    self,\n    env,\n    policy,\n    dataset: Union[Dataset, str],\n    batch_size: int = 1,\n    max_iters: int = 100,\n    max_runtime: Optional[int] = 86_400,\n    save_path: Optional[str] = None,\n    **kwargs,\n):\n    self.save_hyperparameters(logger=False)\n    super().__init__(env, policy, **kwargs)\n    self.dataset = dataset\n    self.automatic_optimization = False  # we optimize manually\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.common.transductive.base.TransductiveModel.setup","title":"setup","text":"<pre><code>setup(stage='fit')\n</code></pre> <p>Setup the dataset and attributes. The RL4COLitModulebase class automatically loads the data.</p> Source code in <code>rl4co/models/common/transductive/base.py</code> <pre><code>def setup(self, stage=\"fit\"):\n    \"\"\"Setup the dataset and attributes.\n    The RL4COLitModulebase class automatically loads the data.\n    \"\"\"\n    if isinstance(self.dataset, str):\n        # load from file\n        self.dataset = self.env.dataset(filename=self.dataset)\n\n    # Set all datasets and batch size as the same\n    for split in [\"train\", \"val\", \"test\"]:\n        setattr(self, f\"{split}_dataset\", self.dataset)\n        setattr(self, f\"{split}_batch_size\", self.hparams.batch_size)\n\n    # Setup loggers\n    self.setup_loggers()\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.common.transductive.base.TransductiveModel.on_train_batch_start","title":"on_train_batch_start","text":"<pre><code>on_train_batch_start(batch: Any, batch_idx: int)\n</code></pre> <p>Called before training (i.e. search) for a new batch begins. This can be used to perform changes to the model or optimizer at the start of each batch.</p> Source code in <code>rl4co/models/common/transductive/base.py</code> <pre><code>def on_train_batch_start(self, batch: Any, batch_idx: int):\n    \"\"\"Called before training (i.e. search) for a new batch begins.\n    This can be used to perform changes to the model or optimizer at the start of each batch.\n    \"\"\"\n    pass  # Implement in subclass\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.common.transductive.base.TransductiveModel.training_step","title":"training_step  <code>abstractmethod</code>","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Main search loop. We use the training step to effectively adapt to a <code>batch</code> of instances.</p> Source code in <code>rl4co/models/common/transductive/base.py</code> <pre><code>@abc.abstractmethod\ndef training_step(self, batch, batch_idx):\n    \"\"\"Main search loop. We use the training step to effectively adapt to a `batch` of instances.\"\"\"\n    raise NotImplementedError(\"Implement in subclass\")\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.common.transductive.base.TransductiveModel.on_train_batch_end","title":"on_train_batch_end","text":"<pre><code>on_train_batch_end(\n    outputs: STEP_OUTPUT, batch: Any, batch_idx: int\n) -&gt; None\n</code></pre> <p>Called when the train batch ends. This can be used for instance for logging or clearing cache.</p> Source code in <code>rl4co/models/common/transductive/base.py</code> <pre><code>def on_train_batch_end(\n    self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int\n) -&gt; None:\n    \"\"\"Called when the train batch ends. This can be used for\n    instance for logging or clearing cache.\n    \"\"\"\n    pass  # Implement in subclass\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.common.transductive.base.TransductiveModel.on_train_epoch_end","title":"on_train_epoch_end","text":"<pre><code>on_train_epoch_end() -&gt; None\n</code></pre> <p>Called when the train ends.</p> Source code in <code>rl4co/models/common/transductive/base.py</code> <pre><code>def on_train_epoch_end(self) -&gt; None:\n    \"\"\"Called when the train ends.\"\"\"\n    pass  # Implement in subclass\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.common.transductive.base.TransductiveModel.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch: Any, batch_idx: int)\n</code></pre> <p>Not used during search</p> Source code in <code>rl4co/models/common/transductive/base.py</code> <pre><code>def validation_step(self, batch: Any, batch_idx: int):\n    \"\"\"Not used during search\"\"\"\n    pass\n</code></pre>"},{"location":"docs/content/api/rl/base/#models.common.transductive.base.TransductiveModel.test_step","title":"test_step","text":"<pre><code>test_step(batch: Any, batch_idx: int)\n</code></pre> <p>Not used during search</p> Source code in <code>rl4co/models/common/transductive/base.py</code> <pre><code>def test_step(self, batch: Any, batch_idx: int):\n    \"\"\"Not used during search\"\"\"\n    pass\n</code></pre>"},{"location":"docs/content/api/rl/ppo/","title":"PPO","text":""},{"location":"docs/content/api/rl/ppo/#models.rl.ppo.ppo.PPO","title":"PPO","text":"<pre><code>PPO(\n    env: RL4COEnvBase,\n    policy: Module,\n    critic: CriticNetwork = None,\n    critic_kwargs: dict = {},\n    clip_range: float = 0.2,\n    ppo_epochs: int = 2,\n    mini_batch_size: Union[int, float] = 0.25,\n    vf_lambda: float = 0.5,\n    entropy_lambda: float = 0.0,\n    normalize_adv: bool = False,\n    max_grad_norm: float = 0.5,\n    metrics: dict = {\n        \"train\": [\n            \"reward\",\n            \"loss\",\n            \"surrogate_loss\",\n            \"value_loss\",\n            \"entropy\",\n        ]\n    },\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RL4COLitModule</code></p> <p>An implementation of the Proximal Policy Optimization (PPO) algorithm (https://arxiv.org/abs/1707.06347) is presented with modifications for autoregressive decoding schemes.</p> <p>In contrast to the original PPO algorithm, this implementation does not consider autoregressive decoding steps as part of the MDP transition. While many Neural Combinatorial Optimization (NCO) studies model decoding steps as transitions in a solution-construction MDP, we treat autoregressive solution construction as an algorithmic choice for tractable CO solution generation. This choice aligns with the Attention Model (AM) (https://openreview.net/forum?id=ByxBFsRqYm), which treats decoding steps as a single-step MDP in Equation 9.</p> <p>Modeling autoregressive decoding steps as a single-step MDP introduces significant changes to the PPO implementation, including:</p> <ul> <li>Generalized Advantage Estimation (GAE) (https://arxiv.org/abs/1506.02438) is not applicable since we are dealing with a single-step MDP.</li> <li>The definition of policy entropy can differ from the commonly implemented manner.</li> </ul> <p>The commonly implemented definition of policy entropy is the entropy of the policy distribution, given by:</p> \\[H(\\pi(x_t)) = - \\sum_{a_t \\in A_t} \\pi(a_t|x_t) \\log \\pi(a_t|x_t)\\] <p>where \\(x_t\\) represents the given state at step \\(t\\), \\(A_t\\) is the set of all (admisible) actions at step \\(t\\), and \\(a_t\\) is the action taken at step \\(t\\).</p> <p>If we interpret autoregressive decoding steps as transition steps of an MDP, the entropy for the entire decoding process can be defined as the sum of entropies for each decoding step:</p> \\[H(\\pi) = \\sum_t H(\\pi(x_t))\\] <p>However, if we consider autoregressive decoding steps as an algorithmic choice, the entropy for the entire decoding process is defined as:</p> \\[H(\\pi) = - \\sum_{a \\in A} \\pi(a|x) \\log \\pi(a|x)\\] <p>where \\(x\\) represents the given CO problem instance, and \\(A\\) is the set of all feasible solutions.</p> <p>Due to the intractability of computing the entropy of the policy distribution over all feasible solutions, we approximate it by computing the entropy over solutions generated by the policy itself. This approximation serves as a proxy for the second definition of entropy, utilizing Monte Carlo sampling.</p> <p>It is worth noting that our modeling of decoding steps and the implementation of the PPO algorithm align with recent work in the Natural Language Processing (NLP) community, specifically RL with Human Feedback (RLHF) (e.g., https://github.com/lucidrains/PaLM-rlhf-pytorch).</p> Source code in <code>rl4co/models/rl/ppo/ppo.py</code> <pre><code>def __init__(\n    self,\n    env: RL4COEnvBase,\n    policy: nn.Module,\n    critic: CriticNetwork = None,\n    critic_kwargs: dict = {},\n    clip_range: float = 0.2,  # epsilon of PPO\n    ppo_epochs: int = 2,  # inner epoch, K\n    mini_batch_size: Union[int, float] = 0.25,  # 0.25,\n    vf_lambda: float = 0.5,  # lambda of Value function fitting\n    entropy_lambda: float = 0.0,  # lambda of entropy bonus\n    normalize_adv: bool = False,  # whether to normalize advantage\n    max_grad_norm: float = 0.5,  # max gradient norm\n    metrics: dict = {\n        \"train\": [\"reward\", \"loss\", \"surrogate_loss\", \"value_loss\", \"entropy\"],\n    },\n    **kwargs,\n):\n    super().__init__(env, policy, metrics=metrics, **kwargs)\n    self.automatic_optimization = False  # PPO uses custom optimization routine\n\n    if critic is None:\n        log.info(\"Creating critic network for {}\".format(env.name))\n        critic = create_critic_from_actor(policy, **critic_kwargs)\n    self.critic = critic\n\n    if isinstance(mini_batch_size, float) and (\n        mini_batch_size &lt;= 0 or mini_batch_size &gt; 1\n    ):\n        default_mini_batch_fraction = 0.25\n        log.warning(\n            f\"mini_batch_size must be an integer or a float in the range (0, 1], got {mini_batch_size}. Setting mini_batch_size to {default_mini_batch_fraction}.\"\n        )\n        mini_batch_size = default_mini_batch_fraction\n\n    if isinstance(mini_batch_size, int) and (mini_batch_size &lt;= 0):\n        default_mini_batch_size = 128\n        log.warning(\n            f\"mini_batch_size must be an integer or a float in the range (0, 1], got {mini_batch_size}. Setting mini_batch_size to {default_mini_batch_size}.\"\n        )\n        mini_batch_size = default_mini_batch_size\n\n    self.ppo_cfg = {\n        \"clip_range\": clip_range,\n        \"ppo_epochs\": ppo_epochs,\n        \"mini_batch_size\": mini_batch_size,\n        \"vf_lambda\": vf_lambda,\n        \"entropy_lambda\": entropy_lambda,\n        \"normalize_adv\": normalize_adv,\n        \"max_grad_norm\": max_grad_norm,\n    }\n</code></pre>"},{"location":"docs/content/api/rl/ppo/#models.rl.ppo.ppo.PPO.on_train_epoch_end","title":"on_train_epoch_end","text":"<pre><code>on_train_epoch_end()\n</code></pre> <p>ToDo: Add support for other schedulers.</p> Source code in <code>rl4co/models/rl/ppo/ppo.py</code> <pre><code>def on_train_epoch_end(self):\n    \"\"\"\n    ToDo: Add support for other schedulers.\n    \"\"\"\n\n    sch = self.lr_schedulers()\n\n    # If the selected scheduler is a MultiStepLR scheduler.\n    if isinstance(sch, torch.optim.lr_scheduler.MultiStepLR):\n        sch.step()\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/","title":"Reinforce","text":""},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.reinforce.REINFORCE","title":"REINFORCE","text":"<pre><code>REINFORCE(\n    env: RL4COEnvBase,\n    policy: Module,\n    baseline: Union[REINFORCEBaseline, str] = \"rollout\",\n    baseline_kwargs: dict = {},\n    reward_scale: str = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RL4COLitModule</code></p> <p>REINFORCE algorithm, also known as policy gradients. See superclass <code>RL4COLitModule</code> for more details.</p> <p>Parameters:</p> <ul> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>Environment to use for the algorithm</p> </li> <li> <code>policy</code>               (<code>Module</code>)           \u2013            <p>Policy to use for the algorithm</p> </li> <li> <code>baseline</code>               (<code>Union[REINFORCEBaseline, str]</code>, default:                   <code>'rollout'</code> )           \u2013            <p>REINFORCE baseline</p> </li> <li> <code>baseline_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments for baseline. Ignored if baseline is not a string</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Keyword arguments passed to the superclass</p> </li> </ul> Source code in <code>rl4co/models/rl/reinforce/reinforce.py</code> <pre><code>def __init__(\n    self,\n    env: RL4COEnvBase,\n    policy: nn.Module,\n    baseline: Union[REINFORCEBaseline, str] = \"rollout\",\n    baseline_kwargs: dict = {},\n    reward_scale: str = None,\n    **kwargs,\n):\n    super().__init__(env, policy, **kwargs)\n\n    self.save_hyperparameters(logger=False)\n\n    if baseline == \"critic\":\n        log.warning(\n            \"Using critic as baseline. If you want more granular support, use the A2C module instead.\"\n        )\n\n    if isinstance(baseline, str):\n        baseline = get_reinforce_baseline(baseline, **baseline_kwargs)\n    else:\n        if baseline_kwargs != {}:\n            log.warning(\"baseline_kwargs is ignored when baseline is not a string\")\n    self.baseline = baseline\n    self.advantage_scaler = RewardScaler(reward_scale)\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.reinforce.REINFORCE.calculate_loss","title":"calculate_loss","text":"<pre><code>calculate_loss(\n    td: TensorDict,\n    batch: TensorDict,\n    policy_out: dict,\n    reward: Optional[Tensor] = None,\n    log_likelihood: Optional[Tensor] = None,\n)\n</code></pre> <p>Calculate loss for REINFORCE algorithm.</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>TensorDict containing the current state of the environment</p> </li> <li> <code>batch</code>               (<code>TensorDict</code>)           \u2013            <p>Batch of data. This is used to get the extra loss terms, e.g., REINFORCE baseline</p> </li> <li> <code>policy_out</code>               (<code>dict</code>)           \u2013            <p>Output of the policy network</p> </li> <li> <code>reward</code>               (<code>Optional[Tensor]</code>, default:                   <code>None</code> )           \u2013            <p>Reward tensor. If None, it is taken from <code>policy_out</code></p> </li> <li> <code>log_likelihood</code>               (<code>Optional[Tensor]</code>, default:                   <code>None</code> )           \u2013            <p>Log-likelihood tensor. If None, it is taken from <code>policy_out</code></p> </li> </ul> Source code in <code>rl4co/models/rl/reinforce/reinforce.py</code> <pre><code>def calculate_loss(\n    self,\n    td: TensorDict,\n    batch: TensorDict,\n    policy_out: dict,\n    reward: Optional[torch.Tensor] = None,\n    log_likelihood: Optional[torch.Tensor] = None,\n):\n    \"\"\"Calculate loss for REINFORCE algorithm.\n\n    Args:\n        td: TensorDict containing the current state of the environment\n        batch: Batch of data. This is used to get the extra loss terms, e.g., REINFORCE baseline\n        policy_out: Output of the policy network\n        reward: Reward tensor. If None, it is taken from `policy_out`\n        log_likelihood: Log-likelihood tensor. If None, it is taken from `policy_out`\n    \"\"\"\n    # Extra: this is used for additional loss terms, e.g., REINFORCE baseline\n    extra = batch.get(\"extra\", None)\n    reward = reward if reward is not None else policy_out[\"reward\"]\n    log_likelihood = (\n        log_likelihood if log_likelihood is not None else policy_out[\"log_likelihood\"]\n    )\n\n    # REINFORCE baseline\n    bl_val, bl_loss = (\n        self.baseline.eval(td, reward, self.env) if extra is None else (extra, 0)\n    )\n\n    # Main loss function\n    advantage = reward - bl_val  # advantage = reward - baseline\n    advantage = self.advantage_scaler(advantage)\n    reinforce_loss = -(advantage * log_likelihood).mean()\n    loss = reinforce_loss + bl_loss\n    policy_out.update(\n        {\n            \"loss\": loss,\n            \"reinforce_loss\": reinforce_loss,\n            \"bl_loss\": bl_loss,\n            \"bl_val\": bl_val,\n        }\n    )\n    return policy_out\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.reinforce.REINFORCE.on_train_epoch_end","title":"on_train_epoch_end","text":"<pre><code>on_train_epoch_end()\n</code></pre> <p>Callback for end of training epoch: we evaluate the baseline</p> Source code in <code>rl4co/models/rl/reinforce/reinforce.py</code> <pre><code>def on_train_epoch_end(self):\n    \"\"\"Callback for end of training epoch: we evaluate the baseline\"\"\"\n    self.baseline.epoch_callback(\n        self.policy,\n        env=self.env,\n        batch_size=self.val_batch_size,\n        device=get_lightning_device(self),\n        epoch=self.current_epoch,\n        dataset_size=self.data_cfg[\"val_data_size\"],\n    )\n    # Need to call super() for the dataset to be reset\n    super().on_train_epoch_end()\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.reinforce.REINFORCE.wrap_dataset","title":"wrap_dataset","text":"<pre><code>wrap_dataset(dataset)\n</code></pre> <p>Wrap dataset from baseline evaluation. Used in greedy rollout baseline</p> Source code in <code>rl4co/models/rl/reinforce/reinforce.py</code> <pre><code>def wrap_dataset(self, dataset):\n    \"\"\"Wrap dataset from baseline evaluation. Used in greedy rollout baseline\"\"\"\n    return self.baseline.wrap_dataset(\n        dataset,\n        self.env,\n        batch_size=self.val_batch_size,\n        device=get_lightning_device(self),\n    )\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.reinforce.REINFORCE.set_decode_type_multistart","title":"set_decode_type_multistart","text":"<pre><code>set_decode_type_multistart(phase: str)\n</code></pre> <p>Set decode type to <code>multistart</code> for train, val and test in policy. For example, if the decode type is <code>greedy</code>, it will be set to <code>multistart_greedy</code>.</p> <p>Parameters:</p> <ul> <li> <code>phase</code>               (<code>str</code>)           \u2013            <p>Phase to set decode type for. Must be one of <code>train</code>, <code>val</code> or <code>test</code>.</p> </li> </ul> Source code in <code>rl4co/models/rl/reinforce/reinforce.py</code> <pre><code>def set_decode_type_multistart(self, phase: str):\n    \"\"\"Set decode type to `multistart` for train, val and test in policy.\n    For example, if the decode type is `greedy`, it will be set to `multistart_greedy`.\n\n    Args:\n        phase: Phase to set decode type for. Must be one of `train`, `val` or `test`.\n    \"\"\"\n    attribute = f\"{phase}_decode_type\"\n    attr_get = getattr(self.policy, attribute)\n    # If does not exist, log error\n    if attr_get is None:\n        log.error(f\"Decode type for {phase} is None. Cannot prepend `multistart_`.\")\n        return\n    elif \"multistart\" in attr_get:\n        return\n    else:\n        setattr(self.policy, attribute, f\"multistart_{attr_get}\")\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.reinforce.REINFORCE.load_from_checkpoint","title":"load_from_checkpoint  <code>classmethod</code>","text":"<pre><code>load_from_checkpoint(\n    checkpoint_path: Union[_PATH, IO],\n    map_location: _MAP_LOCATION_TYPE = None,\n    hparams_file: Optional[_PATH] = None,\n    strict: bool = False,\n    load_baseline: bool = True,\n    **kwargs: Any\n) -&gt; Self\n</code></pre> <p>Load model from checkpoint/</p> Note <p>This is a modified version of <code>load_from_checkpoint</code> from <code>pytorch_lightning.core.saving</code>. It deals with matching keys for the baseline by first running setup</p> Source code in <code>rl4co/models/rl/reinforce/reinforce.py</code> <pre><code>@classmethod\ndef load_from_checkpoint(\n    cls,\n    checkpoint_path: Union[_PATH, IO],\n    map_location: _MAP_LOCATION_TYPE = None,\n    hparams_file: Optional[_PATH] = None,\n    strict: bool = False,\n    load_baseline: bool = True,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Load model from checkpoint/\n\n    Note:\n        This is a modified version of `load_from_checkpoint` from `pytorch_lightning.core.saving`.\n        It deals with matching keys for the baseline by first running setup\n    \"\"\"\n\n    if strict:\n        log.warning(\"Setting strict=False for loading model from checkpoint.\")\n        strict = False\n\n    # Do not use strict\n    loaded = _load_from_checkpoint(\n        cls,\n        checkpoint_path,\n        map_location,\n        hparams_file,\n        strict,\n        **kwargs,\n    )\n\n    # Load baseline state dict\n    if load_baseline:\n        # setup baseline first\n        loaded.setup()\n        loaded.post_setup_hook()\n        # load baseline state dict\n        state_dict = torch.load(checkpoint_path, map_location=map_location)[\"state_dict\"]\n        # get only baseline parameters\n        state_dict = {k: v for k, v in state_dict.items() if \"baseline\" in k}\n        state_dict = {k.replace(\"baseline.\", \"\", 1): v for k, v in state_dict.items()}\n        loaded.baseline.load_state_dict(state_dict)\n\n    return cast(Self, loaded)\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.REINFORCEBaseline","title":"REINFORCEBaseline","text":"<pre><code>REINFORCEBaseline(*args, **kw)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base class for REINFORCE baselines</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def __init__(self, *args, **kw):\n    super().__init__()\n    pass\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.REINFORCEBaseline.wrap_dataset","title":"wrap_dataset","text":"<pre><code>wrap_dataset(dataset: Dataset, *args, **kw)\n</code></pre> <p>Wrap dataset with baseline-specific functionality</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def wrap_dataset(self, dataset: Dataset, *args, **kw):\n    \"\"\"Wrap dataset with baseline-specific functionality\"\"\"\n    return dataset\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.REINFORCEBaseline.eval","title":"eval  <code>abstractmethod</code>","text":"<pre><code>eval(\n    td: TensorDict,\n    reward: Tensor,\n    env: RL4COEnvBase = None,\n    **kwargs\n)\n</code></pre> <p>Evaluate baseline</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>@abc.abstractmethod\ndef eval(\n    self, td: TensorDict, reward: torch.Tensor, env: RL4COEnvBase = None, **kwargs\n):\n    \"\"\"Evaluate baseline\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.REINFORCEBaseline.epoch_callback","title":"epoch_callback","text":"<pre><code>epoch_callback(*args, **kw)\n</code></pre> <p>Callback at the end of each epoch For example, update baseline parameters and obtain baseline values</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def epoch_callback(self, *args, **kw):\n    \"\"\"Callback at the end of each epoch\n    For example, update baseline parameters and obtain baseline values\n    \"\"\"\n    pass\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.REINFORCEBaseline.setup","title":"setup","text":"<pre><code>setup(*args, **kw)\n</code></pre> <p>To be called before training during setup phase This follow PyTorch Lightning's setup() convention</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def setup(self, *args, **kw):\n    \"\"\"To be called before training during setup phase\n    This follow PyTorch Lightning's setup() convention\n    \"\"\"\n    pass\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.NoBaseline","title":"NoBaseline","text":"<pre><code>NoBaseline(*args, **kw)\n</code></pre> <p>               Bases: <code>REINFORCEBaseline</code></p> <p>No baseline: return 0 for baseline and neg_los</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def __init__(self, *args, **kw):\n    super().__init__()\n    pass\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.NoBaseline.eval","title":"eval","text":"<pre><code>eval(td, reward, env=None)\n</code></pre> <p>Evaluate baseline</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def eval(self, td, reward, env=None):\n    return 0, 0  # No baseline, no neg_los\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.SharedBaseline","title":"SharedBaseline","text":"<pre><code>SharedBaseline(*args, **kw)\n</code></pre> <p>               Bases: <code>REINFORCEBaseline</code></p> <p>Shared baseline: return mean of reward as baseline</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def __init__(self, *args, **kw):\n    super().__init__()\n    pass\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.SharedBaseline.eval","title":"eval","text":"<pre><code>eval(td, reward, env=None, on_dim=1)\n</code></pre> <p>Evaluate baseline</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def eval(self, td, reward, env=None, on_dim=1):  # e.g. [batch, pomo, ...]\n    return reward.mean(dim=on_dim, keepdims=True), 0\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.ExponentialBaseline","title":"ExponentialBaseline","text":"<pre><code>ExponentialBaseline(beta=0.8, **kw)\n</code></pre> <p>               Bases: <code>REINFORCEBaseline</code></p> <p>Exponential baseline: return exponential moving average of reward as baseline</p> <p>Parameters:</p> <ul> <li> <code>beta</code>           \u2013            <p>Beta value for the exponential moving average</p> </li> </ul> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def __init__(self, beta=0.8, **kw):\n    super(REINFORCEBaseline, self).__init__()\n\n    self.beta = beta\n    self.v = None\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.ExponentialBaseline.eval","title":"eval","text":"<pre><code>eval(td, reward, env=None)\n</code></pre> <p>Evaluate baseline</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def eval(self, td, reward, env=None):\n    if self.v is None:\n        v = reward.mean()\n    else:\n        v = self.beta * self.v + (1.0 - self.beta) * reward.mean()\n    self.v = v.detach()  # Detach since we never want to backprop\n    return self.v, 0  # No loss\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.MeanBaseline","title":"MeanBaseline","text":"<pre><code>MeanBaseline(*args, **kw)\n</code></pre> <p>               Bases: <code>REINFORCEBaseline</code></p> <p>Mean baseline: return mean of reward as baseline</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def __init__(self, *args, **kw):\n    super().__init__()\n    pass\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.WarmupBaseline","title":"WarmupBaseline","text":"<pre><code>WarmupBaseline(\n    baseline, n_epochs=1, warmup_exp_beta=0.8, **kw\n)\n</code></pre> <p>               Bases: <code>REINFORCEBaseline</code></p> <p>Warmup baseline: return convex combination of baseline and exponential baseline</p> <p>Parameters:</p> <ul> <li> <code>baseline</code>           \u2013            <p>Baseline to use after warmup</p> </li> <li> <code>n_epochs</code>           \u2013            <p>Number of epochs to warmup</p> </li> <li> <code>warmup_exp_beta</code>           \u2013            <p>Beta value for the exponential baseline during warmup</p> </li> </ul> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def __init__(self, baseline, n_epochs=1, warmup_exp_beta=0.8, **kw):\n    super(REINFORCEBaseline, self).__init__()\n\n    self.baseline = baseline\n    assert n_epochs &gt; 0, \"n_epochs to warmup must be positive\"\n    self.warmup_baseline = ExponentialBaseline(warmup_exp_beta)\n    self.alpha = 0\n    self.n_epochs = n_epochs\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.WarmupBaseline.wrap_dataset","title":"wrap_dataset","text":"<pre><code>wrap_dataset(dataset, *args, **kw)\n</code></pre> <p>Wrap dataset with baseline-specific functionality</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def wrap_dataset(self, dataset, *args, **kw):\n    if self.alpha &gt; 0:\n        return self.baseline.wrap_dataset(dataset, *args, **kw)\n    return self.warmup_baseline.wrap_dataset(dataset, *args, **kw)\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.WarmupBaseline.setup","title":"setup","text":"<pre><code>setup(*args, **kw)\n</code></pre> <p>To be called before training during setup phase This follow PyTorch Lightning's setup() convention</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def setup(self, *args, **kw):\n    self.baseline.setup(*args, **kw)\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.WarmupBaseline.eval","title":"eval","text":"<pre><code>eval(td, reward, env=None)\n</code></pre> <p>Evaluate baseline</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def eval(self, td, reward, env=None):\n    if self.alpha == 1:\n        return self.baseline.eval(td, reward, env)\n    if self.alpha == 0:\n        return self.warmup_baseline.eval(td, reward, env)\n    v_b, l_b = self.baseline.eval(td, reward, env)\n    v_wb, l_wb = self.warmup_baseline.eval(td, reward, env)\n    # Return convex combination of baseline and of loss\n    return (\n        self.alpha * v_b + (1 - self.alpha) * v_wb,\n        self.alpha * l_b + (1 - self.alpha) * l_wb,\n    )\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.WarmupBaseline.epoch_callback","title":"epoch_callback","text":"<pre><code>epoch_callback(*args, **kw)\n</code></pre> <p>Callback at the end of each epoch For example, update baseline parameters and obtain baseline values</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def epoch_callback(self, *args, **kw):\n    # Need to call epoch callback of inner policy (also after first epoch if we have not used it)\n    self.baseline.epoch_callback(*args, **kw)\n    if kw[\"epoch\"] &lt; self.n_epochs:\n        self.alpha = (kw[\"epoch\"] + 1) / float(self.n_epochs)\n        log.info(\"Set warmup alpha = {}\".format(self.alpha))\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.CriticBaseline","title":"CriticBaseline","text":"<pre><code>CriticBaseline(critic: CriticNetwork = None, **unused_kw)\n</code></pre> <p>               Bases: <code>REINFORCEBaseline</code></p> <p>Critic baseline: use critic network as baseline</p> <p>Parameters:</p> <ul> <li> <code>critic</code>               (<code>CriticNetwork</code>, default:                   <code>None</code> )           \u2013            <p>Critic network to use as baseline. If None, create a new critic network based on the environment</p> </li> </ul> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def __init__(self, critic: CriticNetwork = None, **unused_kw):\n    super(CriticBaseline, self).__init__()\n    self.critic = critic\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.CriticBaseline.setup","title":"setup","text":"<pre><code>setup(policy, env, **kwargs)\n</code></pre> <p>To be called before training during setup phase This follow PyTorch Lightning's setup() convention</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def setup(self, policy, env, **kwargs):\n    if self.critic is None:\n        log.info(\"Critic not found. Creating critic network for {}\".format(env.name))\n        self.critic = create_critic_from_actor(policy)\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.CriticBaseline.eval","title":"eval","text":"<pre><code>eval(x, c, env=None)\n</code></pre> <p>Evaluate baseline</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def eval(self, x, c, env=None):\n    v = self.critic(x).squeeze(-1)\n    # detach v since actor should not backprop through baseline, only for loss\n    return v.detach(), F.mse_loss(v, c.detach())\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.RolloutBaseline","title":"RolloutBaseline","text":"<pre><code>RolloutBaseline(bl_alpha=0.05, **kw)\n</code></pre> <p>               Bases: <code>REINFORCEBaseline</code></p> <p>Rollout baseline: use greedy rollout as baseline</p> <p>Parameters:</p> <ul> <li> <code>bl_alpha</code>           \u2013            <p>Alpha value for the baseline T-test</p> </li> </ul> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def __init__(self, bl_alpha=0.05, **kw):\n    super(RolloutBaseline, self).__init__()\n    self.bl_alpha = bl_alpha\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.RolloutBaseline.setup","title":"setup","text":"<pre><code>setup(*args, **kw)\n</code></pre> <p>To be called before training during setup phase This follow PyTorch Lightning's setup() convention</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def setup(self, *args, **kw):\n    self._update_policy(*args, **kw)\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.RolloutBaseline.eval","title":"eval","text":"<pre><code>eval(td, reward, env)\n</code></pre> <p>Evaluate rollout baseline</p> Warning <p>This is not differentiable and should only be used for evaluation. Also, it is recommended to use the <code>rollout</code> method directly instead of this method.</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def eval(self, td, reward, env):\n    \"\"\"Evaluate rollout baseline\n\n    Warning:\n        This is not differentiable and should only be used for evaluation.\n        Also, it is recommended to use the `rollout` method directly instead of this method.\n    \"\"\"\n    with torch.inference_mode():\n        reward = self.policy(td, env)[\"reward\"]\n    return reward, 0\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.RolloutBaseline.epoch_callback","title":"epoch_callback","text":"<pre><code>epoch_callback(\n    policy,\n    env,\n    batch_size=64,\n    device=\"cpu\",\n    epoch=None,\n    dataset_size=None,\n)\n</code></pre> <p>Challenges the current baseline with the policy and replaces the baseline policy if it is improved</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def epoch_callback(\n    self, policy, env, batch_size=64, device=\"cpu\", epoch=None, dataset_size=None\n):\n    \"\"\"Challenges the current baseline with the policy and replaces the baseline policy if it is improved\"\"\"\n    log.info(\"Evaluating candidate policy on evaluation dataset\")\n    candidate_vals = self.rollout(policy, env, batch_size, device).cpu().numpy()\n    candidate_mean = candidate_vals.mean()\n\n    log.info(\n        \"Candidate mean: {:.3f}, Baseline mean: {:.3f}\".format(\n            candidate_mean, self.mean\n        )\n    )\n    if candidate_mean - self.mean &gt; 0:\n        # Calc p value with inverse logic (costs)\n        t, p = ttest_rel(-candidate_vals, -self.bl_vals)\n\n        p_val = p / 2  # one-sided\n        assert t &lt; 0, \"T-statistic should be negative\"\n        log.info(\"p-value: {:.3f}\".format(p_val))\n        if p_val &lt; self.bl_alpha:\n            log.info(\"Updating baseline\")\n            self._update_policy(policy, env, batch_size, device, dataset_size)\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.RolloutBaseline.rollout","title":"rollout","text":"<pre><code>rollout(\n    policy, env, batch_size=64, device=\"cpu\", dataset=None\n)\n</code></pre> <p>Rollout the policy on the given dataset</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def rollout(self, policy, env, batch_size=64, device=\"cpu\", dataset=None):\n    \"\"\"Rollout the policy on the given dataset\"\"\"\n\n    # if dataset is None, use the dataset of the baseline\n    dataset = self.dataset if dataset is None else dataset\n\n    policy.eval()\n    policy = policy.to(device)\n\n    def eval_policy(batch):\n        with torch.inference_mode():\n            batch = env.reset(batch.to(device))\n            return policy(batch, env, decode_type=\"greedy\")[\"reward\"]\n\n    dl = DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn)\n\n    rewards = torch.cat([eval_policy(batch) for batch in dl], 0)\n    return rewards\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.RolloutBaseline.wrap_dataset","title":"wrap_dataset","text":"<pre><code>wrap_dataset(\n    dataset, env, batch_size=64, device=\"cpu\", **kw\n)\n</code></pre> <p>Wrap the dataset in a baseline dataset</p> Note <p>This is an alternative to <code>eval</code> that does not require the policy to be passed at every call but just once. Values are added to the dataset. This also allows for larger batch sizes since we evauate the policy without gradients.</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def wrap_dataset(self, dataset, env, batch_size=64, device=\"cpu\", **kw):\n    \"\"\"Wrap the dataset in a baseline dataset\n\n    Note:\n        This is an alternative to `eval` that does not require the policy to be passed\n        at every call but just once. Values are added to the dataset. This also allows for\n        larger batch sizes since we evauate the policy without gradients.\n    \"\"\"\n    rewards = (\n        self.rollout(self.policy, env, batch_size, device, dataset=dataset)\n        .detach()\n        .cpu()\n    )\n    return dataset.add_key(\"extra\", rewards)\n</code></pre>"},{"location":"docs/content/api/rl/reinforce/#models.rl.reinforce.baselines.get_reinforce_baseline","title":"get_reinforce_baseline","text":"<pre><code>get_reinforce_baseline(name, **kw)\n</code></pre> <p>Get a REINFORCE baseline by name The rollout baseline default to warmup baseline with one epoch of exponential baseline and the greedy rollout</p> Source code in <code>rl4co/models/rl/reinforce/baselines.py</code> <pre><code>def get_reinforce_baseline(name, **kw):\n    \"\"\"Get a REINFORCE baseline by name\n    The rollout baseline default to warmup baseline with one epoch of\n    exponential baseline and the greedy rollout\n    \"\"\"\n    if name == \"warmup\":\n        inner_baseline = kw.get(\"baseline\", \"rollout\")\n        if not isinstance(inner_baseline, REINFORCEBaseline):\n            inner_baseline = get_reinforce_baseline(inner_baseline, **kw)\n        return WarmupBaseline(inner_baseline, **kw)\n    elif name == \"rollout\":\n        warmup_epochs = kw.get(\"n_epochs\", 1)\n        warmup_exp_beta = kw.get(\"exp_beta\", 0.8)\n        bl_alpha = kw.get(\"bl_alpha\", 0.05)\n        return WarmupBaseline(\n            RolloutBaseline(bl_alpha=bl_alpha), warmup_epochs, warmup_exp_beta\n        )\n\n    if name is None:\n        name = \"no\"  # default to no baseline\n    baseline_cls = REINFORCE_BASELINES_REGISTRY.get(name, None)\n    if baseline_cls is None:\n        raise ValueError(\n            f\"Unknown baseline {baseline_cls}. Available baselines: {REINFORCE_BASELINES_REGISTRY.keys()}\"\n        )\n    return baseline_cls(**kw)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/","title":"Constructive Autoregressive Methods","text":""},{"location":"docs/content/api/zoo/constructive_ar/#attention-model-am","title":"Attention Model (AM)","text":""},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.am.model.AttentionModel","title":"AttentionModel","text":"<pre><code>AttentionModel(\n    env: RL4COEnvBase,\n    policy: AttentionModelPolicy = None,\n    baseline: Union[REINFORCEBaseline, str] = \"rollout\",\n    policy_kwargs={},\n    baseline_kwargs={},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>REINFORCE</code></p> <p>Attention Model based on REINFORCE: https://arxiv.org/abs/1803.08475. Check :class:<code>REINFORCE</code> and :class:<code>rl4co.models.RL4COLitModule</code> for more details such as additional parameters  including batch size.</p> <p>Parameters:</p> <ul> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>Environment to use for the algorithm</p> </li> <li> <code>policy</code>               (<code>AttentionModelPolicy</code>, default:                   <code>None</code> )           \u2013            <p>Policy to use for the algorithm</p> </li> <li> <code>baseline</code>               (<code>Union[REINFORCEBaseline, str]</code>, default:                   <code>'rollout'</code> )           \u2013            <p>REINFORCE baseline. Defaults to rollout (1 epoch of exponential, then greedy rollout baseline)</p> </li> <li> <code>policy_kwargs</code>           \u2013            <p>Keyword arguments for policy</p> </li> <li> <code>baseline_kwargs</code>           \u2013            <p>Keyword arguments for baseline</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Keyword arguments passed to the superclass</p> </li> </ul> Source code in <code>rl4co/models/zoo/am/model.py</code> <pre><code>def __init__(\n    self,\n    env: RL4COEnvBase,\n    policy: AttentionModelPolicy = None,\n    baseline: Union[REINFORCEBaseline, str] = \"rollout\",\n    policy_kwargs={},\n    baseline_kwargs={},\n    **kwargs,\n):\n    if policy is None:\n        policy = AttentionModelPolicy(env_name=env.name, **policy_kwargs)\n\n    super().__init__(env, policy, baseline, baseline_kwargs, **kwargs)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.am.policy.AttentionModelPolicy","title":"AttentionModelPolicy","text":"<pre><code>AttentionModelPolicy(\n    encoder: Module = None,\n    decoder: Module = None,\n    embed_dim: int = 128,\n    num_encoder_layers: int = 3,\n    num_heads: int = 8,\n    normalization: str = \"batch\",\n    feedforward_hidden: int = 512,\n    env_name: str = \"tsp\",\n    encoder_network: Module = None,\n    init_embedding: Module = None,\n    context_embedding: Module = None,\n    dynamic_embedding: Module = None,\n    use_graph_context: bool = True,\n    linear_bias_decoder: bool = False,\n    sdpa_fn: Callable = None,\n    sdpa_fn_encoder: Callable = None,\n    sdpa_fn_decoder: Callable = None,\n    mask_inner: bool = True,\n    out_bias_pointer_attn: bool = False,\n    check_nan: bool = True,\n    temperature: float = 1.0,\n    tanh_clipping: float = 10.0,\n    mask_logits: bool = True,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"greedy\",\n    test_decode_type: str = \"greedy\",\n    moe_kwargs: dict = {\"encoder\": None, \"decoder\": None},\n    **unused_kwargs\n)\n</code></pre> <p>               Bases: <code>AutoregressivePolicy</code></p> <p>Attention Model Policy based on Kool et al. (2019): https://arxiv.org/abs/1803.08475. This model first encodes the input graph using a Graph Attention Network (GAT) (:class:<code>AttentionModelEncoder</code>) and then decodes the solution using a pointer network (:class:<code>AttentionModelDecoder</code>). Cache is used to store the embeddings of the nodes to be used by the decoder to save computation. See :class:<code>rl4co.models.common.constructive.autoregressive.policy.AutoregressivePolicy</code> for more details on the inference process.</p> <p>Parameters:</p> <ul> <li> <code>encoder</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Encoder module, defaults to :class:<code>AttentionModelEncoder</code></p> </li> <li> <code>decoder</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Decoder module, defaults to :class:<code>AttentionModelDecoder</code></p> </li> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Dimension of the node embeddings</p> </li> <li> <code>num_encoder_layers</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of layers in the encoder</p> </li> <li> <code>num_heads</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of heads in the attention layers</p> </li> <li> <code>normalization</code>               (<code>str</code>, default:                   <code>'batch'</code> )           \u2013            <p>Normalization type in the attention layers</p> </li> <li> <code>feedforward_hidden</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Dimension of the hidden layer in the feedforward network</p> </li> <li> <code>env_name</code>               (<code>str</code>, default:                   <code>'tsp'</code> )           \u2013            <p>Name of the environment used to initialize embeddings</p> </li> <li> <code>encoder_network</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Network to use for the encoder</p> </li> <li> <code>init_embedding</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Module to use for the initialization of the embeddings</p> </li> <li> <code>context_embedding</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Module to use for the context embedding</p> </li> <li> <code>dynamic_embedding</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Module to use for the dynamic embedding</p> </li> <li> <code>use_graph_context</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the graph context</p> </li> <li> <code>linear_bias_decoder</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a bias in the linear layer of the decoder</p> </li> <li> <code>sdpa_fn_encoder</code>               (<code>Callable</code>, default:                   <code>None</code> )           \u2013            <p>Function to use for the scaled dot product attention in the encoder</p> </li> <li> <code>sdpa_fn_decoder</code>               (<code>Callable</code>, default:                   <code>None</code> )           \u2013            <p>Function to use for the scaled dot product attention in the decoder</p> </li> <li> <code>sdpa_fn</code>               (<code>Callable</code>, default:                   <code>None</code> )           \u2013            <p>(deprecated) Function to use for the scaled dot product attention</p> </li> <li> <code>mask_inner</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to mask the inner product</p> </li> <li> <code>out_bias_pointer_attn</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a bias in the pointer attention</p> </li> <li> <code>check_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to check for nan values during decoding</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Temperature for the softmax</p> </li> <li> <code>tanh_clipping</code>               (<code>float</code>, default:                   <code>10.0</code> )           \u2013            <p>Tanh clipping value (see Bello et al., 2016)</p> </li> <li> <code>mask_logits</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to mask the logits during decoding</p> </li> <li> <code>train_decode_type</code>               (<code>str</code>, default:                   <code>'sampling'</code> )           \u2013            <p>Type of decoding to use during training</p> </li> <li> <code>val_decode_type</code>               (<code>str</code>, default:                   <code>'greedy'</code> )           \u2013            <p>Type of decoding to use during validation</p> </li> <li> <code>test_decode_type</code>               (<code>str</code>, default:                   <code>'greedy'</code> )           \u2013            <p>Type of decoding to use during testing</p> </li> <li> <code>moe_kwargs</code>               (<code>dict</code>, default:                   <code>{'encoder': None, 'decoder': None}</code> )           \u2013            <p>Keyword arguments for MoE, e.g., {\"encoder\": {\"hidden_act\": \"ReLU\", \"num_experts\": 4, \"k\": 2, \"noisy_gating\": True},        \"decoder\": {\"light_version\": True, ...}}</p> </li> </ul> Source code in <code>rl4co/models/zoo/am/policy.py</code> <pre><code>def __init__(\n    self,\n    encoder: nn.Module = None,\n    decoder: nn.Module = None,\n    embed_dim: int = 128,\n    num_encoder_layers: int = 3,\n    num_heads: int = 8,\n    normalization: str = \"batch\",\n    feedforward_hidden: int = 512,\n    env_name: str = \"tsp\",\n    encoder_network: nn.Module = None,\n    init_embedding: nn.Module = None,\n    context_embedding: nn.Module = None,\n    dynamic_embedding: nn.Module = None,\n    use_graph_context: bool = True,\n    linear_bias_decoder: bool = False,\n    sdpa_fn: Callable = None,\n    sdpa_fn_encoder: Callable = None,\n    sdpa_fn_decoder: Callable = None,\n    mask_inner: bool = True,\n    out_bias_pointer_attn: bool = False,\n    check_nan: bool = True,\n    temperature: float = 1.0,\n    tanh_clipping: float = 10.0,\n    mask_logits: bool = True,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"greedy\",\n    test_decode_type: str = \"greedy\",\n    moe_kwargs: dict = {\"encoder\": None, \"decoder\": None},\n    **unused_kwargs,\n):\n    if encoder is None:\n        encoder = AttentionModelEncoder(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            num_layers=num_encoder_layers,\n            env_name=env_name,\n            normalization=normalization,\n            feedforward_hidden=feedforward_hidden,\n            net=encoder_network,\n            init_embedding=init_embedding,\n            sdpa_fn=sdpa_fn if sdpa_fn_encoder is None else sdpa_fn_encoder,\n            moe_kwargs=moe_kwargs[\"encoder\"],\n        )\n\n    if decoder is None:\n        decoder = AttentionModelDecoder(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            env_name=env_name,\n            context_embedding=context_embedding,\n            dynamic_embedding=dynamic_embedding,\n            sdpa_fn=sdpa_fn if sdpa_fn_decoder is None else sdpa_fn_decoder,\n            mask_inner=mask_inner,\n            out_bias_pointer_attn=out_bias_pointer_attn,\n            linear_bias=linear_bias_decoder,\n            use_graph_context=use_graph_context,\n            check_nan=check_nan,\n            moe_kwargs=moe_kwargs[\"decoder\"],\n        )\n\n    super(AttentionModelPolicy, self).__init__(\n        encoder=encoder,\n        decoder=decoder,\n        env_name=env_name,\n        temperature=temperature,\n        tanh_clipping=tanh_clipping,\n        mask_logits=mask_logits,\n        train_decode_type=train_decode_type,\n        val_decode_type=val_decode_type,\n        test_decode_type=test_decode_type,\n        **unused_kwargs,\n    )\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#attention-model-ppo-am-ppo","title":"Attention Model - PPO (AM-PPO)","text":""},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.amppo.model.AMPPO","title":"AMPPO","text":"<pre><code>AMPPO(\n    env: RL4COEnvBase,\n    policy: Module = None,\n    critic: CriticNetwork = None,\n    policy_kwargs: dict = {},\n    critic_kwargs: dict = {},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>PPO</code></p> <p>PPO Model based on Proximal Policy Optimization (PPO) with an attention model policy. We default to the attention model policy and the Attention Critic Network.</p> <p>Parameters:</p> <ul> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>Environment to use for the algorithm</p> </li> <li> <code>policy</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Policy to use for the algorithm</p> </li> <li> <code>critic</code>               (<code>CriticNetwork</code>, default:                   <code>None</code> )           \u2013            <p>Critic to use for the algorithm</p> </li> <li> <code>policy_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments for policy</p> </li> <li> <code>critic_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments for critic</p> </li> </ul> Source code in <code>rl4co/models/zoo/amppo/model.py</code> <pre><code>def __init__(\n    self,\n    env: RL4COEnvBase,\n    policy: nn.Module = None,\n    critic: CriticNetwork = None,\n    policy_kwargs: dict = {},\n    critic_kwargs: dict = {},\n    **kwargs,\n):\n    if policy is None:\n        policy = AttentionModelPolicy(env_name=env.name, **policy_kwargs)\n\n    if critic is None:\n        log.info(\"Creating critic network for {}\".format(env.name))\n        # we reuse the parameters of the model\n        encoder = getattr(policy, \"encoder\", None)\n        if encoder is None:\n            raise ValueError(\"Critic network requires an encoder\")\n        critic = CriticNetwork(\n            copy.deepcopy(encoder).to(next(encoder.parameters()).device),\n            **critic_kwargs,\n        )\n\n    super().__init__(env, policy, critic, **kwargs)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#heterogeneous-attention-model-ham","title":"Heterogeneous Attention Model (HAM)","text":""},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.ham.model.HeterogeneousAttentionModel","title":"HeterogeneousAttentionModel","text":"<pre><code>HeterogeneousAttentionModel(\n    env: RL4COEnvBase,\n    policy: HeterogeneousAttentionModelPolicy = None,\n    baseline: Union[REINFORCEBaseline, str] = \"rollout\",\n    policy_kwargs={},\n    baseline_kwargs={},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>REINFORCE</code></p> <p>Heterogenous Attention Model for solving the Pickup and Delivery Problem based on REINFORCE: https://arxiv.org/abs/2110.02634.</p> <p>Parameters:</p> <ul> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>Environment to use for the algorithm</p> </li> <li> <code>policy</code>               (<code>HeterogeneousAttentionModelPolicy</code>, default:                   <code>None</code> )           \u2013            <p>Policy to use for the algorithm</p> </li> <li> <code>baseline</code>               (<code>Union[REINFORCEBaseline, str]</code>, default:                   <code>'rollout'</code> )           \u2013            <p>REINFORCE baseline. Defaults to rollout (1 epoch of exponential, then greedy rollout baseline)</p> </li> <li> <code>policy_kwargs</code>           \u2013            <p>Keyword arguments for policy</p> </li> <li> <code>baseline_kwargs</code>           \u2013            <p>Keyword arguments for baseline</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Keyword arguments passed to the superclass</p> </li> </ul> Source code in <code>rl4co/models/zoo/ham/model.py</code> <pre><code>def __init__(\n    self,\n    env: RL4COEnvBase,\n    policy: HeterogeneousAttentionModelPolicy = None,\n    baseline: Union[REINFORCEBaseline, str] = \"rollout\",\n    policy_kwargs={},\n    baseline_kwargs={},\n    **kwargs,\n):\n    assert (\n        env.name == \"pdp\"\n    ), \"HeterogeneousAttentionModel only works for PDP (Pickup and Delivery Problem)\"\n    if policy is None:\n        policy = HeterogeneousAttentionModelPolicy(env_name=env.name, **policy_kwargs)\n\n    super().__init__(env, policy, baseline, baseline_kwargs, **kwargs)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.ham.policy.HeterogeneousAttentionModelPolicy","title":"HeterogeneousAttentionModelPolicy","text":"<pre><code>HeterogeneousAttentionModelPolicy(\n    encoder: Module = None,\n    env_name: str = \"pdp\",\n    init_embedding: Module = None,\n    embed_dim: int = 128,\n    num_encoder_layers: int = 3,\n    num_heads: int = 8,\n    normalization: str = \"batch\",\n    feedforward_hidden: int = 512,\n    sdpa_fn: Optional[Callable] = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>AttentionModelPolicy</code></p> <p>Heterogeneous Attention Model Policy based on https://ieeexplore.ieee.org/document/9352489. We re-declare the most important arguments here for convenience as in the paper. See :class:<code>rl4co.models.zoo.am.AttentionModelPolicy</code> for more details.</p> <p>Parameters:</p> <ul> <li> <code>encoder</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Encoder module. Can be passed by sub-classes</p> </li> <li> <code>env_name</code>               (<code>str</code>, default:                   <code>'pdp'</code> )           \u2013            <p>Name of the environment used to initialize embeddings</p> </li> <li> <code>init_embedding</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Model to use for the initial embedding. If None, use the default embedding for the environment</p> </li> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Dimension of the embeddings</p> </li> <li> <code>num_encoder_layers</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of layers in the encoder</p> </li> <li> <code>num_heads</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of heads for the attention in encoder</p> </li> <li> <code>normalization</code>               (<code>str</code>, default:                   <code>'batch'</code> )           \u2013            <p>Normalization to use for the attention layers</p> </li> <li> <code>feedforward_hidden</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Dimension of the hidden layer in the feedforward network</p> </li> <li> <code>sdpa_fn</code>               (<code>Optional[Callable]</code>, default:                   <code>None</code> )           \u2013            <p>Function to use for the scaled dot product attention</p> </li> <li> <code>**kwargs</code>           \u2013            <p>keyword arguments passed to the :class:<code>rl4co.models.zoo.am.AttentionModelPolicy</code></p> </li> </ul> Source code in <code>rl4co/models/zoo/ham/policy.py</code> <pre><code>def __init__(\n    self,\n    encoder: nn.Module = None,\n    env_name: str = \"pdp\",\n    init_embedding: nn.Module = None,\n    embed_dim: int = 128,\n    num_encoder_layers: int = 3,\n    num_heads: int = 8,\n    normalization: str = \"batch\",\n    feedforward_hidden: int = 512,\n    sdpa_fn: Optional[Callable] = None,\n    **kwargs,\n):\n    if encoder is None:\n        encoder = GraphHeterogeneousAttentionEncoder(\n            init_embedding=init_embedding,\n            num_heads=num_heads,\n            embed_dim=embed_dim,\n            num_encoder_layers=num_encoder_layers,\n            env_name=env_name,\n            normalization=normalization,\n            feedforward_hidden=feedforward_hidden,\n            sdpa_fn=sdpa_fn,\n        )\n    else:\n        encoder = encoder\n\n    super(HeterogeneousAttentionModelPolicy, self).__init__(\n        env_name=env_name,\n        encoder=encoder,\n        embed_dim=embed_dim,\n        num_encoder_layers=num_encoder_layers,\n        num_heads=num_heads,\n        normalization=normalization,\n        **kwargs,\n    )\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.ham.attention.HeterogenousMHA","title":"HeterogenousMHA","text":"<pre><code>HeterogenousMHA(\n    num_heads,\n    input_dim,\n    embed_dim=None,\n    val_dim=None,\n    key_dim=None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>rl4co/models/zoo/ham/attention.py</code> <pre><code>def __init__(self, num_heads, input_dim, embed_dim=None, val_dim=None, key_dim=None):\n    \"\"\"\n    Heterogenous Multi-Head Attention for Pickup and Delivery problems\n    https://arxiv.org/abs/2110.02634\n    \"\"\"\n    super(HeterogenousMHA, self).__init__()\n\n    if val_dim is None:\n        assert embed_dim is not None, \"Provide either embed_dim or val_dim\"\n        val_dim = embed_dim // num_heads\n    if key_dim is None:\n        key_dim = val_dim\n\n    self.num_heads = num_heads\n    self.input_dim = input_dim\n    self.embed_dim = embed_dim\n    self.val_dim = val_dim\n    self.key_dim = key_dim\n\n    self.norm_factor = 1 / math.sqrt(key_dim)  # See Attention is all you need\n\n    self.W_query = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n    self.W_key = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n    self.W_val = nn.Parameter(torch.Tensor(num_heads, input_dim, val_dim))\n\n    # Pickup weights\n    self.W1_query = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n    self.W2_query = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n    self.W3_query = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n\n    # Delivery weights\n    self.W4_query = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n    self.W5_query = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n    self.W6_query = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n\n    if embed_dim is not None:\n        self.W_out = nn.Parameter(torch.Tensor(num_heads, key_dim, embed_dim))\n\n    self.init_parameters()\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.ham.attention.HeterogenousMHA.forward","title":"forward","text":"<pre><code>forward(q, h=None, mask=None)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>q</code>           \u2013            <p>queries (batch_size, n_query, input_dim)</p> </li> <li> <code>h</code>           \u2013            <p>data (batch_size, graph_size, input_dim)</p> </li> <li> <code>mask</code>           \u2013            <p>mask (batch_size, n_query, graph_size) or viewable as that (i.e. can be 2 dim if n_query == 1)</p> </li> </ul> <p>Mask should contain 1 if attention is not possible (i.e. mask is negative adjacency)</p> Source code in <code>rl4co/models/zoo/ham/attention.py</code> <pre><code>def forward(self, q, h=None, mask=None):\n    \"\"\"\n    Args:\n        q: queries (batch_size, n_query, input_dim)\n        h: data (batch_size, graph_size, input_dim)\n        mask: mask (batch_size, n_query, graph_size) or viewable as that (i.e. can be 2 dim if n_query == 1)\n\n    Mask should contain 1 if attention is not possible (i.e. mask is negative adjacency)\n    \"\"\"\n    if h is None:\n        h = q  # compute self-attention\n\n    # h should be (batch_size, graph_size, input_dim)\n    batch_size, graph_size, input_dim = h.size()\n\n    # Check if graph size is odd number\n    assert (\n        graph_size % 2 == 1\n    ), \"Graph size should have odd number of nodes due to pickup-delivery problem  \\\n                                 (n/2 pickup, n/2 delivery, 1 depot)\"\n\n    n_query = q.size(1)\n    assert q.size(0) == batch_size\n    assert q.size(2) == input_dim\n    assert input_dim == self.input_dim, \"Wrong embedding dimension of input\"\n\n    hflat = h.contiguous().view(-1, input_dim)  # [batch_size * graph_size, embed_dim]\n    qflat = q.contiguous().view(-1, input_dim)  # [batch_size * n_query, embed_dim]\n\n    # last dimension can be different for keys and values\n    shp = (self.num_heads, batch_size, graph_size, -1)\n    shp_q = (self.num_heads, batch_size, n_query, -1)\n\n    # pickup -&gt; its delivery attention\n    n_pick = (graph_size - 1) // 2\n    shp_delivery = (self.num_heads, batch_size, n_pick, -1)\n    shp_q_pick = (self.num_heads, batch_size, n_pick, -1)\n\n    # pickup -&gt; all pickups attention\n    shp_allpick = (self.num_heads, batch_size, n_pick, -1)\n    shp_q_allpick = (self.num_heads, batch_size, n_pick, -1)\n\n    # pickup -&gt; all pickups attention\n    shp_alldelivery = (self.num_heads, batch_size, n_pick, -1)\n    shp_q_alldelivery = (self.num_heads, batch_size, n_pick, -1)\n\n    # Calculate queries, (num_heads, n_query, graph_size, key/val_size)\n    Q = torch.matmul(qflat, self.W_query).view(shp_q)\n    # Calculate keys and values (num_heads, batch_size, graph_size, key/val_size)\n    K = torch.matmul(hflat, self.W_key).view(shp)\n    V = torch.matmul(hflat, self.W_val).view(shp)\n\n    # pickup -&gt; its delivery\n    pick_flat = (\n        h[:, 1 : n_pick + 1, :].contiguous().view(-1, input_dim)\n    )  # [batch_size * n_pick, embed_dim]\n    delivery_flat = (\n        h[:, n_pick + 1 :, :].contiguous().view(-1, input_dim)\n    )  # [batch_size * n_pick, embed_dim]\n\n    # pickup -&gt; its delivery attention\n    Q_pick = torch.matmul(pick_flat, self.W1_query).view(\n        shp_q_pick\n    )  # (self.num_heads, batch_size, n_pick, key_size)\n    K_delivery = torch.matmul(delivery_flat, self.W_key).view(\n        shp_delivery\n    )  # (self.num_heads, batch_size, n_pick, -1)\n    V_delivery = torch.matmul(delivery_flat, self.W_val).view(\n        shp_delivery\n    )  # (num_heads, batch_size, n_pick, key/val_size)\n\n    # pickup -&gt; all pickups attention\n    Q_pick_allpick = torch.matmul(pick_flat, self.W2_query).view(\n        shp_q_allpick\n    )  # (self.num_heads, batch_size, n_pick, -1)\n    K_allpick = torch.matmul(pick_flat, self.W_key).view(\n        shp_allpick\n    )  # [self.num_heads, batch_size, n_pick, key_size]\n    V_allpick = torch.matmul(pick_flat, self.W_val).view(\n        shp_allpick\n    )  # [self.num_heads, batch_size, n_pick, key_size]\n\n    # pickup -&gt; all delivery\n    Q_pick_alldelivery = torch.matmul(pick_flat, self.W3_query).view(\n        shp_q_alldelivery\n    )  # (self.num_heads, batch_size, n_pick, key_size)\n    K_alldelivery = torch.matmul(delivery_flat, self.W_key).view(\n        shp_alldelivery\n    )  # (self.num_heads, batch_size, n_pick, -1)\n    V_alldelivery = torch.matmul(delivery_flat, self.W_val).view(\n        shp_alldelivery\n    )  # (num_heads, batch_size, n_pick, key/val_size)\n\n    # pickup -&gt; its delivery\n    V_additional_delivery = torch.cat(\n        [  # [num_heads, batch_size, graph_size, key_size]\n            torch.zeros(\n                self.num_heads,\n                batch_size,\n                1,\n                self.input_dim // self.num_heads,\n                dtype=V.dtype,\n                device=V.device,\n            ),\n            V_delivery,  # [num_heads, batch_size, n_pick, key/val_size]\n            torch.zeros(\n                self.num_heads,\n                batch_size,\n                n_pick,\n                self.input_dim // self.num_heads,\n                dtype=V.dtype,\n                device=V.device,\n            ),\n        ],\n        2,\n    )\n\n    # delivery -&gt; its pickup attention\n    Q_delivery = torch.matmul(delivery_flat, self.W4_query).view(\n        shp_delivery\n    )  # (self.num_heads, batch_size, n_pick, key_size)\n    K_pick = torch.matmul(pick_flat, self.W_key).view(\n        shp_q_pick\n    )  # (self.num_heads, batch_size, n_pick, -1)\n    V_pick = torch.matmul(pick_flat, self.W_val).view(\n        shp_q_pick\n    )  # (num_heads, batch_size, n_pick, key/val_size)\n\n    # delivery -&gt; all delivery attention\n    Q_delivery_alldelivery = torch.matmul(delivery_flat, self.W5_query).view(\n        shp_alldelivery\n    )  # (self.num_heads, batch_size, n_pick, -1)\n    K_alldelivery2 = torch.matmul(delivery_flat, self.W_key).view(\n        shp_alldelivery\n    )  # [self.num_heads, batch_size, n_pick, key_size]\n    V_alldelivery2 = torch.matmul(delivery_flat, self.W_val).view(\n        shp_alldelivery\n    )  # [self.num_heads, batch_size, n_pick, key_size]\n\n    # delivery -&gt; all pickup\n    Q_delivery_allpickup = torch.matmul(delivery_flat, self.W6_query).view(\n        shp_alldelivery\n    )  # (self.num_heads, batch_size, n_pick, key_size)\n    K_allpickup2 = torch.matmul(pick_flat, self.W_key).view(\n        shp_q_alldelivery\n    )  # (self.num_heads, batch_size, n_pick, -1)\n    V_allpickup2 = torch.matmul(pick_flat, self.W_val).view(\n        shp_q_alldelivery\n    )  # (num_heads, batch_size, n_pick, key/val_size)\n\n    # delivery -&gt; its pick up\n    V_additional_pick = torch.cat(\n        [  # [num_heads, batch_size, graph_size, key_size]\n            torch.zeros(\n                self.num_heads,\n                batch_size,\n                1,\n                self.input_dim // self.num_heads,\n                dtype=V.dtype,\n                device=V.device,\n            ),\n            torch.zeros(\n                self.num_heads,\n                batch_size,\n                n_pick,\n                self.input_dim // self.num_heads,\n                dtype=V.dtype,\n                device=V.device,\n            ),\n            V_pick,  # [num_heads, batch_size, n_pick, key/val_size]\n        ],\n        2,\n    )\n\n    # Calculate compatibility (num_heads, batch_size, n_query, graph_size)\n    compatibility = self.norm_factor * torch.matmul(Q, K.transpose(2, 3))\n\n    ##Pick up pair attention\n    compatibility_pick_delivery = self.norm_factor * torch.sum(\n        Q_pick * K_delivery, -1\n    )  # element_wise, [num_heads, batch_size, n_pick]\n    # [num_heads, batch_size, n_pick, n_pick]\n    compatibility_pick_allpick = self.norm_factor * torch.matmul(\n        Q_pick_allpick, K_allpick.transpose(2, 3)\n    )  # [num_heads, batch_size, n_pick, n_pick]\n    compatibility_pick_alldelivery = self.norm_factor * torch.matmul(\n        Q_pick_alldelivery, K_alldelivery.transpose(2, 3)\n    )  # [num_heads, batch_size, n_pick, n_pick]\n\n    ##Delivery\n    compatibility_delivery_pick = self.norm_factor * torch.sum(\n        Q_delivery * K_pick, -1\n    )  # element_wise, [num_heads, batch_size, n_pick]\n    compatibility_delivery_alldelivery = self.norm_factor * torch.matmul(\n        Q_delivery_alldelivery, K_alldelivery2.transpose(2, 3)\n    )  # [num_heads, batch_size, n_pick, n_pick]\n    compatibility_delivery_allpick = self.norm_factor * torch.matmul(\n        Q_delivery_allpickup, K_allpickup2.transpose(2, 3)\n    )  # [num_heads, batch_size, n_pick, n_pick]\n\n    ##Pick up-&gt;\n    # compatibility_additional?pickup????delivery????attention(size 1),1:n_pick+1??attention,depot?delivery??\n    compatibility_additional_delivery = torch.cat(\n        [  # [num_heads, batch_size, graph_size, 1]\n            float(\"-inf\")\n            * torch.ones(\n                self.num_heads,\n                batch_size,\n                1,\n                dtype=compatibility.dtype,\n                device=compatibility.device,\n            ),\n            compatibility_pick_delivery,  # [num_heads, batch_size, n_pick]\n            float(\"-inf\")\n            * torch.ones(\n                self.num_heads,\n                batch_size,\n                n_pick,\n                dtype=compatibility.dtype,\n                device=compatibility.device,\n            ),\n        ],\n        -1,\n    ).view(self.num_heads, batch_size, graph_size, 1)\n\n    compatibility_additional_allpick = torch.cat(\n        [  # [num_heads, batch_size, graph_size, n_pick]\n            float(\"-inf\")\n            * torch.ones(\n                self.num_heads,\n                batch_size,\n                1,\n                n_pick,\n                dtype=compatibility.dtype,\n                device=compatibility.device,\n            ),\n            compatibility_pick_allpick,  # [num_heads, batch_size, n_pick, n_pick]\n            float(\"-inf\")\n            * torch.ones(\n                self.num_heads,\n                batch_size,\n                n_pick,\n                n_pick,\n                dtype=compatibility.dtype,\n                device=compatibility.device,\n            ),\n        ],\n        2,\n    ).view(self.num_heads, batch_size, graph_size, n_pick)\n\n    compatibility_additional_alldelivery = torch.cat(\n        [  # [num_heads, batch_size, graph_size, n_pick]\n            float(\"-inf\")\n            * torch.ones(\n                self.num_heads,\n                batch_size,\n                1,\n                n_pick,\n                dtype=compatibility.dtype,\n                device=compatibility.device,\n            ),\n            compatibility_pick_alldelivery,  # [num_heads, batch_size, n_pick, n_pick]\n            float(\"-inf\")\n            * torch.ones(\n                self.num_heads,\n                batch_size,\n                n_pick,\n                n_pick,\n                dtype=compatibility.dtype,\n                device=compatibility.device,\n            ),\n        ],\n        2,\n    ).view(self.num_heads, batch_size, graph_size, n_pick)\n    # [num_heads, batch_size, n_query, graph_size+1+n_pick+n_pick]\n\n    # Delivery\n    compatibility_additional_pick = torch.cat(\n        [  # [num_heads, batch_size, graph_size, 1]\n            float(\"-inf\")\n            * torch.ones(\n                self.num_heads,\n                batch_size,\n                1,\n                dtype=compatibility.dtype,\n                device=compatibility.device,\n            ),\n            float(\"-inf\")\n            * torch.ones(\n                self.num_heads,\n                batch_size,\n                n_pick,\n                dtype=compatibility.dtype,\n                device=compatibility.device,\n            ),\n            compatibility_delivery_pick,  # [num_heads, batch_size, n_pick]\n        ],\n        -1,\n    ).view(self.num_heads, batch_size, graph_size, 1)\n\n    compatibility_additional_alldelivery2 = torch.cat(\n        [  # [num_heads, batch_size, graph_size, n_pick]\n            float(\"-inf\")\n            * torch.ones(\n                self.num_heads,\n                batch_size,\n                1,\n                n_pick,\n                dtype=compatibility.dtype,\n                device=compatibility.device,\n            ),\n            float(\"-inf\")\n            * torch.ones(\n                self.num_heads,\n                batch_size,\n                n_pick,\n                n_pick,\n                dtype=compatibility.dtype,\n                device=compatibility.device,\n            ),\n            compatibility_delivery_alldelivery,  # [num_heads, batch_size, n_pick, n_pick]\n        ],\n        2,\n    ).view(self.num_heads, batch_size, graph_size, n_pick)\n\n    compatibility_additional_allpick2 = torch.cat(\n        [  # [num_heads, batch_size, graph_size, n_pick]\n            float(\"-inf\")\n            * torch.ones(\n                self.num_heads,\n                batch_size,\n                1,\n                n_pick,\n                dtype=compatibility.dtype,\n                device=compatibility.device,\n            ),\n            float(\"-inf\")\n            * torch.ones(\n                self.num_heads,\n                batch_size,\n                n_pick,\n                n_pick,\n                dtype=compatibility.dtype,\n                device=compatibility.device,\n            ),\n            compatibility_delivery_allpick,  # [num_heads, batch_size, n_pick, n_pick]\n        ],\n        2,\n    ).view(self.num_heads, batch_size, graph_size, n_pick)\n\n    compatibility = torch.cat(\n        [\n            compatibility,\n            compatibility_additional_delivery,\n            compatibility_additional_allpick,\n            compatibility_additional_alldelivery,\n            compatibility_additional_pick,\n            compatibility_additional_alldelivery2,\n            compatibility_additional_allpick2,\n        ],\n        dim=-1,\n    )\n\n    # Optionally apply mask to prevent attention\n    if mask is not None:\n        mask = mask.view(1, batch_size, n_query, graph_size).expand_as(compatibility)\n        compatibility[mask] = float(\"-inf\")\n\n    attn = torch.softmax(\n        compatibility, dim=-1\n    )  # [num_heads, batch_size, n_query, graph_size+1+n_pick*2] (graph_size include depot)\n\n    # If there are nodes with no neighbours then softmax returns nan so we fix them to 0\n    if mask is not None:\n        attnc = attn.clone()\n        attnc[mask] = 0\n        attn = attnc\n\n    # heads: [num_heads, batrch_size, n_query, val_size] pick -&gt; its delivery\n    heads = torch.matmul(\n        attn[:, :, :, :graph_size], V\n    )  # V: (self.num_heads, batch_size, graph_size, val_size)\n    heads = (\n        heads\n        + attn[:, :, :, graph_size].view(self.num_heads, batch_size, graph_size, 1)\n        * V_additional_delivery\n    )  # V_addi:[num_heads, batch_size, graph_size, key_size]\n\n    # Heads pick -&gt; otherpick, V_allpick: # [num_heads, batch_size, n_pick, key_size]\n    heads = heads + torch.matmul(\n        attn[:, :, :, graph_size + 1 : graph_size + 1 + n_pick].view(\n            self.num_heads, batch_size, graph_size, n_pick\n        ),\n        V_allpick,\n    )\n\n    # V_alldelivery: # (num_heads, batch_size, n_pick, key/val_size)\n    heads = heads + torch.matmul(\n        attn[:, :, :, graph_size + 1 + n_pick : graph_size + 1 + 2 * n_pick].view(\n            self.num_heads, batch_size, graph_size, n_pick\n        ),\n        V_alldelivery,\n    )\n\n    # Delivery\n    heads = (\n        heads\n        + attn[:, :, :, graph_size + 1 + 2 * n_pick].view(\n            self.num_heads, batch_size, graph_size, 1\n        )\n        * V_additional_pick\n    )\n    heads = heads + torch.matmul(\n        attn[\n            :,\n            :,\n            :,\n            graph_size + 1 + 2 * n_pick + 1 : graph_size + 1 + 3 * n_pick + 1,\n        ].view(self.num_heads, batch_size, graph_size, n_pick),\n        V_alldelivery2,\n    )\n    heads = heads + torch.matmul(\n        attn[:, :, :, graph_size + 1 + 3 * n_pick + 1 :].view(\n            self.num_heads, batch_size, graph_size, n_pick\n        ),\n        V_allpickup2,\n    )\n\n    out = torch.mm(\n        heads.permute(1, 2, 0, 3)\n        .contiguous()\n        .view(-1, self.num_heads * self.val_dim),\n        self.W_out.view(-1, self.embed_dim),\n    ).view(batch_size, n_query, self.embed_dim)\n\n    return out\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#matrix-encoding-network-matnet","title":"Matrix Encoding Network (MatNet)","text":""},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.matnet.policy.MatNetPolicy","title":"MatNetPolicy","text":"<pre><code>MatNetPolicy(\n    env_name: str = \"atsp\",\n    embed_dim: int = 256,\n    num_encoder_layers: int = 5,\n    num_heads: int = 16,\n    normalization: str = \"instance\",\n    init_embedding_kwargs: dict = {\"mode\": \"RandomOneHot\"},\n    use_graph_context: bool = False,\n    bias: bool = False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>AutoregressivePolicy</code></p> <p>MatNet Policy from Kwon et al., 2021. Reference: https://arxiv.org/abs/2106.11113</p> Warning <p>This implementation is under development and subject to change.</p> <p>Parameters:</p> <ul> <li> <code>env_name</code>               (<code>str</code>, default:                   <code>'atsp'</code> )           \u2013            <p>Name of the environment used to initialize embeddings</p> </li> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Dimension of the node embeddings</p> </li> <li> <code>num_encoder_layers</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of layers in the encoder</p> </li> <li> <code>num_heads</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>Number of heads in the attention layers</p> </li> <li> <code>normalization</code>               (<code>str</code>, default:                   <code>'instance'</code> )           \u2013            <p>Normalization type in the attention layers</p> </li> <li> <code>**kwargs</code>           \u2013            <p>keyword arguments passed to the <code>AutoregressivePolicy</code></p> </li> </ul> <p>Default paarameters are adopted from the original implementation.</p> Source code in <code>rl4co/models/zoo/matnet/policy.py</code> <pre><code>def __init__(\n    self,\n    env_name: str = \"atsp\",\n    embed_dim: int = 256,\n    num_encoder_layers: int = 5,\n    num_heads: int = 16,\n    normalization: str = \"instance\",\n    init_embedding_kwargs: dict = {\"mode\": \"RandomOneHot\"},\n    use_graph_context: bool = False,\n    bias: bool = False,\n    **kwargs,\n):\n    if env_name not in [\"atsp\", \"ffsp\"]:\n        log.error(f\"env_name {env_name} is not originally implemented in MatNet\")\n\n    if env_name == \"ffsp\":\n        decoder = MatNetFFSPDecoder(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            use_graph_context=use_graph_context,\n            out_bias=True,\n        )\n\n    else:\n        decoder = MatNetDecoder(\n            env_name=env_name,\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            use_graph_context=use_graph_context,\n        )\n\n    super(MatNetPolicy, self).__init__(\n        env_name=env_name,\n        encoder=MatNetEncoder(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            num_layers=num_encoder_layers,\n            normalization=normalization,\n            init_embedding_kwargs=init_embedding_kwargs,\n            bias=bias,\n        ),\n        decoder=decoder,\n        embed_dim=embed_dim,\n        num_encoder_layers=num_encoder_layers,\n        num_heads=num_heads,\n        normalization=normalization,\n        **kwargs,\n    )\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.matnet.policy.MultiStageFFSPPolicy","title":"MultiStageFFSPPolicy","text":"<pre><code>MultiStageFFSPPolicy(\n    stage_cnt: int,\n    embed_dim: int = 512,\n    num_heads: int = 16,\n    num_encoder_layers: int = 5,\n    use_graph_context: bool = False,\n    normalization: str = \"instance\",\n    feedforward_hidden: int = 512,\n    bias: bool = False,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"sampling\",\n    test_decode_type: str = \"sampling\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Policy for solving the FFSP using a seperate encoder and decoder for each stage. This requires the 'while not td[\"done\"].all()'-loop to be on policy level (instead of decoder level).</p> Source code in <code>rl4co/models/zoo/matnet/policy.py</code> <pre><code>def __init__(\n    self,\n    stage_cnt: int,\n    embed_dim: int = 512,\n    num_heads: int = 16,\n    num_encoder_layers: int = 5,\n    use_graph_context: bool = False,\n    normalization: str = \"instance\",\n    feedforward_hidden: int = 512,\n    bias: bool = False,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"sampling\",\n    test_decode_type: str = \"sampling\",\n):\n    super().__init__()\n    self.stage_cnt = stage_cnt\n\n    self.encoders: List[MatNetEncoder] = nn.ModuleList(\n        [\n            MatNetEncoder(\n                embed_dim=embed_dim,\n                num_heads=num_heads,\n                num_layers=num_encoder_layers,\n                normalization=normalization,\n                feedforward_hidden=feedforward_hidden,\n                bias=bias,\n                init_embedding_kwargs={\"mode\": \"RandomOneHot\"},\n            )\n            for _ in range(self.stage_cnt)\n        ]\n    )\n    self.decoders: List[MultiStageFFSPDecoder] = nn.ModuleList(\n        [\n            MultiStageFFSPDecoder(embed_dim, num_heads, use_graph_context)\n            for _ in range(self.stage_cnt)\n        ]\n    )\n\n    self.train_decode_type = train_decode_type\n    self.val_decode_type = val_decode_type\n    self.test_decode_type = test_decode_type\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.matnet.encoder.MixedScoresSDPA","title":"MixedScoresSDPA","text":"<pre><code>MixedScoresSDPA(\n    num_heads: int,\n    num_scores: int = 1,\n    mixer_hidden_dim: int = 16,\n    mix1_init: float = 1 / 2**1 / 2,\n    mix2_init: float = 1 / 16**1 / 2,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>rl4co/models/zoo/matnet/encoder.py</code> <pre><code>def __init__(\n    self,\n    num_heads: int,\n    num_scores: int = 1,\n    mixer_hidden_dim: int = 16,\n    mix1_init: float = (1 / 2) ** (1 / 2),\n    mix2_init: float = (1 / 16) ** (1 / 2),\n):\n    super().__init__()\n    self.num_heads = num_heads\n    self.num_scores = num_scores\n    mix_W1 = torch.torch.distributions.Uniform(low=-mix1_init, high=mix1_init).sample(\n        (num_heads, self.num_scores + 1, mixer_hidden_dim)\n    )\n    mix_b1 = torch.torch.distributions.Uniform(low=-mix1_init, high=mix1_init).sample(\n        (num_heads, mixer_hidden_dim)\n    )\n    self.mix_W1 = nn.Parameter(mix_W1)\n    self.mix_b1 = nn.Parameter(mix_b1)\n\n    mix_W2 = torch.torch.distributions.Uniform(low=-mix2_init, high=mix2_init).sample(\n        (num_heads, mixer_hidden_dim, 1)\n    )\n    mix_b2 = torch.torch.distributions.Uniform(low=-mix2_init, high=mix2_init).sample(\n        (num_heads, 1)\n    )\n    self.mix_W2 = nn.Parameter(mix_W2)\n    self.mix_b2 = nn.Parameter(mix_b2)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.matnet.encoder.MixedScoresSDPA.forward","title":"forward","text":"<pre><code>forward(q, k, v, attn_mask=None, dmat=None, dropout_p=0.0)\n</code></pre> <p>Scaled Dot-Product Attention with MatNet Scores Mixer</p> Source code in <code>rl4co/models/zoo/matnet/encoder.py</code> <pre><code>def forward(self, q, k, v, attn_mask=None, dmat=None, dropout_p=0.0):\n    \"\"\"Scaled Dot-Product Attention with MatNet Scores Mixer\"\"\"\n    assert dmat is not None\n    b, m, n = dmat.shape[:3]\n    dmat = dmat.reshape(b, m, n, self.num_scores)\n\n    # Calculate scaled dot product\n    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (k.size(-1) ** 0.5)\n    # [b, h, m, n, num_scores+1]\n    mix_attn_scores = torch.cat(\n        [\n            attn_scores.unsqueeze(-1),\n            dmat[:, None, ...].expand(b, self.num_heads, m, n, self.num_scores),\n        ],\n        dim=-1,\n    )\n    # [b, h, m, n]\n    attn_scores = (\n        (\n            torch.matmul(\n                F.relu(\n                    torch.matmul(mix_attn_scores.transpose(1, 2), self.mix_W1)\n                    + self.mix_b1[None, None, :, None, :]\n                ),\n                self.mix_W2,\n            )\n            + self.mix_b2[None, None, :, None, :]\n        )\n        .transpose(1, 2)\n        .squeeze(-1)\n    )\n\n    # Apply the provided attention mask\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.bool:\n            attn_mask[~attn_mask.any(-1)] = True\n            attn_scores.masked_fill_(~attn_mask, float(\"-inf\"))\n        else:\n            attn_scores += attn_mask\n\n    # Softmax to get attention weights\n    attn_weights = F.softmax(attn_scores, dim=-1)\n\n    # Apply dropout\n    if dropout_p &gt; 0.0:\n        attn_weights = F.dropout(attn_weights, p=dropout_p)\n\n    # Compute the weighted sum of values\n    return torch.matmul(attn_weights, v)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.matnet.encoder.MatNetMHA","title":"MatNetMHA","text":"<pre><code>MatNetMHA(\n    embed_dim: int, num_heads: int, bias: bool = False\n)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>rl4co/models/zoo/matnet/encoder.py</code> <pre><code>def __init__(self, embed_dim: int, num_heads: int, bias: bool = False):\n    super().__init__()\n    self.row_encoding_block = MatNetCrossMHA(embed_dim, num_heads, bias)\n    self.col_encoding_block = MatNetCrossMHA(embed_dim, num_heads, bias)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.matnet.encoder.MatNetMHA.forward","title":"forward","text":"<pre><code>forward(row_emb, col_emb, dmat, attn_mask=None)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>row_emb</code>               (<code>Tensor</code>)           \u2013            <p>[b, m, d]</p> </li> <li> <code>col_emb</code>               (<code>Tensor</code>)           \u2013            <p>[b, n, d]</p> </li> <li> <code>dmat</code>               (<code>Tensor</code>)           \u2013            <p>[b, m, n]</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Updated row_emb (Tensor): [b, m, d]</p> </li> <li>           \u2013            <p>Updated col_emb (Tensor): [b, n, d]</p> </li> </ul> Source code in <code>rl4co/models/zoo/matnet/encoder.py</code> <pre><code>def forward(self, row_emb, col_emb, dmat, attn_mask=None):\n    \"\"\"\n    Args:\n        row_emb (Tensor): [b, m, d]\n        col_emb (Tensor): [b, n, d]\n        dmat (Tensor): [b, m, n]\n\n    Returns:\n        Updated row_emb (Tensor): [b, m, d]\n        Updated col_emb (Tensor): [b, n, d]\n    \"\"\"\n    updated_row_emb = self.row_encoding_block(\n        row_emb, col_emb, dmat=dmat, cross_attn_mask=attn_mask\n    )\n    attn_mask_t = attn_mask.transpose(-2, -1) if attn_mask is not None else None\n    updated_col_emb = self.col_encoding_block(\n        col_emb,\n        row_emb,\n        dmat=dmat.transpose(-2, -1),\n        cross_attn_mask=attn_mask_t,\n    )\n    return updated_row_emb, updated_col_emb\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.matnet.encoder.MatNetLayer","title":"MatNetLayer","text":"<pre><code>MatNetLayer(\n    embed_dim: int,\n    num_heads: int,\n    bias: bool = False,\n    feedforward_hidden: int = 512,\n    normalization: Optional[str] = \"instance\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>rl4co/models/zoo/matnet/encoder.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int,\n    num_heads: int,\n    bias: bool = False,\n    feedforward_hidden: int = 512,\n    normalization: Optional[str] = \"instance\",\n):\n    super().__init__()\n    self.MHA = MatNetMHA(embed_dim, num_heads, bias)\n    self.F_a = TransformerFFN(embed_dim, feedforward_hidden, normalization)\n    self.F_b = TransformerFFN(embed_dim, feedforward_hidden, normalization)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.matnet.encoder.MatNetLayer.forward","title":"forward","text":"<pre><code>forward(row_emb, col_emb, dmat, attn_mask=None)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>row_emb</code>               (<code>Tensor</code>)           \u2013            <p>[b, m, d]</p> </li> <li> <code>col_emb</code>               (<code>Tensor</code>)           \u2013            <p>[b, n, d]</p> </li> <li> <code>dmat</code>               (<code>Tensor</code>)           \u2013            <p>[b, m, n]</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Updated row_emb (Tensor): [b, m, d]</p> </li> <li>           \u2013            <p>Updated col_emb (Tensor): [b, n, d]</p> </li> </ul> Source code in <code>rl4co/models/zoo/matnet/encoder.py</code> <pre><code>def forward(self, row_emb, col_emb, dmat, attn_mask=None):\n    \"\"\"\n    Args:\n        row_emb (Tensor): [b, m, d]\n        col_emb (Tensor): [b, n, d]\n        dmat (Tensor): [b, m, n]\n\n    Returns:\n        Updated row_emb (Tensor): [b, m, d]\n        Updated col_emb (Tensor): [b, n, d]\n    \"\"\"\n\n    row_emb_out, col_emb_out = self.MHA(row_emb, col_emb, dmat, attn_mask)\n    row_emb_out = self.F_a(row_emb_out, row_emb)\n    col_emb_out = self.F_b(col_emb_out, col_emb)\n    return row_emb_out, col_emb_out\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.matnet.decoder.MultiStageFFSPDecoder","title":"MultiStageFFSPDecoder","text":"<pre><code>MultiStageFFSPDecoder(\n    embed_dim: int,\n    num_heads: int,\n    use_graph_context: bool = True,\n    tanh_clipping: float = 10,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>MatNetFFSPDecoder</code></p> <p>Decoder class for the solving the FFSP using a seperate MatNet decoder for each stage as originally implemented by Kwon et al. (2021)</p> Source code in <code>rl4co/models/zoo/matnet/decoder.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int,\n    num_heads: int,\n    use_graph_context: bool = True,\n    tanh_clipping: float = 10,\n    **kwargs,\n):\n    super().__init__(\n        embed_dim=embed_dim,\n        num_heads=num_heads,\n        use_graph_context=use_graph_context,\n        **kwargs,\n    )\n    self.cached_embs: PrecomputedCache = None\n    self.tanh_clipping = tanh_clipping\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#multi-decoder-attention-model-mdam","title":"Multi-Decoder Attention Model (MDAM)","text":""},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.mdam.model.MDAM","title":"MDAM","text":"<pre><code>MDAM(\n    env: RL4COEnvBase,\n    policy: MDAMPolicy = None,\n    baseline: Union[REINFORCEBaseline, str] = \"rollout\",\n    policy_kwargs={},\n    baseline_kwargs={},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>REINFORCE</code></p> <p>Multi-Decoder Attention Model (MDAM) is a model to train multiple diverse policies, which effectively increases the chance of finding good solutions compared with existing methods that train only one policy. Reference link: https://arxiv.org/abs/2012.10638; Implementation reference: https://github.com/liangxinedu/MDAM.</p> <p>Parameters:</p> <ul> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>Environment to use for the algorithm</p> </li> <li> <code>policy</code>               (<code>MDAMPolicy</code>, default:                   <code>None</code> )           \u2013            <p>Policy to use for the algorithm</p> </li> <li> <code>baseline</code>               (<code>Union[REINFORCEBaseline, str]</code>, default:                   <code>'rollout'</code> )           \u2013            <p>REINFORCE baseline. Defaults to rollout (1 epoch of exponential, then greedy rollout baseline)</p> </li> <li> <code>policy_kwargs</code>           \u2013            <p>Keyword arguments for policy</p> </li> <li> <code>baseline_kwargs</code>           \u2013            <p>Keyword arguments for baseline</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Keyword arguments passed to the superclass</p> </li> </ul> Source code in <code>rl4co/models/zoo/mdam/model.py</code> <pre><code>def __init__(\n    self,\n    env: RL4COEnvBase,\n    policy: MDAMPolicy = None,\n    baseline: Union[REINFORCEBaseline, str] = \"rollout\",\n    policy_kwargs={},\n    baseline_kwargs={},\n    **kwargs,\n):\n    if policy is None:\n        policy = MDAMPolicy(env_name=env.name, **policy_kwargs)\n\n    super().__init__(env, policy, baseline, baseline_kwargs, **kwargs)\n\n    # Change rollout of baseline to the rollout function\n    if isinstance(self.baseline, WarmupBaseline):\n        if isinstance(self.baseline.baseline, RolloutBaseline):\n            self.baseline.baseline.rollout = partial(rollout, self.baseline.baseline)\n    elif isinstance(self.baseline, RolloutBaseline):\n        self.baseline.rollout = partial(rollout, self.baseline)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.mdam.model.MDAM.calculate_loss","title":"calculate_loss","text":"<pre><code>calculate_loss(\n    td, batch, policy_out, reward=None, log_likelihood=None\n)\n</code></pre> <p>Calculate loss for REINFORCE algorithm. Same as in :class:<code>REINFORCE</code>, but the bl_val is calculated is simply unsqueezed to match the reward shape (i.e., [batch, num_paths])</p> <p>Parameters:</p> <ul> <li> <code>td</code>           \u2013            <p>TensorDict containing the current state of the environment</p> </li> <li> <code>batch</code>           \u2013            <p>Batch of data. This is used to get the extra loss terms, e.g., REINFORCE baseline</p> </li> <li> <code>policy_out</code>           \u2013            <p>Output of the policy network</p> </li> <li> <code>reward</code>           \u2013            <p>Reward tensor. If None, it is taken from <code>policy_out</code></p> </li> <li> <code>log_likelihood</code>           \u2013            <p>Log-likelihood tensor. If None, it is taken from <code>policy_out</code></p> </li> </ul> Source code in <code>rl4co/models/zoo/mdam/model.py</code> <pre><code>def calculate_loss(\n    self,\n    td,\n    batch,\n    policy_out,\n    reward=None,\n    log_likelihood=None,\n):\n    \"\"\"Calculate loss for REINFORCE algorithm.\n    Same as in :class:`REINFORCE`, but the bl_val is calculated is simply unsqueezed to match\n    the reward shape (i.e., [batch, num_paths])\n\n    Args:\n        td: TensorDict containing the current state of the environment\n        batch: Batch of data. This is used to get the extra loss terms, e.g., REINFORCE baseline\n        policy_out: Output of the policy network\n        reward: Reward tensor. If None, it is taken from `policy_out`\n        log_likelihood: Log-likelihood tensor. If None, it is taken from `policy_out`\n    \"\"\"\n    # Extra: this is used for additional loss terms, e.g., REINFORCE baseline\n    extra = batch.get(\"extra\", None)\n    reward = reward if reward is not None else policy_out[\"reward\"]\n    log_likelihood = (\n        log_likelihood if log_likelihood is not None else policy_out[\"log_likelihood\"]\n    )\n\n    # REINFORCE baseline\n    bl_val, bl_loss = (\n        self.baseline.eval(td, reward, self.env) if extra is None else (extra, 0)\n    )\n\n    # Main loss function\n    # reward: [batch, num_paths]. Note that the baseline value is the max reward\n    # if bl_val is a tensor, unsqueeze it to match the reward shape\n    if isinstance(bl_val, torch.Tensor):\n        if len(bl_val.shape) &gt; 0:\n            bl_val = bl_val.unsqueeze(1)\n    advantage = reward - bl_val  # advantage = reward - baseline\n    reinforce_loss = -(advantage * log_likelihood).mean()\n    loss = reinforce_loss + bl_loss\n    policy_out.update(\n        {\n            \"loss\": loss,\n            \"reinforce_loss\": reinforce_loss,\n            \"bl_loss\": bl_loss,\n            \"bl_val\": bl_val,\n        }\n    )\n    return policy_out\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.mdam.model.rollout","title":"rollout","text":"<pre><code>rollout(\n    self,\n    model,\n    env,\n    batch_size=64,\n    device=\"cpu\",\n    dataset=None,\n)\n</code></pre> <p>In this case the reward from the model is [batch, num_paths] and the baseline takes the maximum reward from the model as the baseline. https://github.com/liangxinedu/MDAM/blob/19b0bf813fb2dbec2fcde9e22eb50e04675400cd/train.py#L38C29-L38C33</p> Source code in <code>rl4co/models/zoo/mdam/model.py</code> <pre><code>def rollout(self, model, env, batch_size=64, device=\"cpu\", dataset=None):\n    \"\"\"In this case the reward from the model is [batch, num_paths]\n    and the baseline takes the maximum reward from the model as the baseline.\n    https://github.com/liangxinedu/MDAM/blob/19b0bf813fb2dbec2fcde9e22eb50e04675400cd/train.py#L38C29-L38C33\n    \"\"\"\n    # if dataset is None, use the dataset of the baseline\n    dataset = self.dataset if dataset is None else dataset\n\n    model.eval()\n    model = model.to(device)\n\n    def eval_model(batch):\n        with torch.inference_mode():\n            batch = env.reset(batch.to(device))\n            return model(batch, env, decode_type=\"greedy\")[\"reward\"].max(1).values\n\n    dl = DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn)\n\n    rewards = torch.cat([eval_model(batch) for batch in dl], 0)\n    return rewards\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.mdam.policy.MDAMPolicy","title":"MDAMPolicy","text":"<pre><code>MDAMPolicy(\n    encoder: MDAMGraphAttentionEncoder = None,\n    decoder: MDAMDecoder = None,\n    embed_dim: int = 128,\n    env_name: str = \"tsp\",\n    num_encoder_layers: int = 3,\n    num_heads: int = 8,\n    normalization: str = \"batch\",\n    **decoder_kwargs\n)\n</code></pre> <p>               Bases: <code>AutoregressivePolicy</code></p> <p>Multi-Decoder Attention Model (MDAM) policy. Args:</p> Source code in <code>rl4co/models/zoo/mdam/policy.py</code> <pre><code>def __init__(\n    self,\n    encoder: MDAMGraphAttentionEncoder = None,\n    decoder: MDAMDecoder = None,\n    embed_dim: int = 128,\n    env_name: str = \"tsp\",\n    num_encoder_layers: int = 3,\n    num_heads: int = 8,\n    normalization: str = \"batch\",\n    **decoder_kwargs,\n):\n    encoder = (\n        MDAMGraphAttentionEncoder(\n            num_heads=num_heads,\n            embed_dim=embed_dim,\n            num_layers=num_encoder_layers,\n            normalization=normalization,\n        )\n        if encoder is None\n        else encoder\n    )\n\n    decoder = (\n        MDAMDecoder(\n            env_name=env_name,\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            **decoder_kwargs,\n        )\n        if decoder is None\n        else decoder\n    )\n\n    super(MDAMPolicy, self).__init__(\n        env_name=env_name, encoder=encoder, decoder=decoder\n    )\n\n    self.init_embedding = env_init_embedding(env_name, {\"embed_dim\": embed_dim})\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.mdam.encoder.MDAMGraphAttentionEncoder","title":"MDAMGraphAttentionEncoder","text":"<pre><code>MDAMGraphAttentionEncoder(\n    num_heads,\n    embed_dim,\n    num_layers,\n    node_dim=None,\n    normalization=\"batch\",\n    feedforward_hidden=512,\n    sdpa_fn: Optional[Callable] = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>rl4co/models/zoo/mdam/encoder.py</code> <pre><code>def __init__(\n    self,\n    num_heads,\n    embed_dim,\n    num_layers,\n    node_dim=None,\n    normalization=\"batch\",\n    feedforward_hidden=512,\n    sdpa_fn: Optional[Callable] = None,\n):\n    super(MDAMGraphAttentionEncoder, self).__init__()\n\n    # To map input to embedding space\n    self.init_embed = nn.Linear(node_dim, embed_dim) if node_dim is not None else None\n\n    self.layers = nn.Sequential(\n        *(\n            MultiHeadAttentionLayer(\n                embed_dim,\n                num_heads,\n                feedforward_hidden,\n                normalization,\n                sdpa_fn=sdpa_fn,\n            )\n            for _ in range(num_layers - 1)  # because last layer is different\n        )\n    )\n    self.attention_layer = MultiHeadAttentionMDAM(\n        embed_dim, num_heads, sdpa_fn=sdpa_fn, last_one=True\n    )\n    self.BN1 = Normalization(embed_dim, normalization)\n    self.projection = SkipConnection(\n        nn.Sequential(\n            nn.Linear(embed_dim, feedforward_hidden),\n            nn.ReLU(),\n            nn.Linear(feedforward_hidden, embed_dim),\n        )\n        if feedforward_hidden &gt; 0\n        else nn.Linear(embed_dim, embed_dim)\n    )\n    self.BN2 = Normalization(embed_dim, normalization)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.mdam.encoder.MDAMGraphAttentionEncoder.forward","title":"forward","text":"<pre><code>forward(x, mask=None, return_transform_loss=False)\n</code></pre> <p>Returns:</p> <ul> <li>           \u2013            <ul> <li>h [batch_size, graph_size, embed_dim]</li> </ul> </li> <li>           \u2013            <ul> <li>attn [num_head, batch_size, graph_size, graph_size]</li> </ul> </li> <li>           \u2013            <ul> <li>V [num_head, batch_size, graph_size, key_dim]</li> </ul> </li> <li>           \u2013            <ul> <li>h_old [batch_size, graph_size, embed_dim]</li> </ul> </li> </ul> Source code in <code>rl4co/models/zoo/mdam/encoder.py</code> <pre><code>def forward(self, x, mask=None, return_transform_loss=False):\n    \"\"\"\n    Returns:\n        - h [batch_size, graph_size, embed_dim]\n        - attn [num_head, batch_size, graph_size, graph_size]\n        - V [num_head, batch_size, graph_size, key_dim]\n        - h_old [batch_size, graph_size, embed_dim]\n    \"\"\"\n    assert mask is None, \"TODO mask not yet supported!\"\n\n    h_embeded = x\n    h_old = self.layers(h_embeded)\n    h_new, attn, V = self.attention_layer(h_old)\n    h = h_new + h_old\n    h = self.BN1(h)\n    h = self.projection(h)\n    h = self.BN2(h)\n\n    return (h, h.mean(dim=1), attn, V, h_old)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#pomo","title":"POMO","text":""},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.pomo.model.POMO","title":"POMO","text":"<pre><code>POMO(\n    env: RL4COEnvBase,\n    policy: Module = None,\n    policy_kwargs={},\n    baseline: str = \"shared\",\n    num_augment: int = 8,\n    augment_fn: Union[str, callable] = \"dihedral8\",\n    first_aug_identity: bool = True,\n    feats: list = None,\n    num_starts: int = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>REINFORCE</code></p> <p>POMO Model for neural combinatorial optimization based on REINFORCE Based on Kwon et al. (2020) http://arxiv.org/abs/2010.16011.</p> Note <p>If no policy kwargs is passed, we use the Attention Model policy with the following arguments: Differently to the base class:</p> <ul> <li><code>num_encoder_layers=6</code> (instead of 3)</li> <li><code>normalization=\"instance\"</code> (instead of \"batch\")</li> <li><code>use_graph_context=False</code> (instead of True) The latter is due to the fact that the paper does not use the graph context in the policy, which seems to be helpful in overfitting to the training graph size.</li> </ul> <p>Parameters:</p> <ul> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>TorchRL Environment</p> </li> <li> <code>policy</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Policy to use for the algorithm</p> </li> <li> <code>policy_kwargs</code>           \u2013            <p>Keyword arguments for policy</p> </li> <li> <code>baseline</code>               (<code>str</code>, default:                   <code>'shared'</code> )           \u2013            <p>Baseline to use for the algorithm. Note that POMO only supports shared baseline, so we will throw an error if anything else is passed.</p> </li> <li> <code>num_augment</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of augmentations (used only for validation and test)</p> </li> <li> <code>augment_fn</code>               (<code>Union[str, callable]</code>, default:                   <code>'dihedral8'</code> )           \u2013            <p>Function to use for augmentation, defaulting to dihedral8</p> </li> <li> <code>first_aug_identity</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to include the identity augmentation in the first position</p> </li> <li> <code>feats</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>List of features to augment</p> </li> <li> <code>num_starts</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of starts for multi-start. If None, use the number of available actions</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Keyword arguments passed to the superclass</p> </li> </ul> Source code in <code>rl4co/models/zoo/pomo/model.py</code> <pre><code>def __init__(\n    self,\n    env: RL4COEnvBase,\n    policy: nn.Module = None,\n    policy_kwargs={},\n    baseline: str = \"shared\",\n    num_augment: int = 8,\n    augment_fn: Union[str, callable] = \"dihedral8\",\n    first_aug_identity: bool = True,\n    feats: list = None,\n    num_starts: int = None,\n    **kwargs,\n):\n    self.save_hyperparameters(logger=False)\n\n    if policy is None:\n        policy_kwargs_with_defaults = {\n            \"num_encoder_layers\": 6,\n            \"normalization\": \"instance\",\n            \"use_graph_context\": False,\n        }\n        policy_kwargs_with_defaults.update(policy_kwargs)\n        policy = AttentionModelPolicy(env_name=env.name, **policy_kwargs_with_defaults)\n\n    assert baseline == \"shared\", \"POMO only supports shared baseline\"\n\n    # Initialize with the shared baseline\n    super(POMO, self).__init__(env, policy, baseline, **kwargs)\n\n    self.num_starts = num_starts\n    self.num_augment = num_augment\n    if self.num_augment &gt; 1:\n        self.augment = StateAugmentation(\n            num_augment=self.num_augment,\n            augment_fn=augment_fn,\n            first_aug_identity=first_aug_identity,\n            feats=feats,\n        )\n    else:\n        self.augment = None\n\n    # Add `_multistart` to decode type for train, val and test in policy\n    for phase in [\"train\", \"val\", \"test\"]:\n        self.set_decode_type_multistart(phase)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#pointer-network-ptrnet","title":"Pointer Network (PtrNet)","text":""},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.ptrnet.model.PointerNetwork","title":"PointerNetwork","text":"<pre><code>PointerNetwork(\n    env: RL4COEnvBase,\n    policy: PointerNetworkPolicy = None,\n    baseline: Union[REINFORCEBaseline, str] = \"rollout\",\n    policy_kwargs={},\n    baseline_kwargs={},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>REINFORCE</code></p> <p>Pointer Network for neural combinatorial optimization based on REINFORCE Based on Vinyals et al. (2015) https://arxiv.org/abs/1506.03134 Refactored from reference implementation: https://github.com/wouterkool/attention-learn-to-route</p> <p>Parameters:</p> <ul> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>Environment to use for the algorithm</p> </li> <li> <code>policy</code>               (<code>PointerNetworkPolicy</code>, default:                   <code>None</code> )           \u2013            <p>Policy to use for the algorithm</p> </li> <li> <code>baseline</code>               (<code>Union[REINFORCEBaseline, str]</code>, default:                   <code>'rollout'</code> )           \u2013            <p>REINFORCE baseline. Defaults to rollout (1 epoch of exponential, then greedy rollout baseline)</p> </li> <li> <code>policy_kwargs</code>           \u2013            <p>Keyword arguments for policy</p> </li> <li> <code>baseline_kwargs</code>           \u2013            <p>Keyword arguments for baseline</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Keyword arguments passed to the superclass</p> </li> </ul> Source code in <code>rl4co/models/zoo/ptrnet/model.py</code> <pre><code>def __init__(\n    self,\n    env: RL4COEnvBase,\n    policy: PointerNetworkPolicy = None,\n    baseline: Union[REINFORCEBaseline, str] = \"rollout\",\n    policy_kwargs={},\n    baseline_kwargs={},\n    **kwargs,\n):\n    policy = (\n        PointerNetworkPolicy(env=env, **policy_kwargs) if policy is None else policy\n    )\n    super().__init__(env, policy, baseline, baseline_kwargs, **kwargs)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.ptrnet.encoder.Encoder","title":"Encoder","text":"<pre><code>Encoder(input_dim, hidden_dim)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Maps a graph represented as an input sequence to a hidden vector</p> Source code in <code>rl4co/models/zoo/ptrnet/encoder.py</code> <pre><code>def __init__(self, input_dim, hidden_dim):\n    super(Encoder, self).__init__()\n    self.hidden_dim = hidden_dim\n    self.lstm = nn.LSTM(input_dim, hidden_dim)\n    self.init_hx, self.init_cx = self.init_hidden(hidden_dim)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.ptrnet.encoder.Encoder.init_hidden","title":"init_hidden","text":"<pre><code>init_hidden(hidden_dim)\n</code></pre> <p>Trainable initial hidden state</p> Source code in <code>rl4co/models/zoo/ptrnet/encoder.py</code> <pre><code>def init_hidden(self, hidden_dim):\n    \"\"\"Trainable initial hidden state\"\"\"\n    std = 1.0 / math.sqrt(hidden_dim)\n    enc_init_hx = nn.Parameter(torch.FloatTensor(hidden_dim))\n    enc_init_hx.data.uniform_(-std, std)\n\n    enc_init_cx = nn.Parameter(torch.FloatTensor(hidden_dim))\n    enc_init_cx.data.uniform_(-std, std)\n    return enc_init_hx, enc_init_cx\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.ptrnet.decoder.SimpleAttention","title":"SimpleAttention","text":"<pre><code>SimpleAttention(dim, use_tanh=False, C=10)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A generic attention module for a decoder in seq2seq</p> Source code in <code>rl4co/models/zoo/ptrnet/decoder.py</code> <pre><code>def __init__(self, dim, use_tanh=False, C=10):\n    super(SimpleAttention, self).__init__()\n    self.use_tanh = use_tanh\n    self.project_query = nn.Linear(dim, dim)\n    self.project_ref = nn.Conv1d(dim, dim, 1, 1)\n    self.C = C  # tanh exploration\n\n    self.v = nn.Parameter(torch.FloatTensor(dim))\n    self.v.data.uniform_(-(1.0 / math.sqrt(dim)), 1.0 / math.sqrt(dim))\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.ptrnet.decoder.SimpleAttention.forward","title":"forward","text":"<pre><code>forward(query, ref)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>query</code>           \u2013            <p>is the hidden state of the decoder at the current time step. batch x dim</p> </li> <li> <code>ref</code>           \u2013            <p>the set of hidden states from the encoder. sourceL x batch x hidden_dim</p> </li> </ul> Source code in <code>rl4co/models/zoo/ptrnet/decoder.py</code> <pre><code>def forward(self, query, ref):\n    \"\"\"\n    Args:\n        query: is the hidden state of the decoder at the current\n            time step. batch x dim\n        ref: the set of hidden states from the encoder.\n            sourceL x batch x hidden_dim\n    \"\"\"\n    # ref is now [batch_size x hidden_dim x sourceL]\n    ref = ref.permute(1, 2, 0)\n    q = self.project_query(query).unsqueeze(2)  # batch x dim x 1\n    e = self.project_ref(ref)  # batch_size x hidden_dim x sourceL\n    # expand the query by sourceL\n    # batch x dim x sourceL\n    expanded_q = q.repeat(1, 1, e.size(2))\n    # batch x 1 x hidden_dim\n    v_view = self.v.unsqueeze(0).expand(expanded_q.size(0), len(self.v)).unsqueeze(1)\n    # [batch_size x 1 x hidden_dim] * [batch_size x hidden_dim x sourceL]\n    u = torch.bmm(v_view, F.tanh(expanded_q + e)).squeeze(1)\n    if self.use_tanh:\n        logits = self.C * F.tanh(u)\n    else:\n        logits = u\n    return e, logits\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.ptrnet.decoder.Decoder","title":"Decoder","text":"<pre><code>Decoder(\n    embed_dim: int = 128,\n    hidden_dim: int = 128,\n    tanh_exploration: float = 10.0,\n    use_tanh: bool = True,\n    num_glimpses=1,\n    mask_glimpses=True,\n    mask_logits=True,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>rl4co/models/zoo/ptrnet/decoder.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int = 128,\n    hidden_dim: int = 128,\n    tanh_exploration: float = 10.0,\n    use_tanh: bool = True,\n    num_glimpses=1,\n    mask_glimpses=True,\n    mask_logits=True,\n):\n    super(Decoder, self).__init__()\n\n    self.embed_dim = embed_dim\n    self.hidden_dim = hidden_dim\n    self.num_glimpses = num_glimpses\n    self.mask_glimpses = mask_glimpses\n    self.mask_logits = mask_logits\n    self.use_tanh = use_tanh\n    self.tanh_exploration = tanh_exploration\n\n    self.lstm = nn.LSTMCell(embed_dim, hidden_dim)\n    self.pointer = SimpleAttention(hidden_dim, use_tanh=use_tanh, C=tanh_exploration)\n    self.glimpse = SimpleAttention(hidden_dim, use_tanh=False)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.ptrnet.decoder.Decoder.forward","title":"forward","text":"<pre><code>forward(\n    decoder_input,\n    embedded_inputs,\n    hidden,\n    context,\n    decode_type=\"sampling\",\n    eval_tours=None,\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>decoder_input</code>           \u2013            <p>The initial input to the decoder size is [batch_size x embed_dim]. Trainable parameter.</p> </li> <li> <code>embedded_inputs</code>           \u2013            <p>[sourceL x batch_size x embed_dim]</p> </li> <li> <code>hidden</code>           \u2013            <p>the prev hidden state, size is [batch_size x hidden_dim]. Initially this is set to (enc_h[-1], enc_c[-1])</p> </li> <li> <code>context</code>           \u2013            <p>encoder outputs, [sourceL x batch_size x hidden_dim]</p> </li> </ul> Source code in <code>rl4co/models/zoo/ptrnet/decoder.py</code> <pre><code>def forward(\n    self,\n    decoder_input,\n    embedded_inputs,\n    hidden,\n    context,\n    decode_type=\"sampling\",\n    eval_tours=None,\n):\n    \"\"\"\n    Args:\n        decoder_input: The initial input to the decoder\n            size is [batch_size x embed_dim]. Trainable parameter.\n        embedded_inputs: [sourceL x batch_size x embed_dim]\n        hidden: the prev hidden state, size is [batch_size x hidden_dim].\n            Initially this is set to (enc_h[-1], enc_c[-1])\n        context: encoder outputs, [sourceL x batch_size x hidden_dim]\n    \"\"\"\n\n    batch_size = context.size(1)\n    outputs = []\n    selections = []\n    steps = range(embedded_inputs.size(0))\n    idxs = None\n    mask = torch.ones(\n        embedded_inputs.size(1),\n        embedded_inputs.size(0),\n        dtype=torch.bool,\n        device=embedded_inputs.device,\n    )\n\n    for i in steps:\n        hidden, log_p, mask = self.recurrence(\n            decoder_input, hidden, mask, idxs, i, context\n        )\n        # select the next inputs for the decoder [batch_size x hidden_dim]\n        idxs = (\n            decode_logprobs(log_p, mask, decode_type=decode_type)\n            if eval_tours is None\n            else eval_tours[:, i]\n        )\n        # select logp of chosen action\n        log_p = gather_by_index(log_p, idxs, dim=1)\n\n        idxs = (\n            idxs.detach()\n        )  # Otherwise pytorch complains it want's a reward, todo implement this more properly?\n        # Gather input embedding of selected\n        decoder_input = torch.gather(\n            embedded_inputs,\n            0,\n            idxs.contiguous()\n            .view(1, batch_size, 1)\n            .expand(1, batch_size, *embedded_inputs.size()[2:]),\n        ).squeeze(0)\n\n        # use outs to point to next object\n        outputs.append(log_p)\n        selections.append(idxs)\n    return (torch.stack(outputs, 1), torch.stack(selections, 1)), hidden\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.ptrnet.critic.CriticNetworkLSTM","title":"CriticNetworkLSTM","text":"<pre><code>CriticNetworkLSTM(\n    embed_dim,\n    hidden_dim,\n    n_process_block_iters,\n    tanh_exploration,\n    use_tanh,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Useful as a baseline in REINFORCE updates</p> Source code in <code>rl4co/models/zoo/ptrnet/critic.py</code> <pre><code>def __init__(\n    self,\n    embed_dim,\n    hidden_dim,\n    n_process_block_iters,\n    tanh_exploration,\n    use_tanh,\n):\n    super(CriticNetworkLSTM, self).__init__()\n\n    self.hidden_dim = hidden_dim\n    self.n_process_block_iters = n_process_block_iters\n\n    self.encoder = Encoder(embed_dim, hidden_dim)\n\n    self.process_block = SimpleAttention(\n        hidden_dim, use_tanh=use_tanh, C=tanh_exploration\n    )\n    self.sm = nn.Softmax(dim=1)\n    self.decoder = nn.Sequential(\n        nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 1)\n    )\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.ptrnet.critic.CriticNetworkLSTM.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>inputs</code>           \u2013            <p>[embed_dim x batch_size x sourceL] of embedded inputs</p> </li> </ul> Source code in <code>rl4co/models/zoo/ptrnet/critic.py</code> <pre><code>def forward(self, inputs):\n    \"\"\"\n    Args:\n        inputs: [embed_dim x batch_size x sourceL] of embedded inputs\n    \"\"\"\n    inputs = inputs.transpose(0, 1).contiguous()\n\n    encoder_hx = (\n        self.encoder.init_hx.unsqueeze(0).repeat(inputs.size(1), 1).unsqueeze(0)\n    )\n    encoder_cx = (\n        self.encoder.init_cx.unsqueeze(0).repeat(inputs.size(1), 1).unsqueeze(0)\n    )\n\n    # encoder forward pass\n    enc_outputs, (enc_h_t, enc_c_t) = self.encoder(inputs, (encoder_hx, encoder_cx))\n\n    # grab the hidden state and process it via the process block\n    process_block_state = enc_h_t[-1]\n    for i in range(self.n_process_block_iters):\n        ref, logits = self.process_block(process_block_state, enc_outputs)\n        process_block_state = torch.bmm(ref, self.sm(logits).unsqueeze(2)).squeeze(2)\n    # produce the final scalar output\n    out = self.decoder(process_block_state)\n    return out\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#symnco","title":"SymNCO","text":""},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.symnco.model.SymNCO","title":"SymNCO","text":"<pre><code>SymNCO(\n    env: RL4COEnvBase,\n    policy: Union[Module, SymNCOPolicy] = None,\n    policy_kwargs: dict = {},\n    baseline: str = \"symnco\",\n    num_augment: int = 4,\n    augment_fn: Union[str, callable] = \"symmetric\",\n    feats: list = None,\n    alpha: float = 0.2,\n    beta: float = 1,\n    num_starts: int = 0,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>REINFORCE</code></p> <p>SymNCO Model based on REINFORCE with shared baselines. Based on Kim et al. (2022) https://arxiv.org/abs/2205.13209.</p> <p>Parameters:</p> <ul> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>TorchRL environment to use for the algorithm</p> </li> <li> <code>policy</code>               (<code>Union[Module, SymNCOPolicy]</code>, default:                   <code>None</code> )           \u2013            <p>Policy to use for the algorithm</p> </li> <li> <code>policy_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments for policy</p> </li> <li> <code>num_augment</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of augmentations</p> </li> <li> <code>augment_fn</code>               (<code>Union[str, callable]</code>, default:                   <code>'symmetric'</code> )           \u2013            <p>Function to use for augmentation, defaulting to dihedral_8_augmentation</p> </li> <li> <code>feats</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>List of features to augment</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>weight for invariance loss</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>1</code> )           \u2013            <p>weight for solution symmetricity loss</p> </li> <li> <code>num_starts</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of starts for multi-start. If None, use the number of available actions</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Keyword arguments passed to the superclass</p> </li> </ul> Source code in <code>rl4co/models/zoo/symnco/model.py</code> <pre><code>def __init__(\n    self,\n    env: RL4COEnvBase,\n    policy: Union[nn.Module, SymNCOPolicy] = None,\n    policy_kwargs: dict = {},\n    baseline: str = \"symnco\",\n    num_augment: int = 4,\n    augment_fn: Union[str, callable] = \"symmetric\",\n    feats: list = None,\n    alpha: float = 0.2,\n    beta: float = 1,\n    num_starts: int = 0,\n    **kwargs,\n):\n    self.save_hyperparameters(logger=False)\n\n    if policy is None:\n        policy = SymNCOPolicy(env_name=env.name, **policy_kwargs)\n\n    assert baseline == \"symnco\", \"SymNCO only supports custom-symnco baseline\"\n    baseline = \"no\"  # Pass no baseline to superclass since there are multiple custom baselines\n\n    # Pass no baseline to superclass since there are multiple custom baselines\n    super().__init__(env, policy, baseline, **kwargs)\n\n    self.num_starts = num_starts\n    self.num_augment = num_augment\n    self.augment = StateAugmentation(\n        num_augment=self.num_augment, augment_fn=augment_fn, feats=feats\n    )\n    self.alpha = alpha  # weight for invariance loss\n    self.beta = beta  # weight for solution symmetricity loss\n\n    # Add `_multistart` to decode type for train, val and test in policy if num_starts &gt; 1\n    if self.num_starts &gt; 1:\n        for phase in [\"train\", \"val\", \"test\"]:\n            self.set_decode_type_multistart(phase)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.symnco.policy.SymNCOPolicy","title":"SymNCOPolicy","text":"<pre><code>SymNCOPolicy(\n    embed_dim: int = 128,\n    env_name: str = \"tsp\",\n    num_encoder_layers: int = 3,\n    num_heads: int = 8,\n    normalization: str = \"batch\",\n    projection_head: Module = None,\n    use_projection_head: bool = True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>AttentionModelPolicy</code></p> <p>SymNCO Policy based on AutoregressivePolicy. This differs from the default :class:<code>AutoregressivePolicy</code> in that it projects the initial embeddings to a lower dimension using a projection head and returns it. This is used in the SymNCO algorithm to compute the invariance loss. Based on Kim et al. (2022) https://arxiv.org/abs/2205.13209.</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Dimension of the embedding</p> </li> <li> <code>env_name</code>               (<code>str</code>, default:                   <code>'tsp'</code> )           \u2013            <p>Name of the environment</p> </li> <li> <code>num_encoder_layers</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of layers in the encoder</p> </li> <li> <code>num_heads</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of heads in the encoder</p> </li> <li> <code>normalization</code>               (<code>str</code>, default:                   <code>'batch'</code> )           \u2013            <p>Normalization to use in the encoder</p> </li> <li> <code>projection_head</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Projection head to use</p> </li> <li> <code>use_projection_head</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use projection head</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Keyword arguments passed to the superclass</p> </li> </ul> Source code in <code>rl4co/models/zoo/symnco/policy.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int = 128,\n    env_name: str = \"tsp\",\n    num_encoder_layers: int = 3,\n    num_heads: int = 8,\n    normalization: str = \"batch\",\n    projection_head: nn.Module = None,\n    use_projection_head: bool = True,\n    **kwargs,\n):\n    super(SymNCOPolicy, self).__init__(\n        env_name=env_name,\n        embed_dim=embed_dim,\n        num_encoder_layers=num_encoder_layers,\n        num_heads=num_heads,\n        normalization=normalization,\n        **kwargs,\n    )\n\n    self.use_projection_head = use_projection_head\n\n    if self.use_projection_head:\n        self.projection_head = (\n            MLP(embed_dim, embed_dim, 1, embed_dim, nn.ReLU)\n            if projection_head is None\n            else projection_head\n        )\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.symnco.losses.problem_symmetricity_loss","title":"problem_symmetricity_loss","text":"<pre><code>problem_symmetricity_loss(reward, log_likelihood, dim=1)\n</code></pre> <p>REINFORCE loss for problem symmetricity Baseline is the average reward for all augmented problems Corresponds to <code>L_ps</code> in the SymNCO paper</p> Source code in <code>rl4co/models/zoo/symnco/losses.py</code> <pre><code>def problem_symmetricity_loss(reward, log_likelihood, dim=1):\n    \"\"\"REINFORCE loss for problem symmetricity\n    Baseline is the average reward for all augmented problems\n    Corresponds to `L_ps` in the SymNCO paper\n    \"\"\"\n    num_augment = reward.shape[dim]\n    if num_augment &lt; 2:\n        return 0\n    advantage = reward - reward.mean(dim=dim, keepdim=True)\n    loss = -advantage * log_likelihood\n    return loss.mean()\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.symnco.losses.solution_symmetricity_loss","title":"solution_symmetricity_loss","text":"<pre><code>solution_symmetricity_loss(reward, log_likelihood, dim=-1)\n</code></pre> <p>REINFORCE loss for solution symmetricity Baseline is the average reward for all start nodes Corresponds to <code>L_ss</code> in the SymNCO paper</p> Source code in <code>rl4co/models/zoo/symnco/losses.py</code> <pre><code>def solution_symmetricity_loss(reward, log_likelihood, dim=-1):\n    \"\"\"REINFORCE loss for solution symmetricity\n    Baseline is the average reward for all start nodes\n    Corresponds to `L_ss` in the SymNCO paper\n    \"\"\"\n    num_starts = reward.shape[dim]\n    if num_starts &lt; 2:\n        return 0\n    advantage = reward - reward.mean(dim=dim, keepdim=True)\n    loss = -advantage * log_likelihood\n    return loss.mean()\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_ar/#models.zoo.symnco.losses.invariance_loss","title":"invariance_loss","text":"<pre><code>invariance_loss(proj_embed, num_augment)\n</code></pre> <p>Loss for invariant representation on projected nodes Corresponds to <code>L_inv</code> in the SymNCO paper</p> Source code in <code>rl4co/models/zoo/symnco/losses.py</code> <pre><code>def invariance_loss(proj_embed, num_augment):\n    \"\"\"Loss for invariant representation on projected nodes\n    Corresponds to `L_inv` in the SymNCO paper\n    \"\"\"\n    pe = rearrange(proj_embed, \"(b a) ... -&gt; b a ...\", a=num_augment)\n    similarity = sum(\n        [cosine_similarity(pe[:, 0], pe[:, i], dim=-1) for i in range(1, num_augment)]\n    )\n    return similarity.mean()\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_nar/","title":"Constructive NonAutoregressive","text":""},{"location":"docs/content/api/zoo/constructive_nar/#deepaco","title":"DeepACO","text":""},{"location":"docs/content/api/zoo/constructive_nar/#models.zoo.deepaco.antsystem.AntSystem","title":"AntSystem","text":"<pre><code>AntSystem(\n    log_heuristic: Tensor,\n    n_ants: int = 20,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    decay: float = 0.95,\n    Q: Optional[float] = None,\n    temperature: float = 0.1,\n    pheromone: Optional[Tensor] = None,\n    require_logprobs: bool = False,\n    use_local_search: bool = False,\n    use_nls: bool = False,\n    n_perturbations: int = 5,\n    local_search_params: dict = {},\n    perturbation_params: dict = {},\n    start_node: Optional[int] = None,\n)\n</code></pre> <p>Implements the Ant System algorithm: https://doi.org/10.1109/3477.484436.</p> <p>Parameters:</p> <ul> <li> <code>log_heuristic</code>               (<code>Tensor</code>)           \u2013            <p>Logarithm of the heuristic matrix.</p> </li> <li> <code>n_ants</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>Number of ants to be used in the algorithm. Defaults to 20.</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Importance of pheromone in the decision-making process. Defaults to 1.0.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Importance of heuristic information in the decision-making process. Defaults to 1.0.</p> </li> <li> <code>decay</code>               (<code>float</code>, default:                   <code>0.95</code> )           \u2013            <p>Rate at which pheromone evaporates. Should be between 0 and 1. Defaults to 0.95.</p> </li> <li> <code>Q</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Rate at which pheromone deposits. Defaults to <code>1 / n_ants</code>.</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>Temperature for the softmax during decoding. Defaults to 0.1.</p> </li> <li> <code>pheromone</code>               (<code>Optional[Tensor]</code>, default:                   <code>None</code> )           \u2013            <p>Initial pheromone matrix. Defaults to <code>torch.ones_like(log_heuristic)</code>.</p> </li> <li> <code>require_logprobs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to require the log probability of actions. Defaults to False.</p> </li> <li> <code>use_local_search</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use local_search provided by the env. Default to False.</p> </li> <li> <code>use_nls</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use neural-guided local search provided by the env. Default to False.</p> </li> <li> <code>n_perturbations</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of perturbations to be used for nls. Defaults to 5.</p> </li> <li> <code>local_search_params</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Arguments to be passed to the local_search.</p> </li> <li> <code>perturbation_params</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Arguments to be passed to the perturbation used for nls.</p> </li> </ul> Source code in <code>rl4co/models/zoo/deepaco/antsystem.py</code> <pre><code>def __init__(\n    self,\n    log_heuristic: Tensor,\n    n_ants: int = 20,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    decay: float = 0.95,\n    Q: Optional[float] = None,\n    temperature: float = 0.1,\n    pheromone: Optional[Tensor] = None,\n    require_logprobs: bool = False,\n    use_local_search: bool = False,\n    use_nls: bool = False,\n    n_perturbations: int = 5,\n    local_search_params: dict = {},\n    perturbation_params: dict = {},\n    start_node: Optional[int] = None,\n):\n    self.batch_size = log_heuristic.shape[0]\n    self.n_ants = n_ants\n    self.alpha = alpha\n    self.beta = beta\n    self.decay = decay\n    self.Q = 1 / self.n_ants if Q is None else Q\n    self.temperature = temperature\n\n    self.log_heuristic = log_heuristic / self.temperature\n\n    if pheromone is None:\n        self.pheromone = torch.ones_like(log_heuristic)\n        self.pheromone.fill_(0.0005)\n    else:\n        self.pheromone = pheromone\n\n    self.final_actions = self.final_reward = None\n    self.require_logprobs = require_logprobs\n    self.all_records = []\n\n    self.use_local_search = use_local_search\n    assert not (use_nls and not use_local_search), \"use_nls requires use_local_search\"\n    self.use_nls = use_nls\n    self.n_perturbations = n_perturbations\n    self.local_search_params = local_search_params\n    self.perturbation_params = perturbation_params\n    self.start_node = start_node\n\n    self._batchindex = torch.arange(self.batch_size, device=log_heuristic.device)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_nar/#models.zoo.deepaco.antsystem.AntSystem.run","title":"run","text":"<pre><code>run(\n    td_initial: TensorDict,\n    env: RL4COEnvBase,\n    n_iterations: int,\n) -&gt; Tuple[TensorDict, Tensor, Tensor]\n</code></pre> <p>Run the Ant System algorithm for a specified number of iterations.</p> <p>Parameters:</p> <ul> <li> <code>td_initial</code>               (<code>TensorDict</code>)           \u2013            <p>Initial state of the problem.</p> </li> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>Environment representing the problem.</p> </li> <li> <code>n_iterations</code>               (<code>int</code>)           \u2013            <p>Number of iterations to run the algorithm.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>td</code> (              <code>TensorDict</code> )          \u2013            <p>The final state of the problem.</p> </li> <li> <code>actions</code> (              <code>Tensor</code> )          \u2013            <p>The final actions chosen by the algorithm.</p> </li> <li> <code>reward</code> (              <code>Tensor</code> )          \u2013            <p>The final reward achieved by the algorithm.</p> </li> </ul> Source code in <code>rl4co/models/zoo/deepaco/antsystem.py</code> <pre><code>def run(\n    self,\n    td_initial: TensorDict,\n    env: RL4COEnvBase,\n    n_iterations: int,\n) -&gt; Tuple[TensorDict, Tensor, Tensor]:\n    \"\"\"Run the Ant System algorithm for a specified number of iterations.\n\n    Args:\n        td_initial: Initial state of the problem.\n        env: Environment representing the problem.\n        n_iterations: Number of iterations to run the algorithm.\n\n    Returns:\n        td: The final state of the problem.\n        actions: The final actions chosen by the algorithm.\n        reward: The final reward achieved by the algorithm.\n    \"\"\"\n    for _ in range(n_iterations):\n        # reset environment\n        td = td_initial.clone()\n        self._one_step(td, env)\n\n    action_matrix = self._convert_final_action_to_matrix()\n    assert action_matrix is not None and self.final_reward is not None\n    td, env = self._recreate_final_routes(td_initial, env, action_matrix)\n\n    return td, action_matrix, self.final_reward\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_nar/#models.zoo.deepaco.antsystem.AntSystem.local_search","title":"local_search","text":"<pre><code>local_search(\n    td: TensorDict, env: RL4COEnvBase, actions: Tensor\n) -&gt; Tuple[Tensor, Tensor]\n</code></pre> <p>Perform local search on the actions and reward obtained.</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>Current state of the problem.</p> </li> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>Environment representing the problem.</p> </li> <li> <code>actions</code>               (<code>Tensor</code>)           \u2013            <p>Actions chosen by the algorithm.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>actions</code> (              <code>Tensor</code> )          \u2013            <p>The modified actions</p> </li> <li> <code>reward</code> (              <code>Tensor</code> )          \u2013            <p>The modified reward</p> </li> </ul> Source code in <code>rl4co/models/zoo/deepaco/antsystem.py</code> <pre><code>def local_search(\n    self, td: TensorDict, env: RL4COEnvBase, actions: Tensor\n) -&gt; Tuple[Tensor, Tensor]:\n    \"\"\"Perform local search on the actions and reward obtained.\n\n    Args:\n        td: Current state of the problem.\n        env: Environment representing the problem.\n        actions: Actions chosen by the algorithm.\n\n    Returns:\n        actions: The modified actions\n        reward: The modified reward\n    \"\"\"\n    td_cpu = td.detach().cpu()  # Convert to CPU in advance to minimize the overhead from device transfer\n    td_cpu[\"distances\"] = get_distance_matrix(td_cpu[\"locs\"])\n    # TODO: avoid or generalize this, e.g., pre-compute for local search in each env\n    actions = actions.detach().cpu()\n    best_actions = env.local_search(td=td_cpu, actions=actions, **self.local_search_params)\n    best_rewards = env.get_reward(td_cpu, best_actions)\n\n    if self.use_nls:\n        td_cpu_perturb = td_cpu.clone()\n        td_cpu_perturb[\"distances\"] = torch.tile(self.heuristic_dist, (self.n_ants, 1, 1))\n        new_actions = best_actions.clone()\n\n        for _ in range(self.n_perturbations):\n            perturbed_actions = env.local_search(\n                td=td_cpu_perturb, actions=new_actions, **self.perturbation_params\n            )\n            new_actions = env.local_search(td=td_cpu, actions=perturbed_actions, **self.local_search_params)\n            new_rewards = env.get_reward(td_cpu, new_actions)\n\n            improved_indices = new_rewards &gt; best_rewards\n            best_actions = env.replace_selected_actions(best_actions, new_actions, improved_indices)\n            best_rewards[improved_indices] = new_rewards[improved_indices]\n\n    best_actions = best_actions.to(td.device)\n    best_rewards = best_rewards.to(td.device)\n\n    return best_actions, best_rewards\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_nar/#models.zoo.deepaco.antsystem.AntSystem.get_logp","title":"get_logp","text":"<pre><code>get_logp()\n</code></pre> <p>Get the log probability (logprobs) values recorded during the execution of the algorithm.</p> <p>Returns:</p> <ul> <li> <code>results</code>          \u2013            <p>Tuple containing the log probability values, actions chosen, rewards obtained, and mask values (if available).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AssertionError</code>             \u2013            <p>If <code>require_logp</code> is not enabled.</p> </li> </ul> Source code in <code>rl4co/models/zoo/deepaco/antsystem.py</code> <pre><code>def get_logp(self):\n    \"\"\"Get the log probability (logprobs) values recorded during the execution of the algorithm.\n\n    Returns:\n        results: Tuple containing the log probability values,\n            actions chosen, rewards obtained, and mask values (if available).\n\n    Raises:\n        AssertionError: If `require_logp` is not enabled.\n    \"\"\"\n\n    assert (\n        self.require_logprobs\n    ), \"Please enable `require_logp` to record logprobs values\"\n\n    logprobs_list, actions_list, reward_list, mask_list = [], [], [], []\n\n    for logprobs, actions, reward, mask in self.all_records:\n        logprobs_list.append(logprobs)\n        actions_list.append(actions)\n        reward_list.append(reward)\n        mask_list.append(mask)\n\n    if mask_list[0] is None:\n        mask_list = None\n    else:\n        mask_list = torch.stack(mask_list, 0)\n\n    # reset records\n    self.all_records = []\n\n    return (\n        torch.stack(logprobs_list, 0),\n        torch.stack(actions_list, 0),\n        torch.stack(reward_list, 0),\n        mask_list,\n    )\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_nar/#models.zoo.deepaco.model.DeepACO","title":"DeepACO","text":"<pre><code>DeepACO(\n    env: RL4COEnvBase,\n    policy: Optional[DeepACOPolicy] = None,\n    baseline: Union[REINFORCEBaseline, str] = \"no\",\n    policy_kwargs: dict = {},\n    baseline_kwargs: dict = {},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>REINFORCE</code></p> <p>Implements DeepACO: https://arxiv.org/abs/2309.14032.</p> <p>Parameters:</p> <ul> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>Environment to use for the algorithm</p> </li> <li> <code>policy</code>               (<code>Optional[DeepACOPolicy]</code>, default:                   <code>None</code> )           \u2013            <p>Policy to use for the algorithm</p> </li> <li> <code>baseline</code>               (<code>Union[REINFORCEBaseline, str]</code>, default:                   <code>'no'</code> )           \u2013            <p>REINFORCE baseline. Defaults to exponential</p> </li> <li> <code>policy_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments for policy</p> </li> <li> <code>baseline_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments for baseline</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Keyword arguments passed to the superclass</p> </li> </ul> Source code in <code>rl4co/models/zoo/deepaco/model.py</code> <pre><code>def __init__(\n    self,\n    env: RL4COEnvBase,\n    policy: Optional[DeepACOPolicy] = None,\n    baseline: Union[REINFORCEBaseline, str] = \"no\",\n    policy_kwargs: dict = {},\n    baseline_kwargs: dict = {},\n    **kwargs,\n):\n    if policy is None:\n        policy = DeepACOPolicy(env_name=env.name, **policy_kwargs)\n\n    super().__init__(env, policy, baseline, baseline_kwargs, **kwargs)\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_nar/#models.zoo.deepaco.policy.DeepACOPolicy","title":"DeepACOPolicy","text":"<pre><code>DeepACOPolicy(\n    encoder: Optional[NonAutoregressiveEncoder] = None,\n    env_name: str = \"tsp\",\n    temperature: float = 1.0,\n    aco_class: Optional[Type[AntSystem]] = None,\n    aco_kwargs: dict = {},\n    train_with_local_search: bool = True,\n    n_ants: Optional[Union[int, dict]] = None,\n    n_iterations: Optional[Union[int, dict]] = None,\n    ls_reward_aug_W: float = 0.95,\n    **encoder_kwargs\n)\n</code></pre> <p>               Bases: <code>NonAutoregressivePolicy</code></p> <p>Implememts DeepACO policy based on :class:<code>NonAutoregressivePolicy</code>. Introduced by Ye et al. (2023): https://arxiv.org/abs/2309.14032. This policy uses a Non-Autoregressive Graph Neural Network to generate heatmaps, which are then used to run Ant Colony Optimization (ACO) to construct solutions.</p> <p>Parameters:</p> <ul> <li> <code>encoder</code>               (<code>Optional[NonAutoregressiveEncoder]</code>, default:                   <code>None</code> )           \u2013            <p>Encoder module. Can be passed by sub-classes</p> </li> <li> <code>env_name</code>               (<code>str</code>, default:                   <code>'tsp'</code> )           \u2013            <p>Name of the environment used to initialize embeddings</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Temperature for the softmax during decoding. Defaults to 0.1.</p> </li> <li> <code>aco_class</code>               (<code>Optional[Type[AntSystem]]</code>, default:                   <code>None</code> )           \u2013            <p>Class representing the ACO algorithm to be used. Defaults to :class:<code>AntSystem</code>.</p> </li> <li> <code>aco_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments to be passed to the ACO algorithm.</p> </li> <li> <code>n_ants</code>               (<code>Optional[Union[int, dict]]</code>, default:                   <code>None</code> )           \u2013            <p>Number of ants to be used in the ACO algorithm. Can be an integer or dictionary. Defaults to 20.</p> </li> <li> <code>n_iterations</code>               (<code>Optional[Union[int, dict]]</code>, default:                   <code>None</code> )           \u2013            <p>Number of iterations to run the ACO algorithm. Can be an integer or dictionary. Defaults to <code>dict(train=1, val=20, test=100)</code>.</p> </li> <li> <code>ls_reward_aug_W</code>               (<code>float</code>, default:                   <code>0.95</code> )           \u2013            <p>Coefficient to be used for the reward augmentation with the local search. Defaults to 0.95.</p> </li> <li> <code>encoder_kwargs</code>           \u2013            <p>Additional arguments to be passed to the encoder.</p> </li> </ul> Source code in <code>rl4co/models/zoo/deepaco/policy.py</code> <pre><code>def __init__(\n    self,\n    encoder: Optional[NonAutoregressiveEncoder] = None,\n    env_name: str = \"tsp\",\n    temperature: float = 1.0,\n    aco_class: Optional[Type[AntSystem]] = None,\n    aco_kwargs: dict = {},\n    train_with_local_search: bool = True,\n    n_ants: Optional[Union[int, dict]] = None,\n    n_iterations: Optional[Union[int, dict]] = None,\n    ls_reward_aug_W: float = 0.95,\n    **encoder_kwargs,\n):\n    if encoder is None:\n        encoder = NARGNNEncoder(**encoder_kwargs)\n\n    super(DeepACOPolicy, self).__init__(\n        encoder=encoder,\n        env_name=env_name,\n        temperature=temperature,\n        train_decode_type=\"multistart_sampling\",\n        val_decode_type=\"multistart_sampling\",\n        test_decode_type=\"multistart_sampling\",\n    )\n\n    self.aco_class = AntSystem if aco_class is None else aco_class\n    self.aco_kwargs = aco_kwargs\n    self.train_with_local_search = train_with_local_search\n    self.n_ants = merge_with_defaults(n_ants, train=30, val=48, test=48)\n    self.n_iterations = merge_with_defaults(n_iterations, train=1, val=5, test=10)\n    self.ls_reward_aug_W = ls_reward_aug_W\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_nar/#models.zoo.deepaco.policy.DeepACOPolicy.forward","title":"forward","text":"<pre><code>forward(\n    td_initial: TensorDict,\n    env: Optional[Union[str, RL4COEnvBase]] = None,\n    calc_reward: bool = True,\n    phase: str = \"train\",\n    actions=None,\n    return_actions: bool = True,\n    return_hidden: bool = True,\n    **kwargs\n)\n</code></pre> <p>Forward method. During validation and testing, the policy runs the ACO algorithm to construct solutions. See :class:<code>NonAutoregressivePolicy</code> for more details during the training phase.</p> Source code in <code>rl4co/models/zoo/deepaco/policy.py</code> <pre><code>def forward(\n    self,\n    td_initial: TensorDict,\n    env: Optional[Union[str, RL4COEnvBase]] = None,\n    calc_reward: bool = True,\n    phase: str = \"train\",\n    actions=None,\n    return_actions: bool = True,\n    return_hidden: bool = True,\n    **kwargs,\n):\n    \"\"\"\n    Forward method. During validation and testing, the policy runs the ACO algorithm to construct solutions.\n    See :class:`NonAutoregressivePolicy` for more details during the training phase.\n    \"\"\"\n    n_ants = self.n_ants[phase]\n    # Instantiate environment if needed\n    if (phase != \"train\" or self.ls_reward_aug_W &gt; 0) and (env is None or isinstance(env, str)):\n        env_name = self.env_name if env is None else env\n        env = get_env(env_name)\n\n    if phase == \"train\":\n        select_start_nodes_fn = partial(\n            self.aco_class.select_start_node_fn, start_node=self.aco_kwargs.get(\"start_node\", None)\n        )\n        kwargs.update({\"select_start_nodes_fn\": select_start_nodes_fn})\n        #  we just use the constructive policy\n        outdict = super().forward(\n            td_initial,\n            env,\n            phase=phase,\n            decode_type=\"multistart_sampling\",\n            calc_reward=calc_reward,\n            num_starts=n_ants,\n            actions=actions,\n            return_actions=return_actions,\n            return_hidden=return_hidden,\n            **kwargs,\n        )\n\n        # manually compute the advantage\n        reward = unbatchify(outdict[\"reward\"], n_ants)\n        advantage = reward - reward.mean(dim=1, keepdim=True)\n\n        if self.ls_reward_aug_W &gt; 0 and self.train_with_local_search:\n            heatmap_logits = outdict[\"hidden\"]\n            aco = self.aco_class(\n                heatmap_logits,\n                n_ants=n_ants,\n                temperature=self.aco_kwargs.get(\"temperature\", self.temperature),\n                **self.aco_kwargs,\n            )\n\n            actions = outdict[\"actions\"]\n            _, ls_reward = aco.local_search(batchify(td_initial, n_ants), env, actions)\n\n            ls_reward = unbatchify(ls_reward, n_ants)\n            ls_advantage = ls_reward - ls_reward.mean(dim=1, keepdim=True)\n            advantage = advantage * (1 - self.ls_reward_aug_W) + ls_advantage * self.ls_reward_aug_W\n\n        outdict[\"advantage\"] = advantage\n        outdict[\"log_likelihood\"] = unbatchify(outdict[\"log_likelihood\"], n_ants)\n\n        return outdict\n\n    heatmap_logits, _ = self.encoder(td_initial)\n\n    aco = self.aco_class(\n        heatmap_logits,\n        n_ants=self.n_ants[phase],\n        temperature=self.aco_kwargs.get(\"temperature\", self.temperature),\n        **self.aco_kwargs,\n    )\n    td, actions, reward = aco.run(td_initial, env, self.n_iterations[phase])\n\n    out = {}\n    if calc_reward:\n        out[\"reward\"] = reward\n    if return_actions:\n        out[\"actions\"] = actions\n\n    return out\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_nar/#nar-gnn","title":"NAR-GNN","text":""},{"location":"docs/content/api/zoo/constructive_nar/#models.zoo.nargnn.policy.NARGNNPolicy","title":"NARGNNPolicy","text":"<pre><code>NARGNNPolicy(\n    encoder: Optional[NonAutoregressiveEncoder] = None,\n    decoder: Optional[NonAutoregressiveDecoder] = None,\n    embed_dim: int = 64,\n    env_name: str = \"tsp\",\n    init_embedding: Optional[Module] = None,\n    edge_embedding: Optional[Module] = None,\n    graph_network: Optional[Module] = None,\n    heatmap_generator: Optional[Module] = None,\n    num_layers_heatmap_generator: int = 5,\n    num_layers_graph_encoder: int = 15,\n    act_fn=\"silu\",\n    agg_fn=\"mean\",\n    linear_bias: bool = True,\n    train_decode_type: str = \"multistart_sampling\",\n    val_decode_type: str = \"multistart_greedy\",\n    test_decode_type: str = \"multistart_greedy\",\n    **constructive_policy_kw\n)\n</code></pre> <p>               Bases: <code>NonAutoregressivePolicy</code></p> <p>Base Non-autoregressive policy for NCO construction methods. This creates a heatmap of NxN for N nodes (i.e., heuristic) that models the probability to go from one node to another for all nodes.</p> The policy performs the following steps <ol> <li>Encode the environment initial state into node embeddings</li> <li>Decode (non-autoregressively) to construct the solution to the NCO problem</li> </ol> Warning <p>The effectiveness of the non-autoregressive approach can vary significantly across different problem types and configurations. It may require careful tuning of the model architecture and decoding strategy to achieve competitive results.</p> <p>Parameters:</p> <ul> <li> <code>encoder</code>               (<code>Optional[NonAutoregressiveEncoder]</code>, default:                   <code>None</code> )           \u2013            <p>Encoder module. Can be passed by sub-classes</p> </li> <li> <code>decoder</code>               (<code>Optional[NonAutoregressiveDecoder]</code>, default:                   <code>None</code> )           \u2013            <p>Decoder module. Note that this moule defaults to the non-autoregressive decoder</p> </li> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>64</code> )           \u2013            <p>Dimension of the embeddings</p> </li> <li> <code>env_name</code>               (<code>str</code>, default:                   <code>'tsp'</code> )           \u2013            <p>Name of the environment used to initialize embeddings</p> </li> <li> <code>init_embedding</code>               (<code>Optional[Module]</code>, default:                   <code>None</code> )           \u2013            <p>Model to use for the initial embedding. If None, use the default embedding for the environment</p> </li> <li> <code>edge_embedding</code>               (<code>Optional[Module]</code>, default:                   <code>None</code> )           \u2013            <p>Model to use for the edge embedding. If None, use the default embedding for the environment</p> </li> <li> <code>graph_network</code>               (<code>Optional[Module]</code>, default:                   <code>None</code> )           \u2013            <p>Model to use for the graph network. If None, use the default embedding for the environment</p> </li> <li> <code>heatmap_generator</code>               (<code>Optional[Module]</code>, default:                   <code>None</code> )           \u2013            <p>Model to use for the heatmap generator. If None, use the default embedding for the environment</p> </li> <li> <code>num_layers_heatmap_generator</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of layers in the heatmap generator</p> </li> <li> <code>num_layers_graph_encoder</code>               (<code>int</code>, default:                   <code>15</code> )           \u2013            <p>Number of layers in the graph encoder</p> </li> <li> <code>act_fn</code>           \u2013            <p>Activation function to use in the encoder</p> </li> <li> <code>agg_fn</code>           \u2013            <p>Aggregation function to use in the encoder</p> </li> <li> <code>linear_bias</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use bias in the encoder</p> </li> <li> <code>train_decode_type</code>               (<code>str</code>, default:                   <code>'multistart_sampling'</code> )           \u2013            <p>Type of decoding during training</p> </li> <li> <code>val_decode_type</code>               (<code>str</code>, default:                   <code>'multistart_greedy'</code> )           \u2013            <p>Type of decoding during validation</p> </li> <li> <code>test_decode_type</code>               (<code>str</code>, default:                   <code>'multistart_greedy'</code> )           \u2013            <p>Type of decoding during testing</p> </li> <li> <code>**constructive_policy_kw</code>           \u2013            <p>Unused keyword arguments</p> </li> </ul> Source code in <code>rl4co/models/zoo/nargnn/policy.py</code> <pre><code>def __init__(\n    self,\n    encoder: Optional[NonAutoregressiveEncoder] = None,\n    decoder: Optional[NonAutoregressiveDecoder] = None,\n    embed_dim: int = 64,\n    env_name: str = \"tsp\",\n    init_embedding: Optional[nn.Module] = None,\n    edge_embedding: Optional[nn.Module] = None,\n    graph_network: Optional[nn.Module] = None,\n    heatmap_generator: Optional[nn.Module] = None,\n    num_layers_heatmap_generator: int = 5,\n    num_layers_graph_encoder: int = 15,\n    act_fn=\"silu\",\n    agg_fn=\"mean\",\n    linear_bias: bool = True,\n    train_decode_type: str = \"multistart_sampling\",\n    val_decode_type: str = \"multistart_greedy\",\n    test_decode_type: str = \"multistart_greedy\",\n    **constructive_policy_kw,\n):\n    if len(constructive_policy_kw) &gt; 0:\n        log.warn(f\"Unused kwargs: {constructive_policy_kw}\")\n\n    if encoder is None:\n        encoder = NARGNNEncoder(\n            embed_dim=embed_dim,\n            env_name=env_name,\n            init_embedding=init_embedding,\n            edge_embedding=edge_embedding,\n            graph_network=graph_network,\n            heatmap_generator=heatmap_generator,\n            num_layers_heatmap_generator=num_layers_heatmap_generator,\n            num_layers_graph_encoder=num_layers_graph_encoder,\n            act_fn=act_fn,\n            agg_fn=agg_fn,\n            linear_bias=linear_bias,\n        )\n\n    # The decoder generates logits given the current td and heatmap\n    if decoder is None:\n        decoder = NonAutoregressiveDecoder()\n    else:\n        # check if the decoder has trainable parameters\n        if any(p.requires_grad for p in decoder.parameters()):\n            log.error(\n                \"The decoder contains trainable parameters. This should not happen in a non-autoregressive policy.\"\n            )\n\n    # Pass to constructive policy\n    super(NARGNNPolicy, self).__init__(\n        encoder=encoder,\n        decoder=decoder,\n        env_name=env_name,\n        train_decode_type=train_decode_type,\n        val_decode_type=val_decode_type,\n        test_decode_type=test_decode_type,\n        **constructive_policy_kw,\n    )\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_nar/#models.zoo.nargnn.encoder.EdgeHeatmapGenerator","title":"EdgeHeatmapGenerator","text":"<pre><code>EdgeHeatmapGenerator(\n    embed_dim: int,\n    num_layers: int,\n    act_fn: Union[str, Callable] = \"silu\",\n    linear_bias: bool = True,\n    undirected_graph: bool = True,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>MLP for converting edge embeddings to heatmaps.</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>)           \u2013            <p>Dimension of the embeddings</p> </li> <li> <code>num_layers</code>               (<code>int</code>)           \u2013            <p>The number of linear layers in the network.</p> </li> <li> <code>act_fn</code>               (<code>Union[str, Callable]</code>, default:                   <code>'silu'</code> )           \u2013            <p>Activation function. Defaults to \"silu\".</p> </li> <li> <code>linear_bias</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Use bias in linear layers. Defaults to True.</p> </li> <li> <code>undirected_graph</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether the graph is undirected. Defaults to True.</p> </li> </ul> Source code in <code>rl4co/models/zoo/nargnn/encoder.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int,\n    num_layers: int,\n    act_fn: Union[str, Callable] = \"silu\",\n    linear_bias: bool = True,\n    undirected_graph: bool = True,\n) -&gt; None:\n    super(EdgeHeatmapGenerator, self).__init__()\n\n    self.linears = nn.ModuleList(\n        [\n            nn.Linear(embed_dim, embed_dim, bias=linear_bias)\n            for _ in range(num_layers - 1)\n        ]\n    )\n    self.output = nn.Linear(embed_dim, 1, bias=linear_bias)\n\n    self.act = getattr(nn.functional, act_fn) if isinstance(act_fn, str) else act_fn\n\n    self.undirected_graph = undirected_graph\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_nar/#models.zoo.nargnn.encoder.NARGNNEncoder","title":"NARGNNEncoder","text":"<pre><code>NARGNNEncoder(\n    embed_dim: int = 64,\n    env_name: str = \"tsp\",\n    init_embedding: Optional[Module] = None,\n    edge_embedding: Optional[Module] = None,\n    graph_network: Optional[Module] = None,\n    heatmap_generator: Optional[Module] = None,\n    num_layers_heatmap_generator: int = 5,\n    num_layers_graph_encoder: int = 15,\n    act_fn=\"silu\",\n    agg_fn=\"mean\",\n    linear_bias: bool = True,\n    k_sparse: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>NonAutoregressiveEncoder</code></p> <p>Anisotropic Graph Neural Network encoder with edge-gating mechanism as in Joshi et al. (2022), and used in DeepACO (Ye et al., 2023). This creates a heatmap of NxN for N nodes (i.e., heuristic) that models the probability to go from one node to another for all nodes. This model utilizes a multi-layer perceptron (MLP) approach to predict edge attributes directly from the input graph features, which are then transformed into a heatmap representation to facilitate the decoding of the solution. The decoding process is managed by a specified strategy which could vary from simple greedy selection to more complex sampling methods.</p> Tip <p>This decoder's performance heavily relies on the ability of the MLP to capture the dependencies between different parts of the solution without the iterative refinement provided by autoregressive models. It is particularly useful in scenarios where the solution space can be effectively explored in a parallelized manner or when the solution components are largely independent.</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>64</code> )           \u2013            <p>Dimension of the node embeddings</p> </li> <li> <code>env_name</code>               (<code>str</code>, default:                   <code>'tsp'</code> )           \u2013            <p>Name of the environment used to initialize embeddings</p> </li> <li> <code>num_layers</code>           \u2013            <p>Number of layers in the encoder</p> </li> <li> <code>init_embedding</code>               (<code>Optional[Module]</code>, default:                   <code>None</code> )           \u2013            <p>Model to use for the initial embedding. If None, use the default embedding for the environment</p> </li> <li> <code>edge_embedding</code>               (<code>Optional[Module]</code>, default:                   <code>None</code> )           \u2013            <p>Model to use for the edge embedding. If None, use the default embedding for the environment</p> </li> <li> <code>graph_network</code>               (<code>Optional[Module]</code>, default:                   <code>None</code> )           \u2013            <p>Model to use for the graph network. If None, use the default network for the environment</p> </li> <li> <code>heatmap_generator</code>               (<code>Optional[Module]</code>, default:                   <code>None</code> )           \u2013            <p>Model to use for the heatmap generator. If None, use the default network for the environment</p> </li> <li> <code>num_layers_heatmap_generator</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of layers in the heatmap generator</p> </li> <li> <code>num_layers_graph_encoder</code>               (<code>int</code>, default:                   <code>15</code> )           \u2013            <p>Number of layers in the graph encoder</p> </li> <li> <code>act_fn</code>           \u2013            <p>The activation function to use in each GNNLayer, see https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions for available options. Defaults to 'silu'.</p> </li> <li> <code>agg_fn</code>           \u2013            <p>The aggregation function to use in each GNNLayer for pooling features. Options: 'add', 'mean', 'max'. Defaults to 'mean'.</p> </li> <li> <code>linear_bias</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Use bias in linear layers. Defaults to True.</p> </li> <li> <code>k_sparse</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Number of edges to keep for each node. Defaults to None.</p> </li> </ul> Source code in <code>rl4co/models/zoo/nargnn/encoder.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int = 64,\n    env_name: str = \"tsp\",\n    # TODO: pass network\n    init_embedding: Optional[nn.Module] = None,\n    edge_embedding: Optional[nn.Module] = None,\n    graph_network: Optional[nn.Module] = None,\n    heatmap_generator: Optional[nn.Module] = None,\n    num_layers_heatmap_generator: int = 5,\n    num_layers_graph_encoder: int = 15,\n    act_fn=\"silu\",\n    agg_fn=\"mean\",\n    linear_bias: bool = True,\n    k_sparse: Optional[int] = None,\n):\n    super(NonAutoregressiveEncoder, self).__init__()\n    self.env_name = env_name\n\n    self.init_embedding = (\n        env_init_embedding(self.env_name, {\"embed_dim\": embed_dim})\n        if init_embedding is None\n        else init_embedding\n    )\n\n    self.edge_embedding = (\n        env_edge_embedding(self.env_name, {\"embed_dim\": embed_dim, \"k_sparse\": k_sparse})\n        if edge_embedding is None\n        else edge_embedding\n    )\n\n    self.graph_network = (\n        GNNEncoder(\n            embed_dim=embed_dim,\n            num_layers=num_layers_graph_encoder,\n            act_fn=act_fn,\n            agg_fn=agg_fn,\n        )\n        if graph_network is None\n        else graph_network\n    )\n\n    self.heatmap_generator = (\n        EdgeHeatmapGenerator(\n            embed_dim=embed_dim,\n            num_layers=num_layers_heatmap_generator,\n            linear_bias=linear_bias,\n        )\n        if heatmap_generator is None\n        else heatmap_generator\n    )\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_nar/#models.zoo.nargnn.encoder.NARGNNEncoder.forward","title":"forward","text":"<pre><code>forward(td: TensorDict)\n</code></pre> <p>Forward pass of the encoder. Transform the input TensorDict into the latent representation.</p> Source code in <code>rl4co/models/zoo/nargnn/encoder.py</code> <pre><code>def forward(self, td: TensorDict):\n    \"\"\"Forward pass of the encoder.\n    Transform the input TensorDict into the latent representation.\n    \"\"\"\n    # Transfer to embedding space\n    node_embed = self.init_embedding(td)\n    graph = self.edge_embedding(td, node_embed)\n\n    # Process embedding into graph\n    # TODO: standardize?\n    graph.x, graph.edge_attr = self.graph_network(\n        graph.x, graph.edge_index, graph.edge_attr\n    )\n\n    # Generate heatmap logits\n    heatmap_logits = self.heatmap_generator(graph)\n\n    # Return latent representation (i.e. heatmap logits) and initial embeddings\n    return heatmap_logits, node_embed\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_nar/#models.zoo.nargnn.encoder.NARGNNNodeEncoder","title":"NARGNNNodeEncoder","text":"<pre><code>NARGNNNodeEncoder(\n    embed_dim: int = 64,\n    env_name: str = \"tsp\",\n    init_embedding: Optional[Module] = None,\n    edge_embedding: Optional[Module] = None,\n    graph_network: Optional[Module] = None,\n    heatmap_generator: Optional[Module] = None,\n    num_layers_heatmap_generator: int = 5,\n    num_layers_graph_encoder: int = 15,\n    act_fn=\"silu\",\n    agg_fn=\"mean\",\n    linear_bias: bool = True,\n    k_sparse: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>NARGNNEncoder</code></p> <p>In this case, we just use the node embeddings from the graph without transforming them into a heatmap.</p> Source code in <code>rl4co/models/zoo/nargnn/encoder.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int = 64,\n    env_name: str = \"tsp\",\n    # TODO: pass network\n    init_embedding: Optional[nn.Module] = None,\n    edge_embedding: Optional[nn.Module] = None,\n    graph_network: Optional[nn.Module] = None,\n    heatmap_generator: Optional[nn.Module] = None,\n    num_layers_heatmap_generator: int = 5,\n    num_layers_graph_encoder: int = 15,\n    act_fn=\"silu\",\n    agg_fn=\"mean\",\n    linear_bias: bool = True,\n    k_sparse: Optional[int] = None,\n):\n    super(NonAutoregressiveEncoder, self).__init__()\n    self.env_name = env_name\n\n    self.init_embedding = (\n        env_init_embedding(self.env_name, {\"embed_dim\": embed_dim})\n        if init_embedding is None\n        else init_embedding\n    )\n\n    self.edge_embedding = (\n        env_edge_embedding(self.env_name, {\"embed_dim\": embed_dim, \"k_sparse\": k_sparse})\n        if edge_embedding is None\n        else edge_embedding\n    )\n\n    self.graph_network = (\n        GNNEncoder(\n            embed_dim=embed_dim,\n            num_layers=num_layers_graph_encoder,\n            act_fn=act_fn,\n            agg_fn=agg_fn,\n        )\n        if graph_network is None\n        else graph_network\n    )\n\n    self.heatmap_generator = (\n        EdgeHeatmapGenerator(\n            embed_dim=embed_dim,\n            num_layers=num_layers_heatmap_generator,\n            linear_bias=linear_bias,\n        )\n        if heatmap_generator is None\n        else heatmap_generator\n    )\n</code></pre>"},{"location":"docs/content/api/zoo/constructive_nar/#models.zoo.nargnn.encoder.NARGNNNodeEncoder.forward","title":"forward","text":"<pre><code>forward(td: TensorDict)\n</code></pre> <p>Forward pass of the encoder. Transform the input TensorDict into the latent representation.</p> Source code in <code>rl4co/models/zoo/nargnn/encoder.py</code> <pre><code>def forward(self, td: TensorDict):\n    # Transfer to embedding space\n    node_embed = self.init_embedding(td)\n    graph = self.edge_embedding(td, node_embed)\n\n    # Process embedding into graph\n    # TODO: standardize?\n    graph.x, graph.edge_attr = self.graph_network(\n        graph.x, graph.edge_index, graph.edge_attr\n    )\n\n    proc_embeds = graph.x\n    batch_size = node_embed.shape[0]\n    # reshape proc_embeds from [bs*n, h] to [bs, n, h]\n    proc_embeds = proc_embeds.reshape(batch_size, -1, proc_embeds.shape[1])\n    return proc_embeds, node_embed\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/","title":"Improvement Methods","text":"<p>These methods are trained to improve existing solutions iteratively, akin to local search algorithms. They focus on refining existing solutions rather than generating them from scratch.</p>"},{"location":"docs/content/api/zoo/improvement/#dact","title":"DACT","text":""},{"location":"docs/content/api/zoo/improvement/#models.zoo.dact.encoder.DACTEncoder","title":"DACTEncoder","text":"<pre><code>DACTEncoder(\n    embed_dim: int = 64,\n    init_embedding: Module = None,\n    pos_embedding: Module = None,\n    env_name: str = \"tsp_kopt\",\n    pos_type: str = \"CPE\",\n    num_heads: int = 4,\n    num_layers: int = 3,\n    normalization: str = \"layer\",\n    feedforward_hidden: int = 64,\n)\n</code></pre> <p>               Bases: <code>ImprovementEncoder</code></p> <p>Dual-Aspect Collaborative Transformer Encoder as in Ma et al. (2021)</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>64</code> )           \u2013            <p>Dimension of the embedding space</p> </li> <li> <code>init_embedding</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Module to use for the initialization of the node embeddings</p> </li> <li> <code>pos_embedding</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Module to use for the initialization of the positional embeddings</p> </li> <li> <code>env_name</code>               (<code>str</code>, default:                   <code>'tsp_kopt'</code> )           \u2013            <p>Name of the environment used to initialize embeddings</p> </li> <li> <code>pos_type</code>               (<code>str</code>, default:                   <code>'CPE'</code> )           \u2013            <p>Name of the used positional encoding method (CPE or APE)</p> </li> <li> <code>num_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads in the attention layers</p> </li> <li> <code>num_layers</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of layers in the attention network</p> </li> <li> <code>normalization</code>               (<code>str</code>, default:                   <code>'layer'</code> )           \u2013            <p>Normalization type in the attention layers</p> </li> <li> <code>feedforward_hidden</code>               (<code>int</code>, default:                   <code>64</code> )           \u2013            <p>Hidden dimension in the feedforward layers</p> </li> </ul> Source code in <code>rl4co/models/zoo/dact/encoder.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int = 64,\n    init_embedding: nn.Module = None,\n    pos_embedding: nn.Module = None,\n    env_name: str = \"tsp_kopt\",\n    pos_type: str = \"CPE\",\n    num_heads: int = 4,\n    num_layers: int = 3,\n    normalization: str = \"layer\",\n    feedforward_hidden: int = 64,\n):\n    super(DACTEncoder, self).__init__(\n        embed_dim=embed_dim,\n        env_name=env_name,\n        pos_type=pos_type,\n        num_heads=num_heads,\n        num_layers=num_layers,\n        normalization=normalization,\n        feedforward_hidden=feedforward_hidden,\n    )\n\n    assert self.env_name in [\"tsp_kopt\"], NotImplementedError()\n\n    self.net = AdaptiveSequential(\n        *(\n            DACTEncoderLayer(\n                num_heads,\n                embed_dim,\n                feedforward_hidden,\n                normalization,\n            )\n            for _ in range(num_layers)\n        )\n    )\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#models.zoo.dact.decoder.DACTDecoder","title":"DACTDecoder","text":"<pre><code>DACTDecoder(embed_dim: int = 64, num_heads: int = 4)\n</code></pre> <p>               Bases: <code>ImprovementDecoder</code></p> <p>DACT decoder based on Ma et al. (2021) Given the environment state and the dual sets of embeddings (PFE, NFE embeddings), compute the logits for selecting two nodes for the 2-opt local search from the current solution</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>64</code> )           \u2013            <p>Embedding dimension</p> </li> <li> <code>num_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of attention heads</p> </li> </ul> Source code in <code>rl4co/models/zoo/dact/decoder.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int = 64,\n    num_heads: int = 4,\n):\n    super().__init__()\n    self.embed_dim = embed_dim\n    self.n_heads = num_heads\n    self.hidden_dim = embed_dim\n\n    # for MHC sublayer (NFE aspect)\n    self.compater_node = MultiHeadCompat(\n        num_heads, embed_dim, embed_dim, embed_dim, embed_dim\n    )\n\n    # for MHC sublayer (PFE aspect)\n    self.compater_pos = MultiHeadCompat(\n        num_heads, embed_dim, embed_dim, embed_dim, embed_dim\n    )\n\n    self.norm_factor = 1 / math.sqrt(1 * self.hidden_dim)\n\n    # for Max-Pooling sublayer\n    self.project_graph_pos = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n    self.project_graph_node = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n    self.project_node_pos = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n    self.project_node_node = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n\n    # for feed-forward aggregation (FFA)sublayer\n    self.value_head = MLP(\n        input_dim=2 * self.n_heads,\n        output_dim=1,\n        num_neurons=[32, 32],\n        dropout_probs=[0.05, 0.00],\n    )\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#models.zoo.dact.decoder.DACTDecoder.forward","title":"forward","text":"<pre><code>forward(\n    td: TensorDict, final_h: Tensor, final_p: Tensor\n) -&gt; Tensor\n</code></pre> <p>Compute the logits of the removing a node pair from the current solution</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>TensorDict with the current environment state</p> </li> <li> <code>final_h</code>               (<code>Tensor</code>)           \u2013            <p>final NFE embeddings</p> </li> <li> <code>final_p</code>               (<code>Tensor</code>)           \u2013            <p>final pfe embeddings</p> </li> </ul> Source code in <code>rl4co/models/zoo/dact/decoder.py</code> <pre><code>def forward(self, td: TensorDict, final_h: Tensor, final_p: Tensor) -&gt; Tensor:\n    \"\"\"Compute the logits of the removing a node pair from the current solution\n\n    Args:\n        td: TensorDict with the current environment state\n        final_h: final NFE embeddings\n        final_p: final pfe embeddings\n    \"\"\"\n\n    batch_size, graph_size, dim = final_h.size()\n\n    # Max-Pooling sublayer\n    h_node_refined = self.project_node_node(final_h) + self.project_graph_node(\n        final_h.max(1)[0]\n    )[:, None, :].expand(batch_size, graph_size, dim)\n    h_pos_refined = self.project_node_pos(final_p) + self.project_graph_pos(\n        final_p.max(1)[0]\n    )[:, None, :].expand(batch_size, graph_size, dim)\n\n    # MHC sublayer\n    compatibility = torch.zeros(\n        (batch_size, graph_size, graph_size, self.n_heads * 2),\n        device=h_node_refined.device,\n    )\n    compatibility[:, :, :, : self.n_heads] = self.compater_pos(h_pos_refined).permute(\n        1, 2, 3, 0\n    )\n    compatibility[:, :, :, self.n_heads :] = self.compater_node(\n        h_node_refined\n    ).permute(1, 2, 3, 0)\n\n    # FFA sublater\n    return self.value_head(self.norm_factor * compatibility).squeeze(-1)\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#models.zoo.dact.policy.DACTPolicy","title":"DACTPolicy","text":"<pre><code>DACTPolicy(\n    embed_dim: int = 64,\n    num_encoder_layers: int = 3,\n    num_heads: int = 4,\n    normalization: str = \"layer\",\n    feedforward_hidden: int = 64,\n    env_name: str = \"tsp_kopt\",\n    pos_type: str = \"CPE\",\n    init_embedding: Module = None,\n    pos_embedding: Module = None,\n    temperature: float = 1.0,\n    tanh_clipping: float = 6.0,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"sampling\",\n    test_decode_type: str = \"sampling\",\n)\n</code></pre> <p>               Bases: <code>ImprovementPolicy</code></p> <p>DACT Policy based on Ma et al. (2021) This model first encodes the input graph and current solution using a DACT encoder (:class:<code>DACTEncoder</code>) and then decodes the 2-opt action (:class:<code>DACTDecoder</code>)</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>64</code> )           \u2013            <p>Dimension of the node embeddings</p> </li> <li> <code>num_encoder_layers</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of layers in the encoder</p> </li> <li> <code>num_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads in the attention layers</p> </li> <li> <code>normalization</code>               (<code>str</code>, default:                   <code>'layer'</code> )           \u2013            <p>Normalization type in the attention layers</p> </li> <li> <code>feedforward_hidden</code>               (<code>int</code>, default:                   <code>64</code> )           \u2013            <p>Dimension of the hidden layer in the feedforward network</p> </li> <li> <code>env_name</code>               (<code>str</code>, default:                   <code>'tsp_kopt'</code> )           \u2013            <p>Name of the environment used to initialize embeddings</p> </li> <li> <code>pos_type</code>               (<code>str</code>, default:                   <code>'CPE'</code> )           \u2013            <p>Name of the used positional encoding method (CPE or APE)</p> </li> <li> <code>init_embedding</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Module to use for the initialization of the embeddings</p> </li> <li> <code>pos_embedding</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Module to use for the initialization of the positional embeddings</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Temperature for the softmax</p> </li> <li> <code>tanh_clipping</code>               (<code>float</code>, default:                   <code>6.0</code> )           \u2013            <p>Tanh clipping value (see Bello et al., 2016)</p> </li> <li> <code>train_decode_type</code>               (<code>str</code>, default:                   <code>'sampling'</code> )           \u2013            <p>Type of decoding to use during training</p> </li> <li> <code>val_decode_type</code>               (<code>str</code>, default:                   <code>'sampling'</code> )           \u2013            <p>Type of decoding to use during validation</p> </li> <li> <code>test_decode_type</code>               (<code>str</code>, default:                   <code>'sampling'</code> )           \u2013            <p>Type of decoding to use during testing</p> </li> </ul> Source code in <code>rl4co/models/zoo/dact/policy.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int = 64,\n    num_encoder_layers: int = 3,\n    num_heads: int = 4,\n    normalization: str = \"layer\",\n    feedforward_hidden: int = 64,\n    env_name: str = \"tsp_kopt\",\n    pos_type: str = \"CPE\",\n    init_embedding: nn.Module = None,\n    pos_embedding: nn.Module = None,\n    temperature: float = 1.0,\n    tanh_clipping: float = 6.0,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"sampling\",\n    test_decode_type: str = \"sampling\",\n):\n    super(DACTPolicy, self).__init__()\n\n    self.env_name = env_name\n\n    # Encoder and decoder\n    self.encoder = DACTEncoder(\n        embed_dim=embed_dim,\n        init_embedding=init_embedding,\n        pos_embedding=pos_embedding,\n        env_name=env_name,\n        pos_type=pos_type,\n        num_heads=num_heads,\n        num_layers=num_encoder_layers,\n        normalization=normalization,\n        feedforward_hidden=feedforward_hidden,\n    )\n\n    self.decoder = DACTDecoder(embed_dim=embed_dim, num_heads=num_heads)\n\n    # Decoding strategies\n    self.temperature = temperature\n    self.tanh_clipping = tanh_clipping\n    self.train_decode_type = train_decode_type\n    self.val_decode_type = val_decode_type\n    self.test_decode_type = test_decode_type\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#models.zoo.dact.policy.DACTPolicy.forward","title":"forward","text":"<pre><code>forward(\n    td: TensorDict,\n    env: Union[str, RL4COEnvBase] = None,\n    phase: str = \"train\",\n    return_actions: bool = False,\n    return_embeds: bool = False,\n    only_return_embed: bool = False,\n    actions=None,\n    **decoding_kwargs\n) -&gt; dict\n</code></pre> <p>Forward pass of the policy.</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>TensorDict containing the environment state</p> </li> <li> <code>env</code>               (<code>Union[str, RL4COEnvBase]</code>, default:                   <code>None</code> )           \u2013            <p>Environment to use for decoding. If None, the environment is instantiated from <code>env_name</code>. Note that it is more efficient to pass an already instantiated environment each time for fine-grained control</p> </li> <li> <code>phase</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>Phase of the algorithm (train, val, test)</p> </li> <li> <code>return_actions</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the actions</p> </li> <li> <code>actions</code>           \u2013            <p>Actions to use for evaluating the policy. If passed, use these actions instead of sampling from the policy to calculate log likelihood</p> </li> <li> <code>decoding_kwargs</code>           \u2013            <p>Keyword arguments for the decoding strategy. See :class:<code>rl4co.utils.decoding.DecodingStrategy</code> for more information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the reward, log likelihood, and optionally the actions and entropy</p> </li> </ul> Source code in <code>rl4co/models/zoo/dact/policy.py</code> <pre><code>def forward(\n    self,\n    td: TensorDict,\n    env: Union[str, RL4COEnvBase] = None,\n    phase: str = \"train\",\n    return_actions: bool = False,\n    return_embeds: bool = False,\n    only_return_embed: bool = False,\n    actions=None,\n    **decoding_kwargs,\n) -&gt; dict:\n    \"\"\"Forward pass of the policy.\n\n    Args:\n        td: TensorDict containing the environment state\n        env: Environment to use for decoding. If None, the environment is instantiated from `env_name`. Note that\n            it is more efficient to pass an already instantiated environment each time for fine-grained control\n        phase: Phase of the algorithm (train, val, test)\n        return_actions: Whether to return the actions\n        actions: Actions to use for evaluating the policy.\n            If passed, use these actions instead of sampling from the policy to calculate log likelihood\n        decoding_kwargs: Keyword arguments for the decoding strategy. See :class:`rl4co.utils.decoding.DecodingStrategy` for more information.\n\n    Returns:\n        out: Dictionary containing the reward, log likelihood, and optionally the actions and entropy\n    \"\"\"\n\n    # Encoder: get encoder output and initial embeddings from initial state\n    NFE, PFE = self.encoder(td)\n    h_featrues = torch.cat((NFE, PFE), -1)\n\n    if only_return_embed:\n        return {\"embeds\": h_featrues.detach()}\n\n    # Instantiate environment if needed\n    if isinstance(env, str) or env is None:\n        env_name = self.env_name if env is None else env\n        log.info(f\"Instantiated environment not provided; instantiating {env_name}\")\n        env = get_env(env_name)\n    assert env.two_opt_mode, \"DACT only support 2-opt\"\n\n    # Get decode type depending on phase and whether actions are passed for evaluation\n    decode_type = decoding_kwargs.pop(\"decode_type\", None)\n    if actions is not None:\n        decode_type = \"evaluate\"\n    elif decode_type is None:\n        decode_type = getattr(self, f\"{phase}_decode_type\")\n\n    # Setup decoding strategy\n    # we pop arguments that are not part of the decoding strategy\n    decode_strategy: DecodingStrategy = get_decoding_strategy(\n        decode_type,\n        temperature=decoding_kwargs.pop(\"temperature\", self.temperature),\n        tanh_clipping=decoding_kwargs.pop(\"tanh_clipping\", self.tanh_clipping),\n        mask_logits=True,\n        improvement_method_mode=True,\n        **decoding_kwargs,\n    )\n\n    # Perform the decoding\n    batch_size, seq_length = td[\"rec_current\"].size()\n    logits = self.decoder(td, NFE, PFE).view(batch_size, -1)\n\n    # Get mask\n    mask = env.get_mask(td)\n    if \"action\" in td.keys():\n        mask[torch.arange(batch_size), td[\"action\"][:, 0], td[\"action\"][:, 1]] = False\n        mask[torch.arange(batch_size), td[\"action\"][:, 1], td[\"action\"][:, 0]] = False\n    mask = mask.view(batch_size, -1)\n\n    # Get action and log-likelihood\n    logprob, action_sampled = decode_strategy.step(\n        logits,\n        mask,\n        action=actions[:, 0] * seq_length + actions[:, 1]\n        if actions is not None\n        else None,\n    )\n    action_sampled = action_sampled.unsqueeze(-1)\n    if phase == \"train\":\n        log_likelihood = logprob.gather(1, action_sampled)\n    else:\n        log_likelihood = torch.zeros(batch_size, device=td.device)\n\n    ## return\n    DACT_action = torch.cat(\n        (\n            action_sampled // seq_length,\n            action_sampled % seq_length,\n        ),\n        -1,\n    )\n\n    outdict = {\"log_likelihood\": log_likelihood, \"cost_bsf\": td[\"cost_bsf\"]}\n    td.set(\"action\", DACT_action)\n\n    if return_embeds:\n        outdict[\"embeds\"] = h_featrues.detach()\n\n    if return_actions:\n        outdict[\"actions\"] = DACT_action\n\n    return outdict\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#models.zoo.dact.model.DACT","title":"DACT","text":"<pre><code>DACT(\n    env: RL4COEnvBase,\n    policy: Module = None,\n    critic: CriticNetwork = None,\n    policy_kwargs: dict = {},\n    critic_kwargs: dict = {},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>n_step_PPO</code></p> <p>DACT Model based on n_step Proximal Policy Optimization (PPO) with an DACT model policy. We default to the DACT model policy and the improvement Critic Network.</p> <p>Parameters:</p> <ul> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>Environment to use for the algorithm</p> </li> <li> <code>policy</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Policy to use for the algorithm</p> </li> <li> <code>critic</code>               (<code>CriticNetwork</code>, default:                   <code>None</code> )           \u2013            <p>Critic to use for the algorithm</p> </li> <li> <code>policy_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments for policy</p> </li> <li> <code>critic_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments for critic</p> </li> </ul> Source code in <code>rl4co/models/zoo/dact/model.py</code> <pre><code>def __init__(\n    self,\n    env: RL4COEnvBase,\n    policy: nn.Module = None,\n    critic: CriticNetwork = None,\n    policy_kwargs: dict = {},\n    critic_kwargs: dict = {},\n    **kwargs,\n):\n    if policy is None:\n        policy = DACTPolicy(env_name=env.name, **policy_kwargs)\n\n    if critic is None:\n        embed_dim = (\n            policy_kwargs[\"embed_dim\"] * 2 if \"embed_dim\" in policy_kwargs else 128\n        )  # the critic's embed_dim must be as policy's\n\n        encoder = MultiHeadAttentionLayer(\n            embed_dim,\n            critic_kwargs[\"num_heads\"] if \"num_heads\" in critic_kwargs else 4,\n            critic_kwargs[\"feedforward_hidden\"] * 2\n            if \"feedforward_hidden\" in critic_kwargs\n            else 128,\n            critic_kwargs[\"normalization\"]\n            if \"normalization\" in critic_kwargs\n            else \"layer\",\n            bias=False,\n        )\n        value_head = CriticDecoder(embed_dim)\n\n        critic = CriticNetwork(\n            encoder=encoder,\n            value_head=value_head,\n            customized=True,\n        )\n\n    super().__init__(env, policy, critic, **kwargs)\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#n2s","title":"N2S","text":""},{"location":"docs/content/api/zoo/improvement/#models.zoo.n2s.encoder.N2SEncoder","title":"N2SEncoder","text":"<pre><code>N2SEncoder(\n    embed_dim: int = 128,\n    init_embedding: Module = None,\n    pos_embedding: Module = None,\n    env_name: str = \"pdp_ruin_repair\",\n    pos_type: str = \"CPE\",\n    num_heads: int = 4,\n    num_layers: int = 3,\n    normalization: str = \"layer\",\n    feedforward_hidden: int = 128,\n)\n</code></pre> <p>               Bases: <code>ImprovementEncoder</code></p> <p>Neural Neighborhood Search Encoder as in Ma et al. (2022) First embed the input and then process it with a Graph AttepdN2ntion Network.</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Dimension of the embedding space</p> </li> <li> <code>init_embedding</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Module to use for the initialization of the node embeddings</p> </li> <li> <code>pos_embedding</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Module to use for the initialization of the positional embeddings</p> </li> <li> <code>env_name</code>               (<code>str</code>, default:                   <code>'pdp_ruin_repair'</code> )           \u2013            <p>Name of the environment used to initialize embeddings</p> </li> <li> <code>pos_type</code>               (<code>str</code>, default:                   <code>'CPE'</code> )           \u2013            <p>Name of the used positional encoding method (CPE or APE)</p> </li> <li> <code>num_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads in the attention layers</p> </li> <li> <code>num_layers</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of layers in the attention network</p> </li> <li> <code>normalization</code>               (<code>str</code>, default:                   <code>'layer'</code> )           \u2013            <p>Normalization type in the attention layers</p> </li> <li> <code>feedforward_hidden</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Hidden dimension in the feedforward layers</p> </li> </ul> Source code in <code>rl4co/models/zoo/n2s/encoder.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int = 128,\n    init_embedding: nn.Module = None,\n    pos_embedding: nn.Module = None,\n    env_name: str = \"pdp_ruin_repair\",\n    pos_type: str = \"CPE\",\n    num_heads: int = 4,\n    num_layers: int = 3,\n    normalization: str = \"layer\",\n    feedforward_hidden: int = 128,\n):\n    super(N2SEncoder, self).__init__(\n        embed_dim=embed_dim,\n        init_embedding=init_embedding,\n        pos_embedding=pos_embedding,\n        env_name=env_name,\n        pos_type=pos_type,\n        num_heads=num_heads,\n        num_layers=num_layers,\n        normalization=normalization,\n        feedforward_hidden=feedforward_hidden,\n    )\n\n    self.pos_net = MultiHeadCompat(num_heads, embed_dim, feedforward_hidden)\n\n    self.net = AdaptiveSequential(\n        *(\n            N2SEncoderLayer(\n                num_heads,\n                embed_dim,\n                feedforward_hidden,\n                normalization,\n            )\n            for _ in range(num_layers)\n        )\n    )\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#models.zoo.n2s.decoder.NodePairRemovalDecoder","title":"NodePairRemovalDecoder","text":"<pre><code>NodePairRemovalDecoder(\n    embed_dim: int = 128, num_heads: int = 4\n)\n</code></pre> <p>               Bases: <code>ImprovementDecoder</code></p> <p>N2S Node-Pair Removal decoder based on Ma et al. (2022) Given the environment state and the node embeddings (positional embeddings are discarded), compute the logits for selecting a pair of pickup and delivery nodes for node pair removal from the current solution</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Embedding dimension</p> </li> <li> <code>num_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of attention heads</p> </li> </ul> Source code in <code>rl4co/models/zoo/n2s/decoder.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int = 128,\n    num_heads: int = 4,\n):\n    super().__init__()\n    self.input_dim = embed_dim\n    self.n_heads = num_heads\n    self.hidden_dim = embed_dim\n\n    assert embed_dim % num_heads == 0\n\n    self.W_Q = nn.Parameter(\n        torch.Tensor(self.n_heads, self.input_dim, self.hidden_dim)\n    )\n    self.W_K = nn.Parameter(\n        torch.Tensor(self.n_heads, self.input_dim, self.hidden_dim)\n    )\n\n    self.agg = MLP(input_dim=2 * self.n_heads + 4, output_dim=1, num_neurons=[32, 32])\n\n    self.init_parameters()\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#models.zoo.n2s.decoder.NodePairRemovalDecoder.forward","title":"forward","text":"<pre><code>forward(\n    td: TensorDict, final_h: Tensor, final_p: Tensor\n) -&gt; Tensor\n</code></pre> <p>Compute the logits of the removing a node pair from the current solution</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>TensorDict with the current environment state</p> </li> <li> <code>final_h</code>               (<code>Tensor</code>)           \u2013            <p>final node embeddings</p> </li> <li> <code>final_p</code>               (<code>Tensor</code>)           \u2013            <p>final positional embeddings</p> </li> </ul> Source code in <code>rl4co/models/zoo/n2s/decoder.py</code> <pre><code>def forward(self, td: TensorDict, final_h: Tensor, final_p: Tensor) -&gt; Tensor:\n    \"\"\"Compute the logits of the removing a node pair from the current solution\n\n    Args:\n        td: TensorDict with the current environment state\n        final_h: final node embeddings\n        final_p: final positional embeddings\n    \"\"\"\n\n    selection_recent = torch.cat(\n        (td[\"action_record\"][:, -3:], td[\"action_record\"].mean(1, True)), 1\n    )\n    solution = td[\"rec_current\"]\n\n    pre = solution.argsort()  # pre=[1,2,0]\n    post = solution.gather(\n        1, solution\n    )  # post=[1,2,0] # the second neighbour works better\n    batch_size, graph_size_plus1, input_dim = final_h.size()\n\n    hflat = final_h.contiguous().view(-1, input_dim)  #################   reshape\n\n    shp = (self.n_heads, batch_size, graph_size_plus1, self.hidden_dim)\n\n    # Calculate queries, (n_heads, batch_size, graph_size+1, key_size)\n    hidden_Q = torch.matmul(hflat, self.W_Q).view(shp)\n    hidden_K = torch.matmul(hflat, self.W_K).view(shp)\n\n    Q_pre = hidden_Q.gather(\n        2, pre.view(1, batch_size, graph_size_plus1, 1).expand_as(hidden_Q)\n    )\n    K_post = hidden_K.gather(\n        2, post.view(1, batch_size, graph_size_plus1, 1).expand_as(hidden_Q)\n    )\n\n    compatibility = (\n        (Q_pre * hidden_K).sum(-1)\n        + (hidden_Q * K_post).sum(-1)\n        - (Q_pre * K_post).sum(-1)\n    )[\n        :, :, 1:\n    ]  # (n_heads, batch_size, graph_size) (12)\n\n    compatibility_pairing = torch.cat(\n        (\n            compatibility[:, :, : graph_size_plus1 // 2],\n            compatibility[:, :, graph_size_plus1 // 2 :],\n        ),\n        0,\n    )  # (n_heads*2, batch_size, graph_size/2)\n\n    compatibility_pairing = self.agg(\n        torch.cat(\n            (\n                compatibility_pairing.permute(1, 2, 0),\n                selection_recent.permute(0, 2, 1),\n            ),\n            -1,\n        )\n    ).squeeze()  # (batch_size, graph_size/2)\n\n    return compatibility_pairing\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#models.zoo.n2s.decoder.NodePairReinsertionDecoder","title":"NodePairReinsertionDecoder","text":"<pre><code>NodePairReinsertionDecoder(\n    embed_dim: int = 128, num_heads: int = 4\n)\n</code></pre> <p>               Bases: <code>ImprovementDecoder</code></p> <p>N2S Node-Pair Reinsertion decoder based on Ma et al. (2022) Given the environment state, the node embeddings (positional embeddings are discarded), and the removed node from the NodePairRemovalDecoder, compute the logits for finding places to re-insert the removed pair of pickup and delivery nodes to form a new solution</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Embedding dimension</p> </li> <li> <code>num_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of attention heads</p> </li> </ul> Source code in <code>rl4co/models/zoo/n2s/decoder.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int = 128,\n    num_heads: int = 4,\n):\n    super().__init__()\n    self.input_dim = embed_dim\n    self.n_heads = num_heads\n    self.hidden_dim = embed_dim\n\n    assert embed_dim % num_heads == 0\n\n    self.compater_insert1 = MultiHeadCompat(\n        num_heads, embed_dim, embed_dim, embed_dim, embed_dim\n    )\n\n    self.compater_insert2 = MultiHeadCompat(\n        num_heads, embed_dim, embed_dim, embed_dim, embed_dim\n    )\n\n    self.agg = MLP(input_dim=4 * self.n_heads, output_dim=1, num_neurons=[32, 32])\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#models.zoo.n2s.policy.N2SPolicy","title":"N2SPolicy","text":"<pre><code>N2SPolicy(\n    embed_dim: int = 128,\n    num_encoder_layers: int = 3,\n    num_heads: int = 4,\n    normalization: str = \"layer\",\n    feedforward_hidden: int = 128,\n    env_name: str = \"pdp_ruin_repair\",\n    pos_type: str = \"CPE\",\n    init_embedding: Module = None,\n    pos_embedding: Module = None,\n    temperature: float = 1.0,\n    tanh_clipping: float = 6.0,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"sampling\",\n    test_decode_type: str = \"sampling\",\n)\n</code></pre> <p>               Bases: <code>ImprovementPolicy</code></p> <p>N2S Policy based on Ma et al. (2022) This model first encodes the input graph and current solution using a N2S encoder (:class:<code>N2SEncoder</code>) and then decodes the node-pair removal and reinsertion action using the Node-Pair Removal (:class:<code>NodePairRemovalDecoder</code>) and Reinsertion (:class:<code>NodePairReinsertionDecoder</code>) decoders</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Dimension of the node embeddings</p> </li> <li> <code>num_encoder_layers</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of layers in the encoder</p> </li> <li> <code>num_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads in the attention layers</p> </li> <li> <code>normalization</code>               (<code>str</code>, default:                   <code>'layer'</code> )           \u2013            <p>Normalization type in the attention layers</p> </li> <li> <code>feedforward_hidden</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Dimension of the hidden layer in the feedforward network</p> </li> <li> <code>env_name</code>               (<code>str</code>, default:                   <code>'pdp_ruin_repair'</code> )           \u2013            <p>Name of the environment used to initialize embeddings</p> </li> <li> <code>pos_type</code>               (<code>str</code>, default:                   <code>'CPE'</code> )           \u2013            <p>Name of the used positional encoding method (CPE or APE)</p> </li> <li> <code>init_embedding</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Module to use for the initialization of the embeddings</p> </li> <li> <code>pos_embedding</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Module to use for the initialization of the positional embeddings</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Temperature for the softmax</p> </li> <li> <code>tanh_clipping</code>               (<code>float</code>, default:                   <code>6.0</code> )           \u2013            <p>Tanh clipping value (see Bello et al., 2016)</p> </li> <li> <code>train_decode_type</code>               (<code>str</code>, default:                   <code>'sampling'</code> )           \u2013            <p>Type of decoding to use during training</p> </li> <li> <code>val_decode_type</code>               (<code>str</code>, default:                   <code>'sampling'</code> )           \u2013            <p>Type of decoding to use during validation</p> </li> <li> <code>test_decode_type</code>               (<code>str</code>, default:                   <code>'sampling'</code> )           \u2013            <p>Type of decoding to use during testing</p> </li> </ul> Source code in <code>rl4co/models/zoo/n2s/policy.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int = 128,\n    num_encoder_layers: int = 3,\n    num_heads: int = 4,\n    normalization: str = \"layer\",\n    feedforward_hidden: int = 128,\n    env_name: str = \"pdp_ruin_repair\",\n    pos_type: str = \"CPE\",\n    init_embedding: nn.Module = None,\n    pos_embedding: nn.Module = None,\n    temperature: float = 1.0,\n    tanh_clipping: float = 6.0,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"sampling\",\n    test_decode_type: str = \"sampling\",\n):\n    super(N2SPolicy, self).__init__()\n\n    self.env_name = env_name\n\n    # Encoder and decoder\n    self.encoder = N2SEncoder(\n        embed_dim=embed_dim,\n        init_embedding=init_embedding,\n        pos_embedding=pos_embedding,\n        env_name=env_name,\n        pos_type=pos_type,\n        num_heads=num_heads,\n        num_layers=num_encoder_layers,\n        normalization=normalization,\n        feedforward_hidden=feedforward_hidden,\n    )\n\n    self.removal_decoder = NodePairRemovalDecoder(\n        embed_dim=embed_dim, num_heads=num_heads\n    )\n\n    self.reinsertion_decoder = NodePairReinsertionDecoder(\n        embed_dim=embed_dim, num_heads=num_heads\n    )\n\n    self.project_graph = nn.Linear(embed_dim, embed_dim, bias=False)\n    self.project_node = nn.Linear(embed_dim, embed_dim, bias=False)\n\n    # Decoding strategies\n    self.temperature = temperature\n    self.tanh_clipping = tanh_clipping\n    self.train_decode_type = train_decode_type\n    self.val_decode_type = val_decode_type\n    self.test_decode_type = test_decode_type\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#models.zoo.n2s.policy.N2SPolicy.forward","title":"forward","text":"<pre><code>forward(\n    td: TensorDict,\n    env: Union[str, RL4COEnvBase] = None,\n    phase: str = \"train\",\n    return_actions: bool = False,\n    return_embeds: bool = False,\n    only_return_embed: bool = False,\n    actions=None,\n    **decoding_kwargs\n) -&gt; dict\n</code></pre> <p>Forward pass of the policy.</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>TensorDict containing the environment state</p> </li> <li> <code>env</code>               (<code>Union[str, RL4COEnvBase]</code>, default:                   <code>None</code> )           \u2013            <p>Environment to use for decoding. If None, the environment is instantiated from <code>env_name</code>. Note that it is more efficient to pass an already instantiated environment each time for fine-grained control</p> </li> <li> <code>phase</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>Phase of the algorithm (train, val, test)</p> </li> <li> <code>return_actions</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the actions</p> </li> <li> <code>actions</code>           \u2013            <p>Actions to use for evaluating the policy. If passed, use these actions instead of sampling from the policy to calculate log likelihood</p> </li> <li> <code>decoding_kwargs</code>           \u2013            <p>Keyword arguments for the decoding strategy. See :class:<code>rl4co.utils.decoding.DecodingStrategy</code> for more information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the reward, log likelihood, and optionally the actions and entropy</p> </li> </ul> Source code in <code>rl4co/models/zoo/n2s/policy.py</code> <pre><code>def forward(\n    self,\n    td: TensorDict,\n    env: Union[str, RL4COEnvBase] = None,\n    phase: str = \"train\",\n    return_actions: bool = False,\n    return_embeds: bool = False,\n    only_return_embed: bool = False,\n    actions=None,\n    **decoding_kwargs,\n) -&gt; dict:\n    \"\"\"Forward pass of the policy.\n\n    Args:\n        td: TensorDict containing the environment state\n        env: Environment to use for decoding. If None, the environment is instantiated from `env_name`. Note that\n            it is more efficient to pass an already instantiated environment each time for fine-grained control\n        phase: Phase of the algorithm (train, val, test)\n        return_actions: Whether to return the actions\n        actions: Actions to use for evaluating the policy.\n            If passed, use these actions instead of sampling from the policy to calculate log likelihood\n        decoding_kwargs: Keyword arguments for the decoding strategy. See :class:`rl4co.utils.decoding.DecodingStrategy` for more information.\n\n    Returns:\n        out: Dictionary containing the reward, log likelihood, and optionally the actions and entropy\n    \"\"\"\n\n    # Encoder: get encoder output and initial embeddings from initial state\n    h_wave, final_p = self.encoder(td)\n    if only_return_embed:\n        return {\"embeds\": h_wave.detach()}\n    final_h = (\n        self.project_node(h_wave) + self.project_graph(h_wave.max(1)[0])[:, None, :]\n    )\n\n    # Instantiate environment if needed\n    if isinstance(env, str) or env is None:\n        env_name = self.env_name if env is None else env\n        log.info(f\"Instantiated environment not provided; instantiating {env_name}\")\n        env = get_env(env_name)\n\n    # Get decode type depending on phase and whether actions are passed for evaluation\n    decode_type = decoding_kwargs.pop(\"decode_type\", None)\n    if actions is not None:\n        decode_type = \"evaluate\"\n    elif decode_type is None:\n        decode_type = getattr(self, f\"{phase}_decode_type\")\n\n    # Setup decoding strategy\n    # we pop arguments that are not part of the decoding strategy\n    decode_strategy: DecodingStrategy = get_decoding_strategy(\n        decode_type,\n        temperature=decoding_kwargs.pop(\"temperature\", self.temperature),\n        tanh_clipping=decoding_kwargs.pop(\"tanh_clipping\", self.tanh_clipping),\n        mask_logits=True,\n        improvement_method_mode=True,\n        **decoding_kwargs,\n    )\n\n    ## action 1\n\n    # Perform the decoding\n    logits = self.removal_decoder(td, final_h, final_p)\n\n    # Get mask\n    mask = torch.ones_like(td[\"action_record\"][:, 0], device=td.device).bool()\n    if \"action\" in td.keys():\n        mask = mask.scatter(1, td[\"action\"][:, :1], 0)\n\n    # Get action and log-likelihood\n    logprob_removal, action_removal = decode_strategy.step(\n        logits,\n        mask,\n        action=actions[:, 0] if actions is not None else None,\n    )\n    action_removal = action_removal.unsqueeze(-1)\n    if phase == \"train\":\n        selected_log_ll_action1 = logprob_removal.gather(1, action_removal)\n\n    ## action 2\n    td.set(\"action\", action_removal)\n\n    # Perform the decoding\n    batch_size, seq_length = td[\"rec_current\"].size()\n    logits = self.reinsertion_decoder(td, final_h, final_p).view(batch_size, -1)\n\n    # Get mask\n    mask = env.get_mask(action_removal + 1, td).view(batch_size, -1)\n    # Get action and log-likelihood\n    logprob_reinsertion, action_reinsertion = decode_strategy.step(\n        logits,\n        mask,\n        action=actions[:, 1] * seq_length + actions[:, 2]\n        if actions is not None\n        else None,\n    )\n    action_reinsertion = action_reinsertion.unsqueeze(-1)\n    if phase == \"train\":\n        selected_log_ll_action2 = logprob_reinsertion.gather(1, action_reinsertion)\n\n    ## return\n    N2S_action = torch.cat(\n        (\n            action_removal.view(batch_size, -1),\n            action_reinsertion // seq_length,\n            action_reinsertion % seq_length,\n        ),\n        -1,\n    )\n    if phase == \"train\":\n        log_likelihood = selected_log_ll_action1 + selected_log_ll_action2\n    else:\n        log_likelihood = torch.zeros(batch_size, device=td.device)\n\n    outdict = {\"log_likelihood\": log_likelihood, \"cost_bsf\": td[\"cost_bsf\"]}\n    td.set(\"action\", N2S_action)\n\n    if return_embeds:\n        outdict[\"embeds\"] = h_wave.detach()\n\n    if return_actions:\n        outdict[\"actions\"] = N2S_action\n\n    return outdict\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#models.zoo.n2s.model.N2S","title":"N2S","text":"<pre><code>N2S(\n    env: RL4COEnvBase,\n    policy: Module = None,\n    critic: CriticNetwork = None,\n    policy_kwargs: dict = {},\n    critic_kwargs: dict = {},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>n_step_PPO</code></p> <p>N2S Model based on n_step Proximal Policy Optimization (PPO) with an N2S model policy. We default to the N2S model policy and the improvement Critic Network.</p> <p>Parameters:</p> <ul> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>Environment to use for the algorithm</p> </li> <li> <code>policy</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Policy to use for the algorithm</p> </li> <li> <code>critic</code>               (<code>CriticNetwork</code>, default:                   <code>None</code> )           \u2013            <p>Critic to use for the algorithm</p> </li> <li> <code>policy_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments for policy</p> </li> <li> <code>critic_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments for critic</p> </li> </ul> Source code in <code>rl4co/models/zoo/n2s/model.py</code> <pre><code>def __init__(\n    self,\n    env: RL4COEnvBase,\n    policy: nn.Module = None,\n    critic: CriticNetwork = None,\n    policy_kwargs: dict = {},\n    critic_kwargs: dict = {},\n    **kwargs,\n):\n    if policy is None:\n        policy = N2SPolicy(env_name=env.name, **policy_kwargs)\n\n    if critic is None:\n        embed_dim = (\n            policy_kwargs[\"embed_dim\"] if \"embed_dim\" in policy_kwargs else 128\n        )  # the critic's embed_dim must be as policy's\n\n        encoder = MultiHeadAttentionLayer(\n            embed_dim,\n            critic_kwargs[\"num_heads\"] if \"num_heads\" in critic_kwargs else 4,\n            critic_kwargs[\"feedforward_hidden\"]\n            if \"feedforward_hidden\" in critic_kwargs\n            else 128,\n            critic_kwargs[\"normalization\"]\n            if \"normalization\" in critic_kwargs\n            else \"layer\",\n            bias=False,\n        )\n        value_head = CriticDecoder(embed_dim)\n\n        critic = CriticNetwork(\n            encoder=encoder,\n            value_head=value_head,\n            customized=True,\n        )\n\n    super().__init__(env, policy, critic, **kwargs)\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#neuopt","title":"NeuOpt","text":""},{"location":"docs/content/api/zoo/improvement/#models.zoo.neuopt.decoder.RDSDecoder","title":"RDSDecoder","text":"<pre><code>RDSDecoder(embed_dim: int = 128)\n</code></pre> <p>               Bases: <code>ImprovementDecoder</code></p> <p>RDS Decoder for flexible k-opt based on Ma et al. (2023) Given the environment state and the node embeddings (positional embeddings are discarded), compute the logits for selecting a k-opt exchange on basis moves (S-move, I-move, E-move) from the current solution</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Embedding dimension</p> </li> <li> <code>num_heads</code>           \u2013            <p>Number of attention heads</p> </li> </ul> Source code in <code>rl4co/models/zoo/neuopt/decoder.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int = 128,\n):\n    super().__init__()\n    self.embed_dim = embed_dim\n\n    self.linear_K1 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n    self.linear_K2 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n    self.linear_K3 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n    self.linear_K4 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n\n    self.linear_Q1 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n    self.linear_Q2 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n    self.linear_Q3 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n    self.linear_Q4 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n\n    self.linear_V1 = nn.Parameter(torch.Tensor(self.embed_dim))\n    self.linear_V2 = nn.Parameter(torch.Tensor(self.embed_dim))\n\n    self.rnn1 = nn.GRUCell(self.embed_dim, self.embed_dim)\n    self.rnn2 = nn.GRUCell(self.embed_dim, self.embed_dim)\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#models.zoo.neuopt.policy.CustomizeTSPInitEmbedding","title":"CustomizeTSPInitEmbedding","text":"<pre><code>CustomizeTSPInitEmbedding(embed_dim, linear_bias=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Initial embedding for the Traveling Salesman Problems (TSP). Embed the following node features to the embedding space:</p> <pre><code>- locs: x, y coordinates of the cities\n</code></pre> Source code in <code>rl4co/models/zoo/neuopt/policy.py</code> <pre><code>def __init__(self, embed_dim, linear_bias=True):\n    super(CustomizeTSPInitEmbedding, self).__init__()\n    node_dim = 2  # x, y\n    self.init_embed = nn.Sequential(\n        nn.Linear(node_dim, embed_dim // 2, linear_bias),\n        nn.ReLU(inplace=True),\n        nn.Linear(embed_dim // 2, embed_dim, linear_bias),\n    )\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#models.zoo.neuopt.policy.NeuOptPolicy","title":"NeuOptPolicy","text":"<pre><code>NeuOptPolicy(\n    embed_dim: int = 128,\n    num_encoder_layers: int = 3,\n    num_heads: int = 4,\n    normalization: str = \"layer\",\n    feedforward_hidden: int = 128,\n    env_name: str = \"tsp_kopt\",\n    pos_type: str = \"CPE\",\n    init_embedding: Module = None,\n    pos_embedding: Module = None,\n    temperature: float = 1.0,\n    tanh_clipping: float = 6.0,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"sampling\",\n    test_decode_type: str = \"sampling\",\n)\n</code></pre> <p>               Bases: <code>ImprovementPolicy</code></p> <p>NeuOpt Policy based on Ma et al. (2023) This model first encodes the input graph and current solution using a N2S encoder (:class:<code>N2SEncoder</code>) and then decodes the k-opt action (:class:<code>RDSDecoder</code>)</p> <p>Parameters:</p> <ul> <li> <code>embed_dim</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Dimension of the node embeddings</p> </li> <li> <code>num_encoder_layers</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of layers in the encoder</p> </li> <li> <code>num_heads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of heads in the attention layers</p> </li> <li> <code>normalization</code>               (<code>str</code>, default:                   <code>'layer'</code> )           \u2013            <p>Normalization type in the attention layers</p> </li> <li> <code>feedforward_hidden</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Dimension of the hidden layer in the feedforward network</p> </li> <li> <code>env_name</code>               (<code>str</code>, default:                   <code>'tsp_kopt'</code> )           \u2013            <p>Name of the environment used to initialize embeddings</p> </li> <li> <code>pos_type</code>               (<code>str</code>, default:                   <code>'CPE'</code> )           \u2013            <p>Name of the used positional encoding method (CPE or APE)</p> </li> <li> <code>init_embedding</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Module to use for the initialization of the embeddings</p> </li> <li> <code>pos_embedding</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Module to use for the initialization of the positional embeddings</p> </li> <li> <code>temperature</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Temperature for the softmax</p> </li> <li> <code>tanh_clipping</code>               (<code>float</code>, default:                   <code>6.0</code> )           \u2013            <p>Tanh clipping value (see Bello et al., 2016)</p> </li> <li> <code>train_decode_type</code>               (<code>str</code>, default:                   <code>'sampling'</code> )           \u2013            <p>Type of decoding to use during training</p> </li> <li> <code>val_decode_type</code>               (<code>str</code>, default:                   <code>'sampling'</code> )           \u2013            <p>Type of decoding to use during validation</p> </li> <li> <code>test_decode_type</code>               (<code>str</code>, default:                   <code>'sampling'</code> )           \u2013            <p>Type of decoding to use during testing</p> </li> </ul> Source code in <code>rl4co/models/zoo/neuopt/policy.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: int = 128,\n    num_encoder_layers: int = 3,\n    num_heads: int = 4,\n    normalization: str = \"layer\",\n    feedforward_hidden: int = 128,\n    env_name: str = \"tsp_kopt\",\n    pos_type: str = \"CPE\",\n    init_embedding: nn.Module = None,\n    pos_embedding: nn.Module = None,\n    temperature: float = 1.0,\n    tanh_clipping: float = 6.0,\n    train_decode_type: str = \"sampling\",\n    val_decode_type: str = \"sampling\",\n    test_decode_type: str = \"sampling\",\n):\n    super(NeuOptPolicy, self).__init__()\n\n    self.env_name = env_name\n    self.embed_dim = embed_dim\n\n    # Decoding strategies\n    self.temperature = temperature\n    self.tanh_clipping = tanh_clipping\n    self.train_decode_type = train_decode_type\n    self.val_decode_type = val_decode_type\n    self.test_decode_type = test_decode_type\n\n    # Encoder and decoder\n    if init_embedding is None:\n        init_embedding = CustomizeTSPInitEmbedding(self.embed_dim)\n\n    self.encoder = N2SEncoder(\n        embed_dim=embed_dim,\n        init_embedding=init_embedding,\n        pos_embedding=pos_embedding,\n        env_name=env_name,\n        pos_type=pos_type,\n        num_heads=num_heads,\n        num_layers=num_encoder_layers,\n        normalization=normalization,\n        feedforward_hidden=feedforward_hidden,\n    )\n\n    self.decoder = RDSDecoder(embed_dim=embed_dim)\n\n    self.init_hidden_W = nn.Linear(self.embed_dim, self.embed_dim)\n    self.init_query_learnable = nn.Parameter(torch.Tensor(self.embed_dim))\n\n    self.init_parameters()\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#models.zoo.neuopt.policy.NeuOptPolicy.forward","title":"forward","text":"<pre><code>forward(\n    td: TensorDict,\n    env: Union[str, RL4COEnvBase] = None,\n    phase: str = \"train\",\n    return_actions: bool = False,\n    return_embeds: bool = False,\n    only_return_embed: bool = False,\n    actions=None,\n    **decoding_kwargs\n) -&gt; dict\n</code></pre> <p>Forward pass of the policy.</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>TensorDict containing the environment state</p> </li> <li> <code>env</code>               (<code>Union[str, RL4COEnvBase]</code>, default:                   <code>None</code> )           \u2013            <p>Environment to use for decoding. If None, the environment is instantiated from <code>env_name</code>. Note that it is more efficient to pass an already instantiated environment each time for fine-grained control</p> </li> <li> <code>phase</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>Phase of the algorithm (train, val, test)</p> </li> <li> <code>return_actions</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the actions</p> </li> <li> <code>actions</code>           \u2013            <p>Actions to use for evaluating the policy. If passed, use these actions instead of sampling from the policy to calculate log likelihood</p> </li> <li> <code>decoding_kwargs</code>           \u2013            <p>Keyword arguments for the decoding strategy. See :class:<code>rl4co.utils.decoding.DecodingStrategy</code> for more information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the reward, log likelihood, and optionally the actions and entropy</p> </li> </ul> Source code in <code>rl4co/models/zoo/neuopt/policy.py</code> <pre><code>def forward(\n    self,\n    td: TensorDict,\n    env: Union[str, RL4COEnvBase] = None,\n    phase: str = \"train\",\n    return_actions: bool = False,\n    return_embeds: bool = False,\n    only_return_embed: bool = False,\n    actions=None,\n    **decoding_kwargs,\n) -&gt; dict:\n    \"\"\"Forward pass of the policy.\n\n    Args:\n        td: TensorDict containing the environment state\n        env: Environment to use for decoding. If None, the environment is instantiated from `env_name`. Note that\n            it is more efficient to pass an already instantiated environment each time for fine-grained control\n        phase: Phase of the algorithm (train, val, test)\n        return_actions: Whether to return the actions\n        actions: Actions to use for evaluating the policy.\n            If passed, use these actions instead of sampling from the policy to calculate log likelihood\n        decoding_kwargs: Keyword arguments for the decoding strategy. See :class:`rl4co.utils.decoding.DecodingStrategy` for more information.\n\n    Returns:\n        out: Dictionary containing the reward, log likelihood, and optionally the actions and entropy\n    \"\"\"\n\n    # Encoder: get encoder output and initial embeddings from initial state\n    nfe, _ = self.encoder(td)\n    if only_return_embed:\n        return {\"embeds\": nfe.detach()}\n\n    # Instantiate environment if needed\n    if isinstance(env, str) or env is None:\n        env_name = self.env_name if env is None else env\n        log.info(f\"Instantiated environment not provided; instantiating {env_name}\")\n        env = get_env(env_name)\n    assert not env.two_opt_mode, \"NeuOpt only support k-opt with k &gt; 2\"\n\n    # Get decode type depending on phase and whether actions are passed for evaluation\n    decode_type = decoding_kwargs.pop(\"decode_type\", None)\n    if actions is not None:\n        decode_type = \"evaluate\"\n    elif decode_type is None:\n        decode_type = getattr(self, f\"{phase}_decode_type\")\n\n    # Setup decoding strategy\n    # we pop arguments that are not part of the decoding strategy\n    decode_strategy: DecodingStrategy = get_decoding_strategy(\n        decode_type,\n        temperature=decoding_kwargs.pop(\"temperature\", self.temperature),\n        tanh_clipping=decoding_kwargs.pop(\"tanh_clipping\", self.tanh_clipping),\n        mask_logits=True,\n        improvement_method_mode=True,\n        **decoding_kwargs,\n    )\n\n    # Perform the decoding\n    bs, gs, _, ll, action_sampled, rec, visited_time = (\n        *nfe.size(),\n        0.0,\n        None,\n        td[\"rec_current\"],\n        td[\"visited_time\"],\n    )\n    action_index = torch.zeros(bs, env.k_max, dtype=torch.long).to(rec.device)\n    k_action_left = torch.zeros(bs, env.k_max + 1, dtype=torch.long).to(rec.device)\n    k_action_right = torch.zeros(bs, env.k_max, dtype=torch.long).to(rec.device)\n    next_of_last_action = (\n        torch.zeros_like(rec[:, :1], dtype=torch.long).to(rec.device) - 1\n    )\n    mask = torch.zeros_like(rec, dtype=torch.bool).to(rec.device)\n    stopped = torch.ones(bs, dtype=torch.bool).to(rec.device)\n    zeros = torch.zeros((bs, 1), device=td.device)\n\n    # init queries\n    h_mean = nfe.mean(1)\n    init_query = self.init_query_learnable.repeat(bs, 1)\n    input_q1 = input_q2 = init_query.clone()\n    init_hidden = self.init_hidden_W(h_mean)\n    q1 = q2 = init_hidden.clone()\n\n    for i in range(env.k_max):\n        # Pass RDS decoder\n        logits, q1, q2 = self.decoder(nfe, q1, q2, input_q1, input_q2)\n\n        # Calc probs\n        if i == 0 and \"action\" in td.keys():\n            mask = mask.scatter(1, td[\"action\"][:, :1], 1)\n\n        logprob, action_sampled = decode_strategy.step(\n            logits,\n            ~mask.clone(),\n            action=actions[:, i : i + 1].squeeze() if actions is not None else None,\n        )\n        action_sampled = action_sampled.unsqueeze(-1)\n        if i &gt; 0:\n            action_sampled = torch.where(\n                stopped.unsqueeze(-1), action_index[:, :1], action_sampled\n            )\n        if phase == \"train\":\n            loss_now = logprob.gather(1, action_sampled)\n        else:\n            loss_now = zeros.clone()\n\n        # Record log_likelihood and Entropy\n        if i &gt; 0:\n            ll = ll + torch.where(stopped.unsqueeze(-1), zeros * 0, loss_now)\n        else:\n            ll = ll + loss_now\n\n        # Store and Process actions\n        next_of_new_action = rec.gather(1, action_sampled)\n        action_index[:, i] = action_sampled.squeeze().clone()\n        k_action_left[stopped, i] = action_sampled[stopped].squeeze().clone()\n        k_action_right[~stopped, i - 1] = action_sampled[~stopped].squeeze().clone()\n        k_action_left[:, i + 1] = next_of_new_action.squeeze().clone()\n\n        # Prepare next RNN input\n        input_q1 = nfe.gather(\n            1, action_sampled.view(bs, 1, 1).expand(bs, 1, self.embed_dim)\n        ).squeeze(1)\n        input_q2 = torch.where(\n            stopped.view(bs, 1).expand(bs, self.embed_dim),\n            input_q1.clone(),\n            nfe.gather(\n                1,\n                (next_of_last_action % gs)\n                .view(bs, 1, 1)\n                .expand(bs, 1, self.embed_dim),\n            ).squeeze(1),\n        )\n\n        # Process if k-opt close\n        # assert (input_q1[stopped] == input_q2[stopped]).all()\n        if i &gt; 0:\n            stopped = stopped | (action_sampled == next_of_last_action).squeeze()\n        else:\n            stopped = (action_sampled == next_of_last_action).squeeze()\n        # assert (input_q1[stopped] == input_q2[stopped]).all()\n\n        k_action_left[stopped, i] = k_action_left[stopped, i - 1]\n        k_action_right[stopped, i] = k_action_right[stopped, i - 1]\n\n        # Calc next basic masks\n        if i == 0:\n            visited_time_tag = (\n                visited_time - visited_time.gather(1, action_sampled)\n            ) % gs\n        mask &amp;= False\n        mask[(visited_time_tag &lt;= visited_time_tag.gather(1, action_sampled))] = True\n        if i == 0:\n            mask[visited_time_tag &gt; (gs - 2)] = True\n        mask[\n            stopped, action_sampled[stopped].squeeze()\n        ] = False  # allow next k-opt starts immediately\n        # if True:#i == env.k_max - 2: # allow special case: close k-opt at the first selected node\n        index_allow_first_node = (~stopped) &amp; (\n            next_of_new_action.squeeze() == action_index[:, 0]\n        )\n        mask[index_allow_first_node, action_index[index_allow_first_node, 0]] = False\n\n        # Move to next\n        next_of_last_action = next_of_new_action\n        next_of_last_action[stopped] = -1\n\n    # Form final action\n    k_action_right[~stopped, -1] = k_action_left[~stopped, -1].clone()\n    k_action_left = k_action_left[:, : env.k_max]\n    action_all = torch.cat((action_index, k_action_left, k_action_right), -1)\n\n    outdict = {\"log_likelihood\": ll, \"cost_bsf\": td[\"cost_bsf\"]}\n    td.set(\"action\", action_all)\n\n    if return_embeds:\n        outdict[\"embeds\"] = nfe.detach()\n\n    if return_actions:\n        outdict[\"actions\"] = action_all\n\n    return outdict\n</code></pre>"},{"location":"docs/content/api/zoo/improvement/#models.zoo.neuopt.model.NeuOpt","title":"NeuOpt","text":"<pre><code>NeuOpt(\n    env: RL4COEnvBase,\n    policy: Module = None,\n    critic: CriticNetwork = None,\n    policy_kwargs: dict = {},\n    critic_kwargs: dict = {},\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>n_step_PPO</code></p> <p>NeuOpt Model based on n_step Proximal Policy Optimization (PPO) with an NeuOpt model policy. We default to the NeuOpt model policy and the improvement Critic Network.</p> <p>Parameters:</p> <ul> <li> <code>env</code>               (<code>RL4COEnvBase</code>)           \u2013            <p>Environment to use for the algorithm</p> </li> <li> <code>policy</code>               (<code>Module</code>, default:                   <code>None</code> )           \u2013            <p>Policy to use for the algorithm</p> </li> <li> <code>critic</code>               (<code>CriticNetwork</code>, default:                   <code>None</code> )           \u2013            <p>Critic to use for the algorithm</p> </li> <li> <code>policy_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments for policy</p> </li> <li> <code>critic_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments for critic</p> </li> </ul> Source code in <code>rl4co/models/zoo/neuopt/model.py</code> <pre><code>def __init__(\n    self,\n    env: RL4COEnvBase,\n    policy: nn.Module = None,\n    critic: CriticNetwork = None,\n    policy_kwargs: dict = {},\n    critic_kwargs: dict = {},\n    **kwargs,\n):\n    if policy is None:\n        policy = NeuOptPolicy(env_name=env.name, **policy_kwargs)\n\n    if critic is None:\n        embed_dim = (\n            policy_kwargs[\"embed_dim\"] if \"embed_dim\" in policy_kwargs else 128\n        )  # the critic's embed_dim must be as policy's\n\n        encoder = MultiHeadAttentionLayer(\n            embed_dim,\n            critic_kwargs[\"num_heads\"] if \"num_heads\" in critic_kwargs else 4,\n            critic_kwargs[\"feedforward_hidden\"]\n            if \"feedforward_hidden\" in critic_kwargs\n            else 128,\n            critic_kwargs[\"normalization\"]\n            if \"normalization\" in critic_kwargs\n            else \"layer\",\n            bias=False,\n        )\n        value_head = CriticDecoder(embed_dim, dropout_rate=0.001)\n\n        critic = CriticNetwork(\n            encoder=encoder,\n            value_head=value_head,\n            customized=True,\n        )\n\n    super().__init__(env, policy, critic, **kwargs)\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/","title":"Transductive Methods","text":""},{"location":"docs/content/api/zoo/transductive/#transductive-methods","title":"Transductive Methods","text":"<p>These methods update policy parameters during online testing to improve the solutions of a specific instance.</p>"},{"location":"docs/content/api/zoo/transductive/#active-search-as","title":"Active Search (AS)","text":""},{"location":"docs/content/api/zoo/transductive/#models.zoo.active_search.search.ActiveSearch","title":"ActiveSearch","text":"<pre><code>ActiveSearch(\n    env,\n    policy,\n    dataset: Union[Dataset, str],\n    batch_size: int = 1,\n    max_iters: int = 200,\n    augment_size: int = 8,\n    augment_dihedral: bool = True,\n    num_parallel_runs: int = 1,\n    max_runtime: int = 86400,\n    save_path: str = None,\n    optimizer: Union[str, Optimizer, partial] = \"Adam\",\n    optimizer_kwargs: dict = {\n        \"lr\": 0.00026,\n        \"weight_decay\": 1e-06,\n    },\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>TransductiveModel</code></p> <p>Active Search for Neural Combination Optimization from Bello et al. (2016). Fine-tunes the whole policy network (encoder + decoder) on a batch of instances. Reference: https://arxiv.org/abs/1611.09940</p> <p>Parameters:</p> <ul> <li> <code>env</code>           \u2013            <p>RL4CO environment to be solved</p> </li> <li> <code>policy</code>           \u2013            <p>policy network</p> </li> <li> <code>dataset</code>               (<code>Union[Dataset, str]</code>)           \u2013            <p>dataset to be used for training</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>batch size for training</p> </li> <li> <code>max_iters</code>               (<code>int</code>, default:                   <code>200</code> )           \u2013            <p>maximum number of iterations</p> </li> <li> <code>augment_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>number of augmentations per state</p> </li> <li> <code>augment_dihedral</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to augment with dihedral rotations</p> </li> <li> <code>parallel_runs</code>           \u2013            <p>number of parallel runs</p> </li> <li> <code>max_runtime</code>               (<code>int</code>, default:                   <code>86400</code> )           \u2013            <p>maximum runtime in seconds</p> </li> <li> <code>save_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>path to save solution checkpoints</p> </li> <li> <code>optimizer</code>               (<code>Union[str, Optimizer, partial]</code>, default:                   <code>'Adam'</code> )           \u2013            <p>optimizer to use for training</p> </li> <li> <code>optimizer_kwargs</code>               (<code>dict</code>, default:                   <code>{'lr': 0.00026, 'weight_decay': 1e-06}</code> )           \u2013            <p>keyword arguments for optimizer</p> </li> <li> <code>**kwargs</code>           \u2013            <p>additional keyword arguments</p> </li> </ul> Source code in <code>rl4co/models/zoo/active_search/search.py</code> <pre><code>def __init__(\n    self,\n    env,\n    policy,\n    dataset: Union[Dataset, str],\n    batch_size: int = 1,\n    max_iters: int = 200,\n    augment_size: int = 8,\n    augment_dihedral: bool = True,\n    num_parallel_runs: int = 1,\n    max_runtime: int = 86_400,\n    save_path: str = None,\n    optimizer: Union[str, torch.optim.Optimizer, partial] = \"Adam\",\n    optimizer_kwargs: dict = {\"lr\": 2.6e-4, \"weight_decay\": 1e-6},\n    **kwargs,\n):\n    self.save_hyperparameters(logger=False)\n\n    assert batch_size == 1, \"Batch size must be 1 for active search\"\n\n    super(ActiveSearch, self).__init__(\n        env,\n        policy=policy,\n        dataset=dataset,\n        batch_size=batch_size,\n        max_iters=max_iters,\n        max_runtime=max_runtime,\n        save_path=save_path,\n        optimizer=optimizer,\n        optimizer_kwargs=optimizer_kwargs,\n        **kwargs,\n    )\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.active_search.search.ActiveSearch.setup","title":"setup","text":"<pre><code>setup(stage='fit')\n</code></pre> <p>Setup base class and instantiate:</p> <ul> <li>augmentation</li> <li>instance solutions and rewards</li> <li>original policy state dict</li> </ul> Source code in <code>rl4co/models/zoo/active_search/search.py</code> <pre><code>def setup(self, stage=\"fit\"):\n    \"\"\"Setup base class and instantiate:\n    - augmentation\n    - instance solutions and rewards\n    - original policy state dict\n    \"\"\"\n    log.info(\"Setting up active search...\")\n    super(ActiveSearch, self).setup(stage)\n\n    # Instantiate augmentation\n    self.augmentation = StateAugmentation(\n        num_augment=self.hparams.augment_size,\n        augment_fn=\"dihedral8\" if self.hparams.augment_dihedral else \"symmetric\",\n    )\n\n    # Store original policy state dict\n    self.original_policy_state = self.policy.state_dict()\n\n    # Get dataset size and problem size\n    dataset_size = len(self.dataset)\n    _batch = next(iter(self.train_dataloader()))\n    self.problem_size = self.env.reset(_batch)[\"action_mask\"].shape[-1]\n    self.instance_solutions = torch.zeros(\n        dataset_size, self.problem_size * 2, dtype=int\n    )\n    self.instance_rewards = torch.zeros(dataset_size)\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.active_search.search.ActiveSearch.on_train_batch_start","title":"on_train_batch_start","text":"<pre><code>on_train_batch_start(batch: Any, batch_idx: int)\n</code></pre> <p>Called before training (i.e. search) for a new batch begins. We re-load the original policy state dict and configure the optimizer.</p> Source code in <code>rl4co/models/zoo/active_search/search.py</code> <pre><code>def on_train_batch_start(self, batch: Any, batch_idx: int):\n    \"\"\"Called before training (i.e. search) for a new batch begins.\n    We re-load the original policy state dict and configure the optimizer.\n    \"\"\"\n    self.policy.load_state_dict(self.original_policy_state)\n    self.configure_optimizers(self.policy.parameters())\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.active_search.search.ActiveSearch.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Main search loop. We use the training step to effectively adapt to a <code>batch</code> of instances.</p> Source code in <code>rl4co/models/zoo/active_search/search.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Main search loop. We use the training step to effectively adapt to a `batch` of instances.\"\"\"\n    # Augment state\n    batch_size = batch.shape[0]\n    td_init = self.env.reset(batch)\n    n_aug, n_start, n_runs = (\n        self.augmentation.num_augment,\n        self.env.get_num_starts(td_init),\n        self.hparams.num_parallel_runs,\n    )\n    td_init = self.augmentation(td_init)\n    td_init = batchify(td_init, n_runs)\n\n    # Solution and reward buffer\n    max_reward = torch.full((batch_size,), -float(\"inf\"), device=batch.device)\n    best_solutions = torch.zeros(\n        batch_size, self.problem_size * 2, device=batch.device, dtype=int\n    )\n\n    # Init search\n    t_start = time.time()\n    for i in range(self.hparams.max_iters):\n        # Evaluate policy with sampling multistarts (as in POMO)\n        out = self.policy(\n            td_init.clone(),\n            env=self.env,\n            decode_type=\"multistart_sampling\",\n            num_starts=n_start,\n            return_actions=True,\n        )\n\n        if i == 0:\n            log.info(f\"Initial reward: {out['reward'].max():.2f}\")\n\n        # Update best solution and reward found\n        max_reward_iter = out[\"reward\"].max()\n        if max_reward_iter &gt; max_reward:\n            max_reward_idx = out[\"reward\"].argmax()\n            best_solution_iter = out[\"actions\"][max_reward_idx]\n            max_reward = max_reward_iter\n            best_solutions[0, : best_solution_iter.shape[0]] = best_solution_iter\n\n        # Compute REINFORCE loss with shared baseline\n        reward = unbatchify(out[\"reward\"], (n_runs, n_aug, n_start))\n        ll = unbatchify(out[\"log_likelihood\"], (n_runs, n_aug, n_start))\n        advantage = reward - reward.mean(dim=-1, keepdim=True)\n        loss = -(advantage * ll).mean()\n\n        # Backpropagate loss\n        # perform manual optimization following the Lightning routine\n        # https://lightning.ai/docs/pytorch/stable/common/optimization.html\n        opt = self.optimizers()\n        opt.zero_grad()\n        self.manual_backward(loss)\n\n        self.log_dict(\n            {\n                \"loss\": loss,\n                \"max_reward\": max_reward,\n                \"step\": i,\n                \"time\": time.time() - t_start,\n            },\n            on_step=self.log_on_step,\n        )\n\n        # Stop if max runtime is exceeded\n        if time.time() - t_start &gt; self.hparams.max_runtime:\n            break\n\n    return {\"max_reward\": max_reward, \"best_solutions\": best_solutions}\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.active_search.search.ActiveSearch.on_train_batch_end","title":"on_train_batch_end","text":"<pre><code>on_train_batch_end(\n    outputs: STEP_OUTPUT, batch: Any, batch_idx: int\n) -&gt; None\n</code></pre> <p>We store the best solution and reward found.</p> Source code in <code>rl4co/models/zoo/active_search/search.py</code> <pre><code>def on_train_batch_end(\n    self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int\n) -&gt; None:\n    \"\"\"We store the best solution and reward found.\"\"\"\n    max_rewards, best_solutions = outputs[\"max_reward\"], outputs[\"best_solutions\"]\n    self.instance_rewards[batch_idx] = max_rewards\n    self.instance_solutions[batch_idx, :] = best_solutions.squeeze(\n        0\n    )  # only one instance\n    log.info(f\"Best reward: {max_rewards.mean():.2f}\")\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.active_search.search.ActiveSearch.on_train_epoch_end","title":"on_train_epoch_end","text":"<pre><code>on_train_epoch_end() -&gt; None\n</code></pre> <p>Called when the training ends. If the epoch ends, it means we have finished searching over the instances, thus the trainer should stop.</p> Source code in <code>rl4co/models/zoo/active_search/search.py</code> <pre><code>def on_train_epoch_end(self) -&gt; None:\n    \"\"\"Called when the training ends.\n    If the epoch ends, it means we have finished searching over the\n    instances, thus the trainer should stop.\n    \"\"\"\n    save_path = self.hparams.save_path\n    if save_path is not None:\n        log.info(f\"Saving solutions and rewards to {save_path}...\")\n        torch.save(\n            {\"solutions\": self.instance_solutions, \"rewards\": self.instance_rewards},\n            save_path,\n        )\n\n    # https://github.com/Lightning-AI/lightning/issues/1406\n    self.trainer.should_stop = True\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#efficent-active-search-eas","title":"Efficent Active Search (EAS)","text":""},{"location":"docs/content/api/zoo/transductive/#models.zoo.eas.search.EAS","title":"EAS","text":"<pre><code>EAS(\n    env,\n    policy,\n    dataset: Union[Dataset, str],\n    use_eas_embedding: bool = True,\n    use_eas_layer: bool = False,\n    eas_emb_cache_keys: List[str] = [\"logit_key\"],\n    eas_lambda: float = 0.013,\n    batch_size: int = 2,\n    max_iters: int = 200,\n    augment_size: int = 8,\n    augment_dihedral: bool = True,\n    num_parallel_runs: int = 1,\n    baseline: str = \"multistart\",\n    max_runtime: int = 86400,\n    save_path: str = None,\n    optimizer: Union[str, Optimizer, partial] = \"Adam\",\n    optimizer_kwargs: dict = {\n        \"lr\": 0.0041,\n        \"weight_decay\": 1e-06,\n    },\n    verbose: bool = True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>TransductiveModel</code></p> <p>Efficient Active Search for Neural Combination Optimization from Hottung et al. (2022). Fine-tunes a subset of parameters (such as node embeddings or newly added layers) thus avoiding expensive re-encoding of the problem. Reference: https://openreview.net/pdf?id=nO5caZwFwYu</p> <p>Parameters:</p> <ul> <li> <code>env</code>           \u2013            <p>RL4CO environment to be solved</p> </li> <li> <code>policy</code>           \u2013            <p>policy network</p> </li> <li> <code>dataset</code>               (<code>Union[Dataset, str]</code>)           \u2013            <p>dataset to be used for training</p> </li> <li> <code>use_eas_embedding</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to use EAS embedding (EASEmb)</p> </li> <li> <code>use_eas_layer</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use EAS layer (EASLay)</p> </li> <li> <code>eas_emb_cache_keys</code>               (<code>List[str]</code>, default:                   <code>['logit_key']</code> )           \u2013            <p>keys to cache in the embedding</p> </li> <li> <code>eas_lambda</code>               (<code>float</code>, default:                   <code>0.013</code> )           \u2013            <p>lambda parameter for IL loss</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>batch size for training</p> </li> <li> <code>max_iters</code>               (<code>int</code>, default:                   <code>200</code> )           \u2013            <p>maximum number of iterations</p> </li> <li> <code>augment_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>number of augmentations per state</p> </li> <li> <code>augment_dihedral</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to augment with dihedral rotations</p> </li> <li> <code>parallel_runs</code>           \u2013            <p>number of parallel runs</p> </li> <li> <code>baseline</code>               (<code>str</code>, default:                   <code>'multistart'</code> )           \u2013            <p>REINFORCE baseline type (multistart, symmetric, full)</p> </li> <li> <code>max_runtime</code>               (<code>int</code>, default:                   <code>86400</code> )           \u2013            <p>maximum runtime in seconds</p> </li> <li> <code>save_path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>path to save solution checkpoints</p> </li> <li> <code>optimizer</code>               (<code>Union[str, Optimizer, partial]</code>, default:                   <code>'Adam'</code> )           \u2013            <p>optimizer to use for training</p> </li> <li> <code>optimizer_kwargs</code>               (<code>dict</code>, default:                   <code>{'lr': 0.0041, 'weight_decay': 1e-06}</code> )           \u2013            <p>keyword arguments for optimizer</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to print progress for each iteration</p> </li> </ul> Source code in <code>rl4co/models/zoo/eas/search.py</code> <pre><code>def __init__(\n    self,\n    env,\n    policy,\n    dataset: Union[Dataset, str],\n    use_eas_embedding: bool = True,\n    use_eas_layer: bool = False,\n    eas_emb_cache_keys: List[str] = [\"logit_key\"],\n    eas_lambda: float = 0.013,\n    batch_size: int = 2,\n    max_iters: int = 200,\n    augment_size: int = 8,\n    augment_dihedral: bool = True,\n    num_parallel_runs: int = 1,\n    baseline: str = \"multistart\",\n    max_runtime: int = 86_400,\n    save_path: str = None,\n    optimizer: Union[str, torch.optim.Optimizer, partial] = \"Adam\",\n    optimizer_kwargs: dict = {\"lr\": 0.0041, \"weight_decay\": 1e-6},\n    verbose: bool = True,\n    **kwargs,\n):\n    self.save_hyperparameters(logger=False)\n\n    assert (\n        self.hparams.use_eas_embedding or self.hparams.use_eas_layer\n    ), \"At least one of `use_eas_embedding` or `use_eas_layer` must be True.\"\n\n    super(EAS, self).__init__(\n        env,\n        policy=policy,\n        dataset=dataset,\n        batch_size=batch_size,\n        max_iters=max_iters,\n        max_runtime=max_runtime,\n        save_path=save_path,\n        optimizer=optimizer,\n        optimizer_kwargs=optimizer_kwargs,\n        **kwargs,\n    )\n\n    assert self.hparams.baseline in [\n        \"multistart\",\n        \"symmetric\",\n        \"full\",\n    ], f\"Baseline {self.hparams.baseline} not supported.\"\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.eas.search.EAS.setup","title":"setup","text":"<pre><code>setup(stage='fit')\n</code></pre> <p>Setup base class and instantiate:</p> <ul> <li>augmentation</li> <li>instance solutions and rewards</li> <li>original policy state dict</li> </ul> Source code in <code>rl4co/models/zoo/eas/search.py</code> <pre><code>def setup(self, stage=\"fit\"):\n    \"\"\"Setup base class and instantiate:\n    - augmentation\n    - instance solutions and rewards\n    - original policy state dict\n    \"\"\"\n    log.info(\n        f\"Setting up Efficient Active Search (EAS) with: \\n\"\n        f\"- EAS Embedding: {self.hparams.use_eas_embedding} \\n\"\n        f\"- EAS Layer: {self.hparams.use_eas_layer} \\n\"\n    )\n    super(EAS, self).setup(stage)\n\n    # Instantiate augmentation\n    self.augmentation = StateAugmentation(\n        num_augment=self.hparams.augment_size,\n        augment_fn=\"dihedral8\" if self.hparams.augment_dihedral else \"symmetric\",\n    )\n\n    # Store original policy state dict\n    self.original_policy_state = self.policy.state_dict()\n\n    # Get dataset size and problem size\n    len(self.dataset)\n    _batch = next(iter(self.train_dataloader()))\n    self.problem_size = self.env.reset(_batch)[\"action_mask\"].shape[-1]\n    self.instance_solutions = []\n    self.instance_rewards = []\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.eas.search.EAS.on_train_batch_start","title":"on_train_batch_start","text":"<pre><code>on_train_batch_start(batch: Any, batch_idx: int)\n</code></pre> <p>Called before training (i.e. search) for a new batch begins. We re-load the original policy state dict and configure all parameters not to require gradients. We do the rest in the training step.</p> Source code in <code>rl4co/models/zoo/eas/search.py</code> <pre><code>def on_train_batch_start(self, batch: Any, batch_idx: int):\n    \"\"\"Called before training (i.e. search) for a new batch begins.\n    We re-load the original policy state dict and configure all parameters not to require gradients.\n    We do the rest in the training step.\n    \"\"\"\n    self.policy.load_state_dict(self.original_policy_state)\n\n    # Set all policy parameters to not require gradients\n    for param in self.policy.parameters():\n        param.requires_grad = False\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.eas.search.EAS.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Main search loop. We use the training step to effectively adapt to a <code>batch</code> of instances.</p> Source code in <code>rl4co/models/zoo/eas/search.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Main search loop. We use the training step to effectively adapt to a `batch` of instances.\"\"\"\n    # Augment state\n    batch_size = batch.shape[0]\n    td_init = self.env.reset(batch)\n    n_aug, n_start, n_runs = (\n        self.augmentation.num_augment,\n        self.env.get_num_starts(td_init),\n        self.hparams.num_parallel_runs,\n    )\n    td_init = self.augmentation(td_init)\n    td_init = batchify(td_init, n_runs)\n    num_instances = batch_size * n_aug * n_runs  # NOTE: no num_starts!\n    # batch_r = n_runs * batch_size # effective batch size\n    group_s = (\n        n_start + 1\n    )  # number of different rollouts per instance (+1 for incumbent solution construction)\n\n    # Get encoder and decoder for simplicity\n    encoder = self.policy.encoder\n    decoder = self.policy.decoder\n\n    # Precompute the cache of the embeddings (i.e. q,k,v and logit_key)\n    embeddings, _ = encoder(td_init)\n    cached_embeds = decoder._precompute_cache(embeddings)\n\n    # Collect optimizer parameters\n    opt_params = []\n    if self.hparams.use_eas_layer:\n        # EASLay: replace forward of logit attention computation. EASLayer\n        eas_layer = EASLayerNet(num_instances, decoder.embed_dim).to(batch.device)\n        decoder.pointer.eas_layer = partial(eas_layer, decoder.pointer)\n        decoder.pointer.forward = partial(\n            forward_pointer_attn_eas_lay, decoder.pointer\n        )\n        for param in eas_layer.parameters():\n            opt_params.append(param)\n    if self.hparams.use_eas_embedding:\n        # EASEmb: set gradient of emb_key to True\n        # for all the keys, wrap the embedding in a nn.Parameter\n        for key in self.hparams.eas_emb_cache_keys:\n            setattr(\n                cached_embeds, key, torch.nn.Parameter(getattr(cached_embeds, key))\n            )\n            opt_params.append(getattr(cached_embeds, key))\n    decoder.forward_eas = partial(forward_eas, decoder)\n\n    # We pass attributes saved in policy too\n    def set_attr_if_exists(attr):\n        if hasattr(self.policy, attr):\n            setattr(decoder, attr, getattr(self.policy, attr))\n\n    for attr in [\"temperature\", \"tanh_clipping\", \"mask_logits\"]:\n        set_attr_if_exists(attr)\n\n    self.configure_optimizers(opt_params)\n\n    # Solution and reward buffer\n    max_reward = torch.full((batch_size,), -float(\"inf\"), device=batch.device)\n    best_solutions = torch.zeros(\n        batch_size, self.problem_size * 2, device=batch.device, dtype=int\n    )  # i.e. incumbent solutions\n\n    # Init search\n    t_start = time.time()\n    for iter_count in range(self.hparams.max_iters):\n        # Evaluate policy with sampling multistarts passing the cached embeddings\n        best_solutions_expanded = best_solutions.repeat(n_aug, 1).repeat(n_runs, 1)\n        logprobs, actions, td_out, reward = decoder.forward_eas(\n            td_init.clone(),\n            cached_embeds=cached_embeds,\n            best_solutions=best_solutions_expanded,\n            iter_count=iter_count,\n            env=self.env,\n            decode_type=\"multistart_sampling\",\n            num_starts=n_start,\n        )\n\n        # Unbatchify to get correct dimensions\n        ll = get_log_likelihood(logprobs, actions, td_out.get(\"mask\", None))\n        ll = unbatchify(ll, (n_runs * batch_size, n_aug, group_s)).squeeze()\n        reward = unbatchify(reward, (n_runs * batch_size, n_aug, group_s)).squeeze()\n        actions = unbatchify(actions, (n_runs * batch_size, n_aug, group_s)).squeeze()\n\n        # Compute REINFORCE loss with shared baselines\n        # compared to original EAS, we also support symmetric and full baselines\n        group_reward = reward[..., :-1]  # exclude incumbent solution\n        if self.hparams.baseline == \"multistart\":\n            bl_val = group_reward.mean(dim=-1, keepdim=True)\n        elif self.hparams.baseline == \"symmetric\":\n            bl_val = group_reward.mean(dim=-2, keepdim=True)\n        elif self.hparams.baseline == \"full\":\n            bl_val = group_reward.mean(dim=-1, keepdim=True).mean(\n                dim=-2, keepdim=True\n            )\n        else:\n            raise ValueError(f\"Baseline {self.hparams.baseline} not supported.\")\n\n        # REINFORCE loss\n        advantage = group_reward - bl_val\n        loss_rl = -(advantage * ll[..., :-1]).mean()\n        # IL loss\n        loss_il = -ll[..., -1].mean()\n        # Total loss\n        loss = loss_rl + self.hparams.eas_lambda * loss_il\n\n        # Manual backpropagation\n        opt = self.optimizers()\n        opt.zero_grad()\n        self.manual_backward(loss)\n\n        # Save best solutions and rewards\n        # Get max reward for each group and instance\n        max_reward = reward.max(dim=2)[0].max(dim=1)[0]\n\n        # Reshape and rank rewards\n        reward_group = reward.reshape(n_runs * batch_size, -1)\n        _, top_indices = torch.topk(reward_group, k=1, dim=1)\n\n        # Obtain best solutions found so far\n        solutions = actions.reshape(n_runs * batch_size, n_aug * group_s, -1)\n        best_solutions_iter = gather_by_index(solutions, top_indices, dim=1)\n        best_solutions[:, : best_solutions_iter.shape[1]] = best_solutions_iter\n\n        self.log_dict(\n            {\n                \"loss\": loss,\n                \"max_reward\": max_reward.mean(),\n                \"step\": iter_count,\n                \"time\": time.time() - t_start,\n            },\n            on_step=self.log_on_step,\n        )\n\n        log.info(\n            f\"{iter_count}/{self.hparams.max_iters} | \"\n            f\" Reward: {max_reward.mean().item():.2f} \"\n        )\n\n        # Stop if max runtime is exceeded\n        if time.time() - t_start &gt; self.hparams.max_runtime:\n            log.info(f\"Max runtime of {self.hparams.max_runtime} seconds exceeded.\")\n            break\n\n    return {\"max_reward\": max_reward, \"best_solutions\": best_solutions}\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.eas.search.EAS.on_train_batch_end","title":"on_train_batch_end","text":"<pre><code>on_train_batch_end(\n    outputs: STEP_OUTPUT, batch: Any, batch_idx: int\n) -&gt; None\n</code></pre> <p>We store the best solution and reward found.</p> Source code in <code>rl4co/models/zoo/eas/search.py</code> <pre><code>def on_train_batch_end(\n    self, outputs: STEP_OUTPUT, batch: Any, batch_idx: int\n) -&gt; None:\n    \"\"\"We store the best solution and reward found.\"\"\"\n    max_rewards, best_solutions = outputs[\"max_reward\"], outputs[\"best_solutions\"]\n    self.instance_solutions.append(best_solutions)\n    self.instance_rewards.append(max_rewards)\n    log.info(f\"Best reward: {max_rewards.mean():.2f}\")\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.eas.search.EAS.on_train_epoch_end","title":"on_train_epoch_end","text":"<pre><code>on_train_epoch_end() -&gt; None\n</code></pre> <p>Called when the train ends.</p> Source code in <code>rl4co/models/zoo/eas/search.py</code> <pre><code>def on_train_epoch_end(self) -&gt; None:\n    \"\"\"Called when the train ends.\"\"\"\n    save_path = self.hparams.save_path\n    # concatenate solutions and rewards\n    self.instance_solutions = pad_sequence(\n        self.instance_solutions, batch_first=True, padding_value=0\n    ).squeeze()\n    self.instance_rewards = torch.cat(self.instance_rewards, dim=0).squeeze()\n    if save_path is not None:\n        log.info(f\"Saving solutions and rewards to {save_path}...\")\n        torch.save(\n            {\"solutions\": self.instance_solutions, \"rewards\": self.instance_rewards},\n            save_path,\n        )\n\n    # https://github.com/Lightning-AI/lightning/issues/1406\n    self.trainer.should_stop = True\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.eas.search.EASEmb","title":"EASEmb","text":"<pre><code>EASEmb(*args, **kwargs)\n</code></pre> <p>               Bases: <code>EAS</code></p> <p>EAS with embedding adaptation</p> Source code in <code>rl4co/models/zoo/eas/search.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    **kwargs,\n):\n    if not kwargs.get(\"use_eas_embedding\", False) or kwargs.get(\n        \"use_eas_layer\", True\n    ):\n        log.warning(\n            \"Setting `use_eas_embedding` to True and `use_eas_layer` to False. Use EAS base class to override.\"\n        )\n    kwargs[\"use_eas_embedding\"] = True\n    kwargs[\"use_eas_layer\"] = False\n    super(EASEmb, self).__init__(*args, **kwargs)\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.eas.search.EASLay","title":"EASLay","text":"<pre><code>EASLay(*args, **kwargs)\n</code></pre> <p>               Bases: <code>EAS</code></p> <p>EAS with layer adaptation</p> Source code in <code>rl4co/models/zoo/eas/search.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    **kwargs,\n):\n    if kwargs.get(\"use_eas_embedding\", False) or not kwargs.get(\n        \"use_eas_layer\", True\n    ):\n        log.warning(\n            \"Setting `use_eas_embedding` to True and `use_eas_layer` to False. Use EAS base class to override.\"\n        )\n    kwargs[\"use_eas_embedding\"] = False\n    kwargs[\"use_eas_layer\"] = True\n    super(EASLay, self).__init__(*args, **kwargs)\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.eas.decoder.forward_pointer_attn_eas_lay","title":"forward_pointer_attn_eas_lay","text":"<pre><code>forward_pointer_attn_eas_lay(\n    self, query, key, value, logit_key, mask\n)\n</code></pre> <p>Add layer to the forward pass of logit attention, i.e. Single-head attention.</p> Source code in <code>rl4co/models/zoo/eas/decoder.py</code> <pre><code>def forward_pointer_attn_eas_lay(self, query, key, value, logit_key, mask):\n    \"\"\"Add layer to the forward pass of logit attention, i.e.\n    Single-head attention.\n    \"\"\"\n    # Compute inner multi-head attention with no projections.\n    heads = self._inner_mha(query, key, value, mask)\n\n    # Add residual for EAS layer if is set\n    if getattr(self, \"eas_layer\", None) is not None:\n        heads = heads + self.eas_layer(heads)\n\n    glimpse = self.project_out(heads)\n\n    # Batch matrix multiplication to compute logits (batch_size, num_steps, graph_size)\n    # bmm is slightly faster than einsum and matmul\n    logits = (\n        torch.bmm(glimpse, logit_key.squeeze(1).transpose(-2, -1))\n        / math.sqrt(glimpse.size(-1))\n    ).squeeze(1)\n\n    return logits\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.eas.decoder.forward_eas","title":"forward_eas","text":"<pre><code>forward_eas(\n    self,\n    td: TensorDict,\n    cached_embeds,\n    best_solutions,\n    iter_count: int = 0,\n    env: Union[str, RL4COEnvBase] = None,\n    decode_type: str = \"multistart_sampling\",\n    num_starts: int = None,\n    mask_logits: bool = True,\n    temperature: float = 1.0,\n    tanh_clipping: float = 0,\n    **decode_kwargs\n)\n</code></pre> <p>Forward pass of the decoder Given the environment state and the pre-computed embeddings, compute the logits and sample actions</p> <p>Parameters:</p> <ul> <li> <code>td</code>               (<code>TensorDict</code>)           \u2013            <p>Input TensorDict containing the environment state</p> </li> <li> <code>embeddings</code>           \u2013            <p>Precomputed embeddings for the nodes. Can be already precomputed cached in form of q, k, v and</p> </li> <li> <code>env</code>               (<code>Union[str, RL4COEnvBase]</code>, default:                   <code>None</code> )           \u2013            <p>Environment to use for decoding. If None, the environment is instantiated from <code>env_name</code>. Note that it is more efficient to pass an already instantiated environment each time for fine-grained control</p> </li> <li> <code>decode_type</code>               (<code>str</code>, default:                   <code>'multistart_sampling'</code> )           \u2013            <p>Type of decoding to use. Can be one of:</p> <ul> <li>\"sampling\": sample from the logits</li> <li>\"greedy\": take the argmax of the logits</li> <li>\"multistart_sampling\": sample as sampling, but with multi-start decoding</li> <li>\"multistart_greedy\": sample as greedy, but with multi-start decoding</li> </ul> </li> <li> <code>num_starts</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of multi-starts to use. If None, will be calculated from the action mask</p> </li> <li> <code>calc_reward</code>           \u2013            <p>Whether to calculate the reward for the decoded sequence</p> </li> </ul> Source code in <code>rl4co/models/zoo/eas/decoder.py</code> <pre><code>def forward_eas(\n    self,\n    td: TensorDict,\n    cached_embeds,\n    best_solutions,\n    iter_count: int = 0,\n    env: Union[str, RL4COEnvBase] = None,\n    decode_type: str = \"multistart_sampling\",\n    num_starts: int = None,\n    mask_logits: bool = True,\n    temperature: float = 1.0,\n    tanh_clipping: float = 0,\n    **decode_kwargs,\n):\n    \"\"\"Forward pass of the decoder\n    Given the environment state and the pre-computed embeddings, compute the logits and sample actions\n\n    Args:\n        td: Input TensorDict containing the environment state\n        embeddings: Precomputed embeddings for the nodes. Can be already precomputed cached in form of q, k, v and\n        env: Environment to use for decoding. If None, the environment is instantiated from `env_name`. Note that\n            it is more efficient to pass an already instantiated environment each time for fine-grained control\n        decode_type: Type of decoding to use. Can be one of:\n            - \"sampling\": sample from the logits\n            - \"greedy\": take the argmax of the logits\n            - \"multistart_sampling\": sample as sampling, but with multi-start decoding\n            - \"multistart_greedy\": sample as greedy, but with multi-start decoding\n        num_starts: Number of multi-starts to use. If None, will be calculated from the action mask\n        calc_reward: Whether to calculate the reward for the decoded sequence\n    \"\"\"\n    # TODO: this could be refactored by decoding strategies\n\n    # Collect logprobs\n    logprobs = []\n    actions = []\n\n    decode_step = 0\n    # Multi-start decoding: first action is chosen by ad-hoc node selection\n    if num_starts &gt; 1 or \"multistart\" in decode_type:\n        action = env.select_start_nodes(td, num_starts + 1) % num_starts\n        # Append incumbent solutions\n        if iter_count &gt; 0:\n            action = unbatchify(action, num_starts + 1)\n            action[:, -1] = best_solutions[:, decode_step]\n            action = action.permute(1, 0).reshape(-1)\n\n        # Expand td to batch_size * (num_starts + 1)\n        td = batchify(td, num_starts + 1)\n\n        td.set(\"action\", action)\n        td = env.step(td)[\"next\"]\n        logp = torch.zeros_like(\n            td[\"action_mask\"], device=td.device\n        )  # first logprobs is 0, so p = logprobs.exp() = 1\n\n        logprobs.append(logp)\n        actions.append(action)\n\n    # Main decoding: loop until all sequences are done\n    while not td[\"done\"].all():\n        decode_step += 1\n        logits, mask = self.forward(td, cached_embeds, num_starts + 1)\n\n        logp = process_logits(\n            logits,\n            mask,\n            temperature=self.temperature if self.temperature is not None else temperature,\n            tanh_clipping=self.tanh_clipping\n            if self.tanh_clipping is not None\n            else tanh_clipping,\n            mask_logits=self.mask_logits if self.mask_logits is not None else mask_logits,\n        )\n\n        # Select the indices of the next nodes in the sequences, result (batch_size) long\n        action = decode_logprobs(logp, mask, decode_type=decode_type)\n\n        if iter_count &gt; 0:  # append incumbent solutions\n            init_shp = action.shape\n            action = unbatchify(action, num_starts + 1)\n            action[:, -1] = best_solutions[:, decode_step]\n            action = action.permute(1, 0).reshape(init_shp)\n\n        td.set(\"action\", action)\n        td = env.step(td)[\"next\"]\n\n        # Collect output of step\n        logprobs.append(logp)\n        actions.append(action)\n\n    logprobs, actions = torch.stack(logprobs, 1), torch.stack(actions, 1)\n    rewards = env.get_reward(td, actions)\n    return logprobs, actions, td, rewards\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.eas.nn.EASLayerNet","title":"EASLayerNet","text":"<pre><code>EASLayerNet(num_instances: int, emb_dim: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Instantiate weights and biases for the added layer. The layer is defined as: h = relu(emb * W1 + b1); out = h * W2 + b2. Wrapping in <code>nn.Parameter</code> makes the parameters trainable and sets gradient to True.</p> <p>Parameters:</p> <ul> <li> <code>num_instances</code>               (<code>int</code>)           \u2013            <p>Number of instances in the dataset</p> </li> <li> <code>emb_dim</code>               (<code>int</code>)           \u2013            <p>Dimension of the embedding</p> </li> </ul> Source code in <code>rl4co/models/zoo/eas/nn.py</code> <pre><code>def __init__(self, num_instances: int, emb_dim: int):\n    super().__init__()\n    # W2 and b2 are initialized to zero so in the first iteration the layer is identity\n    self.W1 = nn.Parameter(torch.randn(num_instances, emb_dim, emb_dim))\n    self.b1 = nn.Parameter(torch.randn(num_instances, 1, emb_dim))\n    self.W2 = nn.Parameter(torch.zeros(num_instances, emb_dim, emb_dim))\n    self.b2 = nn.Parameter(torch.zeros(num_instances, 1, emb_dim))\n    torch.nn.init.xavier_uniform_(self.W1)\n    torch.nn.init.xavier_uniform_(self.b1)\n</code></pre>"},{"location":"docs/content/api/zoo/transductive/#models.zoo.eas.nn.EASLayerNet.forward","title":"forward","text":"<pre><code>forward(*args)\n</code></pre> <p>emb: [num_instances, group_num, emb_dim]</p> Source code in <code>rl4co/models/zoo/eas/nn.py</code> <pre><code>def forward(self, *args):\n    \"\"\"emb: [num_instances, group_num, emb_dim]\"\"\"\n    # get tensor arg (from partial instantiation)\n    emb = [arg for arg in args if isinstance(arg, torch.Tensor)][0]\n    h = torch.relu(torch.matmul(emb, self.W1) + self.b1.expand_as(emb))\n    return torch.matmul(h, self.W2) + self.b2.expand_as(h)\n</code></pre>"},{"location":"docs/content/general/ai4co/","title":"AI4CO Community","text":"<p>We invite you to join our AI4CO community, an open and inclusive research group in Artificial Intelligence (AI) for Combinatorial Optimization (CO)!</p>"},{"location":"docs/content/general/ai4co/#links","title":"Links","text":"<ul> <li>GitHub</li> <li>Slack</li> <li>Website (coming soon!)</li> </ul>"},{"location":"docs/content/general/contribute/","title":"Contributing to RL4CO","text":"<p>Have a suggestion, request, or found a bug? Feel free to open an issue or submit a pull request. If you would like to contribute, please check out our contribution guidelines   here. We welcome and look forward to all contributions to RL4CO!</p> <p>We are also on Slack if you have any questions or would like to discuss RL4CO with us. We are open to collaborations and would love to hear from you \ud83d\ude80</p>"},{"location":"docs/content/general/contribute/#contributors","title":"Contributors","text":""},{"location":"docs/content/general/faq/","title":"FAQ","text":"<p>You can submit your questions via GitHub Issues or Discussions. You may search for your question in the existing issues or discussions before submitting a new one.</p> <p>If ask more than a few times, we will add it here!</p>"},{"location":"docs/content/general/licensing/","title":"License and Usage","text":"<p>Our library is released under the MIT License, which is an open and permissive license. This means:</p> <ul> <li>You can use, modify, and distribute the code without any restrictions, even for commercial purposes.</li> <li>Your projects will not inherit any additional limitations from our library, even if you modify or extend it.</li> </ul> <p>All contributions to the library are covered by the MIT License, ensuring that everything is free to use under the same open terms. A copy of the license is available here.</p>"},{"location":"docs/content/intro/environments/","title":"Environments","text":""},{"location":"docs/content/intro/environments/#definition","title":"Definition","text":"<p>Given a CO problem instance \\(\\mathbf{x}\\), we formulate the solution-generating procedure as a Markov Decision Process (MDP) characterized by a tuple \\((\\mathcal{S}, \\mathcal{A}, \\mathcal{T}, \\mathcal{R}, \\gamma)\\) as follows:</p> <ul> <li>State \\(\\mathcal{S}\\) is the space of states that represent the given problem \\(\\mathbf{x}\\) and the current partial solution being updated in the MDP.</li> <li>Action \\(\\mathcal{A}\\) is the action space, which includes all feasible actions \\(a_t\\) that can be taken at each step \\(t\\).</li> <li>State Transition \\(\\mathcal{T}\\) is the deterministic state transition function \\(s_{t+1} = \\mathcal{T}(s_t, a_t)\\) that updates a state \\(s_t\\) to the next state \\(s_{t+1}\\).</li> <li>Reward \\(\\mathcal{R}\\) is the reward function \\(\\mathcal{R}(s_t, a_t)\\) representing the immediate reward received after taking action \\(a_t\\) in state \\(s_t\\).</li> <li>Discount Factor \\(\\gamma \\in [0, 1]\\) determines the importance of future rewards.</li> </ul> <p>Since the state transition is deterministic, we represent the solution for a problem \\(\\mathbf{x}\\) as a sequence of \\(T\\) actions \\(\\mathbf{a} = (a_1, \\ldots, a_T)\\). Then the total return \\(\\sum_{t=1}^T \\mathcal{R}(s_t, a_t)\\) translates to the negative cost function of the CO problem.</p>"},{"location":"docs/content/intro/environments/#implementation","title":"Implementation","text":"<p>Environments in our library fully specify the CO problems and their logic. They are based on the <code>RL4COEnvBase</code> class that extends from the <code>EnvBase</code> in TorchRL.</p> <p>Key features:</p> <ul> <li>A modular <code>generator</code> can be provided to the environment.</li> <li>The generator provides CO instances to the environment, and different generators can be used to generate different data distributions.</li> <li>Static instance data and dynamic variables, such as the current state \\(s_t\\), current solution \\(\\mathbf{a}^k\\) for improvement environments, policy actions \\(a_t\\), rewards, and additional information are passed in a stateless fashion in a <code>TensorDict</code>, that we call <code>td</code>, through the environment <code>reset</code> and <code>step</code> functions.</li> </ul> <p>Our environment API contains several functions:</p> <ul> <li><code>render</code></li> <li><code>check_solution_validity</code></li> <li><code>select_start_nodes</code> (i.e., for POMO-based optimization)</li> <li>Optional API such as <code>local_search</code> for solution improvement</li> </ul> <p>It's worth noting that our library enhances the efficiency of environments when compared to vanilla TorchRL, by overriding and optimizing some methods in TorchRL <code>EnvBase</code>. For instance, our new <code>step</code> method brings a decrease of up to 50% in latency and halves the memory impact by avoiding saving duplicate components in the stateless <code>TensorDict</code>.</p>"},{"location":"docs/content/intro/intro/","title":"Introduction","text":"<p>RL4CO is an extensive Reinforcement Learning (RL) for Combinatorial Optimization (CO) benchmark. Our goal is to provide a unified framework for RL-based CO algorithms, and to facilitate reproducible research in this field, decoupling the science from the engineering.</p>"},{"location":"docs/content/intro/intro/#motivation","title":"Motivation","text":""},{"location":"docs/content/intro/intro/#why-nco","title":"Why NCO?","text":"<p>Neural Combinatorial Optimization (NCO) is a subfield of AI that aims to solve combinatorial optimization problems using neural networks. NCO has been successfully applied to a wide range of problems, such as the routing problems in logistics, the scheduling problems in manufacturing, and electronic design automation. The key idea behind NCO is to learn a policy that maps the input data to the optimal solution, without the need for hand-crafted heuristics or domain-specific knowledge.</p>"},{"location":"docs/content/intro/intro/#why-rl","title":"Why RL?","text":"<p>Reinforcement Learning (RL) is a machine learning paradigm that enables agents to learn how to make decisions by interacting with an environment. RL has been successfully applied to a wide range of problems, such as playing games, controlling robots, and optimizing complex systems. The key idea behind RL is to learn a policy that maps the state of the environment to the optimal action, by maximizing a reward signal. Importantly, optimal solutions are not required for training, as RL agents learn from the feedback they receive from the environment.</p>"},{"location":"docs/content/intro/intro/#contents","title":"Contents","text":"<p>We explore in other pages the following components:</p> <ul> <li>Environments: Markov Decision Process (MDP) for CO problems and base classes for environments. These are based on TorchRL.</li> </ul> <ul> <li>Policies: the neural networks that are used to solve CO problems and their base classes. These are based on PyTorch.</li> </ul> <ul> <li>RL Algorithms: (broadly: \"models\"), which are the processes used to train the policies and their base classes. These are based on PyTorch Lightning.</li> </ul>"},{"location":"docs/content/intro/policies/","title":"Policies","text":"<p>The policies can be categorized into constructive policies, which generate a solution from scratch, and improvement policies, which refine an existing solution.</p>"},{"location":"docs/content/intro/policies/#constructive-policies","title":"Constructive policies","text":"<p>A policy \\(\\pi\\) is used to construct a solution from scratch for a given problem instance \\(\\mathbf{x}\\). It can be further categorized into autoregressive (AR) and non-autoregressive (NAR) policies.</p>"},{"location":"docs/content/intro/policies/#autoregressive-ar-policies","title":"Autoregressive (AR) policies","text":"<p>An AR policy is composed of an encoder \\(f\\) that maps the instance \\(\\mathbf{x}\\) into an embedding space \\(\\mathbf{h}=f(\\mathbf{x})\\) and by a decoder \\(g\\) that iteratively determines a sequence of actions \\(\\mathbf{a}\\) as follows:</p> \\[ a_t \\sim g(a_t | a_{t-1}, ... ,a_0, s_t, \\mathbf{h}), \\quad  \\pi(\\mathbf{a}|\\mathbf{x}) \\triangleq \\prod_{t=1}^{T-1} g(a_{t} | a_{t-1}, \\ldots ,a_0, s_t, \\mathbf{h}). \\]"},{"location":"docs/content/intro/policies/#non-autoregressive-nar-policies","title":"Non-autoregressive (NAR) policies","text":"<p>A NAR policy encodes a problem \\(\\mathbf{x}\\) into a heuristic \\(\\mathcal{H} = f(\\mathbf{x}) \\in \\mathbb{R}^{N}_{+}\\), where \\(N\\) is the number of possible assignments across all decision variables. Each number in \\(\\mathcal{H}\\) represents a (unnormalized) probability of a particular assignment. To obtain a solution \\(\\mathbf{a}\\) from \\(\\mathcal{H}\\), one can sample a sequence of assignments from \\(\\mathcal{H}\\) while dynamically masking infeasible assignments to meet problem-specific constraints. It can also guide a search process, e.g., Ant Colony Optimization, or be incorporated into hybrid frameworks. Here, the heuristic helps identify promising transitions and improve the efficiency of finding an optimal or near-optimal solution.</p>"},{"location":"docs/content/intro/policies/#improvement-policies","title":"Improvement policies","text":"<p>A policy can be used for improving an initial solution \\(\\mathbf{a}^{0}=(a_{0}^{0},\\ldots, a_{T-1}^{0})\\) into another one potentially with higher quality, which can be formulated as follows:</p> \\[ \\mathbf{a}^k \\sim g(\\mathbf{a}^{0}, \\mathbf{h}), \\quad\\pi(\\mathbf{a}^K|\\mathbf{a}^0,\\mathbf{x}) \\triangleq \\prod_{k=1}^{K-1} g(\\mathbf{a}^k | \\mathbf{a}^{k-1}, ... ,\\mathbf{a}^0, \\mathbf{h}), \\] <p>where \\(\\mathbf{a}^{k}\\) is the \\(k\\)-th updated solution and \\(K\\) is the budget for number of improvements. This process allows continuous refinement for a long time to enhance the solution quality.</p>"},{"location":"docs/content/intro/policies/#implementation","title":"Implementation","text":"<p>Policies in our library are subclasses of PyTorch's <code>nn.Module</code> and contain the encoding-decoding logic and neural network parameters \\(\\theta\\). Different policies in the RL4CO \"zoo\" can inherit from metaclasses like <code>ConstructivePolicy</code> or <code>ImprovementPolicy</code>. We modularize components to process raw features into the embedding space via a parametrized function \\(\\phi_\\omega\\), called feature embeddings.</p> <ol> <li>Node Embeddings \\(\\phi_n\\): transform \\(m_n\\) node features of instances \\(\\mathbf{x}\\) from the feature space to the embedding space \\(h\\), i.e., \\([B, N, m_n] \\rightarrow [B, N, h]\\).</li> <li>Edge Embeddings \\(\\phi_e\\): transform \\(m_e\\) edge features of instances \\(\\mathbf{x}\\) from the feature space to the embedding space \\(h\\), i.e., \\([B, E, m_e] \\rightarrow [B, E, h]\\), where \\(E\\) is the number of edges.</li> <li>Context Embeddings \\(\\phi_c\\): capture contextual information by transforming \\(m_c\\) context features from the current decoding step \\(s_t\\) from the feature space to the embedding space \\(h\\), i.e., \\([B, m_c] \\rightarrow [B, h]\\), for nodes or edges.</li> </ol> <p>Embeddings can be automatically selected by our library at runtime by simply passing the <code>env_name</code> to the policy. Additionally, we allow for granular control of any higher-level policy component independently, such as encoders and decoders.</p>"},{"location":"docs/content/intro/rl/","title":"RL Algorithms","text":""},{"location":"docs/content/intro/rl/#definitions","title":"Definitions","text":"<p>The RL objective is to learn a policy \\(\\pi\\) that maximizes the expected cumulative reward (or equivalently minimizes the cost) over the distribution of problem instances:</p> \\[ \\theta^{*} = \\underset{\\theta}{\\text{argmax}} \\, \\mathbb{E}_{\\mathbf{x} \\sim P(\\mathbf{x})} \\left[ \\mathbb{E}_{\\pi(\\mathbf{a}|\\mathbf{x})} \\left[ \\sum_{t=0}^{T-1} \\gamma^t \\mathcal{R}(s_t, a_t) \\right] \\right], \\] <p>where \\(\\theta\\) is the set of parameters of \\(\\pi\\) and \\(P(\\mathbf{x})\\) is the distribution of problem instances.</p> <p>This equation can be solved using algorithms such as variations of REINFORCE, Advantage Actor-Critic (A2C) methods, or Proximal Policy Optimization (PPO).</p> <p>These algorithms are employed to train the policy network \\(\\pi\\), by transforming the maximization problem into a minimization problem involving a loss function, which is then optimized using gradient descent algorithms. For instance, the REINFORCE loss function gradient is given by:</p> \\[ \\nabla_{\\theta} \\mathcal{L}_a(\\theta|\\mathbf{x}) = \\mathbb{E}_{\\pi(\\mathbf{a}|\\mathbf{x})} \\left[(R(\\mathbf{a}, \\mathbf{x}) - b(\\mathbf{x})) \\nabla_{\\theta}\\log \\pi(\\mathbf{a}|\\mathbf{x})\\right], \\] <p>where \\(b(\\cdot)\\) is a baseline function used to stabilize training and reduce gradient variance. </p> <p>We also distinguish between two types of RL (pre)training:</p> <ol> <li>Inductive RL: The focus is on learning patterns from the training dataset to generalize to new instances, thus amortizing the inference procedure.</li> <li>Transductive RL (or test-time optimization): Optimizes parameters during testing on target instances.</li> </ol> <p>Typically, a policy \\(\\pi\\) is trained using inductive RL, followed by transductive RL for test-time optimization.</p>"},{"location":"docs/content/intro/rl/#implementation","title":"Implementation","text":"<p>RL algorithms in our library define the process that takes the <code>Environment</code> with its problem instances and the <code>Policy</code> to optimize its parameters \\(\\theta\\). The parent class of algorithms is the <code>RL4COLitModule</code>, inheriting from PyTorch Lightning's <code>pl.LightningModule</code>. This allows for granular support of various methods including the <code>[train, val, test]_step</code>, automatic logging with several logging services such as Wandb via <code>log_metrics</code>, automatic optimizer configuration via <code>configure_optimizers</code> and several useful callbacks for RL methods such as <code>on_train_epoch_end</code>.</p> <p>RL algorithms are additionally attached to an <code>RL4COTrainer</code>, a wrapper we made with additional optimizations around <code>pl.Trainer</code>. This module seamlessly supports features of modern training pipelines, including:</p> <ul> <li>Logging</li> <li>Checkpoint management</li> <li>Mixed-precision training</li> <li>Various hardware acceleration supports (e.g., CPU, GPU, TPU, and Apple Silicon)</li> <li>Multi-device hardware accelerator in distributed settings</li> </ul> <p>For instance, using mixed-precision training significantly decreases training time without sacrificing much convergence and enables us to leverage recent routines, e.g., FlashAttention.</p>"},{"location":"docs/content/start/hydra/","title":"Training with Hydra Configuration","text":"<p>You may find Hydra configurations under configs/ divided into categories (model, env, train, experiment, etc.).</p>"},{"location":"docs/content/start/hydra/#usage","title":"Usage","text":"<p>Train model with default configuration (AM on TSP environment): <pre><code>python run.py\n</code></pre></p> <p>Tip</p> <p>You may check out this notebook to get started with Hydra!</p>"},{"location":"docs/content/start/hydra/#change-experiment","title":"Change experiment","text":"<p>Train model with chosen experiment configuration from configs/experiment/ <pre><code>python run.py experiment=routing/am env=tsp env.generator_params.num_loc=50 model.optimizer_kwargs.lr=2e-4\n</code></pre> Here you may change the environment, e.g. with <code>env=cvrp</code> by command line or by modifying the corresponding experiment e.g. configs/experiment/routing/am.yaml. </p>"},{"location":"docs/content/start/hydra/#disable-logging","title":"Disable logging","text":"<p><pre><code>python run.py experiment=test/am logger=none '~callbacks.learning_rate_monitor'\n</code></pre> Note that <code>~</code> is used to disable a callback that would need a logger.</p>"},{"location":"docs/content/start/hydra/#create-a-sweep-over-hyperparameters","title":"Create a sweep over hyperparameters","text":"<p>We can use -m for multirun:</p> <p><pre><code>python run.py -m experiment=routing/am  model.optimizer_kwargs.lr=1e-3,1e-4,1e-5\n</code></pre> ``` </p>"},{"location":"docs/content/start/installation/","title":"Installation","text":"<p>RL4CO is now available for installation on <code>pip</code>! <pre><code>pip install rl4co\n</code></pre></p>"},{"location":"docs/content/start/installation/#local-install-and-development","title":"Local install and development","text":"<p>If you want to develop RL4CO or access the latest builds, we recommend you to install it locally with <code>pip</code> in editable mode:</p> <pre><code>git clone https://github.com/ai4co/rl4co &amp;&amp; cd rl4co\npip install -e .\n</code></pre> <p>Note: <code>conda</code> is also a good candidate for hassle-free installation of PyTorch: check out the PyTorch website for more details.</p> <p>To get started, we recommend checking out our quickstart notebook or the minimalistic example.</p>"},{"location":"examples/","title":"\ud83e\udde9 Examples and Tutorials","text":"<p>This is a collection of examples and tutorials for using the RL4CO library.</p> <p>The root directory is made of quickstarts and contains the following:</p>"},{"location":"examples/#quickstarts","title":"\u26a1\ufe0f Quickstarts","text":"<p>This is the root directory of the examples. The following quickstarts are available:</p> <ul> <li><code>1-quickstart.ipynb</code>: here we train a model on a simple environment - it takes less than 2 minutes!</li> <li><code>2-full-training.ipynb</code>: similar to the previous notebooks but with a more interesting environment, with checkpointing, logging, and callbacks.<p>- <code>2b-train-simple.py</code>: here we show a simple script that can be called with <code>python 2b-train-simple.py</code>. This is simplified and does not use Hydra - for those who prefer a simpler setup. Note that we also made a Hydra tutorial here. - <code>3-creating-new-env-model.ipynb</code>: here we show how to extend RL4CO to solve new problems and create new models from zero to hero!</p> </li> </ul>"},{"location":"examples/#folders-index","title":"\ud83d\udcc1 Folders Index","text":""},{"location":"examples/#modeling","title":"Modeling","text":"<p>Under the <code>modeling/</code> directory, here are some additional examples for modeling and inference.</p>"},{"location":"examples/#datasets","title":"Datasets","text":"<p>Under the <code>datasets/</code> directory, here are some additional examples for using your custom data to train/evaluate your models</p>"},{"location":"examples/#advanced","title":"Advanced","text":"<p>Under the <code>advanced/</code> directory, here are some additional examples for advanced topics.</p>"},{"location":"examples/1-quickstart/","title":"RL4CO Quickstart Notebook","text":"<p>In this notebook we will train the AttentionModel (AM) on the TSP environment for 20 nodes. On a GPU, this should less than 2 minutes!  \ud83d\ude80</p> <p></p> In\u00a0[6]: Copied! <pre>## Uncomment the following line to install the package from PyPI\n## You may need to restart the runtime in Colab after this\n## Remember to choose a GPU runtime for faster training!\n\n# !pip install rl4co\n</pre> ## Uncomment the following line to install the package from PyPI ## You may need to restart the runtime in Colab after this ## Remember to choose a GPU runtime for faster training!  # !pip install rl4co In\u00a0[7]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport torch\n\nfrom rl4co.envs import TSPEnv\nfrom rl4co.models import AttentionModelPolicy, REINFORCE\nfrom rl4co.utils.trainer import RL4COTrainer\n</pre> %load_ext autoreload %autoreload 2  import torch  from rl4co.envs import TSPEnv from rl4co.models import AttentionModelPolicy, REINFORCE from rl4co.utils.trainer import RL4COTrainer <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[8]: Copied! <pre># RL4CO env based on TorchRL\nenv = TSPEnv(generator_params={'num_loc': 50})\n\n# Policy: neural network, in this case with encoder-decoder architecture\npolicy = AttentionModelPolicy(env_name=env.name, \n                              embed_dim=128,\n                              num_encoder_layers=3,\n                              num_heads=8,\n                            )\n\n# RL Model: REINFORCE and greedy rollout baseline\nmodel = REINFORCE(env, \n                    policy,\n                    baseline=\"rollout\",\n                    batch_size=512,\n                    train_data_size=100_000,\n                    val_data_size=10_000,\n                    optimizer_kwargs={\"lr\": 1e-4},\n                    )\n</pre> # RL4CO env based on TorchRL env = TSPEnv(generator_params={'num_loc': 50})  # Policy: neural network, in this case with encoder-decoder architecture policy = AttentionModelPolicy(env_name=env.name,                                embed_dim=128,                               num_encoder_layers=3,                               num_heads=8,                             )  # RL Model: REINFORCE and greedy rollout baseline model = REINFORCE(env,                      policy,                     baseline=\"rollout\",                     batch_size=512,                     train_data_size=100_000,                     val_data_size=10_000,                     optimizer_kwargs={\"lr\": 1e-4},                     )  In\u00a0[9]: Copied! <pre># Greedy rollouts over untrained policy\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntd_init = env.reset(batch_size=[3]).to(device)\npolicy = policy.to(device)\nout = policy(td_init.clone(), phase=\"test\", decode_type=\"greedy\", return_actions=True)\nactions_untrained = out['actions'].cpu().detach()\nrewards_untrained = out['reward'].cpu().detach()\n\nfor i in range(3):\n    print(f\"Problem {i+1} | Cost: {-rewards_untrained[i]:.3f}\")\n    env.render(td_init[i], actions_untrained[i])\n</pre> # Greedy rollouts over untrained policy device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") td_init = env.reset(batch_size=[3]).to(device) policy = policy.to(device) out = policy(td_init.clone(), phase=\"test\", decode_type=\"greedy\", return_actions=True) actions_untrained = out['actions'].cpu().detach() rewards_untrained = out['reward'].cpu().detach()  for i in range(3):     print(f\"Problem {i+1} | Cost: {-rewards_untrained[i]:.3f}\")     env.render(td_init[i], actions_untrained[i]) <pre>Problem 1 | Cost: 10.648\nProblem 2 | Cost: 9.375\nProblem 3 | Cost: 11.713\n</pre> In\u00a0[10]: Copied! <pre>trainer = RL4COTrainer(\n    max_epochs=3,\n    accelerator=\"gpu\",\n    devices=1,\n    logger=None,\n)\n</pre> trainer = RL4COTrainer(     max_epochs=3,     accelerator=\"gpu\",     devices=1,     logger=None, ) <pre>Using 16bit Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n</pre> In\u00a0[11]: Copied! <pre>trainer.fit(model)\n</pre> trainer.fit(model) <pre>val_file not set. Generating dataset instead\ntest_file not set. Generating dataset instead\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n\n  | Name     | Type                 | Params\n--------------------------------------------------\n0 | env      | TSPEnv               | 0     \n1 | policy   | AttentionModelPolicy | 710 K \n2 | baseline | WarmupBaseline       | 710 K \n--------------------------------------------------\n1.4 M     Trainable params\n0         Non-trainable params\n1.4 M     Total params\n5.681     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n</pre> <pre>Training: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=3` reached.\n</pre> In\u00a0[12]: Copied! <pre># Greedy rollouts over trained model (same states as previous plot)\npolicy = model.policy.to(device)\nout = policy(td_init.clone(), phase=\"test\", decode_type=\"greedy\", return_actions=True)\nactions_trained = out['actions'].cpu().detach()\n\n# Plotting\nimport matplotlib.pyplot as plt\nfor i, td in enumerate(td_init):\n    fig, axs = plt.subplots(1,2, figsize=(11,5))\n    env.render(td, actions_untrained[i], ax=axs[0]) \n    env.render(td, actions_trained[i], ax=axs[1])\n    axs[0].set_title(f\"Untrained | Cost = {-rewards_untrained[i].item():.3f}\")\n    axs[1].set_title(r\"Trained $\\pi_\\theta$\" + f\"| Cost = {-out['reward'][i].item():.3f}\")\n</pre> # Greedy rollouts over trained model (same states as previous plot) policy = model.policy.to(device) out = policy(td_init.clone(), phase=\"test\", decode_type=\"greedy\", return_actions=True) actions_trained = out['actions'].cpu().detach()  # Plotting import matplotlib.pyplot as plt for i, td in enumerate(td_init):     fig, axs = plt.subplots(1,2, figsize=(11,5))     env.render(td, actions_untrained[i], ax=axs[0])      env.render(td, actions_trained[i], ax=axs[1])     axs[0].set_title(f\"Untrained | Cost = {-rewards_untrained[i].item():.3f}\")     axs[1].set_title(r\"Trained $\\pi_\\theta$\" + f\"| Cost = {-out['reward'][i].item():.3f}\") <p>We can see that even after just 3 epochs, our trained AM is able to find much better solutions than the random policy! \ud83c\udf89</p> In\u00a0[13]: Copied! <pre># Optionally, save the checkpoint for later use (e.g. in tutorials/4-search-methods.ipynb)\ntrainer.save_checkpoint(\"tsp-quickstart.ckpt\")\n</pre> # Optionally, save the checkpoint for later use (e.g. in tutorials/4-search-methods.ipynb) trainer.save_checkpoint(\"tsp-quickstart.ckpt\")"},{"location":"examples/1-quickstart/#rl4co-quickstart-notebook","title":"RL4CO Quickstart Notebook\u00b6","text":"<p>Documentation |  Getting Started | Usage | Contributing | Paper | Citation</p>"},{"location":"examples/1-quickstart/#installation","title":"Installation\u00b6","text":""},{"location":"examples/1-quickstart/#imports","title":"Imports\u00b6","text":""},{"location":"examples/1-quickstart/#environment-policy-and-model","title":"Environment, Policy and Model\u00b6","text":"<p>Full documentation of:</p> <ul> <li>Base environment class here</li> <li>Base policy class here</li> <li>Base model class here</li> </ul>"},{"location":"examples/1-quickstart/#test-greedy-rollout-with-untrained-model-and-plot","title":"Test greedy rollout with untrained model and plot\u00b6","text":""},{"location":"examples/1-quickstart/#trainer","title":"Trainer\u00b6","text":"<p>The RL4CO trainer is a wrapper around PyTorch Lightning's <code>Trainer</code> class which adds some functionality and more efficient defaults</p>"},{"location":"examples/1-quickstart/#fit-the-model","title":"Fit the model\u00b6","text":""},{"location":"examples/1-quickstart/#testing","title":"Testing\u00b6","text":""},{"location":"examples/2-full-training/","title":"Training: Checkpoints, Logging, and Callbacks","text":"<p>In this notebook we will cover a quickstart training of the Split Delivery Vehicle Routing Problem (SDVRP), with some additional comments along the way. The SDVRP is a variant of the VRP where a vehicle can deliver a part of the demand of a customer and return later to deliver the rest of the demand.</p> <p></p> In\u00a0[1]: Copied! <pre># !pip install rl4co\n\n## NOTE: to install latest version from Github (may be unstable) install from source instead:\n# !pip install git+https://github.com/ai4co/rl4co.git\n</pre> # !pip install rl4co  ## NOTE: to install latest version from Github (may be unstable) install from source instead: # !pip install git+https://github.com/ai4co/rl4co.git In\u00a0[2]: Copied! <pre>import torch\nfrom lightning.pytorch.callbacks import ModelCheckpoint, RichModelSummary\n\nfrom rl4co.envs import SDVRPEnv\nfrom rl4co.models.zoo import AttentionModel\nfrom rl4co.utils.trainer import RL4COTrainer\n</pre> import torch from lightning.pytorch.callbacks import ModelCheckpoint, RichModelSummary  from rl4co.envs import SDVRPEnv from rl4co.models.zoo import AttentionModel from rl4co.utils.trainer import RL4COTrainer In\u00a0[3]: Copied! <pre># RL4CO env based on TorchRL\nenv = SDVRPEnv(generator_params=dict(num_loc=20))\n\n# Model: default is AM with REINFORCE and greedy rollout baseline\nmodel = AttentionModel(env,\n                       baseline='rollout',\n                       train_data_size=100_000, # really small size for demo\n                       val_data_size=10_000)\n</pre> # RL4CO env based on TorchRL env = SDVRPEnv(generator_params=dict(num_loc=20))  # Model: default is AM with REINFORCE and greedy rollout baseline model = AttentionModel(env,                        baseline='rollout',                        train_data_size=100_000, # really small size for demo                        val_data_size=10_000) <pre>/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n</pre> In\u00a0[4]: Copied! <pre># Greedy rollouts over untrained policy\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntd_init = env.reset(batch_size=[3]).to(device)\npolicy = model.policy.to(device)\nout = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\n\n# Plotting\nprint(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\")\nfor td, actions in zip(td_init, out['actions'].cpu()):\n    env.render(td, actions)\n</pre> # Greedy rollouts over untrained policy device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") td_init = env.reset(batch_size=[3]).to(device) policy = model.policy.to(device) out = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)  # Plotting print(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\") for td, actions in zip(td_init, out['actions'].cpu()):     env.render(td, actions) <pre>Tour lengths: ['29.45', '14.26', '21.15']\n</pre> In\u00a0[5]: Copied! <pre># Checkpointing callback: save models when validation reward improves\ncheckpoint_callback = ModelCheckpoint(  dirpath=\"checkpoints\", # save to checkpoints/\n                                        filename=\"epoch_{epoch:03d}\",  # save as epoch_XXX.ckpt\n                                        save_top_k=1, # save only the best model\n                                        save_last=True, # save the last model\n                                        monitor=\"val/reward\", # monitor validation reward\n                                        mode=\"max\") # maximize validation reward\n\n# Print model summary\nrich_model_summary = RichModelSummary(max_depth=3)\n\n# Callbacks list\ncallbacks = [checkpoint_callback, rich_model_summary]\n</pre> # Checkpointing callback: save models when validation reward improves checkpoint_callback = ModelCheckpoint(  dirpath=\"checkpoints\", # save to checkpoints/                                         filename=\"epoch_{epoch:03d}\",  # save as epoch_XXX.ckpt                                         save_top_k=1, # save only the best model                                         save_last=True, # save the last model                                         monitor=\"val/reward\", # monitor validation reward                                         mode=\"max\") # maximize validation reward  # Print model summary rich_model_summary = RichModelSummary(max_depth=3)  # Callbacks list callbacks = [checkpoint_callback, rich_model_summary] <p>We make sure we're logged into W&amp;B so that our experiments can be associated with our account. You may comment the below line if you don't want to use it.</p> In\u00a0[6]: Copied! <pre># import wandb\n# wandb.login()\n</pre> # import wandb # wandb.login() In\u00a0[7]: Copied! <pre>## Comment following two lines if you don't want logging\nfrom lightning.pytorch.loggers import WandbLogger\n\nlogger = WandbLogger(project=\"rl4co\", name=\"sdvrp-am\")\n\n\n## Keep below if you don't want logging\n# logger = None\n</pre> ## Comment following two lines if you don't want logging from lightning.pytorch.loggers import WandbLogger  logger = WandbLogger(project=\"rl4co\", name=\"sdvrp-am\")   ## Keep below if you don't want logging # logger = None <p>The Trainer handles the logging, checkpointing and more for you.</p> In\u00a0[8]: Copied! <pre>from rl4co.utils.trainer import RL4COTrainer\n\ntrainer = RL4COTrainer(\n    max_epochs=2,\n    accelerator=\"gpu\",\n    devices=1,\n    logger=logger,\n    callbacks=callbacks,\n)\n</pre> from rl4co.utils.trainer import RL4COTrainer  trainer = RL4COTrainer(     max_epochs=2,     accelerator=\"gpu\",     devices=1,     logger=logger,     callbacks=callbacks, ) <pre>Using 16bit Automatic Mixed Precision (AMP)\nTrainer already configured with model summary callbacks: [&lt;class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'&gt;]. Skipping setting a default `ModelSummary` callback.\nGPU available: True (cuda), used: True\nTrainer already configured with model summary callbacks: [&lt;class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'&gt;]. Skipping setting a default `ModelSummary` callback.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</pre> In\u00a0[9]: Copied! <pre>trainer.fit(model)\n</pre> trainer.fit(model) <pre>Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n</pre> <pre>wandb: Currently logged in as: silab-kaist. Use `wandb login --relogin` to force relogin\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/wandb/sdk/lib/ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n  from IPython.core.display import HTML, display  # type: ignore\n</pre>  wandb version 0.16.6 is available!  To upgrade, please run:  $ pip install wandb --upgrade   Tracking run with wandb version 0.16.5   Run data is saved locally in <code>./wandb/run-20240428_182146-xcgdzio4</code>  Syncing run sdvrp-am to Weights &amp; Biases (docs)   View project at https://wandb.ai/silab-kaist/rl4co   View run at https://wandb.ai/silab-kaist/rl4co/runs/xcgdzio4/workspace <pre>val_file not set. Generating dataset instead\ntest_file not set. Generating dataset instead\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503    \u2503 Name                                   \u2503 Type                  \u2503 Params \u2503\n\u2521\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 0  \u2502 env                                    \u2502 SDVRPEnv              \u2502      0 \u2502\n\u2502 1  \u2502 policy                                 \u2502 AttentionModelPolicy  \u2502  694 K \u2502\n\u2502 2  \u2502 policy.encoder                         \u2502 AttentionModelEncoder \u2502  595 K \u2502\n\u2502 3  \u2502 policy.encoder.init_embedding          \u2502 VRPInitEmbedding      \u2502    896 \u2502\n\u2502 4  \u2502 policy.encoder.net                     \u2502 GraphAttentionNetwork \u2502  594 K \u2502\n\u2502 5  \u2502 policy.decoder                         \u2502 AttentionModelDecoder \u2502 98.8 K \u2502\n\u2502 6  \u2502 policy.decoder.context_embedding       \u2502 VRPContext            \u2502 16.5 K \u2502\n\u2502 7  \u2502 policy.decoder.dynamic_embedding       \u2502 SDVRPDynamicEmbedding \u2502    384 \u2502\n\u2502 8  \u2502 policy.decoder.pointer                 \u2502 PointerAttention      \u2502 16.4 K \u2502\n\u2502 9  \u2502 policy.decoder.project_node_embeddings \u2502 Linear                \u2502 49.2 K \u2502\n\u2502 10 \u2502 policy.decoder.project_fixed_context   \u2502 Linear                \u2502 16.4 K \u2502\n\u2502 11 \u2502 baseline                               \u2502 WarmupBaseline        \u2502  694 K \u2502\n\u2502 12 \u2502 baseline.baseline                      \u2502 RolloutBaseline       \u2502  694 K \u2502\n\u2502 13 \u2502 baseline.baseline.policy               \u2502 AttentionModelPolicy  \u2502  694 K \u2502\n\u2502 14 \u2502 baseline.warmup_baseline               \u2502 ExponentialBaseline   \u2502      0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre>Trainable params: 1.4 M                                                                                            \nNon-trainable params: 0                                                                                            \nTotal params: 1.4 M                                                                                                \nTotal estimated model params size (MB): 5                                                                          \n</pre> <pre>Sanity Checking: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n</pre> <pre>Training: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=2` reached.\n</pre> In\u00a0[10]: Copied! <pre># Greedy rollouts over trained model (same states as previous plot)\npolicy = model.policy.to(device)\nout = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\n\n# Plotting\nprint(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\")\nfor td, actions in zip(td_init, out['actions'].cpu()):\n    env.render(td, actions)\n</pre> # Greedy rollouts over trained model (same states as previous plot) policy = model.policy.to(device) out = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)  # Plotting print(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\") for td, actions in zip(td_init, out['actions'].cpu()):     env.render(td, actions) <pre>Tour lengths: ['9.12', '7.16', '9.55']\n</pre> In\u00a0[11]: Copied! <pre>trainer.test(model)\n</pre> trainer.test(model) <pre>val_file not set. Generating dataset instead\ntest_file not set. Generating dataset instead\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n</pre> <pre>Testing: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502        test/reward        \u2502    -7.363526344299316     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> Out[11]: <pre>[{'test/reward': -7.363526344299316}]</pre> In\u00a0[12]: Copied! <pre># Test generalization to 50 nodes (not going to be great due to few epochs, but hey)\nenv = SDVRPEnv(generator_params=dict(num_loc=50))\n\n# Generate data (100) and set as test dataset\nnew_dataset = env.dataset(50)\ndataloader = model._dataloader(new_dataset, batch_size=100)\n</pre> # Test generalization to 50 nodes (not going to be great due to few epochs, but hey) env = SDVRPEnv(generator_params=dict(num_loc=50))  # Generate data (100) and set as test dataset new_dataset = env.dataset(50) dataloader = model._dataloader(new_dataset, batch_size=100) In\u00a0[15]: Copied! <pre># Greedy rollouts over trained policy (same states as previous plot, with 20 nodes)\ninit_states = next(iter(dataloader))[:3]\ntd_init_generalization = env.reset(init_states).to(device)\n\npolicy = model.policy.to(device)\nout = policy(td_init_generalization.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\n\n# Plotting\nprint(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\")\nfor td, actions in zip(td_init_generalization, out['actions'].cpu()):\n    env.render(td, actions)\n</pre> # Greedy rollouts over trained policy (same states as previous plot, with 20 nodes) init_states = next(iter(dataloader))[:3] td_init_generalization = env.reset(init_states).to(device)  policy = model.policy.to(device) out = policy(td_init_generalization.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)  # Plotting print(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\") for td, actions in zip(td_init_generalization, out['actions'].cpu()):     env.render(td, actions) <pre>Tour lengths: ['11.84', '12.49', '12.20']\n</pre> In\u00a0[16]: Copied! <pre># Environment, Model, and Lightning Module (reinstantiate from scratch)\nmodel = AttentionModel(env,\n                       baseline=\"rollout\",\n                       train_data_size=100_000,\n                       test_data_size=10_000,\n                       optimizer_kwargs={'lr': 1e-4}\n                       )\n\n# Note that by default, Lightning will call checkpoints from newer runs with \"-v{version}\" suffix\n# unless you specify the checkpoint path explicitly\nnew_model_checkpoint = AttentionModel.load_from_checkpoint(\"checkpoints/last.ckpt\", strict=False)\n</pre> # Environment, Model, and Lightning Module (reinstantiate from scratch) model = AttentionModel(env,                        baseline=\"rollout\",                        train_data_size=100_000,                        test_data_size=10_000,                        optimizer_kwargs={'lr': 1e-4}                        )  # Note that by default, Lightning will call checkpoints from newer runs with \"-v{version}\" suffix # unless you specify the checkpoint path explicitly new_model_checkpoint = AttentionModel.load_from_checkpoint(\"checkpoints/last.ckpt\", strict=False) <pre>/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['baseline.baseline.policy.encoder.init_embedding.init_embed.weight', 'baseline.baseline.policy.encoder.init_embedding.init_embed.bias', 'baseline.baseline.policy.encoder.init_embedding.init_embed_depot.weight', 'baseline.baseline.policy.encoder.init_embedding.init_embed_depot.bias', 'baseline.baseline.policy.encoder.net.layers.0.0.module.Wqkv.weight', 'baseline.baseline.policy.encoder.net.layers.0.0.module.Wqkv.bias', 'baseline.baseline.policy.encoder.net.layers.0.0.module.out_proj.weight', 'baseline.baseline.policy.encoder.net.layers.0.0.module.out_proj.bias', 'baseline.baseline.policy.encoder.net.layers.0.1.normalizer.weight', 'baseline.baseline.policy.encoder.net.layers.0.1.normalizer.bias', 'baseline.baseline.policy.encoder.net.layers.0.1.normalizer.running_mean', 'baseline.baseline.policy.encoder.net.layers.0.1.normalizer.running_var', 'baseline.baseline.policy.encoder.net.layers.0.1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.net.layers.0.2.module.0.weight', 'baseline.baseline.policy.encoder.net.layers.0.2.module.0.bias', 'baseline.baseline.policy.encoder.net.layers.0.2.module.2.weight', 'baseline.baseline.policy.encoder.net.layers.0.2.module.2.bias', 'baseline.baseline.policy.encoder.net.layers.0.3.normalizer.weight', 'baseline.baseline.policy.encoder.net.layers.0.3.normalizer.bias', 'baseline.baseline.policy.encoder.net.layers.0.3.normalizer.running_mean', 'baseline.baseline.policy.encoder.net.layers.0.3.normalizer.running_var', 'baseline.baseline.policy.encoder.net.layers.0.3.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.net.layers.1.0.module.Wqkv.weight', 'baseline.baseline.policy.encoder.net.layers.1.0.module.Wqkv.bias', 'baseline.baseline.policy.encoder.net.layers.1.0.module.out_proj.weight', 'baseline.baseline.policy.encoder.net.layers.1.0.module.out_proj.bias', 'baseline.baseline.policy.encoder.net.layers.1.1.normalizer.weight', 'baseline.baseline.policy.encoder.net.layers.1.1.normalizer.bias', 'baseline.baseline.policy.encoder.net.layers.1.1.normalizer.running_mean', 'baseline.baseline.policy.encoder.net.layers.1.1.normalizer.running_var', 'baseline.baseline.policy.encoder.net.layers.1.1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.net.layers.1.2.module.0.weight', 'baseline.baseline.policy.encoder.net.layers.1.2.module.0.bias', 'baseline.baseline.policy.encoder.net.layers.1.2.module.2.weight', 'baseline.baseline.policy.encoder.net.layers.1.2.module.2.bias', 'baseline.baseline.policy.encoder.net.layers.1.3.normalizer.weight', 'baseline.baseline.policy.encoder.net.layers.1.3.normalizer.bias', 'baseline.baseline.policy.encoder.net.layers.1.3.normalizer.running_mean', 'baseline.baseline.policy.encoder.net.layers.1.3.normalizer.running_var', 'baseline.baseline.policy.encoder.net.layers.1.3.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.net.layers.2.0.module.Wqkv.weight', 'baseline.baseline.policy.encoder.net.layers.2.0.module.Wqkv.bias', 'baseline.baseline.policy.encoder.net.layers.2.0.module.out_proj.weight', 'baseline.baseline.policy.encoder.net.layers.2.0.module.out_proj.bias', 'baseline.baseline.policy.encoder.net.layers.2.1.normalizer.weight', 'baseline.baseline.policy.encoder.net.layers.2.1.normalizer.bias', 'baseline.baseline.policy.encoder.net.layers.2.1.normalizer.running_mean', 'baseline.baseline.policy.encoder.net.layers.2.1.normalizer.running_var', 'baseline.baseline.policy.encoder.net.layers.2.1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.net.layers.2.2.module.0.weight', 'baseline.baseline.policy.encoder.net.layers.2.2.module.0.bias', 'baseline.baseline.policy.encoder.net.layers.2.2.module.2.weight', 'baseline.baseline.policy.encoder.net.layers.2.2.module.2.bias', 'baseline.baseline.policy.encoder.net.layers.2.3.normalizer.weight', 'baseline.baseline.policy.encoder.net.layers.2.3.normalizer.bias', 'baseline.baseline.policy.encoder.net.layers.2.3.normalizer.running_mean', 'baseline.baseline.policy.encoder.net.layers.2.3.normalizer.running_var', 'baseline.baseline.policy.encoder.net.layers.2.3.normalizer.num_batches_tracked', 'baseline.baseline.policy.decoder.context_embedding.project_context.weight', 'baseline.baseline.policy.decoder.dynamic_embedding.projection.weight', 'baseline.baseline.policy.decoder.pointer.project_out.weight', 'baseline.baseline.policy.decoder.project_node_embeddings.weight', 'baseline.baseline.policy.decoder.project_fixed_context.weight']\nval_file not set. Generating dataset instead\ntest_file not set. Generating dataset instead\n</pre> <p>Now we can load both the model and environment from the checkpoint!</p> In\u00a0[17]: Copied! <pre># Greedy rollouts over trained model (same states as previous plot, with 20 nodes)\npolicy_new = new_model_checkpoint.policy.to(device)\nenv = new_model_checkpoint.env.to(device)\n\nout = policy_new(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\n\n# Plotting\nprint(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\")\nfor td, actions in zip(td_init, out['actions'].cpu()):\n    env.render(td, actions)\n</pre> # Greedy rollouts over trained model (same states as previous plot, with 20 nodes) policy_new = new_model_checkpoint.policy.to(device) env = new_model_checkpoint.env.to(device)  out = policy_new(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)  # Plotting print(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\") for td, actions in zip(td_init, out['actions'].cpu()):     env.render(td, actions) <pre>Tour lengths: ['9.12', '7.16', '9.55']\n</pre>"},{"location":"examples/2-full-training/#training-checkpoints-logging-and-callbacks","title":"Training: Checkpoints, Logging, and Callbacks\u00b6","text":""},{"location":"examples/2-full-training/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install the package from PyPI. Remember to choose a GPU runtime for faster training!</p> <p>Note: You may need to restart the runtime in Colab after this</p>"},{"location":"examples/2-full-training/#imports","title":"Imports\u00b6","text":""},{"location":"examples/2-full-training/#main-setup","title":"Main Setup\u00b6","text":""},{"location":"examples/2-full-training/#environment-model-and-litmodule","title":"Environment, Model and LitModule\u00b6","text":""},{"location":"examples/2-full-training/#test-greedy-rollout-with-untrained-model-and-plot","title":"Test greedy rollout with untrained model and plot\u00b6","text":""},{"location":"examples/2-full-training/#training","title":"Training\u00b6","text":""},{"location":"examples/2-full-training/#callbacks","title":"Callbacks\u00b6","text":"<p>Here we set up a checkpoint callback to save the best model and another callback for demonstration (nice progress bar). You may check other callbacks here</p>"},{"location":"examples/2-full-training/#logging","title":"Logging\u00b6","text":"<p>Here we will use Wandb. You may comment below lines if you don't want to use it. You may check other loggers here</p>"},{"location":"examples/2-full-training/#trainer","title":"Trainer\u00b6","text":"<p>The RL4CO trainer is a wrapper around PyTorch Lightning's <code>Trainer</code> class which adds some functionality and more efficient defaults</p>"},{"location":"examples/2-full-training/#fit-the-model","title":"Fit the model\u00b6","text":""},{"location":"examples/2-full-training/#testing","title":"Testing\u00b6","text":""},{"location":"examples/2-full-training/#plotting","title":"Plotting\u00b6","text":"<p>Here we plot the solution (greedy rollout) of the trained policy to the initial problem</p>"},{"location":"examples/2-full-training/#test-function","title":"Test function\u00b6","text":"<p>By default, the dataset is generated or loaded by the environment. You may load a dataset by setting <code>test_file</code> during the env config:</p> <pre>env = SDVRPEnv(\n    ...\n    test_file=\"path/to/test/file\"\n)\n</pre> <p>In this case, we test directly on the generated test dataset</p>"},{"location":"examples/2-full-training/#test-generalization-to-new-dataset","title":"Test generalization to new dataset\u00b6","text":"<p>Here we can load a new dataset (with 50 nodes) and test the trained model on it</p>"},{"location":"examples/2-full-training/#plotting-generalization","title":"Plotting generalization\u00b6","text":""},{"location":"examples/2-full-training/#loading-model","title":"Loading model\u00b6","text":"<p>Thanks to PyTorch Lightning, we can easily save and load a model to and from a checkpoint! This is declared in the <code>Trainer</code> using the model checkpoint callback. For example, we can load the last model via the <code>last.ckpt</code> file located in the folder we specified in the <code>Trainer</code>.</p>"},{"location":"examples/2-full-training/#checkpointing","title":"Checkpointing\u00b6","text":""},{"location":"examples/2-full-training/#additional-resources","title":"Additional resources\u00b6","text":"<p>Documentation |  Getting Started | Usage | Contributing | Paper | Citation</p> <p>Have feedback about this notebook? Feel free to contribute by either opening an issue or a pull request! ;)</p>"},{"location":"examples/3-creating-new-env-model/","title":"New Environment: Creating and Modeling","text":"<p>In this notebook, we will show how to extend RL4CO to solve new problems from zero to hero! \ud83d\ude80</p> <p></p> In\u00a0[1]: Copied! <pre>## Uncomment the following line to install the package from PyPI\n## You may need to restart the runtime in Colab after this\n## Remember to choose a GPU runtime for faster training!\n\n# !pip install rl4co\n</pre> ## Uncomment the following line to install the package from PyPI ## You may need to restart the runtime in Colab after this ## Remember to choose a GPU runtime for faster training!  # !pip install rl4co In\u00a0[16]: Copied! <pre>from typing import Optional\nimport torch\nimport torch.nn as nn\n\nfrom tensordict.tensordict import TensorDict\nfrom torchrl.data import (\n    BoundedTensorSpec,\n    CompositeSpec,\n    UnboundedContinuousTensorSpec,\n    UnboundedDiscreteTensorSpec,\n)\n\nfrom rl4co.utils.decoding import rollout, random_policy\nfrom rl4co.envs.common import RL4COEnvBase, Generator, get_sampler\nfrom rl4co.models.zoo import AttentionModel, AttentionModelPolicy\nfrom rl4co.utils.ops import gather_by_index, get_tour_length\nfrom rl4co.utils.trainer import RL4COTrainer\n</pre> from typing import Optional import torch import torch.nn as nn  from tensordict.tensordict import TensorDict from torchrl.data import (     BoundedTensorSpec,     CompositeSpec,     UnboundedContinuousTensorSpec,     UnboundedDiscreteTensorSpec, )  from rl4co.utils.decoding import rollout, random_policy from rl4co.envs.common import RL4COEnvBase, Generator, get_sampler from rl4co.models.zoo import AttentionModel, AttentionModelPolicy from rl4co.utils.ops import gather_by_index, get_tour_length from rl4co.utils.trainer import RL4COTrainer <p>We will base environment creation on the <code>RL4COEnvBase</code> class, which is based on TorchRL. More information in documentation!</p> In\u00a0[2]: Copied! <pre>def _reset(self, td: Optional[TensorDict] = None, batch_size=None) -&gt; TensorDict:\n    # Initialize locations\n    init_locs = td[\"locs\"] if td is not None else None\n    if batch_size is None:\n        batch_size = self.batch_size if init_locs is None else init_locs.shape[:-2]\n    device = init_locs.device if init_locs is not None else self.device\n    self.to(device)\n    if init_locs is None:\n        init_locs = self.generate_data(batch_size=batch_size).to(device)[\"locs\"]\n    batch_size = [batch_size] if isinstance(batch_size, int) else batch_size\n\n    # We do not enforce loading from self for flexibility\n    num_loc = init_locs.shape[-2]\n\n    # Other variables\n    current_node = torch.zeros((batch_size), dtype=torch.int64, device=device)\n    available = torch.ones(\n        (*batch_size, num_loc), dtype=torch.bool, device=device\n    )  # 1 means not visited, i.e. action is allowed\n    i = torch.zeros((*batch_size, 1), dtype=torch.int64, device=device)\n\n    return TensorDict(\n        {\n            \"locs\": init_locs,\n            \"first_node\": current_node,\n            \"current_node\": current_node,\n            \"i\": i,\n            \"action_mask\": available,\n            \"reward\": torch.zeros((*batch_size, 1), dtype=torch.float32),\n        },\n        batch_size=batch_size,\n    )\n</pre> def _reset(self, td: Optional[TensorDict] = None, batch_size=None) -&gt; TensorDict:     # Initialize locations     init_locs = td[\"locs\"] if td is not None else None     if batch_size is None:         batch_size = self.batch_size if init_locs is None else init_locs.shape[:-2]     device = init_locs.device if init_locs is not None else self.device     self.to(device)     if init_locs is None:         init_locs = self.generate_data(batch_size=batch_size).to(device)[\"locs\"]     batch_size = [batch_size] if isinstance(batch_size, int) else batch_size      # We do not enforce loading from self for flexibility     num_loc = init_locs.shape[-2]      # Other variables     current_node = torch.zeros((batch_size), dtype=torch.int64, device=device)     available = torch.ones(         (*batch_size, num_loc), dtype=torch.bool, device=device     )  # 1 means not visited, i.e. action is allowed     i = torch.zeros((*batch_size, 1), dtype=torch.int64, device=device)      return TensorDict(         {             \"locs\": init_locs,             \"first_node\": current_node,             \"current_node\": current_node,             \"i\": i,             \"action_mask\": available,             \"reward\": torch.zeros((*batch_size, 1), dtype=torch.float32),         },         batch_size=batch_size,     ) In\u00a0[3]: Copied! <pre>def _step(self, td: TensorDict) -&gt; TensorDict:\n    current_node = td[\"action\"]\n    first_node = current_node if td[\"i\"].all() == 0 else td[\"first_node\"]\n\n    # Set not visited to 0 (i.e., we visited the node)\n    # Note: we may also use a separate function for obtaining the mask for more flexibility\n    available = td[\"action_mask\"].scatter(\n        -1, current_node.unsqueeze(-1).expand_as(td[\"action_mask\"]), 0\n    )\n\n    # We are done there are no unvisited locations\n    done = torch.sum(available, dim=-1) == 0\n\n    # The reward is calculated outside via get_reward for efficiency, so we set it to 0 here\n    reward = torch.zeros_like(done)\n\n    td.update(\n        {\n            \"first_node\": first_node,\n            \"current_node\": current_node,\n            \"i\": td[\"i\"] + 1,\n            \"action_mask\": available,\n            \"reward\": reward,\n            \"done\": done,\n        },\n    )\n    return td\n</pre> def _step(self, td: TensorDict) -&gt; TensorDict:     current_node = td[\"action\"]     first_node = current_node if td[\"i\"].all() == 0 else td[\"first_node\"]      # Set not visited to 0 (i.e., we visited the node)     # Note: we may also use a separate function for obtaining the mask for more flexibility     available = td[\"action_mask\"].scatter(         -1, current_node.unsqueeze(-1).expand_as(td[\"action_mask\"]), 0     )      # We are done there are no unvisited locations     done = torch.sum(available, dim=-1) == 0      # The reward is calculated outside via get_reward for efficiency, so we set it to 0 here     reward = torch.zeros_like(done)      td.update(         {             \"first_node\": first_node,             \"current_node\": current_node,             \"i\": td[\"i\"] + 1,             \"action_mask\": available,             \"reward\": reward,             \"done\": done,         },     )     return td In\u00a0[4]: Copied! <pre>def get_action_mask(self, td: TensorDict) -&gt; TensorDict:\n    # Here: your logic \n    return td[\"action_mask\"]\n</pre> def get_action_mask(self, td: TensorDict) -&gt; TensorDict:     # Here: your logic      return td[\"action_mask\"] In\u00a0[5]: Copied! <pre>def check_solution_validity(self, td: TensorDict, actions: torch.Tensor):\n    \"\"\"Check that solution is valid: nodes are visited exactly once\"\"\"\n    assert (\n        torch.arange(actions.size(1), out=actions.data.new())\n        .view(1, -1)\n        .expand_as(actions)\n        == actions.data.sort(1)[0]\n    ).all(), \"Invalid tour\"\n</pre> def check_solution_validity(self, td: TensorDict, actions: torch.Tensor):     \"\"\"Check that solution is valid: nodes are visited exactly once\"\"\"     assert (         torch.arange(actions.size(1), out=actions.data.new())         .view(1, -1)         .expand_as(actions)         == actions.data.sort(1)[0]     ).all(), \"Invalid tour\" In\u00a0[26]: Copied! <pre>def _get_reward(self, td, actions) -&gt; TensorDict:\n    # Sanity check if enabled\n    if self.check_solution:\n        self.check_solution_validity(td, actions)\n\n    # Gather locations in order of tour and return distance between them (i.e., -reward)\n    locs_ordered = gather_by_index(td[\"locs\"], actions)\n    return -get_tour_length(locs_ordered)\n</pre> def _get_reward(self, td, actions) -&gt; TensorDict:     # Sanity check if enabled     if self.check_solution:         self.check_solution_validity(td, actions)      # Gather locations in order of tour and return distance between them (i.e., -reward)     locs_ordered = gather_by_index(td[\"locs\"], actions)     return -get_tour_length(locs_ordered) In\u00a0[21]: Copied! <pre>def _make_spec(self, generator):\n    \"\"\"Make the observation and action specs from the parameters\"\"\"\n    self.observation_spec = CompositeSpec(\n        locs=BoundedTensorSpec(\n            low=self.generator.min_loc,\n            high=self.generator.max_loc,\n            shape=(self.generator.num_loc, 2),\n            dtype=torch.float32,\n        ),\n        first_node=UnboundedDiscreteTensorSpec(\n            shape=(1),\n            dtype=torch.int64,\n        ),\n        current_node=UnboundedDiscreteTensorSpec(\n            shape=(1),\n            dtype=torch.int64,\n        ),\n        i=UnboundedDiscreteTensorSpec(\n            shape=(1),\n            dtype=torch.int64,\n        ),\n        action_mask=UnboundedDiscreteTensorSpec(\n            shape=(self.generator.num_loc),\n            dtype=torch.bool,\n        ),\n        shape=(),\n    )\n    self.action_spec = BoundedTensorSpec(\n        shape=(1,),\n        dtype=torch.int64,\n        low=0,\n        high=self.generator.num_loc,\n    )\n    self.reward_spec = UnboundedContinuousTensorSpec(shape=(1,))\n    self.done_spec = UnboundedDiscreteTensorSpec(shape=(1,), dtype=torch.bool)\n</pre> def _make_spec(self, generator):     \"\"\"Make the observation and action specs from the parameters\"\"\"     self.observation_spec = CompositeSpec(         locs=BoundedTensorSpec(             low=self.generator.min_loc,             high=self.generator.max_loc,             shape=(self.generator.num_loc, 2),             dtype=torch.float32,         ),         first_node=UnboundedDiscreteTensorSpec(             shape=(1),             dtype=torch.int64,         ),         current_node=UnboundedDiscreteTensorSpec(             shape=(1),             dtype=torch.int64,         ),         i=UnboundedDiscreteTensorSpec(             shape=(1),             dtype=torch.int64,         ),         action_mask=UnboundedDiscreteTensorSpec(             shape=(self.generator.num_loc),             dtype=torch.bool,         ),         shape=(),     )     self.action_spec = BoundedTensorSpec(         shape=(1,),         dtype=torch.int64,         low=0,         high=self.generator.num_loc,     )     self.reward_spec = UnboundedContinuousTensorSpec(shape=(1,))     self.done_spec = UnboundedDiscreteTensorSpec(shape=(1,), dtype=torch.bool) In\u00a0[22]: Copied! <pre>class TSPGenerator(Generator):\n    def __init__(\n        self,\n        num_loc: int = 20,\n        min_loc: float = 0.0,\n        max_loc: float = 1.0,\n    ):\n        self.num_loc = num_loc\n        self.min_loc = min_loc\n        self.max_loc = max_loc\n        self.loc_sampler = torch.distributions.Uniform(\n            low=min_loc, high=max_loc\n        )\n\n    def _generate(self, batch_size) -&gt; TensorDict:\n        # Sample locations\n        locs = self.loc_sampler.sample((*batch_size, self.num_loc, 2))\n        return TensorDict({\"locs\": locs}, batch_size=batch_size)\n    \n# Test generator\ngenerator = TSPGenerator(num_loc=20)\nlocs = generator(32)\nprint(locs[\"locs\"].shape)\n</pre> class TSPGenerator(Generator):     def __init__(         self,         num_loc: int = 20,         min_loc: float = 0.0,         max_loc: float = 1.0,     ):         self.num_loc = num_loc         self.min_loc = min_loc         self.max_loc = max_loc         self.loc_sampler = torch.distributions.Uniform(             low=min_loc, high=max_loc         )      def _generate(self, batch_size) -&gt; TensorDict:         # Sample locations         locs = self.loc_sampler.sample((*batch_size, self.num_loc, 2))         return TensorDict({\"locs\": locs}, batch_size=batch_size)      # Test generator generator = TSPGenerator(num_loc=20) locs = generator(32) print(locs[\"locs\"].shape) <pre>torch.Size([32, 20, 2])\n</pre> In\u00a0[23]: Copied! <pre>def render(self, td, actions=None, ax=None):\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    if ax is None:\n        # Create a plot of the nodes\n        _, ax = plt.subplots()\n\n    td = td.detach().cpu()\n\n    if actions is None:\n        actions = td.get(\"action\", None)\n    # if batch_size greater than 0 , we need to select the first batch element\n    if td.batch_size != torch.Size([]):\n        td = td[0]\n        actions = actions[0]\n\n    locs = td[\"locs\"]\n\n    # gather locs in order of action if available\n    if actions is None:\n        print(\"No action in TensorDict, rendering unsorted locs\")\n    else:\n        actions = actions.detach().cpu()\n        locs = gather_by_index(locs, actions, dim=0)\n\n    # Cat the first node to the end to complete the tour\n    locs = torch.cat((locs, locs[0:1]))\n    x, y = locs[:, 0], locs[:, 1]\n\n    # Plot the visited nodes\n    ax.scatter(x, y, color=\"tab:blue\")\n\n    # Add arrows between visited nodes as a quiver plot\n    dx, dy = np.diff(x), np.diff(y)\n    ax.quiver(\n        x[:-1], y[:-1], dx, dy, scale_units=\"xy\", angles=\"xy\", scale=1, color=\"k\"\n    )\n\n    # Setup limits and show\n    ax.set_xlim(-0.05, 1.05)\n    ax.set_ylim(-0.05, 1.05)\n</pre> def render(self, td, actions=None, ax=None):     import matplotlib.pyplot as plt     import numpy as np      if ax is None:         # Create a plot of the nodes         _, ax = plt.subplots()      td = td.detach().cpu()      if actions is None:         actions = td.get(\"action\", None)     # if batch_size greater than 0 , we need to select the first batch element     if td.batch_size != torch.Size([]):         td = td[0]         actions = actions[0]      locs = td[\"locs\"]      # gather locs in order of action if available     if actions is None:         print(\"No action in TensorDict, rendering unsorted locs\")     else:         actions = actions.detach().cpu()         locs = gather_by_index(locs, actions, dim=0)      # Cat the first node to the end to complete the tour     locs = torch.cat((locs, locs[0:1]))     x, y = locs[:, 0], locs[:, 1]      # Plot the visited nodes     ax.scatter(x, y, color=\"tab:blue\")      # Add arrows between visited nodes as a quiver plot     dx, dy = np.diff(x), np.diff(y)     ax.quiver(         x[:-1], y[:-1], dx, dy, scale_units=\"xy\", angles=\"xy\", scale=1, color=\"k\"     )      # Setup limits and show     ax.set_xlim(-0.05, 1.05)     ax.set_ylim(-0.05, 1.05) In\u00a0[28]: Copied! <pre>class TSPEnv(RL4COEnvBase):\n    \"\"\"Traveling Salesman Problem (TSP) environment\"\"\"\n\n    name = \"tsp\"\n\n    def __init__(\n        self,\n        generator = TSPGenerator,\n        generator_params = {},\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.generator = generator(**generator_params)\n        self._make_spec(self.generator)\n        \n    _reset = _reset\n    _step = _step\n    _get_reward = _get_reward\n    check_solution_validity = check_solution_validity\n    get_action_mask = get_action_mask\n    _make_spec = _make_spec\n    render = render\n</pre> class TSPEnv(RL4COEnvBase):     \"\"\"Traveling Salesman Problem (TSP) environment\"\"\"      name = \"tsp\"      def __init__(         self,         generator = TSPGenerator,         generator_params = {},         **kwargs,     ):         super().__init__(**kwargs)         self.generator = generator(**generator_params)         self._make_spec(self.generator)              _reset = _reset     _step = _step     _get_reward = _get_reward     check_solution_validity = check_solution_validity     get_action_mask = get_action_mask     _make_spec = _make_spec     render = render  In\u00a0[29]: Copied! <pre>batch_size = 2\n\nenv = TSPEnv(generator_params=dict(num_loc=20))\nreward, td, actions = rollout(env, env.reset(batch_size=[batch_size]), random_policy)\nenv.render(td, actions)\n</pre> batch_size = 2  env = TSPEnv(generator_params=dict(num_loc=20)) reward, td, actions = rollout(env, env.reset(batch_size=[batch_size]), random_policy) env.render(td, actions) In\u00a0[30]: Copied! <pre>class TSPInitEmbedding(nn.Module):\n    \"\"\"Initial embedding for the Traveling Salesman Problems (TSP).\n    Embed the following node features to the embedding space:\n        - locs: x, y coordinates of the cities\n    \"\"\"\n\n    def __init__(self, embed_dim, linear_bias=True):\n        super(TSPInitEmbedding, self).__init__()\n        node_dim = 2  # x, y\n        self.init_embed = nn.Linear(node_dim, embed_dim, linear_bias)\n\n    def forward(self, td):\n        out = self.init_embed(td[\"locs\"])\n        return out\n</pre> class TSPInitEmbedding(nn.Module):     \"\"\"Initial embedding for the Traveling Salesman Problems (TSP).     Embed the following node features to the embedding space:         - locs: x, y coordinates of the cities     \"\"\"      def __init__(self, embed_dim, linear_bias=True):         super(TSPInitEmbedding, self).__init__()         node_dim = 2  # x, y         self.init_embed = nn.Linear(node_dim, embed_dim, linear_bias)      def forward(self, td):         out = self.init_embed(td[\"locs\"])         return out In\u00a0[31]: Copied! <pre>class TSPContext(nn.Module):\n    \"\"\"Context embedding for the Traveling Salesman Problem (TSP).\n    Project the following to the embedding space:\n        - first node embedding\n        - current node embedding\n    \"\"\"\n\n    def __init__(self, embed_dim,  linear_bias=True):\n        super(TSPContext, self).__init__()\n        self.W_placeholder = nn.Parameter(\n            torch.Tensor(2 * embed_dim).uniform_(-1, 1)\n        )\n        self.project_context = nn.Linear(\n            embed_dim*2, embed_dim, bias=linear_bias\n        )\n\n    def forward(self, embeddings, td):\n        batch_size = embeddings.size(0)\n        # By default, node_dim = -1 (we only have one node embedding per node)\n        node_dim = (\n            (-1,) if td[\"first_node\"].dim() == 1 else (td[\"first_node\"].size(-1), -1)\n        )\n        if td[\"i\"][(0,) * td[\"i\"].dim()].item() &lt; 1:  # get first item fast\n            context_embedding = self.W_placeholder[None, :].expand(\n                batch_size, self.W_placeholder.size(-1)\n            )\n        else:\n            context_embedding = gather_by_index(\n                embeddings,\n                torch.stack([td[\"first_node\"], td[\"current_node\"]], -1).view(\n                    batch_size, -1\n                ),\n            ).view(batch_size, *node_dim)\n        return self.project_context(context_embedding)\n</pre> class TSPContext(nn.Module):     \"\"\"Context embedding for the Traveling Salesman Problem (TSP).     Project the following to the embedding space:         - first node embedding         - current node embedding     \"\"\"      def __init__(self, embed_dim,  linear_bias=True):         super(TSPContext, self).__init__()         self.W_placeholder = nn.Parameter(             torch.Tensor(2 * embed_dim).uniform_(-1, 1)         )         self.project_context = nn.Linear(             embed_dim*2, embed_dim, bias=linear_bias         )      def forward(self, embeddings, td):         batch_size = embeddings.size(0)         # By default, node_dim = -1 (we only have one node embedding per node)         node_dim = (             (-1,) if td[\"first_node\"].dim() == 1 else (td[\"first_node\"].size(-1), -1)         )         if td[\"i\"][(0,) * td[\"i\"].dim()].item() &lt; 1:  # get first item fast             context_embedding = self.W_placeholder[None, :].expand(                 batch_size, self.W_placeholder.size(-1)             )         else:             context_embedding = gather_by_index(                 embeddings,                 torch.stack([td[\"first_node\"], td[\"current_node\"]], -1).view(                     batch_size, -1                 ),             ).view(batch_size, *node_dim)         return self.project_context(context_embedding)      In\u00a0[32]: Copied! <pre>class StaticEmbedding(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(StaticEmbedding, self).__init__()\n\n    def forward(self, td):\n        return 0, 0, 0\n</pre> class StaticEmbedding(nn.Module):     def __init__(self, *args, **kwargs):         super(StaticEmbedding, self).__init__()      def forward(self, td):         return 0, 0, 0 In\u00a0[33]: Copied! <pre># Instantiate our environment\nenv = TSPEnv(generator_params=dict(num_loc=20))\n\n# Instantiate policy with the embeddings we created above\nemb_dim = 128\npolicy = AttentionModelPolicy(env_name=env.name, # this is actually not needed since we are initializing the embeddings!\n                              embed_dim=emb_dim,\n                              init_embedding=TSPInitEmbedding(emb_dim),\n                              context_embedding=TSPContext(emb_dim),\n                              dynamic_embedding=StaticEmbedding(emb_dim)\n)\n\n\n# Model: default is AM with REINFORCE and greedy rollout baseline\nmodel = AttentionModel(env, \n                       policy=policy,\n                       baseline='rollout',\n                       train_data_size=100_000,\n                       val_data_size=10_000)\n</pre> # Instantiate our environment env = TSPEnv(generator_params=dict(num_loc=20))  # Instantiate policy with the embeddings we created above emb_dim = 128 policy = AttentionModelPolicy(env_name=env.name, # this is actually not needed since we are initializing the embeddings!                               embed_dim=emb_dim,                               init_embedding=TSPInitEmbedding(emb_dim),                               context_embedding=TSPContext(emb_dim),                               dynamic_embedding=StaticEmbedding(emb_dim) )   # Model: default is AM with REINFORCE and greedy rollout baseline model = AttentionModel(env,                         policy=policy,                        baseline='rollout',                        train_data_size=100_000,                        val_data_size=10_000)  <pre>/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n</pre> In\u00a0[34]: Copied! <pre># Greedy rollouts over untrained model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntd_init = env.reset(batch_size=[3]).to(device)\npolicy = model.policy.to(device)\nout = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\nactions_untrained = out['actions'].cpu().detach()\nrewards_untrained = out['reward'].cpu().detach()\n\nfor i in range(3):\n    print(f\"Problem {i+1} | Cost: {-rewards_untrained[i]:.3f}\")\n    env.render(td_init[i], actions_untrained[i])\n</pre> # Greedy rollouts over untrained model device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") td_init = env.reset(batch_size=[3]).to(device) policy = model.policy.to(device) out = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True) actions_untrained = out['actions'].cpu().detach() rewards_untrained = out['reward'].cpu().detach()  for i in range(3):     print(f\"Problem {i+1} | Cost: {-rewards_untrained[i]:.3f}\")     env.render(td_init[i], actions_untrained[i]) <pre>Problem 1 | Cost: 11.545\nProblem 2 | Cost: 8.525\nProblem 3 | Cost: 12.461\n</pre> In\u00a0[35]: Copied! <pre># We use our own wrapper around Lightning's `Trainer` to make it easier to use\ntrainer = RL4COTrainer(max_epochs=3, devices=1)\ntrainer.fit(model)\n</pre> # We use our own wrapper around Lightning's `Trainer` to make it easier to use trainer = RL4COTrainer(max_epochs=3, devices=1) trainer.fit(model) <pre>Using 16bit Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\nval_file not set. Generating dataset instead\ntest_file not set. Generating dataset instead\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n\n  | Name     | Type                 | Params\n--------------------------------------------------\n0 | env      | TSPEnv               | 0     \n1 | policy   | AttentionModelPolicy | 710 K \n2 | baseline | WarmupBaseline       | 710 K \n--------------------------------------------------\n1.4 M     Trainable params\n0         Non-trainable params\n1.4 M     Total params\n5.682     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n</pre> <pre>Training: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=3` reached.\n</pre> In\u00a0[36]: Copied! <pre># Greedy rollouts over trained policy (same states as previous plot)\npolicy = model.policy.to(device)\nout = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\nactions_trained = out['actions'].cpu().detach()\n\n# Plotting\nimport matplotlib.pyplot as plt\nfor i, td in enumerate(td_init):\n    fig, axs = plt.subplots(1,2, figsize=(11,5))\n    env.render(td, actions_untrained[i], ax=axs[0]) \n    env.render(td, actions_trained[i], ax=axs[1])\n    axs[0].set_title(f\"Untrained | Cost = {-rewards_untrained[i].item():.3f}\")\n    axs[1].set_title(r\"Trained $\\pi_\\theta$\" + f\"| Cost = {-out['reward'][i].item():.3f}\")\n</pre> # Greedy rollouts over trained policy (same states as previous plot) policy = model.policy.to(device) out = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True) actions_trained = out['actions'].cpu().detach()  # Plotting import matplotlib.pyplot as plt for i, td in enumerate(td_init):     fig, axs = plt.subplots(1,2, figsize=(11,5))     env.render(td, actions_untrained[i], ax=axs[0])      env.render(td, actions_trained[i], ax=axs[1])     axs[0].set_title(f\"Untrained | Cost = {-rewards_untrained[i].item():.3f}\")     axs[1].set_title(r\"Trained $\\pi_\\theta$\" + f\"| Cost = {-out['reward'][i].item():.3f}\") <p>We can see that solutions are way better than with the untrained model, even just after 3 epochs! \ud83d\ude80</p>"},{"location":"examples/3-creating-new-env-model/#new-environment-creating-and-modeling","title":"New Environment: Creating and Modeling\u00b6","text":""},{"location":"examples/3-creating-new-env-model/#contents","title":"Contents\u00b6","text":"<ol> <li>Environment</li> <li>Modeling</li> <li>Training</li> <li>Evaluation</li> </ol>"},{"location":"examples/3-creating-new-env-model/#problem-tsp","title":"Problem: TSP\u00b6","text":"<p>We will build an environment and model for the Traveling Salesman Problem (TSP). The TSP is a well-known combinatorial optimization problem that consists of finding the shortest route that visits each city in a given list exactly once and returns to the origin city. The TSP is NP-hard, and it is one of the most studied problems in combinatorial optimization.</p>"},{"location":"examples/3-creating-new-env-model/#installation","title":"Installation\u00b6","text":""},{"location":"examples/3-creating-new-env-model/#imports","title":"Imports\u00b6","text":""},{"location":"examples/3-creating-new-env-model/#environment-creation","title":"Environment Creation\u00b6","text":""},{"location":"examples/3-creating-new-env-model/#reset","title":"Reset\u00b6","text":"<p>The <code>_reset</code> function is used to initialize the environment to an initial state. It returns a TensorDict of the initial state.</p>"},{"location":"examples/3-creating-new-env-model/#step","title":"Step\u00b6","text":"<p>Environment <code>_step</code>: this defines the state update of the TSP problem gived a TensorDict (td in the code) of the current state and the action to take:</p>"},{"location":"examples/3-creating-new-env-model/#optional-separate-action-mask-function","title":"[Optional] Separate Action Mask Function\u00b6","text":"<p>The <code>get_action_mask</code> function simply returns a mask of the valid actions for the current updated state. This can be used in <code>_step</code> and <code>_reset</code> for larger environments with several constraints and may be useful for modularity</p>"},{"location":"examples/3-creating-new-env-model/#optional-check-solution-validity","title":"[Optional] Check Solution Validity\u00b6","text":"<p>Another optional utility, this checks whether the solution is feasible and can help identify bugs</p>"},{"location":"examples/3-creating-new-env-model/#reward-function","title":"Reward function\u00b6","text":"<p>The <code>_get_reward</code> function is used to evaluate the reward given the solution (actions).</p>"},{"location":"examples/3-creating-new-env-model/#environment-action-specs","title":"Environment Action Specs\u00b6","text":"<p>This defines the input and output domains of the environment - similar to Gym's <code>spaces</code>. This is not strictly necessary, but it is useful to have a clear definition of the environment's action and observation spaces and if we want to sample actions using TorchRL's utils</p> <p>Note: this is actually not necessary, but it is useful to have a clear definition of the environment's action and observation spaces and if we want to sample actions using TorchRL's utils</p>"},{"location":"examples/3-creating-new-env-model/#data-generator","title":"Data generator\u00b6","text":"<p>The generator allows to generate random instances of the problem. Note that this is a simplified example: this can include additional distributions via the <code>rl4co.envs.common.utils.get_sampler</code> method!</p>"},{"location":"examples/3-creating-new-env-model/#render-function","title":"Render function\u00b6","text":"<p>The <code>render</code> function is optional, but can be useful for quickly visualizing the results of your algorithm!</p>"},{"location":"examples/3-creating-new-env-model/#putting-everything-together","title":"Putting everything together\u00b6","text":""},{"location":"examples/3-creating-new-env-model/#modeling","title":"Modeling\u00b6","text":"<p>Now we need to model the problem by transforming input information into the latent space to be processed. Here we focus on <code>AttentionModel</code>-based embeddings with an encoder-decoder structure. In RL4CO, we divide embeddings in 3 parts:</p> <ul> <li><code>init_embedding</code>: (encoder) embed initial states of the problem</li> <li><code>context_embedding</code>: (decoder) embed context information of the problem for the current partial solution to modify the query</li> <li><code>dynamic_embedding</code>: (decoder) embed dynamic information of the problem for the current partial solution to modify the query, key, and value (i.e. if other nodes also change state)</li> </ul>"},{"location":"examples/3-creating-new-env-model/#init-embedding","title":"Init Embedding\u00b6","text":"<p>Embed initial problem into latent space. In our case, we can project the coordinates of the cities into a latent space.</p>"},{"location":"examples/3-creating-new-env-model/#context-embedding","title":"Context Embedding\u00b6","text":"<p>Context embedding takes the current context and returns a vector representation of it. In TSP, we can take the embedding of the first node visited (since we need to complete the tour) as well as the embedding of current node visited (in the first step we just have a placeholder since they are the same).</p>"},{"location":"examples/3-creating-new-env-model/#dynamic-embedding","title":"Dynamic Embedding\u00b6","text":"<p>Since the states do not change except for visited nodes, we do not need to modify the keys and values. Therefore, we set this to 0</p>"},{"location":"examples/3-creating-new-env-model/#training-our-model","title":"Training our Model\u00b6","text":""},{"location":"examples/3-creating-new-env-model/#rollout-untrained-model","title":"Rollout untrained model\u00b6","text":""},{"location":"examples/3-creating-new-env-model/#training-loop","title":"Training loop\u00b6","text":""},{"location":"examples/3-creating-new-env-model/#evaluation","title":"Evaluation\u00b6","text":""},{"location":"examples/advanced/","title":"Advanced","text":"<p>Collection of advanced examples and tutorials - which at the moment are a bit mixed together.</p>"},{"location":"examples/advanced/#index","title":"Index","text":"<ul> <li><code>1-hydra-config.ipynb</code>: here we show how to use Hydra to configure your training and testing scripts.</li> <li><code>2-flash-attention-2.ipynb</code>: this notebook shows the effects of different SDPA (Scaled Dot-Product Attention) implementations on the training of a model.</li> </ul>"},{"location":"examples/advanced/1-hydra-config/","title":"Hydra Configuration","text":"In\u00a0[1]: Copied! <pre>from hydra import compose, initialize\nfrom omegaconf import OmegaConf\n\nROOT_DIR = \"../../\" # relative to this file\n</pre> from hydra import compose, initialize from omegaconf import OmegaConf  ROOT_DIR = \"../../\" # relative to this file In\u00a0[2]: Copied! <pre># context initialization\nwith initialize(version_base=None, config_path=ROOT_DIR+\"configs\"):\n    cfg = compose(config_name=\"main\")\n</pre> # context initialization with initialize(version_base=None, config_path=ROOT_DIR+\"configs\"):     cfg = compose(config_name=\"main\") <p>Hydra stores the configurations in a dictionary like object called OmegaConf</p> In\u00a0[3]: Copied! <pre>type(cfg)\n</pre> type(cfg) Out[3]: <pre>omegaconf.dictconfig.DictConfig</pre> <p>The different subfolders in the configs folder are represented as distinct keys in the omegaconf</p> In\u00a0[4]: Copied! <pre>list(cfg.keys())\n</pre> list(cfg.keys()) Out[4]: <pre>['mode',\n 'tags',\n 'train',\n 'test',\n 'compile',\n 'ckpt_path',\n 'seed',\n 'matmul_precision',\n 'model',\n 'callbacks',\n 'logger',\n 'trainer',\n 'paths',\n 'extras',\n 'env']</pre> <p>Keys can be accessed using the dot notation (e.g. <code>cfg.model</code>) or via normal dictionaries:</p> In\u00a0[5]: Copied! <pre>print(cfg.model == cfg[\"model\"])\n</pre> print(cfg.model == cfg[\"model\"]) <pre>True\n</pre> <p>The dot notation is however more convenient especially in nested structures</p> In\u00a0[6]: Copied! <pre>print(cfg.model._target_ == cfg[\"model\"][\"_target_\"])\n</pre> print(cfg.model._target_ == cfg[\"model\"][\"_target_\"]) <pre>True\n</pre> <p>For example, lets look at the model configuration (which corresponds the model/default.yaml configuration).</p> In\u00a0[7]: Copied! <pre>print(OmegaConf.to_yaml(cfg.model))\n</pre> print(OmegaConf.to_yaml(cfg.model)) <pre>generate_default_data: true\nmetrics:\n  train:\n  - loss\n  - reward\n  val:\n  - reward\n  test:\n  - reward\n  log_on_step: true\n_target_: rl4co.models.AttentionModel\nbaseline: rollout\nbatch_size: 512\nval_batch_size: 1024\ntest_batch_size: 1024\ntrain_data_size: 1280000\nval_data_size: 10000\ntest_data_size: 10000\noptimizer_kwargs:\n  lr: 0.0001\n\n</pre> <p>If we want to change parts of the configuration, it is generally a good practice to make the changes via the command line when executing the respective python script (in the case of RL4CO for example rl4co/tasks/train.py). For example, if we want to use a different model configuration, we can do something like:</p> <pre>python train.py model=pomo model.batch_size=32\n</pre> <p>Here we use the model/pomo.yaml configuration for the model and also change the batch size during training to 32.</p> <p>Note: check out the see override syntax documentation on the Hydra website for more!</p> In\u00a0[8]: Copied! <pre>with initialize(version_base=None, config_path=ROOT_DIR+\"configs\"):\n    cfg = compose(config_name=\"main\", overrides=[\"model=pomo\",\"model.batch_size=32\"])\n    print(OmegaConf.to_yaml(cfg.model))\n</pre> with initialize(version_base=None, config_path=ROOT_DIR+\"configs\"):     cfg = compose(config_name=\"main\", overrides=[\"model=pomo\",\"model.batch_size=32\"])     print(OmegaConf.to_yaml(cfg.model)) <pre>generate_default_data: true\nmetrics:\n  train:\n  - loss\n  - reward\n  val:\n  - reward\n  - max_reward\n  - max_aug_reward\n  test: ${metrics.val}\n  log_on_step: true\n_target_: rl4co.models.POMO\nnum_augment: 8\nbatch_size: 32\nval_batch_size: 1024\ntest_batch_size: 1024\ntrain_data_size: 1280000\nval_data_size: 10000\ntest_data_size: 10000\noptimizer_kwargs:\n  lr: 0.0001\n\n</pre> <p>It is also possible to add new parameters to a config using the <code>+</code> prefix. Using <code>++</code> will add a new parameter if it does not exist and overwrite it if it does.</p> In\u00a0[9]: Copied! <pre>with initialize(version_base=None, config_path=ROOT_DIR+\"configs\"):\n    cfg = compose(config_name=\"main\", overrides=[\"model=pomo\",\"model.batch_size=32\",\"+model.num_starts=10\"])\n    print(OmegaConf.to_yaml(cfg.model))\n</pre> with initialize(version_base=None, config_path=ROOT_DIR+\"configs\"):     cfg = compose(config_name=\"main\", overrides=[\"model=pomo\",\"model.batch_size=32\",\"+model.num_starts=10\"])     print(OmegaConf.to_yaml(cfg.model)) <pre>generate_default_data: true\nmetrics:\n  train:\n  - loss\n  - reward\n  val:\n  - reward\n  - max_reward\n  - max_aug_reward\n  test: ${metrics.val}\n  log_on_step: true\n_target_: rl4co.models.POMO\nnum_augment: 8\nbatch_size: 32\nval_batch_size: 1024\ntest_batch_size: 1024\ntrain_data_size: 1280000\nval_data_size: 10000\ntest_data_size: 10000\noptimizer_kwargs:\n  lr: 0.0001\nnum_starts: 10\n\n</pre> <p>Likewise, we can also remove unwanted parts of the configuration. For example, if we do not want to use any experiment configuration, we can remove the changes to the configuration made by experiments/base.yaml using the <code>~</code> prefix:</p> In\u00a0[10]: Copied! <pre>with initialize(version_base=None, config_path=ROOT_DIR+\"configs\"):\n    cfg = compose(config_name=\"main\", overrides=[\"model=pomo\",\"~experiment\"])\n    print(OmegaConf.to_yaml(cfg.model))\n</pre> with initialize(version_base=None, config_path=ROOT_DIR+\"configs\"):     cfg = compose(config_name=\"main\", overrides=[\"model=pomo\",\"~experiment\"])     print(OmegaConf.to_yaml(cfg.model)) <pre>generate_default_data: true\nmetrics:\n  train:\n  - loss\n  - reward\n  val:\n  - reward\n  - max_reward\n  - max_aug_reward\n  test: ${metrics.val}\n  log_on_step: true\n_target_: rl4co.models.POMO\nnum_augment: 8\n\n</pre> <p>As you can see, parameters like \"batch_size\" were removed from the model config, as those were set by the experiment config base.yaml. Through the hashbang</p> <pre><code># @package _global_\n</code></pre> <p>in the configs/experiments/base.yaml, this configuration is able to make changes to all parts of the configuration (like model, trainer, logger). So instead of adding a new key to the omegaconf object, configurations with a <code># @package _global_</code> hashbang typically alter other parts of the configuration.</p> <p>Another example of such a configuration is the debug/default.yaml, which sets all parameters into a lightweight debugging mode:</p> In\u00a0[11]: Copied! <pre>with initialize(version_base=None, config_path=ROOT_DIR+\"configs\"):\n    cfg = compose(config_name=\"main\", overrides=[\"debug=default\"])\n    print(OmegaConf.to_yaml(cfg.model))\n</pre> with initialize(version_base=None, config_path=ROOT_DIR+\"configs\"):     cfg = compose(config_name=\"main\", overrides=[\"debug=default\"])     print(OmegaConf.to_yaml(cfg.model)) <pre>generate_default_data: true\nmetrics:\n  train:\n  - loss\n  - reward\n  val:\n  - reward\n  test:\n  - reward\n  log_on_step: true\n_target_: rl4co.models.AttentionModel\nbaseline: rollout\nbatch_size: 8\nval_batch_size: 32\ntest_batch_size: 32\ntrain_data_size: 64\nval_data_size: 1000\ntest_data_size: 1000\noptimizer_kwargs:\n  lr: 0.0001\n\n</pre>"},{"location":"examples/advanced/1-hydra-config/#hydra-configuration","title":"Hydra Configuration\u00b6","text":"<p>Hydra makes it extremely convenient to configure projects with lots of parameter settings like the RL4CO library.</p> <p>While you don't need Hydra to use RL4CO, it is recommended to use it for your own projects to make it easier to manage the configuration of your experiments.</p> <p>Hydra uses config files in <code>.yaml</code> format for this. These files can be found in the configs/ folder, where the subfolders define configurations for specific parts of the framework which are then combined in the main.yaml configuration. In this tutorial we will have a look at how to use these different configuration files and how to add new parameters to the configuration.</p>"},{"location":"examples/advanced/1-hydra-config/#summary","title":"Summary\u00b6","text":"<ul> <li>Reference config files using the CLI flag <code>&lt;key&gt;=&lt;config_file&gt;</code> (e.g. <code>model=am</code>)</li> <li>Add parameters (or even entire keys) to the config using the \"+\" prefix (e.g. <code>+model.batch_size=32</code>)</li> <li>Remove parameters (or even entire keys) to the config using the \"~\" prefix (e.g. <code>~logger.wandb</code>)</li> <li>The <code># @package _global_</code> hashbang allows global access from any config file</li> <li>Turn on debugging mode using <code>debug=default</code></li> </ul>"},{"location":"examples/advanced/2-flash-attention-2/","title":"Using Flash Attention 2 \u26a1","text":"<p>In this notebook we will compare Flash Attention 2 with the <code>torch.nn.functional.scaled_dot_product_attention</code> function and a simple implementation.</p> In\u00a0[1]: Copied! <pre>## Uncomment the following line to install the package from PyPI\n## You may need to restart the runtime in Colab after this\n## Remember to choose a GPU runtime for faster training!\n\n# !pip install rl4co\n</pre> ## Uncomment the following line to install the package from PyPI ## You may need to restart the runtime in Colab after this ## Remember to choose a GPU runtime for faster training!  # !pip install rl4co In\u00a0[2]: Copied! <pre>import torch\nimport torch.utils.benchmark as benchmark\n\n\n# Simple implementation in PyTorch\nfrom rl4co.models.nn.attention import scaled_dot_product_attention_simple\n# PyTorch official implementation of FlashAttention 1\nfrom torch.nn.functional import scaled_dot_product_attention\n# FlashAttention 2\nfrom rl4co.models.nn.flash_attention import scaled_dot_product_attention_flash_attn\n\nfrom rl4co.envs import TSPEnv\nfrom rl4co.models.zoo.am import AttentionModel\nfrom rl4co.utils.trainer import RL4COTrainer\nfrom rl4co.models.common.constructive.autoregressive import GraphAttentionEncoder\n</pre> import torch import torch.utils.benchmark as benchmark   # Simple implementation in PyTorch from rl4co.models.nn.attention import scaled_dot_product_attention_simple # PyTorch official implementation of FlashAttention 1 from torch.nn.functional import scaled_dot_product_attention # FlashAttention 2 from rl4co.models.nn.flash_attention import scaled_dot_product_attention_flash_attn  from rl4co.envs import TSPEnv from rl4co.models.zoo.am import AttentionModel from rl4co.utils.trainer import RL4COTrainer from rl4co.models.common.constructive.autoregressive import GraphAttentionEncoder   <pre>/home/botu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[3]: Copied! <pre>bs, head, length, d = 64, 8, 512, 128\n\nquery = torch.rand(bs, head, length, d, dtype=torch.float16, device=\"cuda\")\nkey = torch.rand(bs, head, length, d, dtype=torch.float16, device=\"cuda\")\nvalue = torch.rand(bs, head, length, d, dtype=torch.float16, device=\"cuda\")\n\n# Simple implementation in PyTorch\nout_simple = scaled_dot_product_attention_simple(query, key, value)\n\n# PyTorch official implementation of FlashAttention 1\nout_pytorch = scaled_dot_product_attention(query, key, value)\n\n# FlashAttention 2\nout_flash_attn = scaled_dot_product_attention_flash_attn(query, key, value)\n\n\nprint(torch.allclose(out_simple, out_pytorch, atol=1e-3))\nprint(torch.allclose(out_flash_attn, out_pytorch, atol=1e-3))\n\nprint(torch.max(torch.abs(out_simple - out_pytorch)), torch.mean(torch.abs(out_simple - out_pytorch)))\nprint(torch.max(torch.abs(out_flash_attn - out_pytorch)), torch.mean(torch.abs(out_flash_attn - out_pytorch)))\n</pre> bs, head, length, d = 64, 8, 512, 128  query = torch.rand(bs, head, length, d, dtype=torch.float16, device=\"cuda\") key = torch.rand(bs, head, length, d, dtype=torch.float16, device=\"cuda\") value = torch.rand(bs, head, length, d, dtype=torch.float16, device=\"cuda\")  # Simple implementation in PyTorch out_simple = scaled_dot_product_attention_simple(query, key, value)  # PyTorch official implementation of FlashAttention 1 out_pytorch = scaled_dot_product_attention(query, key, value)  # FlashAttention 2 out_flash_attn = scaled_dot_product_attention_flash_attn(query, key, value)   print(torch.allclose(out_simple, out_pytorch, atol=1e-3)) print(torch.allclose(out_flash_attn, out_pytorch, atol=1e-3))  print(torch.max(torch.abs(out_simple - out_pytorch)), torch.mean(torch.abs(out_simple - out_pytorch))) print(torch.max(torch.abs(out_flash_attn - out_pytorch)), torch.mean(torch.abs(out_flash_attn - out_pytorch)))  <pre>True\nTrue\ntensor(0.0005, device='cuda:0', dtype=torch.float16) tensor(1.2159e-05, device='cuda:0', dtype=torch.float16)\ntensor(0.0005, device='cuda:0', dtype=torch.float16) tensor(6.3777e-06, device='cuda:0', dtype=torch.float16)\n</pre> In\u00a0[4]: Copied! <pre>env = TSPEnv(generator_params=dict(num_loc=1000))\n\nnum_heads = 8\nembed_dim = 128\nnum_layers = 3\nenc_simple = GraphAttentionEncoder(env, num_heads=num_heads, embed_dim=embed_dim, num_layers=num_layers,\n                            sdpa_fn=scaled_dot_product_attention_simple)\n\nenc_fa1 = GraphAttentionEncoder(env, num_heads=num_heads, embed_dim=embed_dim, num_layers=num_layers,\n                            sdpa_fn=scaled_dot_product_attention)\n\nenc_fa2 = GraphAttentionEncoder(env, num_heads=num_heads, embed_dim=embed_dim, num_layers=num_layers,\n                            sdpa_fn=scaled_dot_product_attention_flash_attn)\n\n# Flash Attention supports only FP16 and BFloat16\nenc_simple.to(\"cuda\").half()\nenc_fa1.to(\"cuda\").half()\nenc_fa2.to(\"cuda\").half()\n</pre> env = TSPEnv(generator_params=dict(num_loc=1000))  num_heads = 8 embed_dim = 128 num_layers = 3 enc_simple = GraphAttentionEncoder(env, num_heads=num_heads, embed_dim=embed_dim, num_layers=num_layers,                             sdpa_fn=scaled_dot_product_attention_simple)  enc_fa1 = GraphAttentionEncoder(env, num_heads=num_heads, embed_dim=embed_dim, num_layers=num_layers,                             sdpa_fn=scaled_dot_product_attention)  enc_fa2 = GraphAttentionEncoder(env, num_heads=num_heads, embed_dim=embed_dim, num_layers=num_layers,                             sdpa_fn=scaled_dot_product_attention_flash_attn)  # Flash Attention supports only FP16 and BFloat16 enc_simple.to(\"cuda\").half() enc_fa1.to(\"cuda\").half() enc_fa2.to(\"cuda\").half() Out[4]: <pre>GraphAttentionEncoder(\n  (init_embedding): TSPInitEmbedding(\n    (init_embed): Linear(in_features=2, out_features=128, bias=True)\n  )\n  (net): GraphAttentionNetwork(\n    (layers): Sequential(\n      (0): MultiHeadAttentionLayer(\n        (0): SkipConnection(\n          (module): MultiHeadAttention(\n            (Wqkv): Linear(in_features=128, out_features=384, bias=True)\n            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n          )\n        )\n        (1): Normalization(\n          (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (2): SkipConnection(\n          (module): Sequential(\n            (0): Linear(in_features=128, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=128, bias=True)\n          )\n        )\n        (3): Normalization(\n          (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): MultiHeadAttentionLayer(\n        (0): SkipConnection(\n          (module): MultiHeadAttention(\n            (Wqkv): Linear(in_features=128, out_features=384, bias=True)\n            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n          )\n        )\n        (1): Normalization(\n          (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (2): SkipConnection(\n          (module): Sequential(\n            (0): Linear(in_features=128, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=128, bias=True)\n          )\n        )\n        (3): Normalization(\n          (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): MultiHeadAttentionLayer(\n        (0): SkipConnection(\n          (module): MultiHeadAttention(\n            (Wqkv): Linear(in_features=128, out_features=384, bias=True)\n            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n          )\n        )\n        (1): Normalization(\n          (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (2): SkipConnection(\n          (module): Sequential(\n            (0): Linear(in_features=128, out_features=512, bias=True)\n            (1): ReLU()\n            (2): Linear(in_features=512, out_features=128, bias=True)\n          )\n        )\n        (3): Normalization(\n          (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n  )\n)</pre> In\u00a0[5]: Copied! <pre>def build_models(num_heads=8, embed_dim=128, num_layers=3):\n    enc_simple = GraphAttentionEncoder(env, num_heads=num_heads, embed_dim=embed_dim, num_layers=num_layers,\n                                sdpa_fn=scaled_dot_product_attention_simple)\n\n    enc_fa1 = GraphAttentionEncoder(env, num_heads=num_heads, embed_dim=embed_dim, num_layers=num_layers,\n                                sdpa_fn=scaled_dot_product_attention)\n\n    enc_fa2 = GraphAttentionEncoder(env, num_heads=num_heads, embed_dim=embed_dim, num_layers=num_layers,\n                                sdpa_fn=scaled_dot_product_attention_flash_attn)\n\n    # Flash Attention supports only FP16 and BFloat16\n    enc_simple.to(\"cuda\").half()\n    enc_fa1.to(\"cuda\").half()\n    enc_fa2.to(\"cuda\").half()\n    return enc_simple, enc_fa1, enc_fa2\n</pre> def build_models(num_heads=8, embed_dim=128, num_layers=3):     enc_simple = GraphAttentionEncoder(env, num_heads=num_heads, embed_dim=embed_dim, num_layers=num_layers,                                 sdpa_fn=scaled_dot_product_attention_simple)      enc_fa1 = GraphAttentionEncoder(env, num_heads=num_heads, embed_dim=embed_dim, num_layers=num_layers,                                 sdpa_fn=scaled_dot_product_attention)      enc_fa2 = GraphAttentionEncoder(env, num_heads=num_heads, embed_dim=embed_dim, num_layers=num_layers,                                 sdpa_fn=scaled_dot_product_attention_flash_attn)      # Flash Attention supports only FP16 and BFloat16     enc_simple.to(\"cuda\").half()     enc_fa1.to(\"cuda\").half()     enc_fa2.to(\"cuda\").half()     return enc_simple, enc_fa1, enc_fa2 In\u00a0[6]: Copied! <pre>threads = 32\nsizes = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]\n\ntimes_simple = []\ntimes_fa1 = []\ntimes_fa2 = []\n\n# for embed_dim in [64, 128, 256]:\nfor embed_dim in [128]:\n    # Get models\n    enc_simple, enc_fa1, enc_fa2 = build_models(embed_dim=embed_dim)\n\n    for problem_size in sizes:\n\n        with torch.no_grad():\n            # initial data\n            env = TSPEnv(generator_params=dict(num_loc=problem_size))\n            td_init = env.reset(batch_size=[2])\n            # set dtype to float16\n            td_init = td_init.to(dest=\"cuda\", dtype=torch.float16)\n\n            t_simple = benchmark.Timer(\n                setup='x = td_init',\n                stmt='encode(x)',\n                globals={'td_init': td_init, 'encode': enc_simple},\n                num_threads=threads)\n\n            t_fa1 = benchmark.Timer(\n                setup='x = td_init',\n                stmt='encode(x)',\n                globals={'td_init': td_init, 'encode': enc_fa1},\n                num_threads=threads)\n            \n            t_fa2 = benchmark.Timer(\n                setup='x = td_init',\n                stmt='encode(x)',\n                globals={'td_init': td_init, 'encode': enc_fa2},\n                num_threads=threads)\n            \n            times_simple.append(torch.tensor(t_simple.blocked_autorange().times).mean())\n            times_fa2.append(torch.tensor(t_fa2.blocked_autorange().times).mean())\n            times_fa1.append(torch.tensor(t_fa1.blocked_autorange().times).mean())\n\n            print(f\"Times for problem size {problem_size}: Simple {times_simple[-1]*1e3:.3f}, FA1 {times_fa1[-1]*1e3:.3f}, FA2 {times_fa2[-1]*1e3:.3f}\")\n\n    # eliminate cache\n    torch.cuda.empty_cache()\n</pre> threads = 32 sizes = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]  times_simple = [] times_fa1 = [] times_fa2 = []  # for embed_dim in [64, 128, 256]: for embed_dim in [128]:     # Get models     enc_simple, enc_fa1, enc_fa2 = build_models(embed_dim=embed_dim)      for problem_size in sizes:          with torch.no_grad():             # initial data             env = TSPEnv(generator_params=dict(num_loc=problem_size))             td_init = env.reset(batch_size=[2])             # set dtype to float16             td_init = td_init.to(dest=\"cuda\", dtype=torch.float16)              t_simple = benchmark.Timer(                 setup='x = td_init',                 stmt='encode(x)',                 globals={'td_init': td_init, 'encode': enc_simple},                 num_threads=threads)              t_fa1 = benchmark.Timer(                 setup='x = td_init',                 stmt='encode(x)',                 globals={'td_init': td_init, 'encode': enc_fa1},                 num_threads=threads)                          t_fa2 = benchmark.Timer(                 setup='x = td_init',                 stmt='encode(x)',                 globals={'td_init': td_init, 'encode': enc_fa2},                 num_threads=threads)                          times_simple.append(torch.tensor(t_simple.blocked_autorange().times).mean())             times_fa2.append(torch.tensor(t_fa2.blocked_autorange().times).mean())             times_fa1.append(torch.tensor(t_fa1.blocked_autorange().times).mean())              print(f\"Times for problem size {problem_size}: Simple {times_simple[-1]*1e3:.3f}, FA1 {times_fa1[-1]*1e3:.3f}, FA2 {times_fa2[-1]*1e3:.3f}\")      # eliminate cache     torch.cuda.empty_cache() <pre>Times for problem size 10: Simple 0.633, FA1 0.511, FA2 0.554\nTimes for problem size 20: Simple 0.646, FA1 0.535, FA2 0.565\nTimes for problem size 50: Simple 0.663, FA1 0.547, FA2 0.580\nTimes for problem size 100: Simple 0.664, FA1 0.547, FA2 0.580\nTimes for problem size 200: Simple 0.670, FA1 0.509, FA2 0.585\nTimes for problem size 500: Simple 0.669, FA1 0.512, FA2 0.582\nTimes for problem size 1000: Simple 1.088, FA1 0.555, FA2 0.609\nTimes for problem size 2000: Simple 3.626, FA1 1.292, FA2 0.790\nTimes for problem size 5000: Simple 20.332, FA1 5.748, FA2 2.943\nTimes for problem size 10000: Simple 80.337, FA1 20.701, FA2 10.230\n</pre> In\u00a0[7]: Copied! <pre># Plot results\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\nax.plot(sizes, times_simple, label=\"Simple\")\nax.plot(sizes, times_fa1, label=\"FlashAttention 1\")\nax.plot(sizes, times_fa2, label=\"FlashAttention 2\")\n\n# fancy grid\nax.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\nax.set_xscale(\"log\")\nax.set_yscale(\"log\")\nax.set_xlabel(\"Problem size\")\nax.set_ylabel(\"Time (ms)\")\nax.legend()\n\n# Instead of 10^1, 10^2... show nuber\nax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}\"))\n\nplt.show()\n</pre> # Plot results import matplotlib.pyplot as plt   fig, ax = plt.subplots(1, 1, figsize=(10, 5)) ax.plot(sizes, times_simple, label=\"Simple\") ax.plot(sizes, times_fa1, label=\"FlashAttention 1\") ax.plot(sizes, times_fa2, label=\"FlashAttention 2\")  # fancy grid ax.grid(True, which=\"both\", ls=\"-\", alpha=0.5) ax.set_xscale(\"log\") ax.set_yscale(\"log\") ax.set_xlabel(\"Problem size\") ax.set_ylabel(\"Time (ms)\") ax.legend()  # Instead of 10^1, 10^2... show nuber ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}\"))  plt.show() <p>Using FlashAttention can speed up inference even at small context lengths (number of nodes in the graph). Difference can be of several times for large graphs between different implementations!</p>"},{"location":"examples/advanced/2-flash-attention-2/#using-flash-attention-2","title":"Using Flash Attention 2 \u26a1\u00b6","text":""},{"location":"examples/advanced/2-flash-attention-2/#installation","title":"Installation\u00b6","text":"<p>Follow instructions here: https://github.com/Dao-AILab/flash-attention</p>"},{"location":"examples/advanced/2-flash-attention-2/#imports","title":"Imports\u00b6","text":""},{"location":"examples/advanced/2-flash-attention-2/#testing-differences-with-simple-tensors","title":"Testing differences with simple tensors\u00b6","text":""},{"location":"examples/advanced/2-flash-attention-2/#testing-graph-attention-encoders-with-flash-attention-2","title":"Testing Graph Attention Encoders with Flash Attention 2\u00b6","text":""},{"location":"examples/advanced/3-local-search/","title":"Local Search","text":"In\u00a0[1]: Copied! <pre># !pip install rl4co[routing]  # include pyvrp\n</pre> # !pip install rl4co[routing]  # include pyvrp In\u00a0[2]: Copied! <pre>import torch\n\nfrom rl4co.envs import TSPEnv\nfrom rl4co.models.zoo import AttentionModel\n</pre> import torch  from rl4co.envs import TSPEnv from rl4co.models.zoo import AttentionModel In\u00a0[3]: Copied! <pre># RL4CO env based on TorchRL\nenv = TSPEnv(num_loc=50) \n\ncheckpoint_path = \"../tsp-quickstart.ckpt\"  # checkpoint from the ../1-quickstart.ipynb\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Model: default is AM with REINFORCE and greedy rollout baseline\nmodel = AttentionModel.load_from_checkpoint(checkpoint_path, load_baseline=False)\n</pre> # RL4CO env based on TorchRL env = TSPEnv(num_loc=50)   checkpoint_path = \"../tsp-quickstart.ckpt\"  # checkpoint from the ../1-quickstart.ipynb  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Model: default is AM with REINFORCE and greedy rollout baseline model = AttentionModel.load_from_checkpoint(checkpoint_path, load_baseline=False) <pre>/home/sanghyeok/NCO/rl4co/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n/home/sanghyeok/NCO/rl4co/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n/home/sanghyeok/NCO/rl4co/.venv/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['baseline.baseline.model.encoder.init_embedding.init_embed.weight', 'baseline.baseline.model.encoder.init_embedding.init_embed.bias', 'baseline.baseline.model.encoder.net.layers.0.0.module.Wqkv.weight', 'baseline.baseline.model.encoder.net.layers.0.0.module.Wqkv.bias', 'baseline.baseline.model.encoder.net.layers.0.0.module.out_proj.weight', 'baseline.baseline.model.encoder.net.layers.0.0.module.out_proj.bias', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.0.2.module.0.weight', 'baseline.baseline.model.encoder.net.layers.0.2.module.0.bias', 'baseline.baseline.model.encoder.net.layers.0.2.module.2.weight', 'baseline.baseline.model.encoder.net.layers.0.2.module.2.bias', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.1.0.module.Wqkv.weight', 'baseline.baseline.model.encoder.net.layers.1.0.module.Wqkv.bias', 'baseline.baseline.model.encoder.net.layers.1.0.module.out_proj.weight', 'baseline.baseline.model.encoder.net.layers.1.0.module.out_proj.bias', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.1.2.module.0.weight', 'baseline.baseline.model.encoder.net.layers.1.2.module.0.bias', 'baseline.baseline.model.encoder.net.layers.1.2.module.2.weight', 'baseline.baseline.model.encoder.net.layers.1.2.module.2.bias', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.2.0.module.Wqkv.weight', 'baseline.baseline.model.encoder.net.layers.2.0.module.Wqkv.bias', 'baseline.baseline.model.encoder.net.layers.2.0.module.out_proj.weight', 'baseline.baseline.model.encoder.net.layers.2.0.module.out_proj.bias', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.2.2.module.0.weight', 'baseline.baseline.model.encoder.net.layers.2.2.module.0.bias', 'baseline.baseline.model.encoder.net.layers.2.2.module.2.weight', 'baseline.baseline.model.encoder.net.layers.2.2.module.2.bias', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.num_batches_tracked', 'baseline.baseline.model.decoder.context_embedding.W_placeholder', 'baseline.baseline.model.decoder.context_embedding.project_context.weight', 'baseline.baseline.model.decoder.project_node_embeddings.weight', 'baseline.baseline.model.decoder.project_fixed_context.weight', 'baseline.baseline.model.decoder.pointer.project_out.weight']\n</pre> In\u00a0[4]: Copied! <pre># Greedy rollouts over trained model (same states as previous plot)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntd_init = env.reset(batch_size=[3]).to(device)\nmodel = model.to(device)\nout = model(td_init.clone(), phase=\"test\", decode_type=\"greedy\", return_actions=True)\nactions = out['actions']\n\n# Improve solutions using LocalSearch\nimproved_actions = env.local_search(td_init, actions, rng=0)\nimproved_rewards = env.get_reward(td_init, improved_actions)\n\n# Plotting\nimport matplotlib.pyplot as plt\nfor i, td in enumerate(td_init):\n    fig, axs = plt.subplots(1,2, figsize=(11,5))\n    env.render(td, actions[i], ax=axs[0]) \n    env.render(td, improved_actions[i], ax=axs[1])\n    axs[0].set_title(f\"Before improvement | Cost = {-out['reward'][i].item():.3f}\")\n    axs[1].set_title(f\"After improvement | Cost = {-improved_rewards[i].item():.3f}\")\n</pre> # Greedy rollouts over trained model (same states as previous plot) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") td_init = env.reset(batch_size=[3]).to(device) model = model.to(device) out = model(td_init.clone(), phase=\"test\", decode_type=\"greedy\", return_actions=True) actions = out['actions']  # Improve solutions using LocalSearch improved_actions = env.local_search(td_init, actions, rng=0) improved_rewards = env.get_reward(td_init, improved_actions)  # Plotting import matplotlib.pyplot as plt for i, td in enumerate(td_init):     fig, axs = plt.subplots(1,2, figsize=(11,5))     env.render(td, actions[i], ax=axs[0])      env.render(td, improved_actions[i], ax=axs[1])     axs[0].set_title(f\"Before improvement | Cost = {-out['reward'][i].item():.3f}\")     axs[1].set_title(f\"After improvement | Cost = {-improved_rewards[i].item():.3f}\") <p>We can see that the solution has improved after using 2-opt.</p>"},{"location":"examples/advanced/3-local-search/#local-search","title":"Local Search\u00b6","text":"<p>In this notebook, we will show how to improve the solution at hand using local search and other techniques. Here we solve TSP and use 2-opt to improve the solution. You can check how the improvement works for other problems in each Env's <code>local_search</code> method.</p> <p>Note that this notebook is based on <code>1-quickstart</code> and we use the checkpoint file from it. If you haven't checked it yet, we recommend you to check it first.</p>"},{"location":"examples/advanced/3-local-search/#installation","title":"Installation\u00b6","text":"<p>We use LocalSearch operator provided by PyVRP. See https://github.com/PyVRP/PyVRP for more details.</p> <p>Uncomment the following line to install the package from PyPI. Remember to choose a GPU runtime for faster training!</p> <p>Note: You may need to restart the runtime in Colab after this</p>"},{"location":"examples/advanced/3-local-search/#imports","title":"Imports\u00b6","text":""},{"location":"examples/advanced/3-local-search/#environment-policy-and-model-from-saved-checkpoint","title":"Environment, Policy, and Model from saved checkpoint\u00b6","text":""},{"location":"examples/advanced/3-local-search/#testing-with-solution-improvement","title":"Testing with Solution Improvement\u00b6","text":""},{"location":"examples/datasets/","title":"Datasets","text":"<p>Collection of examples for training and testing with custom datasets.</p>"},{"location":"examples/datasets/#index","title":"Index","text":"<ul> <li><code>1-test-on-tsplib.ipynb</code>: here we show how to test a model on the TSPLIB dataset.</li> <li><code>2-test-on-cvrplib.ipynb</code>: here we show how to test a model on the CVRPLIB dataset.</li> </ul>"},{"location":"examples/datasets/1-test-on-tsplib/","title":"Test Model on TSPLib","text":"In\u00a0[\u00a0]: Copied! <pre># !pip install rl4co[graph] # include torch-geometric\n\n## NOTE: to install latest version from Github (may be unstable) install from source instead:\n# !pip install git+https://github.com/ai4co/rl4co.git\n</pre> # !pip install rl4co[graph] # include torch-geometric  ## NOTE: to install latest version from Github (may be unstable) install from source instead: # !pip install git+https://github.com/ai4co/rl4co.git In\u00a0[\u00a0]: Copied! <pre># Install the `tsplib95` package\n# !pip install tsplib95\n</pre> # Install the `tsplib95` package # !pip install tsplib95 In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport os\nimport re\nimport torch\n\nfrom rl4co.envs import TSPEnv, CVRPEnv\nfrom rl4co.models.zoo.am import AttentionModel\nfrom rl4co.utils.trainer import RL4COTrainer\nfrom rl4co.utils.decoding import get_log_likelihood\nfrom rl4co.models.zoo import EAS, EASLay, EASEmb, ActiveSearch\n\nfrom math import ceil\nfrom einops import repeat\nfrom tsplib95.loaders import load_problem, load_solution\n</pre> %load_ext autoreload %autoreload 2  import os import re import torch  from rl4co.envs import TSPEnv, CVRPEnv from rl4co.models.zoo.am import AttentionModel from rl4co.utils.trainer import RL4COTrainer from rl4co.utils.decoding import get_log_likelihood from rl4co.models.zoo import EAS, EASLay, EASEmb, ActiveSearch  from math import ceil from einops import repeat from tsplib95.loaders import load_problem, load_solution <pre>/home/cbhua/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre># Load from checkpoint; alternatively, simply instantiate a new model\n# Note the model is trained for TSP problem\ncheckpoint_path = \"../tsp-20.ckpt\" # modify the path to your checkpoint file\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# load checkpoint\n# checkpoint = torch.load(checkpoint_path)\n\nlit_model = AttentionModel.load_from_checkpoint(checkpoint_path, load_baseline=False)\npolicy, env = lit_model.policy, lit_model.env\npolicy = policy.to(device)\n</pre> # Load from checkpoint; alternatively, simply instantiate a new model # Note the model is trained for TSP problem checkpoint_path = \"../tsp-20.ckpt\" # modify the path to your checkpoint file  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # load checkpoint # checkpoint = torch.load(checkpoint_path)  lit_model = AttentionModel.load_from_checkpoint(checkpoint_path, load_baseline=False) policy, env = lit_model.policy, lit_model.env policy = policy.to(device) <pre>/home/cbhua/miniconda3/envs/rl4co-user/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n/home/cbhua/miniconda3/envs/rl4co-user/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n/home/cbhua/miniconda3/envs/rl4co-user/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:177: Found keys that are not in the model state dict but in the checkpoint: ['baseline.baseline.model.encoder.init_embedding.init_embed.weight', 'baseline.baseline.model.encoder.init_embedding.init_embed.bias', 'baseline.baseline.model.encoder.net.layers.0.0.module.Wqkv.weight', 'baseline.baseline.model.encoder.net.layers.0.0.module.Wqkv.bias', 'baseline.baseline.model.encoder.net.layers.0.0.module.out_proj.weight', 'baseline.baseline.model.encoder.net.layers.0.0.module.out_proj.bias', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.0.2.module.0.weight', 'baseline.baseline.model.encoder.net.layers.0.2.module.0.bias', 'baseline.baseline.model.encoder.net.layers.0.2.module.2.weight', 'baseline.baseline.model.encoder.net.layers.0.2.module.2.bias', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.1.0.module.Wqkv.weight', 'baseline.baseline.model.encoder.net.layers.1.0.module.Wqkv.bias', 'baseline.baseline.model.encoder.net.layers.1.0.module.out_proj.weight', 'baseline.baseline.model.encoder.net.layers.1.0.module.out_proj.bias', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.1.2.module.0.weight', 'baseline.baseline.model.encoder.net.layers.1.2.module.0.bias', 'baseline.baseline.model.encoder.net.layers.1.2.module.2.weight', 'baseline.baseline.model.encoder.net.layers.1.2.module.2.bias', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.2.0.module.Wqkv.weight', 'baseline.baseline.model.encoder.net.layers.2.0.module.Wqkv.bias', 'baseline.baseline.model.encoder.net.layers.2.0.module.out_proj.weight', 'baseline.baseline.model.encoder.net.layers.2.0.module.out_proj.bias', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.2.2.module.0.weight', 'baseline.baseline.model.encoder.net.layers.2.2.module.0.bias', 'baseline.baseline.model.encoder.net.layers.2.2.module.2.weight', 'baseline.baseline.model.encoder.net.layers.2.2.module.2.bias', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.num_batches_tracked', 'baseline.baseline.model.decoder.context_embedding.W_placeholder', 'baseline.baseline.model.decoder.context_embedding.project_context.weight', 'baseline.baseline.model.decoder.project_node_embeddings.weight', 'baseline.baseline.model.decoder.project_fixed_context.weight', 'baseline.baseline.model.decoder.logit_attention.project_out.weight']\n</pre> In\u00a0[3]: Copied! <pre># Load the problem from TSPLib\ntsplib_dir = './tsplib'# modify this to the directory of your prepared files\nfiles = os.listdir(tsplib_dir)\nproblem_files_full = [file for file in files if file.endswith('.tsp')]\n\n# Load the optimal solution files from TSPLib\nsolution_files = [file for file in files if file.endswith('.opt.tour')]\n</pre> # Load the problem from TSPLib tsplib_dir = './tsplib'# modify this to the directory of your prepared files files = os.listdir(tsplib_dir) problem_files_full = [file for file in files if file.endswith('.tsp')]  # Load the optimal solution files from TSPLib solution_files = [file for file in files if file.endswith('.opt.tour')]  In\u00a0[4]: Copied! <pre># Utils function\ndef normalize_coord(coord:torch.Tensor) -&gt; torch.Tensor:\n    x, y = coord[:, 0], coord[:, 1]\n    x_min, x_max = x.min(), x.max()\n    y_min, y_max = y.min(), y.max()\n    \n    x_scaled = (x - x_min) / (x_max - x_min) \n    y_scaled = (y - y_min) / (y_max - y_min)\n    coord_scaled = torch.stack([x_scaled, y_scaled], dim=1)\n    return coord_scaled\n</pre> # Utils function def normalize_coord(coord:torch.Tensor) -&gt; torch.Tensor:     x, y = coord[:, 0], coord[:, 1]     x_min, x_max = x.min(), x.max()     y_min, y_max = y.min(), y.max()          x_scaled = (x - x_min) / (x_max - x_min)      y_scaled = (y - y_min) / (y_max - y_min)     coord_scaled = torch.stack([x_scaled, y_scaled], dim=1)     return coord_scaled  In\u00a0[9]: Copied! <pre># Customized problem cases\nproblem_files_custom = [\n    \"eil51.tsp\", \"berlin52.tsp\", \"st70.tsp\", \"eil76.tsp\", \n    \"pr76.tsp\", \"rat99.tsp\", \"kroA100.tsp\", \"kroB100.tsp\", \n    \"kroC100.tsp\", \"kroD100.tsp\", \"kroE100.tsp\", \"rd100.tsp\", \n    \"eil101.tsp\", \"lin105.tsp\", \"pr124.tsp\", \"bier127.tsp\", \n    \"ch130.tsp\", \"pr136.tsp\", \"pr144.tsp\", \"kroA150.tsp\", \n    \"kroB150.tsp\", \"pr152.tsp\", \"u159.tsp\", \"rat195.tsp\", \n    \"kroA200.tsp\", \"ts225.tsp\", \"tsp225.tsp\", \"pr226.tsp\"\n]\n</pre> # Customized problem cases problem_files_custom = [     \"eil51.tsp\", \"berlin52.tsp\", \"st70.tsp\", \"eil76.tsp\",      \"pr76.tsp\", \"rat99.tsp\", \"kroA100.tsp\", \"kroB100.tsp\",      \"kroC100.tsp\", \"kroD100.tsp\", \"kroE100.tsp\", \"rd100.tsp\",      \"eil101.tsp\", \"lin105.tsp\", \"pr124.tsp\", \"bier127.tsp\",      \"ch130.tsp\", \"pr136.tsp\", \"pr144.tsp\", \"kroA150.tsp\",      \"kroB150.tsp\", \"pr152.tsp\", \"u159.tsp\", \"rat195.tsp\",      \"kroA200.tsp\", \"ts225.tsp\", \"tsp225.tsp\", \"pr226.tsp\" ] In\u00a0[12]: Copied! <pre># problem_files = problem_files_full # if you want to test on all the problems\nproblem_files = problem_files_custom # if you want to test on the customized problems\n\nfor problem_idx in range(len(problem_files)):\n    problem = load_problem(os.path.join(tsplib_dir, problem_files[problem_idx]))\n\n    # NOTE: in some problem files (e.g. hk48), the node coordinates are not available\n    # we temporarily skip these problems\n    if not len(problem.node_coords):\n        continue\n\n    # Load the node coordinates\n    coords = []\n    for _, v in problem.node_coords.items():\n        coords.append(v)\n    coords = torch.tensor(coords).float().to(device) # [n, 2]\n    coords_norm = normalize_coord(coords)\n\n    # Prepare the tensordict\n    batch_size = 2\n    td = env.reset(batch_size=(batch_size,)).to(device)\n    td['locs'] = repeat(coords_norm, 'n d -&gt; b n d', b=batch_size, d=2)\n    td['action_mask'] = torch.ones(batch_size, coords_norm.shape[0], dtype=torch.bool)\n\n    # Get the solution from the policy\n    with torch.no_grad():\n        out = policy(\n            td.clone(), \n            decode_type=\"greedy\", \n            return_actions=True,\n            num_starts=0\n        )\n\n    # Calculate the cost on the original scale\n    td['locs'] = repeat(coords, 'n d -&gt; b n d', b=batch_size, d=2)\n    neg_reward = env.get_reward(td, out['actions'])\n    cost = ceil(-1 * neg_reward[0].item())\n\n    # Check if there exists an optimal solution\n    try:\n        # Load the optimal solution\n        solution = load_solution(os.path.join(tsplib_dir, problem_files[problem_idx][:-4] + '.opt.tour'))\n        matches = re.findall(r'\\((.*?)\\)', solution.comment)\n\n        # NOTE: in some problem solution file (e.g. ch130), the optimal cost is not writen with a brace\n        # we temporarily skip to calculate the gap for these problems\n        optimal_cost = int(matches[0])\n        gap = (cost - optimal_cost) / optimal_cost\n        print(f'Problem: {problem_files[problem_idx][:-4]:&lt;10} Cost: {cost:&lt;10} Optimal Cost: {optimal_cost:&lt;10}\\t Gap: {gap:.2%}')\n    except:\n        continue\n    finally:\n        print(f'problem: {problem_files[problem_idx][:-4]:&lt;10} cost: {cost:&lt;10}')\n</pre> # problem_files = problem_files_full # if you want to test on all the problems problem_files = problem_files_custom # if you want to test on the customized problems  for problem_idx in range(len(problem_files)):     problem = load_problem(os.path.join(tsplib_dir, problem_files[problem_idx]))      # NOTE: in some problem files (e.g. hk48), the node coordinates are not available     # we temporarily skip these problems     if not len(problem.node_coords):         continue      # Load the node coordinates     coords = []     for _, v in problem.node_coords.items():         coords.append(v)     coords = torch.tensor(coords).float().to(device) # [n, 2]     coords_norm = normalize_coord(coords)      # Prepare the tensordict     batch_size = 2     td = env.reset(batch_size=(batch_size,)).to(device)     td['locs'] = repeat(coords_norm, 'n d -&gt; b n d', b=batch_size, d=2)     td['action_mask'] = torch.ones(batch_size, coords_norm.shape[0], dtype=torch.bool)      # Get the solution from the policy     with torch.no_grad():         out = policy(             td.clone(),              decode_type=\"greedy\",              return_actions=True,             num_starts=0         )      # Calculate the cost on the original scale     td['locs'] = repeat(coords, 'n d -&gt; b n d', b=batch_size, d=2)     neg_reward = env.get_reward(td, out['actions'])     cost = ceil(-1 * neg_reward[0].item())      # Check if there exists an optimal solution     try:         # Load the optimal solution         solution = load_solution(os.path.join(tsplib_dir, problem_files[problem_idx][:-4] + '.opt.tour'))         matches = re.findall(r'\\((.*?)\\)', solution.comment)          # NOTE: in some problem solution file (e.g. ch130), the optimal cost is not writen with a brace         # we temporarily skip to calculate the gap for these problems         optimal_cost = int(matches[0])         gap = (cost - optimal_cost) / optimal_cost         print(f'Problem: {problem_files[problem_idx][:-4]:&lt;10} Cost: {cost:&lt;10} Optimal Cost: {optimal_cost:&lt;10}\\t Gap: {gap:.2%}')     except:         continue     finally:         print(f'problem: {problem_files[problem_idx][:-4]:&lt;10} cost: {cost:&lt;10}') <pre>/tmp/ipykernel_3883036/2632546508.py:5: DeprecationWarning: Call to deprecated function (or staticmethod) load_problem. (Will be removed in newer versions. Use `tsplib95.load` instead.) -- Deprecated since version 7.0.0.\n  problem = load_problem(os.path.join(tsplib_dir, problem_files[problem_idx]))\n/tmp/ipykernel_3883036/2632546508.py:43: DeprecationWarning: Call to deprecated function (or staticmethod) load_solution. (Will be removed in newer versions. Use `tsplib95.load` instead.) -- Deprecated since version 7.0.0.\n  solution = load_solution(os.path.join(tsplib_dir, problem_files[problem_idx][:-4] + '.opt.tour'))\n</pre> <pre>Problem: eil51      Cost: 493        Optimal Cost: 426       \t Gap: 15.73%\nproblem: eil51      cost: 493       \nproblem: berlin52   cost: 8957      \nProblem: st70       Cost: 806        Optimal Cost: 675       \t Gap: 19.41%\nproblem: st70       cost: 806       \nProblem: eil76      Cost: 693        Optimal Cost: 538       \t Gap: 28.81%\nproblem: eil76      cost: 693       \nProblem: pr76       Cost: 132292     Optimal Cost: 108159    \t Gap: 22.31%\nproblem: pr76       cost: 132292    \nproblem: rat99      cost: 2053      \nProblem: kroA100    Cost: 30791      Optimal Cost: 21282     \t Gap: 44.68%\nproblem: kroA100    cost: 30791     \nproblem: kroB100    cost: 30347     \nProblem: kroC100    Cost: 28339      Optimal Cost: 20749     \t Gap: 36.58%\nproblem: kroC100    cost: 28339     \nProblem: kroD100    Cost: 27600      Optimal Cost: 21294     \t Gap: 29.61%\nproblem: kroD100    cost: 27600     \nproblem: kroE100    cost: 28396     \nProblem: rd100      Cost: 10695      Optimal Cost: 7910      \t Gap: 35.21%\nproblem: rd100      cost: 10695     \nproblem: eil101     cost: 919       \nProblem: lin105     Cost: 21796      Optimal Cost: 14379     \t Gap: 51.58%\nproblem: lin105     cost: 21796     \nproblem: pr124      cost: 75310     \nproblem: bier127    cost: 177471    \nproblem: ch130      cost: 8169      \nproblem: pr136      cost: 135974    \nproblem: pr144      cost: 71599     \nproblem: kroA150    cost: 40376     \nproblem: kroB150    cost: 37076     \nproblem: pr152      cost: 94805     \nproblem: u159       cost: 64768     \nproblem: rat195     cost: 4465      \nproblem: kroA200    cost: 44181     \nproblem: ts225      cost: 210475    \nProblem: tsp225     Cost: 6212       Optimal Cost: 3919      \t Gap: 58.51%\nproblem: tsp225     cost: 6212      \nproblem: pr226      cost: 98849     \n</pre> In\u00a0[16]: Copied! <pre># problem_files = problem_files_full # if you want to test on all the problems\nproblem_files = problem_files_custom # if you want to test on the customized problems\n\n# Import augmented utils\nfrom rl4co.data.transforms import (\n    StateAugmentation as SymmetricStateAugmentation)\nfrom rl4co.utils.ops import batchify, unbatchify\n\nnum_augment = 100\naugmentation = SymmetricStateAugmentation(num_augment=num_augment)\n\nfor problem_idx in range(len(problem_files)):\n    problem = load_problem(os.path.join(tsplib_dir, problem_files[problem_idx]))\n\n    # NOTE: in some problem files (e.g. hk48), the node coordinates are not available\n    # we temporarily skip these problems\n    if not len(problem.node_coords):\n        continue\n\n    # Load the node coordinates\n    coords = []\n    for _, v in problem.node_coords.items():\n        coords.append(v)\n    coords = torch.tensor(coords).float().to(device) # [n, 2]\n    coords_norm = normalize_coord(coords)\n\n    # Prepare the tensordict\n    batch_size = 2\n    td = env.reset(batch_size=(batch_size,)).to(device)\n    td['locs'] = repeat(coords_norm, 'n d -&gt; b n d', b=batch_size, d=2)\n    td['action_mask'] = torch.ones(batch_size, coords_norm.shape[0], dtype=torch.bool)\n\n    # Augmentation\n    td = augmentation(td)\n\n    # Get the solution from the policy\n    with torch.no_grad():\n        out = policy(\n            td.clone(), \n            decode_type=\"greedy\", \n            return_actions=True,\n            num_starts=0\n        )\n\n    # Calculate the cost on the original scale\n    coords_repeat = repeat(coords, 'n d -&gt; b n d', b=batch_size, d=2)\n    td['locs'] = batchify(coords_repeat, num_augment)\n    reward = env.get_reward(td, out['actions'])\n    reward = unbatchify(reward, num_augment)\n    cost = ceil(-1 * torch.max(reward).item())\n\n    # Check if there exists an optimal solution\n    try:\n        # Load the optimal solution\n        solution = load_solution(os.path.join(tsplib_dir, problem_files[problem_idx][:-4] + '.opt.tour'))\n        matches = re.findall(r'\\((.*?)\\)', solution.comment)\n\n        # NOTE: in some problem solution file (e.g. ch130), the optimal cost is not writen with a brace\n        # we temporarily skip to calculate the gap for these problems\n        optimal_cost = int(matches[0])\n        gap = (cost - optimal_cost) / optimal_cost\n        print(f'Problem: {problem_files[problem_idx][:-4]}\\t Cost: {cost}\\t Optimal Cost: {optimal_cost}\\t Gap: {gap:.2%}')\n    except:\n        continue\n    finally:\n        print(f'problem: {problem_files[problem_idx][:-4]}\\t cost: {cost}\\t')\n</pre> # problem_files = problem_files_full # if you want to test on all the problems problem_files = problem_files_custom # if you want to test on the customized problems  # Import augmented utils from rl4co.data.transforms import (     StateAugmentation as SymmetricStateAugmentation) from rl4co.utils.ops import batchify, unbatchify  num_augment = 100 augmentation = SymmetricStateAugmentation(num_augment=num_augment)  for problem_idx in range(len(problem_files)):     problem = load_problem(os.path.join(tsplib_dir, problem_files[problem_idx]))      # NOTE: in some problem files (e.g. hk48), the node coordinates are not available     # we temporarily skip these problems     if not len(problem.node_coords):         continue      # Load the node coordinates     coords = []     for _, v in problem.node_coords.items():         coords.append(v)     coords = torch.tensor(coords).float().to(device) # [n, 2]     coords_norm = normalize_coord(coords)      # Prepare the tensordict     batch_size = 2     td = env.reset(batch_size=(batch_size,)).to(device)     td['locs'] = repeat(coords_norm, 'n d -&gt; b n d', b=batch_size, d=2)     td['action_mask'] = torch.ones(batch_size, coords_norm.shape[0], dtype=torch.bool)      # Augmentation     td = augmentation(td)      # Get the solution from the policy     with torch.no_grad():         out = policy(             td.clone(),              decode_type=\"greedy\",              return_actions=True,             num_starts=0         )      # Calculate the cost on the original scale     coords_repeat = repeat(coords, 'n d -&gt; b n d', b=batch_size, d=2)     td['locs'] = batchify(coords_repeat, num_augment)     reward = env.get_reward(td, out['actions'])     reward = unbatchify(reward, num_augment)     cost = ceil(-1 * torch.max(reward).item())      # Check if there exists an optimal solution     try:         # Load the optimal solution         solution = load_solution(os.path.join(tsplib_dir, problem_files[problem_idx][:-4] + '.opt.tour'))         matches = re.findall(r'\\((.*?)\\)', solution.comment)          # NOTE: in some problem solution file (e.g. ch130), the optimal cost is not writen with a brace         # we temporarily skip to calculate the gap for these problems         optimal_cost = int(matches[0])         gap = (cost - optimal_cost) / optimal_cost         print(f'Problem: {problem_files[problem_idx][:-4]}\\t Cost: {cost}\\t Optimal Cost: {optimal_cost}\\t Gap: {gap:.2%}')     except:         continue     finally:         print(f'problem: {problem_files[problem_idx][:-4]}\\t cost: {cost}\\t') <pre>/tmp/ipykernel_3883036/2898406631.py:13: DeprecationWarning: Call to deprecated function (or staticmethod) load_problem. (Will be removed in newer versions. Use `tsplib95.load` instead.) -- Deprecated since version 7.0.0.\n  problem = load_problem(os.path.join(tsplib_dir, problem_files[problem_idx]))\n/tmp/ipykernel_3883036/2898406631.py:56: DeprecationWarning: Call to deprecated function (or staticmethod) load_solution. (Will be removed in newer versions. Use `tsplib95.load` instead.) -- Deprecated since version 7.0.0.\n  solution = load_solution(os.path.join(tsplib_dir, problem_files[problem_idx][:-4] + '.opt.tour'))\n</pre> <pre>Problem: eil51\t Cost: 457\t Optimal Cost: 426\t Gap: 7.28%\nproblem: eil51\t cost: 457\t\nproblem: berlin52\t cost: 8256\t\nProblem: st70\t Cost: 777\t Optimal Cost: 675\t Gap: 15.11%\nproblem: st70\t cost: 777\t\nProblem: eil76\t Cost: 652\t Optimal Cost: 538\t Gap: 21.19%\nproblem: eil76\t cost: 652\t\nProblem: pr76\t Cost: 124939\t Optimal Cost: 108159\t Gap: 15.51%\nproblem: pr76\t cost: 124939\t\nproblem: rat99\t cost: 1614\t\nProblem: kroA100\t Cost: 27694\t Optimal Cost: 21282\t Gap: 30.13%\nproblem: kroA100\t cost: 27694\t\nproblem: kroB100\t cost: 28244\t\nProblem: kroC100\t Cost: 25032\t Optimal Cost: 20749\t Gap: 20.64%\nproblem: kroC100\t cost: 25032\t\nProblem: kroD100\t Cost: 26811\t Optimal Cost: 21294\t Gap: 25.91%\nproblem: kroD100\t cost: 26811\t\nproblem: kroE100\t cost: 27831\t\nProblem: rd100\t Cost: 9657\t Optimal Cost: 7910\t Gap: 22.09%\nproblem: rd100\t cost: 9657\t\nproblem: eil101\t cost: 781\t\nProblem: lin105\t Cost: 18769\t Optimal Cost: 14379\t Gap: 30.53%\nproblem: lin105\t cost: 18769\t\nproblem: pr124\t cost: 72115\t\nproblem: bier127\t cost: 154518\t\nproblem: ch130\t cost: 7543\t\nproblem: pr136\t cost: 128134\t\nproblem: pr144\t cost: 69755\t\nproblem: kroA150\t cost: 35967\t\nproblem: kroB150\t cost: 35196\t\nproblem: pr152\t cost: 88602\t\nproblem: u159\t cost: 59484\t\nproblem: rat195\t cost: 3631\t\nproblem: kroA200\t cost: 42061\t\nproblem: ts225\t cost: 196545\t\nProblem: tsp225\t Cost: 5680\t Optimal Cost: 3919\t Gap: 44.93%\nproblem: tsp225\t cost: 5680\t\nproblem: pr226\t cost: 94540\t\n</pre> In\u00a0[18]: Copied! <pre># problem_files = problem_files_full # if you want to test on all the problems\nproblem_files = problem_files_custom # if you want to test on the customized problems\n\n# Parameters for sampling\nnum_samples = 100\nsoftmax_temp = 0.05\n\nfor problem_idx in range(len(problem_files)):\n    problem = load_problem(os.path.join(tsplib_dir, problem_files[problem_idx]))\n\n    # NOTE: in some problem files (e.g. hk48), the node coordinates are not available\n    # we temporarily skip these problems\n    if not len(problem.node_coords):\n        continue\n\n    # Load the node coordinates\n    coords = []\n    for _, v in problem.node_coords.items():\n        coords.append(v)\n    coords = torch.tensor(coords).float().to(device) # [n, 2]\n    coords_norm = normalize_coord(coords)\n\n    # Prepare the tensordict\n    batch_size = 2\n    td = env.reset(batch_size=(batch_size,)).to(device)\n    td['locs'] = repeat(coords_norm, 'n d -&gt; b n d', b=batch_size, d=2)\n    td['action_mask'] = torch.ones(batch_size, coords_norm.shape[0], dtype=torch.bool)\n\n    # Sampling\n    td = batchify(td, num_samples)\n\n    # Get the solution from the policy\n    with torch.no_grad():\n        out = policy(\n            td.clone(), \n            decode_type=\"sampling\", \n            return_actions=True,\n            num_starts=0,\n            softmax_temp=softmax_temp\n        )\n\n    # Calculate the cost on the original scale\n    coords_repeat = repeat(coords, 'n d -&gt; b n d', b=batch_size, d=2)\n    td['locs'] = batchify(coords_repeat, num_samples)\n    reward = env.get_reward(td, out['actions'])\n    reward = unbatchify(reward, num_samples)\n    cost = ceil(-1 * torch.max(reward).item())\n\n    # Check if there exists an optimal solution\n    try:\n        # Load the optimal solution\n        solution = load_solution(os.path.join(tsplib_dir, problem_files[problem_idx][:-4] + '.opt.tour'))\n        matches = re.findall(r'\\((.*?)\\)', solution.comment)\n\n        # NOTE: in some problem solution file (e.g. ch130), the optimal cost is not writen with a brace\n        # we temporarily skip to calculate the gap for these problems\n        optimal_cost = int(matches[0])\n        gap = (cost - optimal_cost) / optimal_cost\n        print(f'Problem: {problem_files[problem_idx][:-4]}\\t Cost: {cost}\\t Optimal Cost: {optimal_cost}\\t Gap: {gap:.2%}')\n    except:\n        continue\n    finally:\n        print(f'problem: {problem_files[problem_idx][:-4]}\\t cost: {cost}\\t')\n</pre> # problem_files = problem_files_full # if you want to test on all the problems problem_files = problem_files_custom # if you want to test on the customized problems  # Parameters for sampling num_samples = 100 softmax_temp = 0.05  for problem_idx in range(len(problem_files)):     problem = load_problem(os.path.join(tsplib_dir, problem_files[problem_idx]))      # NOTE: in some problem files (e.g. hk48), the node coordinates are not available     # we temporarily skip these problems     if not len(problem.node_coords):         continue      # Load the node coordinates     coords = []     for _, v in problem.node_coords.items():         coords.append(v)     coords = torch.tensor(coords).float().to(device) # [n, 2]     coords_norm = normalize_coord(coords)      # Prepare the tensordict     batch_size = 2     td = env.reset(batch_size=(batch_size,)).to(device)     td['locs'] = repeat(coords_norm, 'n d -&gt; b n d', b=batch_size, d=2)     td['action_mask'] = torch.ones(batch_size, coords_norm.shape[0], dtype=torch.bool)      # Sampling     td = batchify(td, num_samples)      # Get the solution from the policy     with torch.no_grad():         out = policy(             td.clone(),              decode_type=\"sampling\",              return_actions=True,             num_starts=0,             softmax_temp=softmax_temp         )      # Calculate the cost on the original scale     coords_repeat = repeat(coords, 'n d -&gt; b n d', b=batch_size, d=2)     td['locs'] = batchify(coords_repeat, num_samples)     reward = env.get_reward(td, out['actions'])     reward = unbatchify(reward, num_samples)     cost = ceil(-1 * torch.max(reward).item())      # Check if there exists an optimal solution     try:         # Load the optimal solution         solution = load_solution(os.path.join(tsplib_dir, problem_files[problem_idx][:-4] + '.opt.tour'))         matches = re.findall(r'\\((.*?)\\)', solution.comment)          # NOTE: in some problem solution file (e.g. ch130), the optimal cost is not writen with a brace         # we temporarily skip to calculate the gap for these problems         optimal_cost = int(matches[0])         gap = (cost - optimal_cost) / optimal_cost         print(f'Problem: {problem_files[problem_idx][:-4]}\\t Cost: {cost}\\t Optimal Cost: {optimal_cost}\\t Gap: {gap:.2%}')     except:         continue     finally:         print(f'problem: {problem_files[problem_idx][:-4]}\\t cost: {cost}\\t') <pre>/tmp/ipykernel_3883036/2154301274.py:9: DeprecationWarning: Call to deprecated function (or staticmethod) load_problem. (Will be removed in newer versions. Use `tsplib95.load` instead.) -- Deprecated since version 7.0.0.\n  problem = load_problem(os.path.join(tsplib_dir, problem_files[problem_idx]))\n/tmp/ipykernel_3883036/2154301274.py:53: DeprecationWarning: Call to deprecated function (or staticmethod) load_solution. (Will be removed in newer versions. Use `tsplib95.load` instead.) -- Deprecated since version 7.0.0.\n  solution = load_solution(os.path.join(tsplib_dir, problem_files[problem_idx][:-4] + '.opt.tour'))\n</pre> <pre>Problem: eil51\t Cost: 482\t Optimal Cost: 426\t Gap: 13.15%\nproblem: eil51\t cost: 482\t\nproblem: berlin52\t cost: 8955\t\nProblem: st70\t Cost: 794\t Optimal Cost: 675\t Gap: 17.63%\nproblem: st70\t cost: 794\t\nProblem: eil76\t Cost: 673\t Optimal Cost: 538\t Gap: 25.09%\nproblem: eil76\t cost: 673\t\nProblem: pr76\t Cost: 127046\t Optimal Cost: 108159\t Gap: 17.46%\nproblem: pr76\t cost: 127046\t\nproblem: rat99\t cost: 1886\t\nProblem: kroA100\t Cost: 29517\t Optimal Cost: 21282\t Gap: 38.69%\nproblem: kroA100\t cost: 29517\t\nproblem: kroB100\t cost: 28892\t\nProblem: kroC100\t Cost: 26697\t Optimal Cost: 20749\t Gap: 28.67%\nproblem: kroC100\t cost: 26697\t\nProblem: kroD100\t Cost: 27122\t Optimal Cost: 21294\t Gap: 27.37%\nproblem: kroD100\t cost: 27122\t\nproblem: kroE100\t cost: 28016\t\nProblem: rd100\t Cost: 10424\t Optimal Cost: 7910\t Gap: 31.78%\nproblem: rd100\t cost: 10424\t\nproblem: eil101\t cost: 837\t\nProblem: lin105\t Cost: 19618\t Optimal Cost: 14379\t Gap: 36.44%\nproblem: lin105\t cost: 19618\t\nproblem: pr124\t cost: 74699\t\nproblem: bier127\t cost: 170255\t\nproblem: ch130\t cost: 7985\t\nproblem: pr136\t cost: 129964\t\nproblem: pr144\t cost: 70477\t\nproblem: kroA150\t cost: 37185\t\nproblem: kroB150\t cost: 35172\t\nproblem: pr152\t cost: 97244\t\nproblem: u159\t cost: 59792\t\nproblem: rat195\t cost: 4325\t\nproblem: kroA200\t cost: 42059\t\nproblem: ts225\t cost: 205982\t\nProblem: tsp225\t Cost: 5970\t Optimal Cost: 3919\t Gap: 52.33%\nproblem: tsp225\t cost: 5970\t\nproblem: pr226\t cost: 103135\t\n</pre>"},{"location":"examples/datasets/1-test-on-tsplib/#test-model-on-tsplib","title":"Test Model on TSPLib\u00b6","text":"<p>In this notebook, we will test the trained model's performance on the TSPLib benchmark. We will use the trained model from the previous notebook.</p> <p>TSPLib is a library of sample instances for the TSP (and related problems) from various sources and of various types. In the TSPLib, there are several problems, including TSP, HCP, ATSP, etc. In this notebook, we will focus on testing the model on the TSP problem.</p>"},{"location":"examples/datasets/1-test-on-tsplib/#before-we-start","title":"Before we start\u00b6","text":"<p>Before we test the model on TSPLib dataset, we need to prepare the dataset first by the following steps:</p> <p>Step 1. You may come to here to download the symmetric traveling salesman problem data in TSPLib dataset and unzip to a folder;</p> <p>Note that the downloaded data is <code>gzip</code> file with the following file tree:</p> <pre><code>.\n\u2514\u2500\u2500 ALL_tsp/\n    \u251c\u2500\u2500 a280.opt.tour.gz\n    \u251c\u2500\u2500 a280.tsp.gz\n    \u251c\u2500\u2500 ali535.tsp.gz\n    \u2514\u2500\u2500 ... (other problems)\n</code></pre> <p>We need to unzip the <code>gzip</code> file to get the <code>.tsp</code> and <code>.opt.tour</code> files. We can use the following command to unzip them to the same folder:</p> <pre>find . -name \"*.gz\" -exec gunzip {} +\n</pre> <p>After doing this, we will get the following file tree:</p> <pre><code>.\n\u2514\u2500\u2500 ALL_tsp/\n    \u251c\u2500\u2500 a280.opt.tour\n    \u251c\u2500\u2500 a280.tsp\n    \u251c\u2500\u2500 ali535.tsp\n    \u2514\u2500\u2500 ... (other problems)\n</code></pre> <p>Step 2. To read the TSPLib problem and optimal solution, we choose to use the <code>tsplib95</code> package. Use <code>pip install tsplib95</code> to install the package.</p>"},{"location":"examples/datasets/1-test-on-tsplib/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install the package from PyPI. Remember to choose a GPU runtime for faster training!</p> <p>Note: You may need to restart the runtime in Colab after this</p>"},{"location":"examples/datasets/1-test-on-tsplib/#imports","title":"Imports\u00b6","text":""},{"location":"examples/datasets/1-test-on-tsplib/#load-a-trained-model","title":"Load a trained model\u00b6","text":""},{"location":"examples/datasets/1-test-on-tsplib/#load-tsp-problems","title":"Load tsp problems\u00b6","text":"<p>Note that in the TSPLib, only part of the problems have optimal solutions with the same problem name but with <code>.opt.tour</code> suffix. For example, <code>a280.tsp</code> has the optimal solution <code>a280.opt.tour</code>.</p>"},{"location":"examples/datasets/1-test-on-tsplib/#test-the-greedy","title":"Test the greedy\u00b6","text":"<p>Note that run all experiments will take long time and require large VRAM. For simple testing, we can use a subset of the problems.</p>"},{"location":"examples/datasets/1-test-on-tsplib/#test-the-augmentation","title":"Test the Augmentation\u00b6","text":""},{"location":"examples/datasets/1-test-on-tsplib/#test-the-sampling","title":"Test the Sampling\u00b6","text":""},{"location":"examples/datasets/2-test-on-cvrplib/","title":"Test Model on VRPLib","text":"In\u00a0[\u00a0]: Copied! <pre># !pip install rl4co[graph] # include torch-geometric\n\n## NOTE: to install latest version from Github (may be unstable) install from source instead:\n# !pip install git+https://github.com/ai4co/rl4co.git\n</pre> # !pip install rl4co[graph] # include torch-geometric  ## NOTE: to install latest version from Github (may be unstable) install from source instead: # !pip install git+https://github.com/ai4co/rl4co.git In\u00a0[\u00a0]: Copied! <pre># Install the `tsplib95` package\n# !pip install vrplib\n</pre> # Install the `tsplib95` package # !pip install vrplib In\u00a0[8]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport os\nimport re\nimport torch\nimport vrplib\n\nfrom rl4co.envs import TSPEnv, CVRPEnv\nfrom rl4co.models.zoo.am import AttentionModel\nfrom rl4co.utils.trainer import RL4COTrainer\nfrom rl4co.utils.decoding import get_log_likelihood\nfrom rl4co.models.zoo import EAS, EASLay, EASEmb, ActiveSearch\n\nfrom tqdm import tqdm\nfrom math import ceil\nfrom einops import repeat\n</pre> %load_ext autoreload %autoreload 2  import os import re import torch import vrplib  from rl4co.envs import TSPEnv, CVRPEnv from rl4co.models.zoo.am import AttentionModel from rl4co.utils.trainer import RL4COTrainer from rl4co.utils.decoding import get_log_likelihood from rl4co.models.zoo import EAS, EASLay, EASEmb, ActiveSearch  from tqdm import tqdm from math import ceil from einops import repeat <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[15]: Copied! <pre># Load from checkpoint; alternatively, simply instantiate a new model\n# Note the model is trained for CVRP problem\ncheckpoint_path = \"../cvrp-20.ckpt\" # modify the path to your checkpoint file\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# load checkpoint\n# checkpoint = torch.load(checkpoint_path)\n\nlit_model = AttentionModel.load_from_checkpoint(checkpoint_path, load_baseline=False)\npolicy, env = lit_model.policy, lit_model.env\npolicy = policy.to(device)\n</pre> # Load from checkpoint; alternatively, simply instantiate a new model # Note the model is trained for CVRP problem checkpoint_path = \"../cvrp-20.ckpt\" # modify the path to your checkpoint file  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # load checkpoint # checkpoint = torch.load(checkpoint_path)  lit_model = AttentionModel.load_from_checkpoint(checkpoint_path, load_baseline=False) policy, env = lit_model.policy, lit_model.env policy = policy.to(device) <pre>/home/cbhua/miniconda3/envs/rl4co-user/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n/home/cbhua/miniconda3/envs/rl4co-user/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n/home/cbhua/miniconda3/envs/rl4co-user/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:177: Found keys that are not in the model state dict but in the checkpoint: ['baseline.baseline.model.encoder.init_embedding.init_embed.weight', 'baseline.baseline.model.encoder.init_embedding.init_embed.bias', 'baseline.baseline.model.encoder.init_embedding.init_embed_depot.weight', 'baseline.baseline.model.encoder.init_embedding.init_embed_depot.bias', 'baseline.baseline.model.encoder.net.layers.0.0.module.Wqkv.weight', 'baseline.baseline.model.encoder.net.layers.0.0.module.Wqkv.bias', 'baseline.baseline.model.encoder.net.layers.0.0.module.out_proj.weight', 'baseline.baseline.model.encoder.net.layers.0.0.module.out_proj.bias', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.0.2.module.0.weight', 'baseline.baseline.model.encoder.net.layers.0.2.module.0.bias', 'baseline.baseline.model.encoder.net.layers.0.2.module.2.weight', 'baseline.baseline.model.encoder.net.layers.0.2.module.2.bias', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.1.0.module.Wqkv.weight', 'baseline.baseline.model.encoder.net.layers.1.0.module.Wqkv.bias', 'baseline.baseline.model.encoder.net.layers.1.0.module.out_proj.weight', 'baseline.baseline.model.encoder.net.layers.1.0.module.out_proj.bias', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.1.2.module.0.weight', 'baseline.baseline.model.encoder.net.layers.1.2.module.0.bias', 'baseline.baseline.model.encoder.net.layers.1.2.module.2.weight', 'baseline.baseline.model.encoder.net.layers.1.2.module.2.bias', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.2.0.module.Wqkv.weight', 'baseline.baseline.model.encoder.net.layers.2.0.module.Wqkv.bias', 'baseline.baseline.model.encoder.net.layers.2.0.module.out_proj.weight', 'baseline.baseline.model.encoder.net.layers.2.0.module.out_proj.bias', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.2.2.module.0.weight', 'baseline.baseline.model.encoder.net.layers.2.2.module.0.bias', 'baseline.baseline.model.encoder.net.layers.2.2.module.2.weight', 'baseline.baseline.model.encoder.net.layers.2.2.module.2.bias', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.num_batches_tracked', 'baseline.baseline.model.decoder.context_embedding.project_context.weight', 'baseline.baseline.model.decoder.project_node_embeddings.weight', 'baseline.baseline.model.decoder.project_fixed_context.weight', 'baseline.baseline.model.decoder.logit_attention.project_out.weight']\n</pre> In\u00a0[11]: Copied! <pre>problem_names = vrplib.list_names(low=50, high=200, vrp_type='cvrp') \n\ninstances = [] # Collect Set A, B, E, F, M datasets\nfor name in problem_names:\n    if 'A' in name:\n        instances.append(name)\n    elif 'B' in name:\n        instances.append(name)\n    elif 'E' in name:\n        instances.append(name)\n    elif 'F' in name:\n        instances.append(name)\n    elif 'M' in name and 'CMT' not in name:\n        instances.append(name)\n\n# Modify the path you want to save \n# Note: we don't have to create this folder in advance\npath_to_save = './vrplib/' \n\ntry:\n    os.makedirs(path_to_save)\n    for instance in tqdm(instances):\n        vrplib.download_instance(instance, path_to_save)\n        vrplib.download_solution(instance, path_to_save)\nexcept: # already exist\n    pass\n</pre> problem_names = vrplib.list_names(low=50, high=200, vrp_type='cvrp')   instances = [] # Collect Set A, B, E, F, M datasets for name in problem_names:     if 'A' in name:         instances.append(name)     elif 'B' in name:         instances.append(name)     elif 'E' in name:         instances.append(name)     elif 'F' in name:         instances.append(name)     elif 'M' in name and 'CMT' not in name:         instances.append(name)  # Modify the path you want to save  # Note: we don't have to create this folder in advance path_to_save = './vrplib/'   try:     os.makedirs(path_to_save)     for instance in tqdm(instances):         vrplib.download_instance(instance, path_to_save)         vrplib.download_solution(instance, path_to_save) except: # already exist     pass  <pre>  0%|          | 0/37 [00:00&lt;?, ?it/s]</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 37/37 [00:42&lt;00:00,  1.16s/it]\n</pre> In\u00a0[12]: Copied! <pre># Utils function\ndef normalize_coord(coord:torch.Tensor) -&gt; torch.Tensor:\n    x, y = coord[:, 0], coord[:, 1]\n    x_min, x_max = x.min(), x.max()\n    y_min, y_max = y.min(), y.max()\n    \n    x_scaled = (x - x_min) / (x_max - x_min) \n    y_scaled = (y - y_min) / (y_max - y_min)\n    coord_scaled = torch.stack([x_scaled, y_scaled], dim=1)\n    return coord_scaled\n</pre> # Utils function def normalize_coord(coord:torch.Tensor) -&gt; torch.Tensor:     x, y = coord[:, 0], coord[:, 1]     x_min, x_max = x.min(), x.max()     y_min, y_max = y.min(), y.max()          x_scaled = (x - x_min) / (x_max - x_min)      y_scaled = (y - y_min) / (y_max - y_min)     coord_scaled = torch.stack([x_scaled, y_scaled], dim=1)     return coord_scaled  In\u00a0[18]: Copied! <pre>for instance in instances:\n    problem = vrplib.read_instance(os.path.join(path_to_save, instance+'.vrp'))\n\n    coords = torch.tensor(problem['node_coord']).float()\n    coords_norm = normalize_coord(coords)\n    demand = torch.tensor(problem['demand'][1:]).float()\n    capacity = problem['capacity']\n    n = coords.shape[0]\n\n    # Prepare the tensordict\n    batch_size = 2\n    td = env.reset(batch_size=(batch_size,)).to(device)\n    td['locs'] = repeat(coords_norm, 'n d -&gt; b n d', b=batch_size, d=2)\n    td['demand'] = repeat(demand, 'n -&gt; b n', b=batch_size) / capacity\n    td['visited'] = torch.zeros((batch_size, 1, n), dtype=torch.uint8)\n    action_mask = torch.ones(batch_size, n, dtype=torch.bool)\n    action_mask[:, 0] = False\n    td['action_mask']  = action_mask\n\n    # Get the solution from the policy\n    with torch.no_grad():\n        out = policy(td.clone(), decode_type='greedy', return_actions=True)\n\n    # Calculate the cost on the original scale\n    td['locs'] = repeat(coords, 'n d -&gt; b n d', b=batch_size, d=2)\n    neg_reward = env.get_reward(td, out['actions'])\n    cost = ceil(-1 * neg_reward[0].item())\n\n    # Load the optimal cost\n    solution = vrplib.read_solution(os.path.join(path_to_save, instance+'.sol'))\n    optimal_cost = solution['cost']\n\n    # Calculate the gap and print\n    gap = (cost - optimal_cost) / optimal_cost\n    print(f'Problem: {instance:&lt;15} Cost: {cost:&lt;10} Optimal Cost: {optimal_cost:&lt;10}\\t Gap: {gap:.2%}')\n</pre> for instance in instances:     problem = vrplib.read_instance(os.path.join(path_to_save, instance+'.vrp'))      coords = torch.tensor(problem['node_coord']).float()     coords_norm = normalize_coord(coords)     demand = torch.tensor(problem['demand'][1:]).float()     capacity = problem['capacity']     n = coords.shape[0]      # Prepare the tensordict     batch_size = 2     td = env.reset(batch_size=(batch_size,)).to(device)     td['locs'] = repeat(coords_norm, 'n d -&gt; b n d', b=batch_size, d=2)     td['demand'] = repeat(demand, 'n -&gt; b n', b=batch_size) / capacity     td['visited'] = torch.zeros((batch_size, 1, n), dtype=torch.uint8)     action_mask = torch.ones(batch_size, n, dtype=torch.bool)     action_mask[:, 0] = False     td['action_mask']  = action_mask      # Get the solution from the policy     with torch.no_grad():         out = policy(td.clone(), decode_type='greedy', return_actions=True)      # Calculate the cost on the original scale     td['locs'] = repeat(coords, 'n d -&gt; b n d', b=batch_size, d=2)     neg_reward = env.get_reward(td, out['actions'])     cost = ceil(-1 * neg_reward[0].item())      # Load the optimal cost     solution = vrplib.read_solution(os.path.join(path_to_save, instance+'.sol'))     optimal_cost = solution['cost']      # Calculate the gap and print     gap = (cost - optimal_cost) / optimal_cost     print(f'Problem: {instance:&lt;15} Cost: {cost:&lt;10} Optimal Cost: {optimal_cost:&lt;10}\\t Gap: {gap:.2%}') <pre>Problem: A-n53-k7        Cost: 1371       Optimal Cost: 1010      \t Gap: 35.74%\nProblem: A-n54-k7        Cost: 1426       Optimal Cost: 1167      \t Gap: 22.19%\nProblem: A-n55-k9        Cost: 1333       Optimal Cost: 1073      \t Gap: 24.23%\nProblem: A-n60-k9        Cost: 1728       Optimal Cost: 1354      \t Gap: 27.62%\nProblem: A-n61-k9        Cost: 1297       Optimal Cost: 1034      \t Gap: 25.44%\nProblem: A-n62-k8        Cost: 1818       Optimal Cost: 1288      \t Gap: 41.15%\nProblem: A-n63-k9        Cost: 2166       Optimal Cost: 1616      \t Gap: 34.03%\nProblem: A-n63-k10       Cost: 1698       Optimal Cost: 1314      \t Gap: 29.22%\nProblem: A-n64-k9        Cost: 1805       Optimal Cost: 1401      \t Gap: 28.84%\nProblem: A-n65-k9        Cost: 1592       Optimal Cost: 1174      \t Gap: 35.60%\nProblem: A-n69-k9        Cost: 1641       Optimal Cost: 1159      \t Gap: 41.59%\nProblem: A-n80-k10       Cost: 2230       Optimal Cost: 1763      \t Gap: 26.49%\nProblem: B-n51-k7        Cost: 1270       Optimal Cost: 1032      \t Gap: 23.06%\nProblem: B-n52-k7        Cost: 994        Optimal Cost: 747       \t Gap: 33.07%\nProblem: B-n56-k7        Cost: 931        Optimal Cost: 707       \t Gap: 31.68%\nProblem: B-n57-k7        Cost: 1422       Optimal Cost: 1153      \t Gap: 23.33%\nProblem: B-n57-k9        Cost: 1889       Optimal Cost: 1598      \t Gap: 18.21%\nProblem: B-n63-k10       Cost: 1807       Optimal Cost: 1496      \t Gap: 20.79%\nProblem: B-n64-k9        Cost: 1150       Optimal Cost: 861       \t Gap: 33.57%\nProblem: B-n66-k9        Cost: 1746       Optimal Cost: 1316      \t Gap: 32.67%\nProblem: B-n67-k10       Cost: 1368       Optimal Cost: 1032      \t Gap: 32.56%\nProblem: B-n68-k9        Cost: 1737       Optimal Cost: 1272      \t Gap: 36.56%\nProblem: B-n78-k10       Cost: 1706       Optimal Cost: 1221      \t Gap: 39.72%\nProblem: E-n51-k5        Cost: 690        Optimal Cost: 521       \t Gap: 32.44%\nProblem: E-n76-k7        Cost: 1019       Optimal Cost: 682       \t Gap: 49.41%\nProblem: E-n76-k8        Cost: 1031       Optimal Cost: 735       \t Gap: 40.27%\nProblem: E-n76-k10       Cost: 1156       Optimal Cost: 830       \t Gap: 39.28%\nProblem: E-n76-k14       Cost: 1335       Optimal Cost: 1021      \t Gap: 30.75%\nProblem: E-n101-k8       Cost: 1265       Optimal Cost: 815       \t Gap: 55.21%\nProblem: E-n101-k14      Cost: 1567       Optimal Cost: 1067      \t Gap: 46.86%\nProblem: F-n72-k4        Cost: 425        Optimal Cost: 237       \t Gap: 79.32%\nProblem: F-n135-k7       Cost: 4219       Optimal Cost: 1162      \t Gap: 263.08%\nProblem: M-n101-k10      Cost: 1388       Optimal Cost: 820       \t Gap: 69.27%\nProblem: M-n121-k7       Cost: 1746       Optimal Cost: 1034      \t Gap: 68.86%\nProblem: M-n151-k12      Cost: 1906       Optimal Cost: 1015      \t Gap: 87.78%\nProblem: M-n200-k16      Cost: 2509       Optimal Cost: 1274      \t Gap: 96.94%\nProblem: M-n200-k17      Cost: 2339       Optimal Cost: 1275      \t Gap: 83.45%\n</pre> In\u00a0[20]: Copied! <pre># Import augmented utils\nfrom rl4co.data.transforms import (\n    StateAugmentation as SymmetricStateAugmentation)\nfrom rl4co.utils.ops import batchify, unbatchify\n\nnum_augment = 100\naugmentation = SymmetricStateAugmentation(num_augment=num_augment)\n\nfor instance in instances:\n    problem = vrplib.read_instance(os.path.join(path_to_save, instance+'.vrp'))\n\n    coords = torch.tensor(problem['node_coord']).float()\n    coords_norm = normalize_coord(coords)\n    demand = torch.tensor(problem['demand'][1:]).float()\n    capacity = problem['capacity']\n    n = coords.shape[0]\n\n    # Prepare the tensordict\n    batch_size = 2\n    td = env.reset(batch_size=(batch_size,)).to(device)\n    td['locs'] = repeat(coords_norm, 'n d -&gt; b n d', b=batch_size, d=2)\n    td['demand'] = repeat(demand, 'n -&gt; b n', b=batch_size) / capacity\n    td['visited'] = torch.zeros((batch_size, 1, n), dtype=torch.uint8)\n    action_mask = torch.ones(batch_size, n, dtype=torch.bool)\n    action_mask[:, 0] = False\n    td['action_mask']  = action_mask\n    \n    # Augmentation\n    td = augmentation(td)\n\n    # Get the solution from the policy\n    with torch.no_grad():\n        out = policy(\n            td.clone(), decode_type='greedy', num_starts=0, return_actions=True\n        )\n\n    # Calculate the cost on the original scale\n    coords_repeat = repeat(coords, 'n d -&gt; b n d', b=batch_size, d=2)\n    td['locs'] = batchify(coords_repeat, num_augment)\n    reward = env.get_reward(td, out['actions'])\n    reward = unbatchify(reward, num_augment)\n    cost = ceil(-1 * torch.max(reward).item())\n\n    # Load the optimal cost\n    solution = vrplib.read_solution(os.path.join(path_to_save, instance+'.sol'))\n    optimal_cost = solution['cost']\n\n    # Calculate the gap and print\n    gap = (cost - optimal_cost) / optimal_cost\n    print(f'Problem: {instance:&lt;15} Cost: {cost:&lt;10} Optimal Cost: {optimal_cost:&lt;10}\\t Gap: {gap:.2%}')\n</pre> # Import augmented utils from rl4co.data.transforms import (     StateAugmentation as SymmetricStateAugmentation) from rl4co.utils.ops import batchify, unbatchify  num_augment = 100 augmentation = SymmetricStateAugmentation(num_augment=num_augment)  for instance in instances:     problem = vrplib.read_instance(os.path.join(path_to_save, instance+'.vrp'))      coords = torch.tensor(problem['node_coord']).float()     coords_norm = normalize_coord(coords)     demand = torch.tensor(problem['demand'][1:]).float()     capacity = problem['capacity']     n = coords.shape[0]      # Prepare the tensordict     batch_size = 2     td = env.reset(batch_size=(batch_size,)).to(device)     td['locs'] = repeat(coords_norm, 'n d -&gt; b n d', b=batch_size, d=2)     td['demand'] = repeat(demand, 'n -&gt; b n', b=batch_size) / capacity     td['visited'] = torch.zeros((batch_size, 1, n), dtype=torch.uint8)     action_mask = torch.ones(batch_size, n, dtype=torch.bool)     action_mask[:, 0] = False     td['action_mask']  = action_mask          # Augmentation     td = augmentation(td)      # Get the solution from the policy     with torch.no_grad():         out = policy(             td.clone(), decode_type='greedy', num_starts=0, return_actions=True         )      # Calculate the cost on the original scale     coords_repeat = repeat(coords, 'n d -&gt; b n d', b=batch_size, d=2)     td['locs'] = batchify(coords_repeat, num_augment)     reward = env.get_reward(td, out['actions'])     reward = unbatchify(reward, num_augment)     cost = ceil(-1 * torch.max(reward).item())      # Load the optimal cost     solution = vrplib.read_solution(os.path.join(path_to_save, instance+'.sol'))     optimal_cost = solution['cost']      # Calculate the gap and print     gap = (cost - optimal_cost) / optimal_cost     print(f'Problem: {instance:&lt;15} Cost: {cost:&lt;10} Optimal Cost: {optimal_cost:&lt;10}\\t Gap: {gap:.2%}') <pre>Problem: A-n53-k7        Cost: 1123       Optimal Cost: 1010      \t Gap: 11.19%\nProblem: A-n54-k7        Cost: 1305       Optimal Cost: 1167      \t Gap: 11.83%\nProblem: A-n55-k9        Cost: 1199       Optimal Cost: 1073      \t Gap: 11.74%\nProblem: A-n60-k9        Cost: 1534       Optimal Cost: 1354      \t Gap: 13.29%\nProblem: A-n61-k9        Cost: 1187       Optimal Cost: 1034      \t Gap: 14.80%\nProblem: A-n62-k8        Cost: 1474       Optimal Cost: 1288      \t Gap: 14.44%\nProblem: A-n63-k9        Cost: 1820       Optimal Cost: 1616      \t Gap: 12.62%\nProblem: A-n63-k10       Cost: 1505       Optimal Cost: 1314      \t Gap: 14.54%\nProblem: A-n64-k9        Cost: 1582       Optimal Cost: 1401      \t Gap: 12.92%\nProblem: A-n65-k9        Cost: 1332       Optimal Cost: 1174      \t Gap: 13.46%\nProblem: A-n69-k9        Cost: 1305       Optimal Cost: 1159      \t Gap: 12.60%\nProblem: A-n80-k10       Cost: 2044       Optimal Cost: 1763      \t Gap: 15.94%\nProblem: B-n51-k7        Cost: 1073       Optimal Cost: 1032      \t Gap: 3.97%\nProblem: B-n52-k7        Cost: 815        Optimal Cost: 747       \t Gap: 9.10%\nProblem: B-n56-k7        Cost: 792        Optimal Cost: 707       \t Gap: 12.02%\nProblem: B-n57-k7        Cost: 1219       Optimal Cost: 1153      \t Gap: 5.72%\nProblem: B-n57-k9        Cost: 1744       Optimal Cost: 1598      \t Gap: 9.14%\nProblem: B-n63-k10       Cost: 1611       Optimal Cost: 1496      \t Gap: 7.69%\nProblem: B-n64-k9        Cost: 931        Optimal Cost: 861       \t Gap: 8.13%\nProblem: B-n66-k9        Cost: 1427       Optimal Cost: 1316      \t Gap: 8.43%\nProblem: B-n67-k10       Cost: 1122       Optimal Cost: 1032      \t Gap: 8.72%\nProblem: B-n68-k9        Cost: 1382       Optimal Cost: 1272      \t Gap: 8.65%\nProblem: B-n78-k10       Cost: 1437       Optimal Cost: 1221      \t Gap: 17.69%\nProblem: E-n51-k5        Cost: 606        Optimal Cost: 521       \t Gap: 16.31%\nProblem: E-n76-k7        Cost: 816        Optimal Cost: 682       \t Gap: 19.65%\nProblem: E-n76-k8        Cost: 892        Optimal Cost: 735       \t Gap: 21.36%\nProblem: E-n76-k10       Cost: 943        Optimal Cost: 830       \t Gap: 13.61%\nProblem: E-n76-k14       Cost: 1160       Optimal Cost: 1021      \t Gap: 13.61%\nProblem: E-n101-k8       Cost: 1042       Optimal Cost: 815       \t Gap: 27.85%\nProblem: E-n101-k14      Cost: 1302       Optimal Cost: 1067      \t Gap: 22.02%\nProblem: F-n72-k4        Cost: 286        Optimal Cost: 237       \t Gap: 20.68%\nProblem: F-n135-k7       Cost: 1570       Optimal Cost: 1162      \t Gap: 35.11%\nProblem: M-n101-k10      Cost: 1037       Optimal Cost: 820       \t Gap: 26.46%\nProblem: M-n121-k7       Cost: 1283       Optimal Cost: 1034      \t Gap: 24.08%\nProblem: M-n151-k12      Cost: 1407       Optimal Cost: 1015      \t Gap: 38.62%\nProblem: M-n200-k16      Cost: 1811       Optimal Cost: 1274      \t Gap: 42.15%\nProblem: M-n200-k17      Cost: 1812       Optimal Cost: 1275      \t Gap: 42.12%\n</pre> In\u00a0[21]: Copied! <pre># Parameters for sampling\nnum_samples = 100\nsoftmax_temp = 0.05\n\nfor instance in instances:\n    problem = vrplib.read_instance(os.path.join(path_to_save, instance+'.vrp'))\n\n    coords = torch.tensor(problem['node_coord']).float()\n    coords_norm = normalize_coord(coords)\n    demand = torch.tensor(problem['demand'][1:]).float()\n    capacity = problem['capacity']\n    n = coords.shape[0]\n\n    # Prepare the tensordict\n    batch_size = 2\n    td = env.reset(batch_size=(batch_size,)).to(device)\n    td['locs'] = repeat(coords_norm, 'n d -&gt; b n d', b=batch_size, d=2)\n    td['demand'] = repeat(demand, 'n -&gt; b n', b=batch_size) / capacity\n    td['visited'] = torch.zeros((batch_size, 1, n), dtype=torch.uint8)\n    action_mask = torch.ones(batch_size, n, dtype=torch.bool)\n    action_mask[:, 0] = False\n    td['action_mask']  = action_mask\n    \n    # Sampling\n    td = batchify(td, num_samples)\n\n    # Get the solution from the policy\n    with torch.no_grad():\n        out = policy(\n            td.clone(), decode_type='sampling', num_starts=0, return_actions=True\n        )\n\n    # Calculate the cost on the original scale\n    coords_repeat = repeat(coords, 'n d -&gt; b n d', b=batch_size, d=2)\n    td['locs'] = batchify(coords_repeat, num_samples)\n    reward = env.get_reward(td, out['actions'])\n    reward = unbatchify(reward, num_samples)\n    cost = ceil(-1 * torch.max(reward).item())\n\n    # Load the optimal cost\n    solution = vrplib.read_solution(os.path.join(path_to_save, instance+'.sol'))\n    optimal_cost = solution['cost']\n\n    # Calculate the gap and print\n    gap = (cost - optimal_cost) / optimal_cost\n    print(f'Problem: {instance:&lt;15} Cost: {cost:&lt;10} Optimal Cost: {optimal_cost:&lt;10}\\t Gap: {gap:.2%}')\n</pre> # Parameters for sampling num_samples = 100 softmax_temp = 0.05  for instance in instances:     problem = vrplib.read_instance(os.path.join(path_to_save, instance+'.vrp'))      coords = torch.tensor(problem['node_coord']).float()     coords_norm = normalize_coord(coords)     demand = torch.tensor(problem['demand'][1:]).float()     capacity = problem['capacity']     n = coords.shape[0]      # Prepare the tensordict     batch_size = 2     td = env.reset(batch_size=(batch_size,)).to(device)     td['locs'] = repeat(coords_norm, 'n d -&gt; b n d', b=batch_size, d=2)     td['demand'] = repeat(demand, 'n -&gt; b n', b=batch_size) / capacity     td['visited'] = torch.zeros((batch_size, 1, n), dtype=torch.uint8)     action_mask = torch.ones(batch_size, n, dtype=torch.bool)     action_mask[:, 0] = False     td['action_mask']  = action_mask          # Sampling     td = batchify(td, num_samples)      # Get the solution from the policy     with torch.no_grad():         out = policy(             td.clone(), decode_type='sampling', num_starts=0, return_actions=True         )      # Calculate the cost on the original scale     coords_repeat = repeat(coords, 'n d -&gt; b n d', b=batch_size, d=2)     td['locs'] = batchify(coords_repeat, num_samples)     reward = env.get_reward(td, out['actions'])     reward = unbatchify(reward, num_samples)     cost = ceil(-1 * torch.max(reward).item())      # Load the optimal cost     solution = vrplib.read_solution(os.path.join(path_to_save, instance+'.sol'))     optimal_cost = solution['cost']      # Calculate the gap and print     gap = (cost - optimal_cost) / optimal_cost     print(f'Problem: {instance:&lt;15} Cost: {cost:&lt;10} Optimal Cost: {optimal_cost:&lt;10}\\t Gap: {gap:.2%}') <pre>Problem: A-n53-k7        Cost: 1191       Optimal Cost: 1010      \t Gap: 17.92%\nProblem: A-n54-k7        Cost: 1328       Optimal Cost: 1167      \t Gap: 13.80%\nProblem: A-n55-k9        Cost: 1286       Optimal Cost: 1073      \t Gap: 19.85%\nProblem: A-n60-k9        Cost: 1631       Optimal Cost: 1354      \t Gap: 20.46%\nProblem: A-n61-k9        Cost: 1230       Optimal Cost: 1034      \t Gap: 18.96%\nProblem: A-n62-k8        Cost: 1505       Optimal Cost: 1288      \t Gap: 16.85%\nProblem: A-n63-k9        Cost: 1840       Optimal Cost: 1616      \t Gap: 13.86%\nProblem: A-n63-k10       Cost: 1590       Optimal Cost: 1314      \t Gap: 21.00%\nProblem: A-n64-k9        Cost: 1643       Optimal Cost: 1401      \t Gap: 17.27%\nProblem: A-n65-k9        Cost: 1381       Optimal Cost: 1174      \t Gap: 17.63%\nProblem: A-n69-k9        Cost: 1451       Optimal Cost: 1159      \t Gap: 25.19%\nProblem: A-n80-k10       Cost: 2170       Optimal Cost: 1763      \t Gap: 23.09%\nProblem: B-n51-k7        Cost: 1187       Optimal Cost: 1032      \t Gap: 15.02%\nProblem: B-n52-k7        Cost: 884        Optimal Cost: 747       \t Gap: 18.34%\nProblem: B-n56-k7        Cost: 853        Optimal Cost: 707       \t Gap: 20.65%\nProblem: B-n57-k7        Cost: 1314       Optimal Cost: 1153      \t Gap: 13.96%\nProblem: B-n57-k9        Cost: 1744       Optimal Cost: 1598      \t Gap: 9.14%\nProblem: B-n63-k10       Cost: 1698       Optimal Cost: 1496      \t Gap: 13.50%\nProblem: B-n64-k9        Cost: 1045       Optimal Cost: 861       \t Gap: 21.37%\nProblem: B-n66-k9        Cost: 1506       Optimal Cost: 1316      \t Gap: 14.44%\nProblem: B-n67-k10       Cost: 1254       Optimal Cost: 1032      \t Gap: 21.51%\nProblem: B-n68-k9        Cost: 1510       Optimal Cost: 1272      \t Gap: 18.71%\nProblem: B-n78-k10       Cost: 1514       Optimal Cost: 1221      \t Gap: 24.00%\nProblem: E-n51-k5        Cost: 613        Optimal Cost: 521       \t Gap: 17.66%\nProblem: E-n76-k7        Cost: 882        Optimal Cost: 682       \t Gap: 29.33%\nProblem: E-n76-k8        Cost: 952        Optimal Cost: 735       \t Gap: 29.52%\nProblem: E-n76-k10       Cost: 1015       Optimal Cost: 830       \t Gap: 22.29%\nProblem: E-n76-k14       Cost: 1185       Optimal Cost: 1021      \t Gap: 16.06%\nProblem: E-n101-k8       Cost: 1189       Optimal Cost: 815       \t Gap: 45.89%\nProblem: E-n101-k14      Cost: 1420       Optimal Cost: 1067      \t Gap: 33.08%\nProblem: F-n72-k4        Cost: 344        Optimal Cost: 237       \t Gap: 45.15%\nProblem: F-n135-k7       Cost: 3130       Optimal Cost: 1162      \t Gap: 169.36%\nProblem: M-n101-k10      Cost: 1221       Optimal Cost: 820       \t Gap: 48.90%\nProblem: M-n121-k7       Cost: 1538       Optimal Cost: 1034      \t Gap: 48.74%\nProblem: M-n151-k12      Cost: 1688       Optimal Cost: 1015      \t Gap: 66.31%\nProblem: M-n200-k16      Cost: 2252       Optimal Cost: 1274      \t Gap: 76.77%\nProblem: M-n200-k17      Cost: 2260       Optimal Cost: 1275      \t Gap: 77.25%\n</pre>"},{"location":"examples/datasets/2-test-on-cvrplib/#test-model-on-vrplib","title":"Test Model on VRPLib\u00b6","text":"<p>In this notebook, we will test the trained model's performance on the VRPLib benchmark. We will use the trained model from the previous notebook.</p> <p>VRPLIB is a collection of instances related to the CVRP, which is a classic optimization challenge in the field of logistics and transportation.</p>"},{"location":"examples/datasets/2-test-on-cvrplib/#before-we-start","title":"Before we start\u00b6","text":"<p>To use the VRPLib, we strongly recomment to use the Python <code>vrplib</code> tool:</p> <p>VRPLib is a Python package for working with Vehicle Routing Problem (VRP) instances. This tool can help us easily load the VRPLib instances and visualize the results.</p>"},{"location":"examples/datasets/2-test-on-cvrplib/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install the package from PyPI. Remember to choose a GPU runtime for faster training!</p> <p>Note: You may need to restart the runtime in Colab after this</p>"},{"location":"examples/datasets/2-test-on-cvrplib/#imports","title":"Imports\u00b6","text":""},{"location":"examples/datasets/2-test-on-cvrplib/#load-a-trained-model","title":"Load a trained model\u00b6","text":""},{"location":"examples/datasets/2-test-on-cvrplib/#download-vrp-problems","title":"Download vrp problems\u00b6","text":""},{"location":"examples/datasets/2-test-on-cvrplib/#test-the-greedy","title":"Test the greedy\u00b6","text":""},{"location":"examples/datasets/2-test-on-cvrplib/#test-the-augmentation","title":"Test the Augmentation\u00b6","text":""},{"location":"examples/datasets/2-test-on-cvrplib/#test-the-sampling","title":"Test the Sampling\u00b6","text":""},{"location":"examples/modeling/","title":"Modeling","text":"<p>Collection of examples on models and related topics.</p>"},{"location":"examples/modeling/#index","title":"Index","text":"<ul> <li><code>1-decoding-strategies.ipynb</code>: here we show how to use different decoding strategies at inference time, such as greedy evaluation, beam search, and various sampling methods including top-k and nucleus sampling.</li> <li><code>2-search-methods.ipynb</code>: here we show how to use search methods such as EAS for test-time optimization.</li> <li><code>3-change-encoder.ipynb</code>: here we show how to change the encoder of a model.</li> </ul>"},{"location":"examples/modeling/1-decoding-strategies/","title":"RL4CO Decoding Strategies Notebook","text":"In\u00a0[1]: Copied! <pre>## Uncomment the following line to install the package from PyPI\n## You may need to restart the runtime in Colab after this\n## Remember to choose a GPU runtime for faster training!\n\n# !pip install rl4co\n</pre> ## Uncomment the following line to install the package from PyPI ## You may need to restart the runtime in Colab after this ## Remember to choose a GPU runtime for faster training!  # !pip install rl4co In\u00a0[4]: Copied! <pre>import torch\n\nfrom rl4co.envs import TSPEnv\nfrom rl4co.models.zoo import AttentionModel, AttentionModelPolicy\nfrom rl4co.utils.trainer import RL4COTrainer\nfrom rl4co.utils.ops import batchify\n</pre> import torch  from rl4co.envs import TSPEnv from rl4co.models.zoo import AttentionModel, AttentionModelPolicy from rl4co.utils.trainer import RL4COTrainer from rl4co.utils.ops import batchify In\u00a0[5]: Copied! <pre>%%capture\n# RL4CO env based on TorchRL\nenv = TSPEnv(generator_params=dict(num_loc=50)) \n\n# Policy: neural network, in this case with encoder-decoder architecture\npolicy = AttentionModelPolicy(env_name=env.name, \n                              embed_dim=128,\n                              num_encoder_layers=3,\n                              num_heads=8,\n                            )\n\n# Model: default is AM with REINFORCE and greedy rollout baseline\nmodel = AttentionModel(env, \n                       baseline=\"rollout\",\n                       batch_size = 512,\n                       val_batch_size = 64, \n                       test_batch_size = 64, \n                       train_data_size=100_000, # fast training for demo\n                       val_data_size=1_000,\n                       test_data_size=1_000,\n                       optimizer_kwargs={\"lr\": 1e-4},\n                       policy_kwargs={  # we can specify the decode types using the policy_kwargs\n                           \"train_decode_type\": \"sampling\",\n                           \"val_decode_type\": \"greedy\",\n                           \"test_decode_type\": \"beam_search\",\n                       }\n                       )\n</pre> %%capture # RL4CO env based on TorchRL env = TSPEnv(generator_params=dict(num_loc=50))   # Policy: neural network, in this case with encoder-decoder architecture policy = AttentionModelPolicy(env_name=env.name,                                embed_dim=128,                               num_encoder_layers=3,                               num_heads=8,                             )  # Model: default is AM with REINFORCE and greedy rollout baseline model = AttentionModel(env,                         baseline=\"rollout\",                        batch_size = 512,                        val_batch_size = 64,                         test_batch_size = 64,                         train_data_size=100_000, # fast training for demo                        val_data_size=1_000,                        test_data_size=1_000,                        optimizer_kwargs={\"lr\": 1e-4},                        policy_kwargs={  # we can specify the decode types using the policy_kwargs                            \"train_decode_type\": \"sampling\",                            \"val_decode_type\": \"greedy\",                            \"test_decode_type\": \"beam_search\",                        }                        )  In\u00a0[4]: Copied! <pre>trainer = RL4COTrainer(\n    max_epochs=3,\n    devices=1,\n)\n\ntrainer.fit(model)\n</pre> trainer = RL4COTrainer(     max_epochs=3,     devices=1, )  trainer.fit(model) <pre>Using 16bit Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\nval_file not set. Generating dataset instead\ntest_file not set. Generating dataset instead\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n\n  | Name     | Type                 | Params\n--------------------------------------------------\n0 | env      | TSPEnv               | 0     \n1 | policy   | AttentionModelPolicy | 710 K \n2 | baseline | WarmupBaseline       | 710 K \n--------------------------------------------------\n1.4 M     Trainable params\n0         Non-trainable params\n1.4 M     Total params\n5.681     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n</pre> <pre>Training: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=3` reached.\n</pre> In\u00a0[6]: Copied! <pre># here we evaluate the model on the test set using the beam search decoding strategy as declared in the model constructor\ntrainer.test(model=model)\n</pre> # here we evaluate the model on the test set using the beam search decoding strategy as declared in the model constructor trainer.test(model=model) In\u00a0[9]: Copied! <pre># we can simply change the decoding type of the current model instance\nmodel.policy.test_decode_type = \"greedy\"\ntrainer.test(model=model)\n</pre> # we can simply change the decoding type of the current model instance model.policy.test_decode_type = \"greedy\" trainer.test(model=model) In\u00a0[8]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntest_td_raw = next(iter(model.test_dataloader())).to(device)\ntd_test = env.reset(test_td_raw)\nmodel = model.to(device)\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  test_td_raw = next(iter(model.test_dataloader())).to(device) td_test = env.reset(test_td_raw) model = model.to(device) In\u00a0[10]: Copied! <pre># Example over full dataset\nrewards = []\nfor batch in model.test_dataloader():\n    with torch.inference_mode():\n        td = env.reset(batch).to(device)\n        out = model(td, decode_type=\"greedy\")\n    rewards.append(out[\"reward\"])\nprint(\"Average reward over all dataset: %.3f\" % torch.cat(rewards).mean().item())\n\n# Example over a single instance\nwith torch.inference_mode():\n    out = model(test_td_raw.clone(), decode_type=\"greedy\")\n    print(\"Average reward: %.3f\" % out[\"reward\"].mean().item())\n</pre> # Example over full dataset rewards = [] for batch in model.test_dataloader():     with torch.inference_mode():         td = env.reset(batch).to(device)         out = model(td, decode_type=\"greedy\")     rewards.append(out[\"reward\"]) print(\"Average reward over all dataset: %.3f\" % torch.cat(rewards).mean().item())  # Example over a single instance with torch.inference_mode():     out = model(test_td_raw.clone(), decode_type=\"greedy\")     print(\"Average reward: %.3f\" % out[\"reward\"].mean().item()) <pre>Average reward over all dataset: -6.376\nAverage reward: -6.415\n</pre> In\u00a0[11]: Copied! <pre># Example over a single instance\nwith torch.inference_mode():\n    bs = td_test.batch_size[0]\n    out = model(td_test.clone(), decode_type=\"multistart_greedy\", num_starts=20, return_actions=True)\n    rewards = torch.stack(out[\"reward\"].split(bs), 1).max(1).values\n    print(\"Average reward: %.3f\" % rewards.mean().item())\n</pre> # Example over a single instance with torch.inference_mode():     bs = td_test.batch_size[0]     out = model(td_test.clone(), decode_type=\"multistart_greedy\", num_starts=20, return_actions=True)     rewards = torch.stack(out[\"reward\"].split(bs), 1).max(1).values     print(\"Average reward: %.3f\" % rewards.mean().item()) <pre>Average reward: -6.279\n</pre> In\u00a0[44]: Copied! <pre>num_samples = 32\nwith torch.inference_mode():\n    bs = td_test.batch_size[0]\n    td_test_batched = batchify(td_test, num_samples) # repeat the same instance num_samples times\n    out = model(td_test_batched.clone(), decode_type=\"sampling\", return_actions=True)\n    rewards = torch.stack(out[\"reward\"].split(bs), 1).max(1).values # take the max reward over the num_samples samples\n    print(\"Average reward: %.3f\" % rewards.mean().item())\n</pre> num_samples = 32 with torch.inference_mode():     bs = td_test.batch_size[0]     td_test_batched = batchify(td_test, num_samples) # repeat the same instance num_samples times     out = model(td_test_batched.clone(), decode_type=\"sampling\", return_actions=True)     rewards = torch.stack(out[\"reward\"].split(bs), 1).max(1).values # take the max reward over the num_samples samples     print(\"Average reward: %.3f\" % rewards.mean().item()) <pre>Average reward: -6.157\n</pre> In\u00a0[75]: Copied! <pre>num_samples = 32\ntop_p = 0.9\nwith torch.inference_mode():\n    bs = td_test.batch_size[0]\n    td_test_batched = batchify(td_test, num_samples) # repeat the same instance num_samples times\n    out = model(td_test_batched.clone(), decode_type=\"sampling\", return_actions=True, top_p=top_p)\n    rewards = torch.stack(out[\"reward\"].split(bs), 1).max(1).values # take the max reward over the num_samples samples\n    print(\"Average reward: %.3f\" % rewards.mean().item())\n</pre> num_samples = 32 top_p = 0.9 with torch.inference_mode():     bs = td_test.batch_size[0]     td_test_batched = batchify(td_test, num_samples) # repeat the same instance num_samples times     out = model(td_test_batched.clone(), decode_type=\"sampling\", return_actions=True, top_p=top_p)     rewards = torch.stack(out[\"reward\"].split(bs), 1).max(1).values # take the max reward over the num_samples samples     print(\"Average reward: %.3f\" % rewards.mean().item()) <pre>Average reward: -6.136\n</pre> In\u00a0[67]: Copied! <pre>num_samples = 32\ntop_k = 10\nwith torch.inference_mode():\n    bs = td_test.batch_size[0]\n    td_test_batched = batchify(td_test, num_samples) # repeat the same instance num_samples times\n    out = model(td_test_batched.clone(), decode_type=\"sampling\", return_actions=True, top_k=top_k)\n    rewards = torch.stack(out[\"reward\"].split(bs), 1).max(1).values # take the max reward over the num_samples samples\n    print(\"Average reward: %.3f\" % rewards.mean().item())\n</pre> num_samples = 32 top_k = 10 with torch.inference_mode():     bs = td_test.batch_size[0]     td_test_batched = batchify(td_test, num_samples) # repeat the same instance num_samples times     out = model(td_test_batched.clone(), decode_type=\"sampling\", return_actions=True, top_k=top_k)     rewards = torch.stack(out[\"reward\"].split(bs), 1).max(1).values # take the max reward over the num_samples samples     print(\"Average reward: %.3f\" % rewards.mean().item()) <pre>Average reward: -6.158\n</pre> In\u00a0[88]: Copied! <pre>with torch.inference_mode():\n    bs = td_test.batch_size[0]\n    out = model(td_test.clone(), decode_type=\"beam_search\", beam_width=20)\n    rewards = torch.stack(out[\"reward\"].split(bs), 1).max(1).values # take the max reward over the num_samples samples\n    print(\"Average reward: %.3f\" % rewards.mean().item())\n</pre> with torch.inference_mode():     bs = td_test.batch_size[0]     out = model(td_test.clone(), decode_type=\"beam_search\", beam_width=20)     rewards = torch.stack(out[\"reward\"].split(bs), 1).max(1).values # take the max reward over the num_samples samples     print(\"Average reward: %.3f\" % rewards.mean().item()) <pre>Average reward: -6.195\n</pre> <p>We can see that beam search finds a better solution than the greedy decoder</p> <p>We can also analyze the different solutions obtained via beam search when passing \"select_best=False\" to the forward pass of the policy. The solutions in this case are sorted per instance-wise, that is:</p> <ul> <li>instance1_solution1</li> <li>instance2_solution1</li> <li>instance3_solution1</li> <li>instance1_solution2</li> <li>instance2_solution2</li> <li>instance3_solution2</li> </ul> In\u00a0[90]: Copied! <pre>out = model(td_test.clone(), decode_type=\"beam_search\", beam_width=5, select_best=False, return_actions=True)\n</pre> out = model(td_test.clone(), decode_type=\"beam_search\", beam_width=5, select_best=False, return_actions=True) In\u00a0[91]: Copied! <pre># we split the sequence ofter every \"batch_size\" instances, then stack the different solutions obtained for each minibatch instance by the beam search together.\nactions_stacked = torch.stack(out[\"actions\"].split(bs), 1)\nrewards_stacked = torch.stack(out[\"reward\"].split(bs), 1)\n</pre> # we split the sequence ofter every \"batch_size\" instances, then stack the different solutions obtained for each minibatch instance by the beam search together. actions_stacked = torch.stack(out[\"actions\"].split(bs), 1) rewards_stacked = torch.stack(out[\"reward\"].split(bs), 1) In\u00a0[95]: Copied! <pre>import matplotlib.pyplot as plt\nbatch_instance = 0\nfor i, actions in enumerate(actions_stacked[batch_instance].cpu()):\n    reward = rewards_stacked[batch_instance, i]\n    _, ax = plt.subplots()\n    \n    env.render(td[0], actions, ax=ax)\n    ax.set_title(\"Reward: %s\" % reward.item())\n</pre> import matplotlib.pyplot as plt batch_instance = 0 for i, actions in enumerate(actions_stacked[batch_instance].cpu()):     reward = rewards_stacked[batch_instance, i]     _, ax = plt.subplots()          env.render(td[0], actions, ax=ax)     ax.set_title(\"Reward: %s\" % reward.item()) <p>For evaluation, we can also use additional decoding strategies used during evaluatin, such as sampling N times or greedy augmentations, available in rl4co/tasks/eval.py</p>"},{"location":"examples/modeling/1-decoding-strategies/#rl4co-decoding-strategies-notebook","title":"RL4CO Decoding Strategies Notebook\u00b6","text":"<p>This notebook demonstrates how to utilize the different decoding strategies available in rl4co/utils/decoding.py during the different phases of model development. We will also demonstrate how to evaluate the model for different decoding strategies on the test dataset.</p> <p></p>"},{"location":"examples/modeling/1-decoding-strategies/#installation","title":"Installation\u00b6","text":""},{"location":"examples/modeling/1-decoding-strategies/#setup-policy-and-environment","title":"Setup Policy and Environment\u00b6","text":""},{"location":"examples/modeling/1-decoding-strategies/#setup-trainer-and-train-model","title":"Setup Trainer and train model\u00b6","text":""},{"location":"examples/modeling/1-decoding-strategies/#test-the-model-using-trainer-class","title":"Test the model using Trainer class\u00b6","text":""},{"location":"examples/modeling/1-decoding-strategies/#test-loop","title":"Test Loop\u00b6","text":"<p>Let's compare different decoding strategies on some test samples - for simplicity, we don't loop over the entire test dataset, but only over the on a single iteration of the test dataloader.</p>"},{"location":"examples/modeling/1-decoding-strategies/#greedy-decoding","title":"Greedy Decoding\u00b6","text":""},{"location":"examples/modeling/1-decoding-strategies/#greedy-decoding","title":"Greedy decoding\u00b6","text":""},{"location":"examples/modeling/1-decoding-strategies/#greedy-multistart-decoding","title":"Greedy multistart decoding\u00b6","text":"<p>Start from different nodes as done in POMO</p>"},{"location":"examples/modeling/1-decoding-strategies/#sampling","title":"Sampling\u00b6","text":""},{"location":"examples/modeling/1-decoding-strategies/#decoding-via-sampling","title":"Decoding via sampling\u00b6","text":"<p>In this case, we can parallelize the decoding process by batching the samples and decoding them in parallel.</p>"},{"location":"examples/modeling/1-decoding-strategies/#top-p-sampling-nucleus-sampling","title":"Top-p sampling (nucleus sampling)\u00b6","text":"<p>Top-p sampling is a sampling strategy where the top-p most likely tokens are selected and the probability mass is redistributed among them. This is useful when we want to sample from a subset of the nodes and we want to exclude from the lower-end tail of the distribution.</p>"},{"location":"examples/modeling/1-decoding-strategies/#top-k-sampling","title":"Top-k sampling\u00b6","text":"<p>In this case we only sample from the top-k most likely tokens.</p>"},{"location":"examples/modeling/1-decoding-strategies/#beam-search","title":"Beam search\u00b6","text":"<p>Beam search is a popular decoding strategy in sequence-to-sequence models. It maintains a list of the top-k most likely sequences and expands them by adding the next token in the sequence. The sequences are scored based on the log-likelihood of the sequence. The sequences are expanded until the end token is reached or the maximum length is reached.</p>"},{"location":"examples/modeling/1-decoding-strategies/#beam-search-decoding","title":"Beam search decoding\u00b6","text":""},{"location":"examples/modeling/1-decoding-strategies/#digging-deeper-into-beam-search-solutions","title":"Digging deeper into beam search solutions\u00b6","text":""},{"location":"examples/modeling/1-decoding-strategies/#final-notes","title":"Final notes\u00b6","text":""},{"location":"examples/modeling/2-transductive-methods/","title":"Transductive Methods","text":"In\u00a0[\u00a0]: Copied! <pre># !pip install rl4co[graph] # include torch-geometric\n\n## NOTE: to install latest version from Github (may be unstable) install from source instead:\n# !pip install git+https://github.com/ai4co/rl4co.git\n</pre> # !pip install rl4co[graph] # include torch-geometric  ## NOTE: to install latest version from Github (may be unstable) install from source instead: # !pip install git+https://github.com/ai4co/rl4co.git In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport torch\n\nfrom rl4co.envs import TSPEnv, CVRPEnv\nfrom rl4co.models.zoo.am import AttentionModel\nfrom rl4co.utils.trainer import RL4COTrainer\nfrom rl4co.utils.decoding import get_log_likelihood\nfrom rl4co.models.zoo import EAS, EASLay, EASEmb, ActiveSearch\n\nimport logging\n</pre> %load_ext autoreload %autoreload 2  import torch  from rl4co.envs import TSPEnv, CVRPEnv from rl4co.models.zoo.am import AttentionModel from rl4co.utils.trainer import RL4COTrainer from rl4co.utils.decoding import get_log_likelihood from rl4co.models.zoo import EAS, EASLay, EASEmb, ActiveSearch  import logging <pre>2023-08-22 16:29:17.903805: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-08-22 16:29:17.923169: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-08-22 16:29:18.249479: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n</pre> In\u00a0[2]: Copied! <pre># Load from checkpoint; alternatively, simply instantiate a new model\ncheckpoint_path = \"last.ckpt\" # model trained for one epoch only just for showing the examples\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# load checkpoint\n# checkpoint = torch.load(checkpoint_path)\n\nmodel = AttentionModel.load_from_checkpoint(checkpoint_path, load_baseline=False)\npolicy = model.policy.to(device)\n</pre> # Load from checkpoint; alternatively, simply instantiate a new model checkpoint_path = \"last.ckpt\" # model trained for one epoch only just for showing the examples  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # load checkpoint # checkpoint = torch.load(checkpoint_path)  model = AttentionModel.load_from_checkpoint(checkpoint_path, load_baseline=False) policy = model.policy.to(device) <pre>/home/botu/miniconda3/envs/rl4co/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n  rank_zero_warn(\n/home/botu/miniconda3/envs/rl4co/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n  rank_zero_warn(\n/home/botu/miniconda3/envs/rl4co/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:164: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['baseline.baseline.model.encoder.init_embedding.init_embed.weight', 'baseline.baseline.model.encoder.init_embedding.init_embed.bias', 'baseline.baseline.model.encoder.net.layers.0.0.module.Wqkv.weight', 'baseline.baseline.model.encoder.net.layers.0.0.module.Wqkv.bias', 'baseline.baseline.model.encoder.net.layers.0.0.module.out_proj.weight', 'baseline.baseline.model.encoder.net.layers.0.0.module.out_proj.bias', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.0.1.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.0.2.module.0.weight', 'baseline.baseline.model.encoder.net.layers.0.2.module.0.bias', 'baseline.baseline.model.encoder.net.layers.0.2.module.2.weight', 'baseline.baseline.model.encoder.net.layers.0.2.module.2.bias', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.0.3.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.1.0.module.Wqkv.weight', 'baseline.baseline.model.encoder.net.layers.1.0.module.Wqkv.bias', 'baseline.baseline.model.encoder.net.layers.1.0.module.out_proj.weight', 'baseline.baseline.model.encoder.net.layers.1.0.module.out_proj.bias', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.1.1.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.1.2.module.0.weight', 'baseline.baseline.model.encoder.net.layers.1.2.module.0.bias', 'baseline.baseline.model.encoder.net.layers.1.2.module.2.weight', 'baseline.baseline.model.encoder.net.layers.1.2.module.2.bias', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.1.3.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.2.0.module.Wqkv.weight', 'baseline.baseline.model.encoder.net.layers.2.0.module.Wqkv.bias', 'baseline.baseline.model.encoder.net.layers.2.0.module.out_proj.weight', 'baseline.baseline.model.encoder.net.layers.2.0.module.out_proj.bias', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.2.1.normalizer.num_batches_tracked', 'baseline.baseline.model.encoder.net.layers.2.2.module.0.weight', 'baseline.baseline.model.encoder.net.layers.2.2.module.0.bias', 'baseline.baseline.model.encoder.net.layers.2.2.module.2.weight', 'baseline.baseline.model.encoder.net.layers.2.2.module.2.bias', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.weight', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.bias', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.running_mean', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.running_var', 'baseline.baseline.model.encoder.net.layers.2.3.normalizer.num_batches_tracked', 'baseline.baseline.model.decoder.context_embedding.W_placeholder', 'baseline.baseline.model.decoder.context_embedding.project_context.weight', 'baseline.baseline.model.decoder.project_node_embeddings.weight', 'baseline.baseline.model.decoder.project_fixed_context.weight', 'baseline.baseline.model.decoder.logit_attention.project_out.weight']\n  rank_zero_warn(\n</pre> In\u00a0[3]: Copied! <pre># env = CVRPEnv(generator_params=dict(num_loc=50))\n# policy = AttentionModel(env).policy.to(device)\n\nenv = TSPEnv(generator_params=dict(num_loc=50))\n\ntd = env.reset(batch_size=3).to(device)\n\nout = policy(td, return_actions=True)\n</pre> # env = CVRPEnv(generator_params=dict(num_loc=50)) # policy = AttentionModel(env).policy.to(device)  env = TSPEnv(generator_params=dict(num_loc=50))  td = env.reset(batch_size=3).to(device)  out = policy(td, return_actions=True) In\u00a0[4]: Copied! <pre>env.render(td.cpu(), out[\"actions\"].cpu())\n</pre> env.render(td.cpu(), out[\"actions\"].cpu()) In\u00a0[5]: Copied! <pre>logging.basicConfig(level=logging.DEBUG)\n\nenv.generator.num_loc = 200\n\ndataset = env.dataset(batch_size=[2])\n# eas_model = EASEmb(env, policy, dataset, batch_size=2, max_iters=20, save_path=\"eas_sols.pt\") # alternative\neas_model = EASLay(env, policy, dataset, batch_size=2, max_iters=20, save_path=\"eas_sols.pt\")\n\neas_model.setup()\n</pre> logging.basicConfig(level=logging.DEBUG)  env.generator.num_loc = 200  dataset = env.dataset(batch_size=[2]) # eas_model = EASEmb(env, policy, dataset, batch_size=2, max_iters=20, save_path=\"eas_sols.pt\") # alternative eas_model = EASLay(env, policy, dataset, batch_size=2, max_iters=20, save_path=\"eas_sols.pt\")  eas_model.setup() <pre>/home/botu/miniconda3/envs/rl4co/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n  rank_zero_warn(\n/home/botu/miniconda3/envs/rl4co/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n  rank_zero_warn(\nINFO:rl4co.models.rl.common.base:No metrics specified, using default\nINFO:rl4co.models.zoo.eas.search:Setting up Efficient Active Search (EAS) with: \n- EAS Embedding: False \n- EAS Layer: True \n\n</pre> In\u00a0[6]: Copied! <pre># Plot initial solution\ntd_dataset = next(iter(eas_model.train_dataloader()))\ntd_dataset = env.reset(td_dataset).to(device)\nout = policy(td_dataset, return_actions=True)\n\nenv.render(td_dataset.cpu(), out[\"actions\"].cpu())\n</pre> # Plot initial solution td_dataset = next(iter(eas_model.train_dataloader())) td_dataset = env.reset(td_dataset).to(device) out = policy(td_dataset, return_actions=True)  env.render(td_dataset.cpu(), out[\"actions\"].cpu()) <pre>INFO:rl4co.models.common.constructive.autoregressive.policy:Instantiated environment not provided; instantiating tsp\n</pre> In\u00a0[7]: Copied! <pre>from rl4co.utils.trainer import RL4COTrainer\n\ntrainer = RL4COTrainer(\n    max_epochs=1,\n    gradient_clip_val=None,\n)\n\ntrainer.fit(eas_model)\n</pre> from rl4co.utils.trainer import RL4COTrainer  trainer = RL4COTrainer(     max_epochs=1,     gradient_clip_val=None, )  trainer.fit(eas_model) <pre>WARNING:rl4co.utils.trainer:gradient_clip_val is set to None. This may lead to unstable training.\nUsing 16bit Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nINFO:rl4co.models.zoo.eas.search:Setting up Efficient Active Search (EAS) with: \n- EAS Embedding: False \n- EAS Layer: True \n\nDEBUG:fsspec.local:open file: /home/botu/Dev/rl4co-rebuttal/notebooks/dev/lightning_logs/version_181/hparams.yaml\nDEBUG:fsspec.local:open file: /home/botu/Dev/rl4co-rebuttal/notebooks/dev/lightning_logs/version_181/hparams.yaml\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:rl4co.models.rl.common.base:Instantiating optimizer &lt;Adam&gt;\n\n  | Name   | Type                 | Params\n------------------------------------------------\n0 | env    | TSPEnv               | 0     \n1 | policy | AttentionModelPolicy | 710 K \n------------------------------------------------\n710 K     Trainable params\n0         Non-trainable params\n710 K     Total params\n2.841     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: 0it [00:00, ?it/s]</pre> <pre>/home/botu/miniconda3/envs/rl4co/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n/home/botu/miniconda3/envs/rl4co/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n/home/botu/miniconda3/envs/rl4co/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\n</pre> <pre>Training: 0it [00:00, ?it/s]</pre> <pre>/home/botu/Dev/rl4co-rebuttal/notebooks/dev/../../rl4co/models/zoo/eas/nn.py:22: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n  torch.nn.init.xavier_uniform(self.W1)\n/home/botu/Dev/rl4co-rebuttal/notebooks/dev/../../rl4co/models/zoo/eas/nn.py:23: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n  torch.nn.init.xavier_uniform(self.b1)\nINFO:rl4co.models.rl.common.base:Instantiating optimizer &lt;Adam&gt;\n</pre> <pre>/home/botu/miniconda3/envs/rl4co/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n  warning_cache.warn(\nINFO:rl4co.models.zoo.eas.search:0/20 |  Reward: -15.52 \nINFO:rl4co.models.zoo.eas.search:1/20 |  Reward: -15.32 \nINFO:rl4co.models.zoo.eas.search:2/20 |  Reward: -15.30 \nINFO:rl4co.models.zoo.eas.search:3/20 |  Reward: -15.28 \nINFO:rl4co.models.zoo.eas.search:4/20 |  Reward: -15.01 \nINFO:rl4co.models.zoo.eas.search:5/20 |  Reward: -15.01 \nINFO:rl4co.models.zoo.eas.search:6/20 |  Reward: -15.01 \nINFO:rl4co.models.zoo.eas.search:7/20 |  Reward: -15.01 \nINFO:rl4co.models.zoo.eas.search:8/20 |  Reward: -15.01 \nINFO:rl4co.models.zoo.eas.search:9/20 |  Reward: -15.01 \nINFO:rl4co.models.zoo.eas.search:10/20 |  Reward: -15.01 \nINFO:rl4co.models.zoo.eas.search:11/20 |  Reward: -15.01 \nINFO:rl4co.models.zoo.eas.search:12/20 |  Reward: -15.01 \nINFO:rl4co.models.zoo.eas.search:13/20 |  Reward: -15.01 \nINFO:rl4co.models.zoo.eas.search:14/20 |  Reward: -15.01 \nINFO:rl4co.models.zoo.eas.search:15/20 |  Reward: -15.01 \nINFO:rl4co.models.zoo.eas.search:16/20 |  Reward: -15.01 \nINFO:rl4co.models.zoo.eas.search:17/20 |  Reward: -15.01 \nINFO:rl4co.models.zoo.eas.search:18/20 |  Reward: -14.84 \nINFO:rl4co.models.zoo.eas.search:19/20 |  Reward: -14.74 \nINFO:rl4co.models.zoo.eas.search:Best reward: -14.74\n</pre> <pre>Validation: 0it [00:00, ?it/s]</pre> <pre>INFO:rl4co.models.zoo.eas.search:Saving solutions and rewards to eas_sols.pt...\n`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> In\u00a0[10]: Copied! <pre># Load\nactions = torch.load(\"eas_sols.pt\")[\"solutions\"][0].cpu()\nactions = actions[:torch.count_nonzero(actions, dim=-1)] # remove trailing zeros\nstate = td_dataset.cpu()[0]\n\nenv.render(state, actions)\n</pre> # Load actions = torch.load(\"eas_sols.pt\")[\"solutions\"][0].cpu() actions = actions[:torch.count_nonzero(actions, dim=-1)] # remove trailing zeros state = td_dataset.cpu()[0]  env.render(state, actions) <p>Even with few iterations, the search method can clearly find better solutions than the initial ones!</p>"},{"location":"examples/modeling/2-transductive-methods/#transductive-methods","title":"Transductive Methods\u00b6","text":"<p>In this notebook, we will showcase how to use the Efficient Active Search (EAS) algorithm to find better solutions to existing problems!</p> <p>Tip: in transductive RL) we train (or finetune) to solve only specific ones.</p> <p></p>"},{"location":"examples/modeling/2-transductive-methods/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install the package from PyPI. Remember to choose a GPU runtime for faster training!</p> <p>Note: You may need to restart the runtime in Colab after this</p>"},{"location":"examples/modeling/2-transductive-methods/#imports","title":"Imports\u00b6","text":""},{"location":"examples/modeling/2-transductive-methods/#eas","title":"EAS\u00b6","text":"<p>We perform few iterations of EASLay for demonstration</p>"},{"location":"examples/modeling/2-transductive-methods/#perform-search","title":"Perform search\u00b6","text":""},{"location":"examples/modeling/2-transductive-methods/#load-actions","title":"Load actions\u00b6","text":""},{"location":"examples/modeling/3-change-encoder/","title":"Encoder Customization","text":"In\u00a0[1]: Copied! <pre># !pip install rl4co[graph] # include torch-geometric\n\n## NOTE: to install latest version from Github (may be unstable) install from source instead:\n# !pip install git+https://github.com/ai4co/rl4co.git\n</pre> # !pip install rl4co[graph] # include torch-geometric  ## NOTE: to install latest version from Github (may be unstable) install from source instead: # !pip install git+https://github.com/ai4co/rl4co.git In\u00a0[1]: Copied! <pre>from rl4co.envs import CVRPEnv\n\nfrom rl4co.models.zoo import AttentionModel\nfrom rl4co.utils.trainer import RL4COTrainer\n</pre> from rl4co.envs import CVRPEnv  from rl4co.models.zoo import AttentionModel from rl4co.utils.trainer import RL4COTrainer In\u00a0[3]: Copied! <pre># Init env, model, trainer\nenv = CVRPEnv(generator_params=dict(num_loc=20))\n\nmodel = AttentionModel(\n    env, \n    baseline='rollout',\n    train_data_size=100_000, # really small size for demo\n    val_data_size=10_000\n)\n \ntrainer = RL4COTrainer(\n    max_epochs=3, # few epochs for demo\n    accelerator='gpu',\n    devices=1,\n    logger=False,\n)\n\n# By default the AM uses the Graph Attention Encoder\nprint(f'Encoder: {model.policy.encoder._get_name()}')\n</pre> # Init env, model, trainer env = CVRPEnv(generator_params=dict(num_loc=20))  model = AttentionModel(     env,      baseline='rollout',     train_data_size=100_000, # really small size for demo     val_data_size=10_000 )   trainer = RL4COTrainer(     max_epochs=3, # few epochs for demo     accelerator='gpu',     devices=1,     logger=False, )  # By default the AM uses the Graph Attention Encoder print(f'Encoder: {model.policy.encoder._get_name()}') <pre>/datasets/home/botu/mambaforge/envs/rl4co-new/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n/datasets/home/botu/mambaforge/envs/rl4co-new/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\nUsing 16bit Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\n</pre> <pre>TPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</pre> <pre>Encoder: GraphAttentionEncoder\n</pre> In\u00a0[4]: Copied! <pre># Train the model\ntrainer.fit(model)\n</pre> # Train the model trainer.fit(model) <pre>/datasets/home/botu/mambaforge/envs/rl4co-new/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory /datasets/home/botu/Dev/rl4co/notebooks/tutorials/checkpoints exists and is not empty.\nval_file not set. Generating dataset instead\ntest_file not set. Generating dataset instead\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n\n  | Name     | Type                 | Params\n--------------------------------------------------\n0 | env      | CVRPEnv              | 0     \n1 | policy   | AttentionModelPolicy | 694 K \n2 | baseline | WarmupBaseline       | 694 K \n--------------------------------------------------\n1.4 M     Trainable params\n0         Non-trainable params\n1.4 M     Total params\n5.553     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>/datasets/home/botu/mambaforge/envs/rl4co-new/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n/datasets/home/botu/mambaforge/envs/rl4co-new/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n</pre> <pre>Training: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=3` reached.\n</pre> In\u00a0[5]: Copied! <pre># Before we init, we need to install the graph neural network dependencies\n# !pip install rl4co[graph]\n</pre> # Before we init, we need to install the graph neural network dependencies # !pip install rl4co[graph] In\u00a0[7]: Copied! <pre># Init the model with different encoder\nfrom rl4co.models.nn.graph.gcn import GCNEncoder\nfrom rl4co.models.nn.graph.mpnn import MessagePassingEncoder\n\ngcn_encoder = GCNEncoder(\n    env_name='cvrp', \n    embed_dim=128,\n    num_nodes=20, \n    num_layers=3,\n)\n\nmpnn_encoder = MessagePassingEncoder(\n    env_name='cvrp', \n    embed_dim=128,\n    num_nodes=20, \n    num_layers=3,\n)\n\nmodel = AttentionModel(\n    env, \n    baseline='rollout',\n    train_data_size=100_000, # really small size for demo\n    val_data_size=10_000, \n    policy_kwargs={\n        'encoder': gcn_encoder # gcn_encoder or mpnn_encoder\n    }\n)\n \ntrainer = RL4COTrainer(\n    max_epochs=3, # few epochs for demo\n    accelerator='gpu',\n    devices=1,\n    logger=False,\n)\n</pre> # Init the model with different encoder from rl4co.models.nn.graph.gcn import GCNEncoder from rl4co.models.nn.graph.mpnn import MessagePassingEncoder  gcn_encoder = GCNEncoder(     env_name='cvrp',      embed_dim=128,     num_nodes=20,      num_layers=3, )  mpnn_encoder = MessagePassingEncoder(     env_name='cvrp',      embed_dim=128,     num_nodes=20,      num_layers=3, )  model = AttentionModel(     env,      baseline='rollout',     train_data_size=100_000, # really small size for demo     val_data_size=10_000,      policy_kwargs={         'encoder': gcn_encoder # gcn_encoder or mpnn_encoder     } )   trainer = RL4COTrainer(     max_epochs=3, # few epochs for demo     accelerator='gpu',     devices=1,     logger=False, ) <pre>/datasets/home/botu/mambaforge/envs/rl4co-new/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n/datasets/home/botu/mambaforge/envs/rl4co-new/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\nUsing 16bit Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</pre> In\u00a0[8]: Copied! <pre># Train the model\ntrainer.fit(model)\n</pre> # Train the model trainer.fit(model) <pre>/datasets/home/botu/mambaforge/envs/rl4co-new/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory /datasets/home/botu/Dev/rl4co/notebooks/tutorials/checkpoints exists and is not empty.\nval_file not set. Generating dataset instead\ntest_file not set. Generating dataset instead\n</pre> <pre>LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n\n  | Name     | Type                 | Params\n--------------------------------------------------\n0 | env      | CVRPEnv              | 0     \n1 | policy   | AttentionModelPolicy | 148 K \n2 | baseline | WarmupBaseline       | 148 K \n--------------------------------------------------\n297 K     Trainable params\n0         Non-trainable params\n297 K     Total params\n1.191     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>/datasets/home/botu/mambaforge/envs/rl4co-new/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n/datasets/home/botu/mambaforge/envs/rl4co-new/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n</pre> <pre>Training: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=3` reached.\n</pre> In\u00a0[9]: Copied! <pre># Import necessary packages\nimport torch.nn as nn\nfrom torch import Tensor\nfrom tensordict import TensorDict\nfrom typing import Tuple, Union\nfrom rl4co.models.nn.env_embeddings import env_init_embedding\n\n\nclass BaseEncoder(nn.Module):\n    def __init__(\n            self,\n            env_name: str,\n            embed_dim: int,\n            init_embedding: nn.Module = None,\n        ):\n        super(BaseEncoder, self).__init__()\n        self.env_name = env_name\n        \n        # Init embedding for each environment\n        self.init_embedding = (\n            env_init_embedding(self.env_name, {\"embed_dim\": embed_dim})\n            if init_embedding is None\n            else init_embedding\n        )\n\n    def forward(\n        self, td: TensorDict, mask: Union[Tensor, None] = None\n    ) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"\n        Args:\n            td: Input TensorDict containing the environment state\n            mask: Mask to apply to the attention\n\n        Returns:\n            h: Latent representation of the input\n            init_h: Initial embedding of the input\n        \"\"\"\n        init_h = self.init_embedding(td)\n        h = None\n        return h, init_h\n</pre> # Import necessary packages import torch.nn as nn from torch import Tensor from tensordict import TensorDict from typing import Tuple, Union from rl4co.models.nn.env_embeddings import env_init_embedding   class BaseEncoder(nn.Module):     def __init__(             self,             env_name: str,             embed_dim: int,             init_embedding: nn.Module = None,         ):         super(BaseEncoder, self).__init__()         self.env_name = env_name                  # Init embedding for each environment         self.init_embedding = (             env_init_embedding(self.env_name, {\"embed_dim\": embed_dim})             if init_embedding is None             else init_embedding         )      def forward(         self, td: TensorDict, mask: Union[Tensor, None] = None     ) -&gt; Tuple[Tensor, Tensor]:         \"\"\"         Args:             td: Input TensorDict containing the environment state             mask: Mask to apply to the attention          Returns:             h: Latent representation of the input             init_h: Initial embedding of the input         \"\"\"         init_h = self.init_embedding(td)         h = None         return h, init_h"},{"location":"examples/modeling/3-change-encoder/#encoder-customization","title":"Encoder Customization\u00b6","text":"<p>In this notebook we will cover a tutorial for the flexible encoders!</p> <p></p>"},{"location":"examples/modeling/3-change-encoder/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install the package from PyPI. Remember to choose a GPU runtime for faster training!</p> <p>Note: You may need to restart the runtime in Colab after this</p>"},{"location":"examples/modeling/3-change-encoder/#imports","title":"Imports\u00b6","text":""},{"location":"examples/modeling/3-change-encoder/#a-default-minimal-training-script","title":"A default minimal training script\u00b6","text":"<p>Here we use the CVRP environment and AM model as a minimal example of training script. By default, the AM is initialized with a Graph Attention Encoder, but we can change it to anything we want.</p>"},{"location":"examples/modeling/3-change-encoder/#change-the-encoder","title":"Change the Encoder\u00b6","text":"<p>In RL4CO, we provides two graph neural network encoders: Graph Convolutionsal Network (GCN) encoder and Message Passing Neural Network (MPNN) encoder. In this tutorial, we will show how to change the encoder.</p> <p>Note: while we provide these examples, you can also implement your own encoder and use it in RL4CO! For instance, you may use different encoders (and decoders) to solve problems that require e.g. distance matrices as input</p>"},{"location":"examples/modeling/3-change-encoder/#or-you-want-to-create-your-own-encoder","title":"Or you want to create your own encoder\u00b6","text":"<p>If you want to create a new encoder, you may want to follow the following base class to create the encoder class with the folowing components:</p> <ol> <li>RL4CO provides the <code>env_init_embedding</code> method for each environment. You may want to use it to get the initial embedding of the environment.</li> <li><code>h</code> and <code>init_h</code> as return hidden features have the shape <code>([batch_size], num_node, hidden_size)</code></li> <li>In RL4CO, we put the graph neural network encoders in the <code>rl4co/models/nn/graph</code> folder. You may want to put your customized encoder to the same folder. Feel free to send a PR to add your encoder to RL4CO!</li> </ol>"},{"location":"examples/other/1-mtvrp/","title":"MTVRP: Multi-task VRP environment","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nfrom rl4co.envs.routing.mtvrp.env import MTVRPEnv\nfrom rl4co.envs.routing.mtvrp.generator import MTVRPGenerator\n</pre> %load_ext autoreload %autoreload 2  from rl4co.envs.routing.mtvrp.env import MTVRPEnv from rl4co.envs.routing.mtvrp.generator import MTVRPGenerator <pre>/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning_utilities/core/imports.py:14: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  import pkg_resources\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/pkg_resources/__init__.py:2832: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning.fabric')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/pkg_resources/__init__.py:2317: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('lightning')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(parent)\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('lightning.pytorch')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/pkg_resources/__init__.py:2317: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('lightning')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(parent)\n</pre> <p>Let's now generate some variants! By default, we can generate all variants with the <code>variants_preset</code> variable</p> In\u00a0[2]: Copied! <pre># Single feat: generate a distribution of single-featured environments\ngenerator = MTVRPGenerator(num_loc=50, variant_preset=\"all\")\nenv = MTVRPEnv(generator, check_solution=False)\n\ntd_data = env.generator(8)\nenv.get_variant_names(td_data)\n</pre> # Single feat: generate a distribution of single-featured environments generator = MTVRPGenerator(num_loc=50, variant_preset=\"all\") env = MTVRPEnv(generator, check_solution=False)  td_data = env.generator(8) env.get_variant_names(td_data) Out[2]: <pre>['VRPLTW', 'OVRP', 'VRPLTW', 'OVRPLTW', 'OVRPL', 'VRPB', 'OVRPTW', 'OVRPB']</pre> In\u00a0[3]: Copied! <pre># Here is the list of presets and their probabilities of being generated (fully customizable)\nenv.print_presets()\n</pre> # Here is the list of presets and their probabilities of being generated (fully customizable) env.print_presets() <pre>all: {'O': 0.5, 'TW': 0.5, 'L': 0.5, 'B': 0.5}\nsingle_feat: {'O': 0.5, 'TW': 0.5, 'L': 0.5, 'B': 0.5}\nsingle_feat_otw: {'O': 0.5, 'TW': 0.5, 'L': 0.5, 'B': 0.5, 'OTW': 0.5}\ncvrp: {'O': 0.0, 'TW': 0.0, 'L': 0.0, 'B': 0.0}\novrp: {'O': 1.0, 'TW': 0.0, 'L': 0.0, 'B': 0.0}\nvrpb: {'O': 0.0, 'TW': 0.0, 'L': 0.0, 'B': 1.0}\nvrpl: {'O': 0.0, 'TW': 0.0, 'L': 1.0, 'B': 0.0}\nvrptw: {'O': 0.0, 'TW': 1.0, 'L': 0.0, 'B': 0.0}\novrptw: {'O': 1.0, 'TW': 1.0, 'L': 0.0, 'B': 0.0}\novrpb: {'O': 1.0, 'TW': 0.0, 'L': 0.0, 'B': 1.0}\novrpl: {'O': 1.0, 'TW': 0.0, 'L': 1.0, 'B': 0.0}\nvrpbl: {'O': 0.0, 'TW': 0.0, 'L': 1.0, 'B': 1.0}\nvrpbtw: {'O': 0.0, 'TW': 1.0, 'L': 0.0, 'B': 1.0}\nvrpltw: {'O': 0.0, 'TW': 1.0, 'L': 1.0, 'B': 0.0}\novrpbl: {'O': 1.0, 'TW': 0.0, 'L': 1.0, 'B': 1.0}\novrpbtw: {'O': 1.0, 'TW': 1.0, 'L': 0.0, 'B': 1.0}\novrpltw: {'O': 1.0, 'TW': 1.0, 'L': 1.0, 'B': 0.0}\nvrpbltw: {'O': 0.0, 'TW': 1.0, 'L': 1.0, 'B': 1.0}\novrpbltw: {'O': 1.0, 'TW': 1.0, 'L': 1.0, 'B': 1.0}\n</pre> <p>We can change the preset to generate some specific variant, for instance the VRPB</p> In\u00a0[4]: Copied! <pre># Change generator\ngenerator = MTVRPGenerator(num_loc=50, variant_preset=\"vrpb\")\nenv.generator = generator\ntd_data = env.generator(8)\nenv.get_variant_names(td_data)\n</pre> # Change generator generator = MTVRPGenerator(num_loc=50, variant_preset=\"vrpb\") env.generator = generator td_data = env.generator(8) env.get_variant_names(td_data) <pre>vrpb selected. Will not use feature combination!\n</pre> Out[4]: <pre>['VRPB', 'VRPB', 'VRPB', 'VRPB', 'VRPB', 'VRPB', 'VRPB', 'VRPB']</pre> In\u00a0[5]: Copied! <pre>import torch\nfrom rl4co.utils.ops import gather_by_index\n\n\n# Simple heuristics (nearest neighbor + capacity check)\ndef greedy_policy(td):\n    \"\"\"Select closest available action\"\"\"\n    available_actions = td[\"action_mask\"]\n    # distances\n    curr_node = td[\"current_node\"]\n    loc_cur = gather_by_index(td[\"locs\"], curr_node)\n    distances_next = torch.cdist(loc_cur[:, None, :], td[\"locs\"], p=2.0).squeeze(1)\n\n    distances_next[~available_actions.bool()] = float(\"inf\")\n    # do not select depot if some capacity is left\n    distances_next[:, 0] = float(\"inf\") * (\n        td[\"used_capacity_linehaul\"] &lt; td[\"vehicle_capacity\"]\n    ).float().squeeze(-1)\n\n    # # if sum of available actions is 0, select depot\n    # distances_next[available_actions.sum(-1) == 0, 0] = 0\n    action = torch.argmin(distances_next, dim=-1)\n    td.set(\"action\", action)\n    return td\n\n\ndef rollout(env, td, policy=greedy_policy, max_steps: int = None):\n    \"\"\"Helper function to rollout a policy. Currently, TorchRL does not allow to step\n    over envs when done with `env.rollout()`. We need this because for environments that complete at different steps.\n    \"\"\"\n\n    max_steps = float(\"inf\") if max_steps is None else max_steps\n    actions = []\n    steps = 0\n\n    while not td[\"done\"].all():\n        td = policy(td)\n        actions.append(td[\"action\"])\n        td = env.step(td)[\"next\"]\n        steps += 1\n        if steps &gt; max_steps:\n            print(\"Max steps reached\")\n            break\n    return torch.stack(actions, dim=1)\n</pre> import torch from rl4co.utils.ops import gather_by_index   # Simple heuristics (nearest neighbor + capacity check) def greedy_policy(td):     \"\"\"Select closest available action\"\"\"     available_actions = td[\"action_mask\"]     # distances     curr_node = td[\"current_node\"]     loc_cur = gather_by_index(td[\"locs\"], curr_node)     distances_next = torch.cdist(loc_cur[:, None, :], td[\"locs\"], p=2.0).squeeze(1)      distances_next[~available_actions.bool()] = float(\"inf\")     # do not select depot if some capacity is left     distances_next[:, 0] = float(\"inf\") * (         td[\"used_capacity_linehaul\"] &lt; td[\"vehicle_capacity\"]     ).float().squeeze(-1)      # # if sum of available actions is 0, select depot     # distances_next[available_actions.sum(-1) == 0, 0] = 0     action = torch.argmin(distances_next, dim=-1)     td.set(\"action\", action)     return td   def rollout(env, td, policy=greedy_policy, max_steps: int = None):     \"\"\"Helper function to rollout a policy. Currently, TorchRL does not allow to step     over envs when done with `env.rollout()`. We need this because for environments that complete at different steps.     \"\"\"      max_steps = float(\"inf\") if max_steps is None else max_steps     actions = []     steps = 0      while not td[\"done\"].all():         td = policy(td)         actions.append(td[\"action\"])         td = env.step(td)[\"next\"]         steps += 1         if steps &gt; max_steps:             print(\"Max steps reached\")             break     return torch.stack(actions, dim=1) In\u00a0[6]: Copied! <pre># NOTE: if we don't select ovrpbltw, the below does not work and there is still some\n# minor bug in either masking or variant subselection\n\ngenerator = MTVRPGenerator(num_loc=50, variant_preset=\"all\")\nenv.generator = generator\ntd_data = env.generator(3)\nvariant_names = env.get_variant_names(td_data)\n\ntd = env.reset(td_data)\n\nactions = rollout(env, td.clone(), greedy_policy)\nrewards = env.get_reward(td, actions)\n\nfor idx in [0, 1, 2]:\n    env.render(td[idx], actions[idx])\n    print(\"Cost: \", - rewards[idx].item())\n    print(\"Problem: \", variant_names[idx])\n</pre> # NOTE: if we don't select ovrpbltw, the below does not work and there is still some # minor bug in either masking or variant subselection  generator = MTVRPGenerator(num_loc=50, variant_preset=\"all\") env.generator = generator td_data = env.generator(3) variant_names = env.get_variant_names(td_data)  td = env.reset(td_data)  actions = rollout(env, td.clone(), greedy_policy) rewards = env.get_reward(td, actions)  for idx in [0, 1, 2]:     env.render(td[idx], actions[idx])     print(\"Cost: \", - rewards[idx].item())     print(\"Problem: \", variant_names[idx])  <pre>Cost:  17.503389358520508\nProblem:  OVRPLTW\n</pre> <pre>Cost:  18.86773109436035\nProblem:  CVRP\n</pre> <pre>Cost:  15.39835262298584\nProblem:  VRPB\n</pre> In\u00a0[7]: Copied! <pre>from rl4co.utils.trainer import RL4COTrainer\nfrom rl4co.models.zoo import MVMoE_POMO\n\ndevice_id = 0\ndevice = torch.device(f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\")\ngenerator = MTVRPGenerator(num_loc=50, variant_preset=\"single_feat\")\nenv = MTVRPEnv(generator, check_solution=False)\n</pre> from rl4co.utils.trainer import RL4COTrainer from rl4co.models.zoo import MVMoE_POMO  device_id = 0 device = torch.device(f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\") generator = MTVRPGenerator(num_loc=50, variant_preset=\"single_feat\") env = MTVRPEnv(generator, check_solution=False) <pre>single_feat selected. Will not use feature combination!\n</pre> In\u00a0[8]: Copied! <pre>moe_kwargs = {\"encoder\": {\"hidden_act\": \"ReLU\", \"num_experts\": 4, \"k\": 2, \"noisy_gating\": True},\n              \"decoder\": {\"light_version\": False, \"num_experts\": 4, \"k\": 2, \"noisy_gating\": True}}\nmodel = MVMoE_POMO(\n    env,\n    moe_kwargs=moe_kwargs,\n    batch_size=128,\n    train_data_size=10000,  # each epoch,\n    val_batch_size=100,\n    val_data_size=1000,\n    optimizer=\"Adam\",\n    optimizer_kwargs={\"lr\": 1e-4, \"weight_decay\": 1e-6},\n    lr_scheduler=\"MultiStepLR\",\n    lr_scheduler_kwargs={\"milestones\": [451, ], \"gamma\": 0.1},\n)\n\ntrainer = RL4COTrainer(\n        max_epochs=3,\n        accelerator=\"gpu\",\n        devices=[device_id],\n        logger=None\n    )\n\ntrainer.fit(model)\n</pre> moe_kwargs = {\"encoder\": {\"hidden_act\": \"ReLU\", \"num_experts\": 4, \"k\": 2, \"noisy_gating\": True},               \"decoder\": {\"light_version\": False, \"num_experts\": 4, \"k\": 2, \"noisy_gating\": True}} model = MVMoE_POMO(     env,     moe_kwargs=moe_kwargs,     batch_size=128,     train_data_size=10000,  # each epoch,     val_batch_size=100,     val_data_size=1000,     optimizer=\"Adam\",     optimizer_kwargs={\"lr\": 1e-4, \"weight_decay\": 1e-6},     lr_scheduler=\"MultiStepLR\",     lr_scheduler_kwargs={\"milestones\": [451, ], \"gamma\": 0.1}, )  trainer = RL4COTrainer(         max_epochs=3,         accelerator=\"gpu\",         devices=[device_id],         logger=None     )  trainer.fit(model) <pre>/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\nUsing 16bit Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\nMissing logger folder: /home/botu/Dev/rl4co/examples/other/lightning_logs\nval_file not set. Generating dataset instead\ntest_file not set. Generating dataset instead\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n\n  | Name     | Type                 | Params\n--------------------------------------------------\n0 | env      | MTVRPEnv             | 0     \n1 | policy   | AttentionModelPolicy | 3.7 M \n2 | baseline | SharedBaseline       | 0     \n--------------------------------------------------\n3.7 M     Trainable params\n0         Non-trainable params\n3.7 M     Total params\n14.868    Total estimated model params size (MB)\n</pre> <pre>Sanity Checking: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n</pre> <pre>Training: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=3` reached.\n</pre> In\u00a0[34]: Copied! <pre># Greedy rollouts over trained model (same states as previous plot)\npolicy = model.policy.to(device)\nout = policy(td.to(device).clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\nactions_mvmoe = out['actions'].cpu().detach()\nrewards_mvmoe = out['reward'].cpu().detach()\n\nfor idx in [0, 1, 2]:\n    env.render(td[idx], actions_mvmoe[idx])\n    print(\"Cost: \", -rewards_mvmoe[idx].item())\n    print(\"Problem: \", variant_names[idx])\n</pre> # Greedy rollouts over trained model (same states as previous plot) policy = model.policy.to(device) out = policy(td.to(device).clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True) actions_mvmoe = out['actions'].cpu().detach() rewards_mvmoe = out['reward'].cpu().detach()  for idx in [0, 1, 2]:     env.render(td[idx], actions_mvmoe[idx])     print(\"Cost: \", -rewards_mvmoe[idx].item())     print(\"Problem: \", variant_names[idx]) <pre>Cost:  17.188127517700195\nProblem:  OVRPLTW\n</pre> <pre>Cost:  14.578388214111328\nProblem:  CVRP\n</pre> <pre>Cost:  12.24499797821045\nProblem:  VRPB\n</pre> In\u00a0[31]: Copied! <pre># PyVRP - HGS\npyvrp_actions, pyvrp_costs = env.solve(td, max_runtime=5, num_procs=10, solver=\"pyvrp\")\nrewards_pyvrp = env.get_reward(td, pyvrp_actions)\n</pre> # PyVRP - HGS pyvrp_actions, pyvrp_costs = env.solve(td, max_runtime=5, num_procs=10, solver=\"pyvrp\") rewards_pyvrp = env.get_reward(td, pyvrp_actions) In\u00a0[36]: Copied! <pre>def calculate_gap(cost, bks):   \n    gaps = (cost - bks) / bks\n    return gaps.mean() * 100\n\n# Nearest insertion\nactions = rollout(env, td.clone(), greedy_policy)\nrewards_ni = env.get_reward(td, actions)\n\nprint(rewards_mvmoe, rewards_ni, rewards_pyvrp)   \nprint(f\"Gap to HGS (NI): {calculate_gap(-rewards_ni, -rewards_pyvrp):.2f}%\")\nprint(f\"Gap to HGS (MVMoE): {calculate_gap(-rewards_mvmoe, -rewards_pyvrp):.2f}%\")\n</pre> def calculate_gap(cost, bks):        gaps = (cost - bks) / bks     return gaps.mean() * 100  # Nearest insertion actions = rollout(env, td.clone(), greedy_policy) rewards_ni = env.get_reward(td, actions)  print(rewards_mvmoe, rewards_ni, rewards_pyvrp)    print(f\"Gap to HGS (NI): {calculate_gap(-rewards_ni, -rewards_pyvrp):.2f}%\") print(f\"Gap to HGS (MVMoE): {calculate_gap(-rewards_mvmoe, -rewards_pyvrp):.2f}%\") <pre>tensor([-17.1881, -14.5784, -12.2450]) tensor([-17.5034, -18.8677, -15.3984]) tensor([-12.6954, -11.9107,  -9.9261])\nGap to HGS (NI): 50.47%\nGap to HGS (MVMoE): 27.05%\n</pre> <p>With only two short epochs, we can already get better than NI!</p>"},{"location":"examples/other/1-mtvrp/#mtvrp-multi-task-vrp-environment","title":"MTVRP: Multi-task VRP environment\u00b6","text":"<p>This environment can handle any of the following variants:</p> VRP Variant Capacity (C) Open Route (O) Backhaul (B) Duration Limit (L) Time Window (TW) CVRP \u2714 OVRP \u2714 \u2714 VRPB \u2714 \u2714 VRPL \u2714 \u2714 VRPTW \u2714 \u2714 OVRPTW \u2714 \u2714 \u2714 OVRPB \u2714 \u2714 \u2714 OVRPL \u2714 \u2714 \u2714 VRPBL \u2714 \u2714 \u2714 VRPBTW \u2714 \u2714 \u2714 VRPLTW \u2714 \u2714 \u2714 OVRPBL \u2714 \u2714 \u2714 \u2714 OVRPBTW \u2714 \u2714 \u2714 \u2714 OVRPLTW \u2714 \u2714 \u2714 \u2714 VRPBLTW \u2714 \u2714 \u2714 \u2714 OVRPBLTW \u2714 \u2714 \u2714 \u2714 \u2714 <p>It is fully batched, meaning that different variants can be in the same batch too!</p>"},{"location":"examples/other/1-mtvrp/#greedy-rollout-and-plot","title":"Greedy rollout and plot\u00b6","text":""},{"location":"examples/other/1-mtvrp/#train-mvmoe-on-multiple-problems","title":"Train MVMoE on Multiple Problems\u00b6","text":""},{"location":"examples/other/1-mtvrp/#getting-gaps-to-classical-solvers","title":"Getting gaps to classical solvers\u00b6","text":"<p>We additionally offer an optional <code>solve</code> API to get solutions from classical solvers. We can use this to get the gaps to the optimal solutions.</p>"},{"location":"examples/other/2-scheduling/","title":"Solving the Flexible Job-Shop Scheduling Problem (FJSP)","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.display import display, clear_output\nimport time\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom rl4co.envs import FJSPEnv\nfrom rl4co.models.zoo.l2d import L2DModel\nfrom rl4co.models.zoo.l2d.policy import L2DPolicy\nfrom rl4co.models.zoo.l2d.decoder import L2DDecoder\nfrom rl4co.models.nn.graph.hgnn import HetGNNEncoder\nfrom rl4co.utils.trainer import RL4COTrainer\n</pre> %load_ext autoreload %autoreload 2  import torch import numpy as np import matplotlib.pyplot as plt import numpy as np from IPython.display import display, clear_output import time import networkx as nx import matplotlib.pyplot as plt from rl4co.envs import FJSPEnv from rl4co.models.zoo.l2d import L2DModel from rl4co.models.zoo.l2d.policy import L2DPolicy from rl4co.models.zoo.l2d.decoder import L2DDecoder from rl4co.models.nn.graph.hgnn import HetGNNEncoder from rl4co.utils.trainer import RL4COTrainer <pre>/home/laurin.luttmann/miniconda3/envs/cuda1203/lib/python3.10/site-packages/lightning_utilities/core/imports.py:14: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  import pkg_resources\n/home/laurin.luttmann/miniconda3/envs/cuda1203/lib/python3.10/site-packages/lightning/fabric/__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning.fabric')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n/home/laurin.luttmann/miniconda3/envs/cuda1203/lib/python3.10/site-packages/pkg_resources/__init__.py:2317: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('lightning')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(parent)\n/home/laurin.luttmann/miniconda3/envs/cuda1203/lib/python3.10/site-packages/lightning/pytorch/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('lightning.pytorch')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n/home/laurin.luttmann/miniconda3/envs/cuda1203/lib/python3.10/site-packages/pkg_resources/__init__.py:2317: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('lightning')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(parent)\n/home/laurin.luttmann/miniconda3/envs/cuda1203/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[3]: Copied! <pre>generator_params = {\n  \"num_jobs\": 5,  # the total number of jobs\n  \"num_machines\": 5,  # the total number of machines that can process operations\n  \"min_ops_per_job\": 1,  # minimum number of operatios per job\n  \"max_ops_per_job\": 2,  # maximum number of operations per job\n  \"min_processing_time\": 1,  # the minimum time required for a machine to process an operation\n  \"max_processing_time\": 20,  # the maximum time required for a machine to process an operation\n  \"min_eligible_ma_per_op\": 1,  # the minimum number of machines capable to process an operation\n  \"max_eligible_ma_per_op\": 2,  # the maximum number of machines capable to process an operation\n}\n</pre> generator_params = {   \"num_jobs\": 5,  # the total number of jobs   \"num_machines\": 5,  # the total number of machines that can process operations   \"min_ops_per_job\": 1,  # minimum number of operatios per job   \"max_ops_per_job\": 2,  # maximum number of operations per job   \"min_processing_time\": 1,  # the minimum time required for a machine to process an operation   \"max_processing_time\": 20,  # the maximum time required for a machine to process an operation   \"min_eligible_ma_per_op\": 1,  # the minimum number of machines capable to process an operation   \"max_eligible_ma_per_op\": 2,  # the maximum number of machines capable to process an operation } In\u00a0[4]: Copied! <pre>env = FJSPEnv(generator_params=generator_params)\ntd = env.reset(batch_size=[1])\n</pre> env = FJSPEnv(generator_params=generator_params) td = env.reset(batch_size=[1]) In\u00a0[5]: Copied! <pre># Create a bipartite graph from the adjacency matrix\nG = nx.Graph()\nproc_times = td[\"proc_times\"].squeeze(0)\njob_ops_adj = td[\"job_ops_adj\"].squeeze(0)\norder = td[\"ops_sequence_order\"].squeeze(0) + 1\n\nnum_machines, num_operations = proc_times.shape\nnum_jobs = job_ops_adj.size(0)\n\njobs = [f\"j{i+1}\" for i in range(num_jobs)]\nmachines = [f\"m{i+1}\" for i in range(num_machines)]\noperations = [f\"o{i+1}\" for i in range(num_operations)]\n\n# Add nodes from each set\nG.add_nodes_from(machines, bipartite=0)\nG.add_nodes_from(operations, bipartite=1)\nG.add_nodes_from(jobs, bipartite=2)\n\n# Add edges based on the adjacency matrix\nfor i in range(num_machines):\n    for j in range(num_operations):\n        edge_weigth = proc_times[i][j]\n        if edge_weigth != 0:\n            G.add_edge(f\"m{i+1}\", f\"o{j+1}\", weight=edge_weigth)\n\n\n# Add edges based on the adjacency matrix\nfor i in range(num_jobs):\n    for j in range(num_operations):\n        edge_weigth = job_ops_adj[i][j]\n        if edge_weigth != 0:\n            G.add_edge(f\"j{i+1}\", f\"o{j+1}\", weight=3, label=order[j])\n\n\nwidths = [x / 3 for x in nx.get_edge_attributes(G, 'weight').values()]\n\nplt.figure(figsize=(10,6))\n# Plot the graph\n\nmachines = [n for n, d in G.nodes(data=True) if d['bipartite'] == 0]\noperations = [n for n, d in G.nodes(data=True) if d['bipartite'] == 1]\njobs = [n for n, d in G.nodes(data=True) if d['bipartite'] == 2]\n\npos = {}\npos.update((node, (1, index)) for index, node in enumerate(machines))\npos.update((node, (2, index)) for index, node in enumerate(operations))\npos.update((node, (3, index)) for index, node in enumerate(jobs))\n\nedge_labels = {(u, v): d['label'].item() for u, v, d in G.edges(data=True) if d.get(\"label\") is not None}\nnx.draw_networkx_edge_labels(G, {k: (v[0]+.12, v[1]) for k,v in pos.items()}, edge_labels=edge_labels, rotate=False)\n\nnx.draw_networkx_nodes(G, pos, nodelist=machines, node_color='b', label=\"Machine\")\nnx.draw_networkx_nodes(G, pos, nodelist=operations, node_color='r', label=\"Operation\")\nnx.draw_networkx_nodes(G, pos, nodelist=jobs, node_color='y', label=\"jobs\")\nnx.draw_networkx_edges(G, pos, width=widths, alpha=0.6)\n\nplt.title('Visualization of the FJSP')\nplt.legend(bbox_to_anchor=(.95, 1.05))\nplt.axis('off')\nplt.show()\n</pre> # Create a bipartite graph from the adjacency matrix G = nx.Graph() proc_times = td[\"proc_times\"].squeeze(0) job_ops_adj = td[\"job_ops_adj\"].squeeze(0) order = td[\"ops_sequence_order\"].squeeze(0) + 1  num_machines, num_operations = proc_times.shape num_jobs = job_ops_adj.size(0)  jobs = [f\"j{i+1}\" for i in range(num_jobs)] machines = [f\"m{i+1}\" for i in range(num_machines)] operations = [f\"o{i+1}\" for i in range(num_operations)]  # Add nodes from each set G.add_nodes_from(machines, bipartite=0) G.add_nodes_from(operations, bipartite=1) G.add_nodes_from(jobs, bipartite=2)  # Add edges based on the adjacency matrix for i in range(num_machines):     for j in range(num_operations):         edge_weigth = proc_times[i][j]         if edge_weigth != 0:             G.add_edge(f\"m{i+1}\", f\"o{j+1}\", weight=edge_weigth)   # Add edges based on the adjacency matrix for i in range(num_jobs):     for j in range(num_operations):         edge_weigth = job_ops_adj[i][j]         if edge_weigth != 0:             G.add_edge(f\"j{i+1}\", f\"o{j+1}\", weight=3, label=order[j])   widths = [x / 3 for x in nx.get_edge_attributes(G, 'weight').values()]  plt.figure(figsize=(10,6)) # Plot the graph  machines = [n for n, d in G.nodes(data=True) if d['bipartite'] == 0] operations = [n for n, d in G.nodes(data=True) if d['bipartite'] == 1] jobs = [n for n, d in G.nodes(data=True) if d['bipartite'] == 2]  pos = {} pos.update((node, (1, index)) for index, node in enumerate(machines)) pos.update((node, (2, index)) for index, node in enumerate(operations)) pos.update((node, (3, index)) for index, node in enumerate(jobs))  edge_labels = {(u, v): d['label'].item() for u, v, d in G.edges(data=True) if d.get(\"label\") is not None} nx.draw_networkx_edge_labels(G, {k: (v[0]+.12, v[1]) for k,v in pos.items()}, edge_labels=edge_labels, rotate=False)  nx.draw_networkx_nodes(G, pos, nodelist=machines, node_color='b', label=\"Machine\") nx.draw_networkx_nodes(G, pos, nodelist=operations, node_color='r', label=\"Operation\") nx.draw_networkx_nodes(G, pos, nodelist=jobs, node_color='y', label=\"jobs\") nx.draw_networkx_edges(G, pos, width=widths, alpha=0.6)  plt.title('Visualization of the FJSP') plt.legend(bbox_to_anchor=(.95, 1.05)) plt.axis('off') plt.show() In\u00a0[6]: Copied! <pre># Lets generate a more complex instance\n\ngenerator_params = {\n  \"num_jobs\": 10,  # the total number of jobs\n  \"num_machines\": 5,  # the total number of machines that can process operations\n  \"min_ops_per_job\": 4,  # minimum number of operatios per job\n  \"max_ops_per_job\": 6,  # maximum number of operations per job\n  \"min_processing_time\": 1,  # the minimum time required for a machine to process an operation\n  \"max_processing_time\": 20,  # the maximum time required for a machine to process an operation\n  \"min_eligible_ma_per_op\": 1,  # the minimum number of machines capable to process an operation\n  \"max_eligible_ma_per_op\": 5,  # the maximum number of machines capable to process an operation\n}\n\nenv = FJSPEnv(generator_params=generator_params)\ntd = env.reset(batch_size=[1])\n</pre> # Lets generate a more complex instance  generator_params = {   \"num_jobs\": 10,  # the total number of jobs   \"num_machines\": 5,  # the total number of machines that can process operations   \"min_ops_per_job\": 4,  # minimum number of operatios per job   \"max_ops_per_job\": 6,  # maximum number of operations per job   \"min_processing_time\": 1,  # the minimum time required for a machine to process an operation   \"max_processing_time\": 20,  # the maximum time required for a machine to process an operation   \"min_eligible_ma_per_op\": 1,  # the minimum number of machines capable to process an operation   \"max_eligible_ma_per_op\": 5,  # the maximum number of machines capable to process an operation }  env = FJSPEnv(generator_params=generator_params) td = env.reset(batch_size=[1]) In\u00a0[7]: Copied! <pre>encoder = HetGNNEncoder(embed_dim=32, num_layers=2)\n(ma_emb, op_emb), init = encoder(td)\nprint(ma_emb.shape)\nprint(op_emb.shape)\n</pre> encoder = HetGNNEncoder(embed_dim=32, num_layers=2) (ma_emb, op_emb), init = encoder(td) print(ma_emb.shape) print(op_emb.shape) <pre>torch.Size([1, 60, 32])\ntorch.Size([1, 5, 32])\n</pre> <p>The decoder return logits over a composite action-space of size (1 + num_jobs * num_machines), where each entry corresponds to a machine-job combination plus one waiting-operation. The selected action specifies, which job is processed next by which machine. To be more precise, the next operation of the selected job is processed. This operation can be retrieved from td[\"next_op\"]</p> In\u00a0[8]: Copied! <pre># next operation per job\ntd[\"next_op\"]\n</pre> # next operation per job td[\"next_op\"] Out[8]: <pre>tensor([[ 0,  4,  9, 15, 21, 27, 31, 37, 41, 45]])</pre> In\u00a0[9]: Copied! <pre>decoder = L2DDecoder(env_name=env.name, embed_dim=32)\nlogits, mask = decoder(td, (ma_emb, op_emb), num_starts=0)\n# (1 + num_jobs * num_machines)\nprint(logits.shape)\n</pre> decoder = L2DDecoder(env_name=env.name, embed_dim=32) logits, mask = decoder(td, (ma_emb, op_emb), num_starts=0) # (1 + num_jobs * num_machines) print(logits.shape) <pre>torch.Size([1, 51])\n</pre> In\u00a0[10]: Copied! <pre>def make_step(td):\n    logits, mask = decoder(td, (ma_emb, op_emb), num_starts=0)\n    action = logits.masked_fill(~mask, -torch.inf).argmax(1)\n    td[\"action\"] = action\n    td = env.step(td)[\"next\"]\n    return td\n</pre> def make_step(td):     logits, mask = decoder(td, (ma_emb, op_emb), num_starts=0)     action = logits.masked_fill(~mask, -torch.inf).argmax(1)     td[\"action\"] = action     td = env.step(td)[\"next\"]     return td In\u00a0[20]: Copied! <pre>env.render(td, 0)\n# Update plot within a for loop\nwhile not td[\"done\"].all():\n    # Clear the previous output for the next iteration\n    clear_output(wait=True)\n\n    td = make_step(td)\n    env.render(td, 0)\n    # Display updated plot\n    display(plt.gcf())\n    \n    # Pause for a moment to see the changes\n    time.sleep(.4)\n</pre> env.render(td, 0) # Update plot within a for loop while not td[\"done\"].all():     # Clear the previous output for the next iteration     clear_output(wait=True)      td = make_step(td)     env.render(td, 0)     # Display updated plot     display(plt.gcf())          # Pause for a moment to see the changes     time.sleep(.4) <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre> <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre> <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre> In\u00a0[20]: Copied! <pre>if torch.cuda.is_available():\n    accelerator = \"gpu\"\n    batch_size = 256\n    train_data_size = 2_000\n    embed_dim = 128\n    num_encoder_layers = 4\nelse:\n    accelerator = \"cpu\"\n    batch_size = 32\n    train_data_size = 1_000\n    embed_dim = 64\n    num_encoder_layers = 2\n</pre> if torch.cuda.is_available():     accelerator = \"gpu\"     batch_size = 256     train_data_size = 2_000     embed_dim = 128     num_encoder_layers = 4 else:     accelerator = \"cpu\"     batch_size = 32     train_data_size = 1_000     embed_dim = 64     num_encoder_layers = 2 In\u00a0[21]: Copied! <pre># Policy: neural network, in this case with encoder-decoder architecture\npolicy = L2DPolicy(embed_dim=embed_dim, num_encoder_layers=num_encoder_layers, env_name=\"fjsp\")\n\n# Model: default is AM with REINFORCE and greedy rollout baseline\nmodel = L2DModel(env,\n                 policy=policy, \n                 baseline=\"rollout\",\n                 batch_size=batch_size,\n                 train_data_size=train_data_size,\n                 val_data_size=1_000,\n                 optimizer_kwargs={\"lr\": 1e-4})\n\ntrainer = RL4COTrainer(\n    max_epochs=3,\n    accelerator=accelerator,\n    devices=1,\n    logger=None,\n)\n\ntrainer.fit(model)\n</pre> # Policy: neural network, in this case with encoder-decoder architecture policy = L2DPolicy(embed_dim=embed_dim, num_encoder_layers=num_encoder_layers, env_name=\"fjsp\")  # Model: default is AM with REINFORCE and greedy rollout baseline model = L2DModel(env,                  policy=policy,                   baseline=\"rollout\",                  batch_size=batch_size,                  train_data_size=train_data_size,                  val_data_size=1_000,                  optimizer_kwargs={\"lr\": 1e-4})  trainer = RL4COTrainer(     max_epochs=3,     accelerator=accelerator,     devices=1,     logger=None, )  trainer.fit(model) <pre>Using 16bit Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nval_file not set. Generating dataset instead\ntest_file not set. Generating dataset instead\n</pre> <pre>\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[21], line 20\n      5 model = L2DModel(env,\n      6                  policy=policy, \n      7                  baseline=\"rollout\",\n   (...)\n     10                  val_data_size=1_000,\n     11                  optimizer_kwargs={\"lr\": 1e-4})\n     13 trainer = RL4COTrainer(\n     14     max_epochs=3,\n     15     accelerator=accelerator,\n     16     devices=1,\n     17     logger=None,\n     18 )\n---&gt; 20 trainer.fit(model)\n\nFile ~/repos/ai4co/rl4co/rl4co/utils/trainer.py:146, in RL4COTrainer.fit(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\n    141         log.warning(\n    142             \"Overriding gradient_clip_val to None for 'automatic_optimization=False' models\"\n    143         )\n    144         self.gradient_clip_val = None\n--&gt; 146 super().fit(\n    147     model=model,\n    148     train_dataloaders=train_dataloaders,\n    149     val_dataloaders=val_dataloaders,\n    150     datamodule=datamodule,\n    151     ckpt_path=ckpt_path,\n    152 )\n\nFile ~/miniconda3/envs/cuda1203/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:544, in Trainer.fit(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\n    542 self.state.status = TrainerStatus.RUNNING\n    543 self.training = True\n--&gt; 544 call._call_and_handle_interrupt(\n    545     self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n    546 )\n\nFile ~/miniconda3/envs/cuda1203/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44, in _call_and_handle_interrupt(trainer, trainer_fn, *args, **kwargs)\n     42     if trainer.strategy.launcher is not None:\n     43         return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n---&gt; 44     return trainer_fn(*args, **kwargs)\n     46 except _TunerExitException:\n     47     _call_teardown_hook(trainer)\n\nFile ~/miniconda3/envs/cuda1203/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:580, in Trainer._fit_impl(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\n    573 assert self.state.fn is not None\n    574 ckpt_path = self._checkpoint_connector._select_ckpt_path(\n    575     self.state.fn,\n    576     ckpt_path,\n    577     model_provided=True,\n    578     model_connected=self.lightning_module is not None,\n    579 )\n--&gt; 580 self._run(model, ckpt_path=ckpt_path)\n    582 assert self.state.stopped\n    583 self.training = False\n\nFile ~/miniconda3/envs/cuda1203/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:949, in Trainer._run(self, model, ckpt_path)\n    946 log.debug(f\"{self.__class__.__name__}: preparing data\")\n    947 self._data_connector.prepare_data()\n--&gt; 949 call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment\n    950 log.debug(f\"{self.__class__.__name__}: configuring model\")\n    951 call._call_configure_model(self)\n\nFile ~/miniconda3/envs/cuda1203/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:94, in _call_setup_hook(trainer)\n     92     _call_lightning_datamodule_hook(trainer, \"setup\", stage=fn)\n     93 _call_callback_hooks(trainer, \"setup\", stage=fn)\n---&gt; 94 _call_lightning_module_hook(trainer, \"setup\", stage=fn)\n     96 trainer.strategy.barrier(\"post_setup\")\n\nFile ~/miniconda3/envs/cuda1203/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:157, in _call_lightning_module_hook(trainer, hook_name, pl_module, *args, **kwargs)\n    154 pl_module._current_fx_name = hook_name\n    156 with trainer.profiler.profile(f\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"):\n--&gt; 157     output = fn(*args, **kwargs)\n    159 # restore current_fx when nested context\n    160 pl_module._current_fx_name = prev_fx_name\n\nFile ~/repos/ai4co/rl4co/rl4co/models/rl/common/base.py:155, in RL4COLitModule.setup(self, stage)\n    153 self.dataloader_names = None\n    154 self.setup_loggers()\n--&gt; 155 self.post_setup_hook()\n\nFile ~/repos/ai4co/rl4co/rl4co/models/rl/reinforce/reinforce.py:119, in REINFORCE.post_setup_hook(self, stage)\n    117 def post_setup_hook(self, stage=\"fit\"):\n    118     # Make baseline taking model itself and train_dataloader from model as input\n--&gt; 119     self.baseline.setup(\n    120         self.policy,\n    121         self.env,\n    122         batch_size=self.val_batch_size,\n    123         device=get_lightning_device(self),\n    124         dataset_size=self.data_cfg[\"val_data_size\"],\n    125     )\n\nFile ~/repos/ai4co/rl4co/rl4co/models/rl/reinforce/baselines.py:117, in WarmupBaseline.setup(self, *args, **kw)\n    116 def setup(self, *args, **kw):\n--&gt; 117     self.baseline.setup(*args, **kw)\n\nFile ~/repos/ai4co/rl4co/rl4co/models/rl/reinforce/baselines.py:174, in RolloutBaseline.setup(self, *args, **kw)\n    173 def setup(self, *args, **kw):\n--&gt; 174     self._update_policy(*args, **kw)\n\nFile ~/repos/ai4co/rl4co/rl4co/models/rl/reinforce/baselines.py:187, in RolloutBaseline._update_policy(self, policy, env, batch_size, device, dataset_size, dataset)\n    183     self.dataset = env.dataset(batch_size=[dataset_size])\n    185 log.info(\"Evaluating baseline policy on evaluation dataset\")\n    186 self.bl_vals = (\n--&gt; 187     self.rollout(self.policy, env, batch_size, device, self.dataset).cpu().numpy()\n    188 )\n    189 self.mean = self.bl_vals.mean()\n\nFile ~/repos/ai4co/rl4co/rl4co/models/rl/reinforce/baselines.py:242, in RolloutBaseline.rollout(self, policy, env, batch_size, device, dataset)\n    238         return policy(batch, env, decode_type=\"greedy\")[\"reward\"]\n    240 dl = DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn)\n--&gt; 242 rewards = torch.cat([eval_policy(batch) for batch in dl], 0)\n    243 return rewards\n\nFile ~/repos/ai4co/rl4co/rl4co/models/rl/reinforce/baselines.py:242, in &lt;listcomp&gt;(.0)\n    238         return policy(batch, env, decode_type=\"greedy\")[\"reward\"]\n    240 dl = DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.collate_fn)\n--&gt; 242 rewards = torch.cat([eval_policy(batch) for batch in dl], 0)\n    243 return rewards\n\nFile ~/repos/ai4co/rl4co/rl4co/models/rl/reinforce/baselines.py:238, in RolloutBaseline.rollout.&lt;locals&gt;.eval_policy(batch)\n    236 with torch.inference_mode():\n    237     batch = env.reset(batch.to(device))\n--&gt; 238     return policy(batch, env, decode_type=\"greedy\")[\"reward\"]\n\nFile ~/miniconda3/envs/cuda1203/lib/python3.10/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-&gt; 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/cuda1203/lib/python3.10/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/repos/ai4co/rl4co/rl4co/models/common/constructive/base.py:231, in ConstructivePolicy.forward(self, td, env, phase, calc_reward, return_actions, return_entropy, return_hidden, return_init_embeds, return_sum_log_likelihood, actions, max_steps, **decoding_kwargs)\n    229 while not td[\"done\"].all():\n    230     logits, mask = self.decoder(td, hidden, num_starts)\n--&gt; 231     td = decode_strategy.step(\n    232         logits,\n    233         mask,\n    234         td,\n    235         action=actions[..., step] if actions is not None else None,\n    236     )\n    237     td = env.step(td)[\"next\"]\n    238     step += 1\n\nFile ~/repos/ai4co/rl4co/rl4co/utils/decoding.py:343, in DecodingStrategy.step(self, logits, mask, td, action, **kwargs)\n    340 if not self.mask_logits:  # set mask_logit to None if mask_logits is False\n    341     mask = None\n--&gt; 343 logprobs = process_logits(\n    344     logits,\n    345     mask,\n    346     temperature=self.temperature,\n    347     top_p=self.top_p,\n    348     top_k=self.top_k,\n    349     tanh_clipping=self.tanh_clipping,\n    350     mask_logits=self.mask_logits,\n    351 )\n    352 logprobs, selected_action, td = self._step(\n    353     logprobs, mask, td, action=action, **kwargs\n    354 )\n    356 # directly return for improvement methods, since the action for improvement methods is finalized in its own policy\n\nFile ~/repos/ai4co/rl4co/rl4co/utils/decoding.py:177, in process_logits(logits, mask, temperature, top_p, top_k, tanh_clipping, mask_logits)\n    175 if mask_logits:\n    176     assert mask is not None, \"mask must be provided if mask_logits is True\"\n--&gt; 177     logits[~mask] = float(\"-inf\")\n    179 logits = logits / temperature  # temperature scaling\n    181 if top_k &gt; 0:\n\nIndexError: The shape of the mask [256, 11] at index 1 does not match the shape of the indexed tensor [256, 101] at index 1</pre> In\u00a0[2]: Copied! <pre>import gc\nfrom rl4co.envs import JSSPEnv\nfrom rl4co.models.zoo.l2d.model import L2DPPOModel\nfrom rl4co.models.zoo.l2d.policy import L2DPolicy4PPO\nfrom torch.utils.data import DataLoader\n</pre> import gc from rl4co.envs import JSSPEnv from rl4co.models.zoo.l2d.model import L2DPPOModel from rl4co.models.zoo.l2d.policy import L2DPolicy4PPO from torch.utils.data import DataLoader In\u00a0[3]: Copied! <pre># Lets generate a more complex instance\n\ngenerator_params = {\n  \"num_jobs\": 15,  # the total number of jobs\n  \"num_machines\": 15,  # the total number of machines that can process operations\n  \"min_processing_time\": 1,  # the minimum time required for a machine to process an operation\n  \"max_processing_time\": 99,  # the maximum time required for a machine to process an operation\n}\n\nenv = JSSPEnv(\n    generator_params=generator_params, \n    _torchrl_mode=True, \n    stepwise_reward=True\n)\n</pre> # Lets generate a more complex instance  generator_params = {   \"num_jobs\": 15,  # the total number of jobs   \"num_machines\": 15,  # the total number of machines that can process operations   \"min_processing_time\": 1,  # the minimum time required for a machine to process an operation   \"max_processing_time\": 99,  # the maximum time required for a machine to process an operation }  env = JSSPEnv(     generator_params=generator_params,      _torchrl_mode=True,      stepwise_reward=True ) In\u00a0[36]: Copied! <pre># Policy: neural network, in this case with encoder-decoder architecture\npolicy = L2DPolicy4PPO(\n    embed_dim=embed_dim, \n    num_encoder_layers=num_encoder_layers, \n    env_name=\"jssp\",\n    het_emb=False\n)\n\nmodel = L2DPPOModel(\n    env=env,\n    policy=policy,\n    batch_size=batch_size,\n    train_data_size=train_data_size,\n    val_data_size=1_000,\n    optimizer_kwargs={\"lr\": 1e-4}\n)\n</pre> # Policy: neural network, in this case with encoder-decoder architecture policy = L2DPolicy4PPO(     embed_dim=embed_dim,      num_encoder_layers=num_encoder_layers,      env_name=\"jssp\",     het_emb=False )  model = L2DPPOModel(     env=env,     policy=policy,     batch_size=batch_size,     train_data_size=train_data_size,     val_data_size=1_000,     optimizer_kwargs={\"lr\": 1e-4} ) <pre>Using 16bit Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nOverriding gradient_clip_val to None for 'automatic_optimization=False' models\nval_file not set. Generating dataset instead\nProvided file name data/../../data/jssp/taillard/15j_15m not found. Make sure to provide a file in the right path first or unset test_file to generate data automatically instead\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4]\n\n  | Name       | Type          | Params\n---------------------------------------------\n0 | env        | JSSPEnv       | 0     \n1 | policy     | L2DPolicy4PPO | 133 K \n2 | policy_old | L2DPolicy4PPO | 133 K \n---------------------------------------------\n266 K     Trainable params\n0         Non-trainable params\n266 K     Total params\n1.066     Total estimated model params size (MB)\n</pre> <pre>Epoch 0: 100%|\u2588| 8/8 [03:40&lt;00:00,  0.04it/s, v_num=9, train/loss=1.45e+3, train\nValidation: |                                             | 0/? [00:00&lt;?, ?it/s]\nValidation:   0%|                                         | 0/4 [00:00&lt;?, ?it/s]\nValidation DataLoader 0:   0%|                            | 0/4 [00:00&lt;?, ?it/s]\nValidation DataLoader 0:  25%|\u2588\u2588\u2588\u2588\u2588               | 1/4 [00:04&lt;00:13,  0.22it/s]\nValidation DataLoader 0:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588          | 2/4 [00:09&lt;00:09,  0.22it/s]\nValidation DataLoader 0:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     | 3/4 [00:13&lt;00:04,  0.21it/s]\nValidation DataLoader 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:18&lt;00:00,  0.22it/s]\nEpoch 0: 100%|\u2588| 8/8 [03:58&lt;00:00,  0.03it/s, v_num=9, train/loss=1.45e+3, train</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> <pre>Epoch 0: 100%|\u2588| 8/8 [03:58&lt;00:00,  0.03it/s, v_num=9, train/loss=1.45e+3, train\n</pre> In\u00a0[\u00a0]: Copied! <pre>CHECKPOINT_PATH = \"last.ckpt\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntry:\n    model = L2DPPOModel.load_from_checkpoint(CHECKPOINT_PATH)\nexcept FileNotFoundError:\n\n    trainer = RL4COTrainer(\n        max_epochs=1,\n        accelerator=accelerator,\n        devices=1,\n        logger=None,\n    )\n\n    trainer.fit(model)\nfinally:\n    model = model.to(device)\n</pre> CHECKPOINT_PATH = \"last.ckpt\" device = \"cuda\" if torch.cuda.is_available() else \"cpu\" try:     model = L2DPPOModel.load_from_checkpoint(CHECKPOINT_PATH) except FileNotFoundError:      trainer = RL4COTrainer(         max_epochs=1,         accelerator=accelerator,         devices=1,         logger=None,     )      trainer.fit(model) finally:     model = model.to(device) In\u00a0[8]: Copied! <pre># path to taillard instances\nDATA_PATH = \"../../ai4co/rl4co/data/jssp/taillard/{instance_type}\"\n\nresults = {}\ninstance_types = [\"15j_15m\", \"20j_15m\", \"20j_20m\", \"30j_15m\", \"30j_20m\"]\n\nfor instance_type in instance_types:\n    \n    dataset = env.dataset(batch_size=[10], phase=\"test\", filename=DATA_PATH.format(instance_type=instance_type))\n    dl = DataLoader(dataset, batch_size=5, collate_fn=dataset.collate_fn)\n    rewards = []\n    \n    for batch in dl:\n        td = env.reset(batch).to(device)\n        # use policy.generate to avoid grad calculations which can lead to oom \n        out = model.policy.generate(td, env=env, phase=\"test\", decode_type=\"multistart_sampling\", num_starts=100, select_best=True)\n        rewards.append(out[\"reward\"])\n\n    reward = torch.cat(rewards, dim=0).mean().item()\n    results[instance_type] = reward\n\n    print(\"Done evaluating instance type %s with reward %s\" % (instance_type, reward))\n\n    # avoid ooms due to cache not being cleared \n    model.rb.empty()\n    gc.collect()\n    torch.cuda.empty_cache()\n</pre> # path to taillard instances DATA_PATH = \"../../ai4co/rl4co/data/jssp/taillard/{instance_type}\"  results = {} instance_types = [\"15j_15m\", \"20j_15m\", \"20j_20m\", \"30j_15m\", \"30j_20m\"]  for instance_type in instance_types:          dataset = env.dataset(batch_size=[10], phase=\"test\", filename=DATA_PATH.format(instance_type=instance_type))     dl = DataLoader(dataset, batch_size=5, collate_fn=dataset.collate_fn)     rewards = []          for batch in dl:         td = env.reset(batch).to(device)         # use policy.generate to avoid grad calculations which can lead to oom          out = model.policy.generate(td, env=env, phase=\"test\", decode_type=\"multistart_sampling\", num_starts=100, select_best=True)         rewards.append(out[\"reward\"])      reward = torch.cat(rewards, dim=0).mean().item()     results[instance_type] = reward      print(\"Done evaluating instance type %s with reward %s\" % (instance_type, reward))      # avoid ooms due to cache not being cleared      model.rb.empty()     gc.collect()     torch.cuda.empty_cache() <pre>Done evaluating instance type 30j_20m with reward -2357.900146484375\n</pre>"},{"location":"examples/other/2-scheduling/#solving-the-flexible-job-shop-scheduling-problem-fjsp","title":"Solving the Flexible Job-Shop Scheduling Problem (FJSP)\u00b6","text":"<p>The following notebook explains the FJSP and explains the solution construction process using an encoder-decoder architecture based on a Heterogeneous Graph Neural Network (HetGNN)</p> <p></p>"},{"location":"examples/other/2-scheduling/#visualize-the-problem","title":"Visualize the Problem\u00b6","text":"<p>Below we visualize the generated instance of the FJSP. Blue nodes correspond to machines, red nodes to operations and yellow nodes to jobs. A machine may process an operation if there exists an edge between the two.</p> <p>The thickness of the connection between a machine and an operation node specifies the processing time the respective machine needs to process the operation (thicker line := longer processing).</p> <p>Each operation belongs to exactly one job, where an edge between a job and an operation node indicates that the respective operation belongs to the job. The number above an operation-job edge specifies the precedence-order in which the operations of a job need to be processed. A job is done when all operations belonging to it are scheduled. The instance is solved when all jobs are fully scheduled.</p> <p>Also note that some operation nodes are not connected. These operation nodes are padded, so that all instances in a batch have the same number of operations (where we determine the maximum number of operations as num_jobs * max_ops_per_job).</p>"},{"location":"examples/other/2-scheduling/#build-a-model-to-solve-the-fjsp","title":"Build a Model to Solve the FJSP\u00b6","text":"<p>In the FJSP we typically encode Operations and Machines separately, since they pose different node types in a k-partite Graph. Therefore, the encoder for the FJSP returns two hidden representations, the first containing machine embeddings and the second containing operation embeddings:</p>"},{"location":"examples/other/2-scheduling/#visualize-solution-construction","title":"Visualize solution construction\u00b6","text":"<p>Starting at $t=0$, the decoder uses the machine-operation embeddings of the encoder to decide which machine-job-combination to schedule next. Note, that due to the precedence relationship, the operations to be scheduled next are fixed per job. Therefore, it is sufficient to determine the next job to be scheduled, which significantly reduces the action space.</p> <p>After some operations have been scheduled, either all the machines are busy or all the jobs have been scheduled with their currently active operation. In this case, the environment transitions to a new time step $t$. The new $t$ will be equal to the first time step where a machine finishes an operation in the partial schedule. When an operation is finished, the machine that has processed it is immediately ready to process the next operation. Also, the next operation of the respective job can then be scheduled.</p> <p>The start time of an operation is always equal to the time step in which it is scheduled. The finish time of an operation is equal to its start time plus the processing time required by the machine on which it is being processed.</p> <p>The figure below visualises this process.</p>"},{"location":"examples/other/2-scheduling/#solving-the-job-shop-scheduling-problem-jssp","title":"Solving the Job-Shop Scheduling Problem (JSSP)\u00b6","text":""},{"location":"examples/other/2-scheduling/#train-on-synthetic-data-and-test-on-taillard-benchmark","title":"Train on synthetic data and test on Taillard benchmark\u00b6","text":""},{"location":"examples/other/3-data-generator-distributions/","title":"Generating data in RL4CO","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nfrom rl4co.envs.routing import TSPEnv, TSPGenerator\nfrom rl4co.envs.common.distribution_utils import Cluster, Mix_Distribution, Mix_Multi_Distributions, Gaussian_Mixture, Mixed\n\n# Instantiate the environment and generator\ngenerator = TSPGenerator(num_loc=100)\nenv = TSPEnv(generator=generator)\n\n# Simple plot\nfig, axs = plt.subplots(1, 3, figsize=(10, 3))\ntd = env.generator(3) # generate 3 instances\nfor i in range(3):\n    axs[i].scatter(td[\"locs\"][i][:, 0], td[\"locs\"][i][:, 1])\n    axs[i].set_xticks([]); axs[i].set_yticks([])\nfig.suptitle(\"TSP with 100 locations, uniform distribution\")\n</pre> import matplotlib.pyplot as plt from rl4co.envs.routing import TSPEnv, TSPGenerator from rl4co.envs.common.distribution_utils import Cluster, Mix_Distribution, Mix_Multi_Distributions, Gaussian_Mixture, Mixed  # Instantiate the environment and generator generator = TSPGenerator(num_loc=100) env = TSPEnv(generator=generator)  # Simple plot fig, axs = plt.subplots(1, 3, figsize=(10, 3)) td = env.generator(3) # generate 3 instances for i in range(3):     axs[i].scatter(td[\"locs\"][i][:, 0], td[\"locs\"][i][:, 1])     axs[i].set_xticks([]); axs[i].set_yticks([]) fig.suptitle(\"TSP with 100 locations, uniform distribution\") <pre>/home/botu/anaconda3/envs/rl4co/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> Out[1]: <pre>Text(0.5, 0.98, 'TSP with 100 locations, uniform distribution')</pre> <p>Generating data with different sizes</p> In\u00a0[2]: Copied! <pre>generator = TSPGenerator(num_loc=1000)\nenv.generator = generator\n\nfig, axs = plt.subplots(1, 3, figsize=(10, 3))\ntd = env.generator(3) # generate 3 instances\nfor i in range(3):\n    axs[i].scatter(td[\"locs\"][i][:, 0], td[\"locs\"][i][:, 1])\n    axs[i].set_xticks([]); axs[i].set_yticks([])\nfig.suptitle(\"TSP with 1000 locations, uniform distribution\")\n</pre> generator = TSPGenerator(num_loc=1000) env.generator = generator  fig, axs = plt.subplots(1, 3, figsize=(10, 3)) td = env.generator(3) # generate 3 instances for i in range(3):     axs[i].scatter(td[\"locs\"][i][:, 0], td[\"locs\"][i][:, 1])     axs[i].set_xticks([]); axs[i].set_yticks([]) fig.suptitle(\"TSP with 1000 locations, uniform distribution\") Out[2]: <pre>Text(0.5, 0.98, 'TSP with 1000 locations, uniform distribution')</pre> <p>Changing distribution of the data to normal distribution. We can pass the arguments to it by using <code>loc_</code> + distribution name as well as its keyword arguments, including here the <code>mean</code> and <code>std</code> of the normal distribution</p> In\u00a0[3]: Copied! <pre>generator = TSPGenerator(num_loc=100, loc_distribution=\"normal\", loc_mean=0, loc_std=1)\nenv.generator = generator\n\nfig, axs = plt.subplots(1, 3, figsize=(10, 3))\ntd = env.generator(3) # generate 3 instances\nfor i in range(3):\n    axs[i].scatter(td[\"locs\"][i][:, 0], td[\"locs\"][i][:, 1])\n    axs[i].set_xticks([]); axs[i].set_yticks([])\nfig.suptitle(\"TSP with 100 locations, normal distribution\")\n</pre> generator = TSPGenerator(num_loc=100, loc_distribution=\"normal\", loc_mean=0, loc_std=1) env.generator = generator  fig, axs = plt.subplots(1, 3, figsize=(10, 3)) td = env.generator(3) # generate 3 instances for i in range(3):     axs[i].scatter(td[\"locs\"][i][:, 0], td[\"locs\"][i][:, 1])     axs[i].set_xticks([]); axs[i].set_yticks([]) fig.suptitle(\"TSP with 100 locations, normal distribution\") Out[3]: <pre>Text(0.5, 0.98, 'TSP with 100 locations, normal distribution')</pre> <p>We can pass a custom <code>loc_sampler</code> to the generator (we can make it ourselves!) to generate data from a custom distribution. In this case we use the mixture of three exemplar distributions in batch-level, i.e. Uniform, Cluster, Mixed following the setting in Bi et al. 2022 (https://arxiv.org/abs/2210.07686)</p> In\u00a0[4]: Copied! <pre>loc_sampler = Mix_Distribution(n_cluster=3)\ngenerator = TSPGenerator(num_loc=200, loc_sampler=loc_sampler)\nenv.generator = generator\n\nfig, axs = plt.subplots(1, 3, figsize=(10, 3))\ntd = env.generator(3) # generate 3 instances\nfor i in range(3):\n    axs[i].scatter(td[\"locs\"][i][:, 0], td[\"locs\"][i][:, 1])\n    axs[i].set_xticks([]); axs[i].set_yticks([])\nfig.suptitle(\"TSP with 200 locations, mixed distribution\")\n</pre> loc_sampler = Mix_Distribution(n_cluster=3) generator = TSPGenerator(num_loc=200, loc_sampler=loc_sampler) env.generator = generator  fig, axs = plt.subplots(1, 3, figsize=(10, 3)) td = env.generator(3) # generate 3 instances for i in range(3):     axs[i].scatter(td[\"locs\"][i][:, 0], td[\"locs\"][i][:, 1])     axs[i].set_xticks([]); axs[i].set_yticks([]) fig.suptitle(\"TSP with 200 locations, mixed distribution\") Out[4]: <pre>Text(0.5, 0.98, 'TSP with 200 locations, mixed distribution')</pre> In\u00a0[5]: Copied! <pre>from rl4co.envs.graph import MCPEnv, MCPGenerator\nfrom matplotlib import pyplot as plt\nimport torch\nfrom collections import Counter\n\ngenerator = MCPGenerator(size_distribution=\"uniform\", weight_distribution=\"uniform\")\nenv = MCPEnv(generator=generator)\ndata = env.generator(100)\n\nsizes = torch.count_nonzero(data[\"membership\"], dim=-1).flatten().tolist()\nsize2cnt = Counter(sizes)\nweights = data[\"weights\"].flatten().tolist()\nweight2cnt = Counter(weights)\n\n# plot the size distributions and the weight distributions\nplt.figure()\nplt.bar(size2cnt.keys(), size2cnt.values())\nplt.title(\"Size distribution\")\nplt.xlabel(\"Size\")\nplt.ylabel(\"Probability\")\nplt.show()\n\n# Note: the size distributions are not perfectly uniform since there might be repeated items and are removed in post-processing\n\nplt.figure()\nplt.bar(weight2cnt.keys(), weight2cnt.values())\nplt.title(\"Weight distribution\")\nplt.xlabel(\"Weight\")\nplt.ylabel(\"Probability\")\nplt.show()\n</pre> from rl4co.envs.graph import MCPEnv, MCPGenerator from matplotlib import pyplot as plt import torch from collections import Counter  generator = MCPGenerator(size_distribution=\"uniform\", weight_distribution=\"uniform\") env = MCPEnv(generator=generator) data = env.generator(100)  sizes = torch.count_nonzero(data[\"membership\"], dim=-1).flatten().tolist() size2cnt = Counter(sizes) weights = data[\"weights\"].flatten().tolist() weight2cnt = Counter(weights)  # plot the size distributions and the weight distributions plt.figure() plt.bar(size2cnt.keys(), size2cnt.values()) plt.title(\"Size distribution\") plt.xlabel(\"Size\") plt.ylabel(\"Probability\") plt.show()  # Note: the size distributions are not perfectly uniform since there might be repeated items and are removed in post-processing  plt.figure() plt.bar(weight2cnt.keys(), weight2cnt.values()) plt.title(\"Weight distribution\") plt.xlabel(\"Weight\") plt.ylabel(\"Probability\") plt.show()   <p>We can also pass a custom <code>sampler</code> to generate data:</p> In\u00a0[6]: Copied! <pre>from collections import Counter\nfrom torch.distributions import Normal\n\nsize_sampler = Normal(10, 2)\nweight_sampler = Normal(5, 1)\n\ngenerator = MCPGenerator(size_sampler=size_sampler, weight_sampler=weight_sampler)\nenv = MCPEnv(generator=generator)\ndata = env.generator(100)\n\nsizes = torch.count_nonzero(data[\"membership\"], dim=-1).flatten().tolist()\nsize2cnt = Counter(sizes)\nweights = data[\"weights\"].flatten().tolist()\nweight2cnt = Counter(weights)\n\n# plot the size distributions and the weight distributions\nplt.figure()\nplt.bar(size2cnt.keys(), size2cnt.values())\nplt.title(\"Size distribution\")\nplt.xlabel(\"Size\")\nplt.ylabel(\"Probability\")\nplt.show()\n\nplt.figure()\nplt.bar(weight2cnt.keys(), weight2cnt.values())\nplt.title(\"Weight distribution\")\nplt.xlabel(\"Weight\")\nplt.ylabel(\"Probability\")\nplt.show()\n</pre> from collections import Counter from torch.distributions import Normal  size_sampler = Normal(10, 2) weight_sampler = Normal(5, 1)  generator = MCPGenerator(size_sampler=size_sampler, weight_sampler=weight_sampler) env = MCPEnv(generator=generator) data = env.generator(100)  sizes = torch.count_nonzero(data[\"membership\"], dim=-1).flatten().tolist() size2cnt = Counter(sizes) weights = data[\"weights\"].flatten().tolist() weight2cnt = Counter(weights)  # plot the size distributions and the weight distributions plt.figure() plt.bar(size2cnt.keys(), size2cnt.values()) plt.title(\"Size distribution\") plt.xlabel(\"Size\") plt.ylabel(\"Probability\") plt.show()  plt.figure() plt.bar(weight2cnt.keys(), weight2cnt.values()) plt.title(\"Weight distribution\") plt.xlabel(\"Weight\") plt.ylabel(\"Probability\") plt.show() <p>Tl;dr: RL4CO allows for easily generating data for CO problems! \ud83d\ude80</p>"},{"location":"examples/other/3-data-generator-distributions/#generating-data-in-rl4co","title":"Generating data in RL4CO\u00b6","text":"<p>RL4CO allows for easily generating data from different distributions for CO problems</p>"},{"location":"examples/other/3-data-generator-distributions/#generating-different-distributions-for-tsp","title":"Generating different distributions for TSP\u00b6","text":""},{"location":"examples/other/3-data-generator-distributions/#generating-different-distributions-for-mcp","title":"Generating different distributions for MCP\u00b6","text":"<p>In here we visualize the different weight and size distributions for MCP by passing the distribution name, which is automatically parsed:</p>"},{"location":"rl4co/tasks/","title":"Evaluation","text":"<p>To evaluate your trained model, here are some steps to follow:</p> <p>Step 1. Prepare your pre-trained model checkpoint and test instances data file. Put them in your preferred place. e.g., we will test the <code>AttentionModel</code> on TSP50:</p> <pre><code>.\n\u251c\u2500\u2500 rl4co/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 checkpoints/\n\u2502   \u2514\u2500\u2500 am-tsp50.ckpt\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 tsp/\n        \u2514\u2500\u2500 tsp50_test_seed1234.npz\n</code></pre> <p>You can generate the test instances data file by running the following command:</p> <pre><code>python -c \"from rl4co.data.generate_data import generate_default_datasets; generate_default_datasets('data')\"\n</code></pre> <p>Step 2. Run the <code>eval.py</code> with your customized setting. e.g., let's use the <code>sampling</code> method with a <code>top_p=0.95</code> sampling strategy:</p> <pre><code>python rl4co/tasks/eval.py --problem tsp --data-path data/tsp/tsp50_test_seed1234.npz --model AttentionModel --ckpt-path checkpoints/am-tsp50.ckpt --method sampling --top-p 0.95\n</code></pre> <p>Arguments guideline:</p> <ul> <li><code>--problem</code>: the problem name, e.g., <code>tsp</code>, <code>cvrp</code>, <code>pdp</code>, etc. This should be consistent with the <code>env.name</code>. Default is <code>tsp</code>.</li> <li><code>--generator-params</code>: the generator parameters for the test instances. You could specify the <code>num_loc</code> etc. Default is <code>{'num_loc': 50}</code>.</li> <li><code>--data-path</code>: the path to the test instances data file. Default is <code>data/tsp/tsp50_test_seed1234.npz</code>.</li> <li><code>--model</code>: the model class name, e.g., <code>AttentionModel</code>, <code>POMO</code>, <code>SymNCO</code>, etc. It will be dynamically imported and instantiated. Default is <code>AttentionModel</code>.</li> <li><code>--ckpt-path</code>: the path to the pre-trained model checkpoint. Default is <code>checkpoints/am-tsp50.ckpt</code>.</li> <li><code>--device</code>: the device to run the evaluation, e.g., <code>cuda:0</code>, <code>cpu</code>, etc. Default is <code>cuda:0</code>.</li> <li><code>--method</code>: the evaluation method, e.g., <code>greedy</code>, <code>sampling</code>, <code>multistart_greedy</code>, <code>augment_dihedral_8</code>, <code>augment</code>, <code>multistart_greedy_augment_dihedral_8</code>, and <code>multistart_greedy_augment</code>. Default is <code>greedy</code>.</li> <li><code>--save-results</code>: whether to save the evaluation results as a <code>.pkl</code> file. Deafult is <code>True</code>. The results include <code>actions</code>, <code>rewards</code>, <code>inference_time</code>, and <code>avg_reward</code>.</li> <li><code>--save-path</code>: the path to save the evaluation results. Default is <code>results/</code>.</li> <li><code>--num-instances</code>: the number of test instances to evaluate. Default is <code>1000</code>. </li> </ul> <p>If you use the <code>sampling</code> method, you may need to specify the following parameters:</p> <ul> <li><code>--samples</code>: the number of samples for the sampling method. Default is <code>1280</code>.</li> <li><code>--temperature</code>: the temperature for the sampling method. Default is <code>1.0</code>.</li> <li><code>--top-p</code>: the top-p for the sampling method. Default is <code>0.0</code>, i.e. not activated.</li> <li><code>--top-k</code>: the top-k for the sampling method. Deafult is <code>0</code>, i.e. not activated.</li> <li><code>--select-best</code>: whether to select the best action from the sampling results. If <code>False</code>, the results will include all sampled rewards, i.e., <code>[num_instances * num_samples]</code>.</li> </ul> <p>If you use the <code>augment</code> method, you may need to specify the following parameters:</p> <ul> <li><code>--num-augments</code>: the number of augmented instances for the augment method. Default is <code>8</code>.</li> <li><code>--force-dihedral-8</code>: whether to force the augmented instances to be dihedral 8. Default is <code>True</code>.</li> </ul> <p>Step 3. If you want to launch several evaluations with various parameters, you may refer to the following examples:</p> <ul> <li>Evaluate POMO on TSP50 with a sampling of different Top-p and temperature:<pre><code>    #!/bin/bash\n\n    top_p_list=(0.5 0.6 0.7 0.8 0.9 0.95 0.98 0.99 0.995 1.0)\n    temp_list=(0.1 0.3 0.5 0.7 0.8 0.9 1.0 1.1 1.2 1.5 1.8 2.0 2.2 2.5 2.8 3.0)\n\n    device=cuda:0\n\n    problem=tsp\n    model=POMO\n    ckpt_path=checkpoints/pomo-tsp50.ckpt\n    data_path=data/tsp/tsp50_test_seed1234.npz\n\n    num_instances=1000\n    save_path=results/tsp50-pomo-topp-1k\n\n    for top_p in ${top_p_list[@]}; do\n        for temp in ${temp_list[@]}; do\n            python rl4co/tasks/eval.py --problem ${problem} --model ${model} --ckpt_path ${ckpt_path} --data_path ${data_path} --save_path ${save_path} --method sampling --temperature=${temp} --top_p=${top_p} --top_k=0 --device ${device}\n        done\n    done\n</code></pre> </li> </ul> <ul> <li>Evaluate POMO on CVRP50 with a sampling of different Top-k and temperature:<pre><code>    #!/bin/bash\n\n    top_k_list=(5 10 15 20 25)\n    temp_list=(0.1 0.3 0.5 0.7 0.8 0.9 1.0 1.1 1.2 1.5 1.8 2.0 2.2 2.5 2.8 3.0)\n\n    device=cuda:1\n\n    problem=cvrp\n    model=POMO\n    ckpt_path=checkpoints/pomo-cvrp50.ckpt\n    data_path=data/vrp/vrp50_test_seed1234.npz\n\n    num_instances=1000\n    save_path=results/cvrp50-pomo-topk-1k\n\n    for top_k in ${top_k_list[@]}; do\n        for temp in ${temp_list[@]}; do\n            python rl4co/tasks/eval.py --problem ${problem} --model ${model} --ckpt_path ${ckpt_path} --data_path ${data_path} --save_path ${save_path} --method sampling --temperature=${temp} --top_p=0.0 --top_k=${top_k} --device ${device}\n        done\n    done\n</code></pre> </li> </ul>"},{"location":"tests/luop221/","title":"Luop221","text":"In\u00a0[1]: Copied! <pre>import wandb\nwandb.login()\n</pre> import wandb wandb.login() <pre>wandb: Currently logged in as: huailegedan (upc-lbw). Use `wandb login --relogin` to force relogin\n</pre> Out[1]: <pre>True</pre> In\u00a0[2]: Copied! <pre>from lightning.pytorch.loggers import WandbLogger\nlogger = WandbLogger(project=\"rl4co\", name=\"luop-am221\")\n</pre> from lightning.pytorch.loggers import WandbLogger logger = WandbLogger(project=\"rl4co\", name=\"luop-am221\") In\u00a0[3]: Copied! <pre>from rl4co.envs import landuseOptEnv\nimport torch\nfrom rl4co.models import AttentionModel, AttentionModelPolicy\nfrom rl4co.models.nn.env_embeddings.context import LOPContext\nfrom rl4co.models.nn.env_embeddings.dynamic import StaticEmbedding\nfrom rl4co.models.nn.env_embeddings.init import lopInitEmbedding\nfrom rl4co.utils import RL4COTrainer\nfrom rl4co.utils.decoding import random_policy, rollout\n</pre> from rl4co.envs import landuseOptEnv import torch from rl4co.models import AttentionModel, AttentionModelPolicy from rl4co.models.nn.env_embeddings.context import LOPContext from rl4co.models.nn.env_embeddings.dynamic import StaticEmbedding from rl4co.models.nn.env_embeddings.init import lopInitEmbedding from rl4co.utils import RL4COTrainer from rl4co.utils.decoding import random_policy, rollout In\u00a0[\u00a0]: Copied! <pre>batch_size = 1024\nenv = landuseOptEnv(\n    generator_params=dict(num_loc=221),\n    test_file=\"/test221\",\n    val_file=\"/val221\",\n)\nemb_dim = 128\npolicy = AttentionModelPolicy(env_name=env.name, # this is actually not needed since we are initializing the embeddings!\n                              embed_dim=emb_dim,\n                              init_embedding=lopInitEmbedding(emb_dim),\n                              context_embedding=LOPContext(emb_dim),\n                              dynamic_embedding=StaticEmbedding(emb_dim)\n)\n# Model: default is AM with REINFORCE and greedy rollout baseline\nmodel = AttentionModel(env,\n                       baseline='rollout',\n                       policy=policy,\n                       batch_size=512,\n                       val_batch_size= 1024,\n                       test_batch_size= 1024,\n                       train_data_size=640_000, # really small size for demo\n                       val_data_size=10_000,\n                       test_data_size=10_000,\n                       optimizer_kwargs={\n                           \"lr\":1e-4,\n                           \"weight_decay\": 1e-6,\n                       },\n                       lr_scheduler=\"MultiStepLR\",\n                       lr_scheduler_kwargs={\n                            \"milestones\": [80, 95],\n                            \"gamma\":0.1,\n                       },\n                       policy_kwargs={  # we can specify the decode types using the policy_kwargs\n                           \"train_decode_type\": \"sampling\",\n                           \"val_decode_type\": \"greedy\",\n                           \"test_decode_type\": \"greedy\",\n                       }\n                       )\n</pre> batch_size = 1024 env = landuseOptEnv(     generator_params=dict(num_loc=221),     test_file=\"/test221\",     val_file=\"/val221\", ) emb_dim = 128 policy = AttentionModelPolicy(env_name=env.name, # this is actually not needed since we are initializing the embeddings!                               embed_dim=emb_dim,                               init_embedding=lopInitEmbedding(emb_dim),                               context_embedding=LOPContext(emb_dim),                               dynamic_embedding=StaticEmbedding(emb_dim) ) # Model: default is AM with REINFORCE and greedy rollout baseline model = AttentionModel(env,                        baseline='rollout',                        policy=policy,                        batch_size=512,                        val_batch_size= 1024,                        test_batch_size= 1024,                        train_data_size=640_000, # really small size for demo                        val_data_size=10_000,                        test_data_size=10_000,                        optimizer_kwargs={                            \"lr\":1e-4,                            \"weight_decay\": 1e-6,                        },                        lr_scheduler=\"MultiStepLR\",                        lr_scheduler_kwargs={                             \"milestones\": [80, 95],                             \"gamma\":0.1,                        },                        policy_kwargs={  # we can specify the decode types using the policy_kwargs                            \"train_decode_type\": \"sampling\",                            \"val_decode_type\": \"greedy\",                            \"test_decode_type\": \"greedy\",                        }                        )  In\u00a0[5]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntd_init = env.reset(batch_size=[3]).to(device)\npolicy = model.policy.to(device)\nout = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\n\nactions_untrained = out['actions'].cpu().detach()\nrewards_untrained = out['reward'].cpu().detach()\n\nfor i in range(3):\n    print(f\"Problem {i+1} | Cost: {-rewards_untrained[i]:.3f}\")\n    env.render(td_init[i], actions_untrained[i])\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") td_init = env.reset(batch_size=[3]).to(device) policy = model.policy.to(device) out = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)  actions_untrained = out['actions'].cpu().detach() rewards_untrained = out['reward'].cpu().detach()  for i in range(3):     print(f\"Problem {i+1} | Cost: {-rewards_untrained[i]:.3f}\")     env.render(td_init[i], actions_untrained[i]) <pre>E:\\temp\\DRLtest\\rl4co\\rl4co\\models\\nn\\attention.py:128: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n  out = self.sdpa_fn(\n</pre> <pre>Problem 1 | Cost: -0.591\n</pre> <pre>D:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\IPython\\core\\pylabtools.py:77: DeprecationWarning: backend2gui is deprecated since IPython 8.24, backends are managed in matplotlib and can be externally registered.\n  warnings.warn(\nE:\\temp\\DRLtest\\rl4co\\rl4co\\envs\\urbanplan\\cityplan\\render.py:42: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n  colors = plt.cm.get_cmap('tab20', np.unique(current_plan).size)\n</pre> <pre>Problem 2 | Cost: -0.588\n</pre> <pre>Problem 3 | Cost: -0.620\n</pre> In\u00a0[6]: Copied! <pre>from lightning.pytorch.callbacks import ModelCheckpoint, RichModelSummary\n# Checkpointing callback: save models when validation reward improves\ncheckpoint_callback = ModelCheckpoint(  dirpath=\"checkpoints221\", # save to checkpoints/\n                                        filename=\"epoch_{epoch:03d}\",  # save as epoch_XXX.ckpt\n                                        save_top_k=1, # save only the best model\n                                        save_last=True, # save the last model\n                                        monitor=\"val/reward\", # monitor validation reward\n                                        mode=\"max\") # maximize validation reward\n\n# Print model summary\nrich_model_summary = RichModelSummary(max_depth=3)\n# Callbacks list\ncallbacks = [checkpoint_callback, rich_model_summary]\n</pre> from lightning.pytorch.callbacks import ModelCheckpoint, RichModelSummary # Checkpointing callback: save models when validation reward improves checkpoint_callback = ModelCheckpoint(  dirpath=\"checkpoints221\", # save to checkpoints/                                         filename=\"epoch_{epoch:03d}\",  # save as epoch_XXX.ckpt                                         save_top_k=1, # save only the best model                                         save_last=True, # save the last model                                         monitor=\"val/reward\", # monitor validation reward                                         mode=\"max\") # maximize validation reward  # Print model summary rich_model_summary = RichModelSummary(max_depth=3) # Callbacks list callbacks = [checkpoint_callback, rich_model_summary] In\u00a0[7]: Copied! <pre>from rl4co.utils.trainer import RL4COTrainer\n\ntrainer = RL4COTrainer(\n    max_epochs=5,\n    accelerator=\"gpu\",\n    devices=1,\n    logger=logger,\n    callbacks=callbacks,\n)\n</pre> from rl4co.utils.trainer import RL4COTrainer  trainer = RL4COTrainer(     max_epochs=5,     accelerator=\"gpu\",     devices=1,     logger=logger,     callbacks=callbacks, ) <pre>Using 16bit Automatic Mixed Precision (AMP)\nTrainer already configured with model summary callbacks: [&lt;class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'&gt;]. Skipping setting a default `ModelSummary` callback.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n</pre> In\u00a0[\u00a0]: Copied! <pre>trainer.fit(model)\n</pre> trainer.fit(model) <pre>D:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\wandb\\sdk\\lib\\ipython.py:89: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n  from IPython.core.display import display\n</pre> <pre>VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888925108, max=1.0\u2026</pre>  wandb version 0.17.7 is available!  To upgrade, please run:  $ pip install wandb --upgrade   Tracking run with wandb version 0.17.4   Run data is saved locally in <code>.\\wandb\\run-20240819_010731-mk0gzyrh</code>  Syncing run luop-am to Weights &amp; Biases (docs)   View project at https://wandb.ai/upc-lbw/rl4co   View run at https://wandb.ai/upc-lbw/rl4co/runs/mk0gzyrh <pre>D:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory E:\\temp\\DRLtest\\rl4co\\tests\\checkpoints exists and is not empty.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Greedy rollouts over trained model (same states as previous plot)\npolicy = model.policy.to(device)\nout = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\n\n# Plotting\nprint(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\")\nfor td, actions in zip(td_init, out['actions'].cpu()):\n    env.render(td, actions)\n</pre> # Greedy rollouts over trained model (same states as previous plot) policy = model.policy.to(device) out = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)  # Plotting print(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\") for td, actions in zip(td_init, out['actions'].cpu()):     env.render(td, actions) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tests/luop50/","title":"Luop50","text":"In\u00a0[1]: Copied! <pre>import wandb\nwandb.login()\n</pre> import wandb wandb.login() <pre>wandb: Currently logged in as: huailegedan (upc-lbw). Use `wandb login --relogin` to force relogin\n</pre> Out[1]: <pre>True</pre> In\u00a0[2]: Copied! <pre>from lightning.pytorch.loggers import WandbLogger\nlogger = WandbLogger(project=\"rl4co\", name=\"luop-am\")\n</pre> from lightning.pytorch.loggers import WandbLogger logger = WandbLogger(project=\"rl4co\", name=\"luop-am\") In\u00a0[3]: Copied! <pre>from rl4co.envs import landuseOptEnv\nimport torch\nfrom rl4co.models import AttentionModel, AttentionModelPolicy\nfrom rl4co.models.nn.env_embeddings.context import LOPContext\nfrom rl4co.models.nn.env_embeddings.dynamic import StaticEmbedding\nfrom rl4co.models.nn.env_embeddings.init import lopInitEmbedding\nfrom rl4co.utils import RL4COTrainer\nfrom rl4co.utils.decoding import random_policy, rollout\n</pre> from rl4co.envs import landuseOptEnv import torch from rl4co.models import AttentionModel, AttentionModelPolicy from rl4co.models.nn.env_embeddings.context import LOPContext from rl4co.models.nn.env_embeddings.dynamic import StaticEmbedding from rl4co.models.nn.env_embeddings.init import lopInitEmbedding from rl4co.utils import RL4COTrainer from rl4co.utils.decoding import random_policy, rollout In\u00a0[4]: Copied! <pre>batch_size = 1024\nenv = landuseOptEnv(generator_params=dict(num_loc=50))\nemb_dim = 128\npolicy = AttentionModelPolicy(env_name=env.name, # this is actually not needed since we are initializing the embeddings!\n                              embed_dim=emb_dim,\n                              init_embedding=lopInitEmbedding(emb_dim),\n                              context_embedding=LOPContext(emb_dim),\n                              dynamic_embedding=StaticEmbedding(emb_dim)\n)\n# Model: default is AM with REINFORCE and greedy rollout baseline\nmodel = AttentionModel(env,\n                       baseline='rollout',\n                       policy=policy,\n                       batch_size=512,\n                       val_batch_size= 1024,\n                       test_batch_size= 1024,\n                       train_data_size=640_000, # really small size for demo\n                       val_data_size=10_000,\n                       test_data_size=10_000,\n                       optimizer_kwargs={\n                           \"lr\":1e-4,\n                           \"weight_decay\": 1e-6,\n                       },\n                       lr_scheduler=\"MultiStepLR\",\n                       lr_scheduler_kwargs={\n                            \"milestones\": [80, 95],\n                            \"gamma\":0.1,\n                       },\n                       policy_kwargs={  # we can specify the decode types using the policy_kwargs\n                           \"train_decode_type\": \"sampling\",\n                           \"val_decode_type\": \"greedy\",\n                           \"test_decode_type\": \"greedy\",\n                       }\n                       )\n</pre> batch_size = 1024 env = landuseOptEnv(generator_params=dict(num_loc=50)) emb_dim = 128 policy = AttentionModelPolicy(env_name=env.name, # this is actually not needed since we are initializing the embeddings!                               embed_dim=emb_dim,                               init_embedding=lopInitEmbedding(emb_dim),                               context_embedding=LOPContext(emb_dim),                               dynamic_embedding=StaticEmbedding(emb_dim) ) # Model: default is AM with REINFORCE and greedy rollout baseline model = AttentionModel(env,                        baseline='rollout',                        policy=policy,                        batch_size=512,                        val_batch_size= 1024,                        test_batch_size= 1024,                        train_data_size=640_000, # really small size for demo                        val_data_size=10_000,                        test_data_size=10_000,                        optimizer_kwargs={                            \"lr\":1e-4,                            \"weight_decay\": 1e-6,                        },                        lr_scheduler=\"MultiStepLR\",                        lr_scheduler_kwargs={                             \"milestones\": [80, 95],                             \"gamma\":0.1,                        },                        policy_kwargs={  # we can specify the decode types using the policy_kwargs                            \"train_decode_type\": \"sampling\",                            \"val_decode_type\": \"greedy\",                            \"test_decode_type\": \"greedy\",                        }                        )  <pre>D:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\nD:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n</pre> In\u00a0[5]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntd_init = env.reset(batch_size=[3]).to(device)\npolicy = model.policy.to(device)\nout = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\n\nactions_untrained = out['actions'].cpu().detach()\nrewards_untrained = out['reward'].cpu().detach()\n\nfor i in range(3):\n    print(f\"Problem {i+1} | Cost: {-rewards_untrained[i]:.3f}\")\n    env.render(td_init[i], actions_untrained[i])\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") td_init = env.reset(batch_size=[3]).to(device) policy = model.policy.to(device) out = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)  actions_untrained = out['actions'].cpu().detach() rewards_untrained = out['reward'].cpu().detach()  for i in range(3):     print(f\"Problem {i+1} | Cost: {-rewards_untrained[i]:.3f}\")     env.render(td_init[i], actions_untrained[i]) <pre>E:\\temp\\DRLtest\\rl4co\\rl4co\\models\\nn\\attention.py:128: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n  out = self.sdpa_fn(\nE:\\temp\\DRLtest\\rl4co\\rl4co\\envs\\urbanplan\\cityplan\\env.py:218: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  neighbors = torch.tensor(neighbors, dtype=torch.long, device=device)  # Shape: [batch_size, num_centers, num_neighbours + 1]\n</pre> <pre>Problem 1 | Cost: -0.639\n</pre> <pre>D:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\IPython\\core\\pylabtools.py:77: DeprecationWarning: backend2gui is deprecated since IPython 8.24, backends are managed in matplotlib and can be externally registered.\n  warnings.warn(\n</pre> <pre>Problem 2 | Cost: -0.797\n</pre> <pre>Problem 3 | Cost: -0.549\n</pre> In\u00a0[6]: Copied! <pre>from lightning.pytorch.callbacks import ModelCheckpoint, RichModelSummary\n# Checkpointing callback: save models when validation reward improves\ncheckpoint_callback = ModelCheckpoint(  dirpath=\"checkpoints\", # save to checkpoints/\n                                        filename=\"epoch_{epoch:03d}\",  # save as epoch_XXX.ckpt\n                                        save_top_k=1, # save only the best model\n                                        save_last=True, # save the last model\n                                        monitor=\"val/reward\", # monitor validation reward\n                                        mode=\"max\") # maximize validation reward\n\n# Print model summary\nrich_model_summary = RichModelSummary(max_depth=3)\n# Callbacks list\ncallbacks = [checkpoint_callback, rich_model_summary]\n</pre> from lightning.pytorch.callbacks import ModelCheckpoint, RichModelSummary # Checkpointing callback: save models when validation reward improves checkpoint_callback = ModelCheckpoint(  dirpath=\"checkpoints\", # save to checkpoints/                                         filename=\"epoch_{epoch:03d}\",  # save as epoch_XXX.ckpt                                         save_top_k=1, # save only the best model                                         save_last=True, # save the last model                                         monitor=\"val/reward\", # monitor validation reward                                         mode=\"max\") # maximize validation reward  # Print model summary rich_model_summary = RichModelSummary(max_depth=3) # Callbacks list callbacks = [checkpoint_callback, rich_model_summary] In\u00a0[7]: Copied! <pre>from rl4co.utils.trainer import RL4COTrainer\n\ntrainer = RL4COTrainer(\n    max_epochs=5,\n    accelerator=\"gpu\",\n    devices=1,\n    logger=logger,\n    callbacks=callbacks,\n)\n</pre> from rl4co.utils.trainer import RL4COTrainer  trainer = RL4COTrainer(     max_epochs=5,     accelerator=\"gpu\",     devices=1,     logger=logger,     callbacks=callbacks, ) <pre>Using 16bit Automatic Mixed Precision (AMP)\nTrainer already configured with model summary callbacks: [&lt;class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'&gt;]. Skipping setting a default `ModelSummary` callback.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n</pre> In\u00a0[8]: Copied! <pre>trainer.fit(model)\n</pre> trainer.fit(model) <pre>D:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\wandb\\sdk\\lib\\ipython.py:89: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n  from IPython.core.display import display\n</pre> <pre>VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888925108, max=1.0\u2026</pre> <pre>D:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n</pre> In\u00a0[9]: Copied! <pre># Greedy rollouts over trained model (same states as previous plot)\npolicy = model.policy.to(device)\nout = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\n\n# Plotting\nprint(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\")\nfor td, actions in zip(td_init, out['actions'].cpu()):\n    env.render(td, actions)\n</pre> # Greedy rollouts over trained model (same states as previous plot) policy = model.policy.to(device) out = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)  # Plotting print(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\") for td, actions in zip(td_init, out['actions'].cpu()):     env.render(td, actions) <pre>Tour lengths: ['-0.64', '-0.80', '-0.55']\n</pre> In\u00a0[9]: Copied! <pre>\n</pre>"},{"location":"tests/luop50test/","title":"Luop50test","text":"In\u00a0[1]: Copied! <pre>from rl4co.envs import landuseOptEnv\nimport torch\nfrom rl4co.models import AttentionModel, AttentionModelPolicy\nfrom rl4co.models.nn.env_embeddings.context import LOPContext\nfrom rl4co.models.nn.env_embeddings.dynamic import StaticEmbedding\nfrom rl4co.models.nn.env_embeddings.init import lopInitEmbedding\nfrom rl4co.envs.urbanplan.cityplan import init\nimport torch\nfrom tensordict.tensordict import TensorDict\n</pre> from rl4co.envs import landuseOptEnv import torch from rl4co.models import AttentionModel, AttentionModelPolicy from rl4co.models.nn.env_embeddings.context import LOPContext from rl4co.models.nn.env_embeddings.dynamic import StaticEmbedding from rl4co.models.nn.env_embeddings.init import lopInitEmbedding from rl4co.envs.urbanplan.cityplan import init import torch from tensordict.tensordict import TensorDict <pre>D:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\lightning\\fabric\\__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning.fabric')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\nD:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\pkg_resources\\__init__.py:2317: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('lightning')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(parent)\nD:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\lightning\\pytorch\\__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('lightning.pytorch')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\nD:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\pkg_resources\\__init__.py:2317: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('lightning')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(parent)\n</pre> In\u00a0[2]: Copied! <pre>''' neighbour to list '''\npolygoncount = 221\nneighbourlist = init.getneighbourlist('../Data/queen.csv', polygoncount)\nbasiclanduse = init.readlanduselist('../Data/baseParcels.shp', polygoncount)\nlandusePalette = {'Commercial': 'coral',\n                  'Residential': 'peachpuff',\n                  'Office': 'indianred',\n                  'Residential&amp;Commercial': 'lightsalmon',\n                  'Green Space': 'lightgreen',\n                  'Education': 'lightskyblue',\n                  'Hospital': 'royalblue',\n                  'SOHO': 'lightcoral'\n                  }\n\nlandtype = ['Commercial', 'Residential', 'Office', 'Residential&amp;Commercial', 'Green Space', 'Education', 'Hospital',\n            'SOHO']\nadj_matrix = init.get_adjacency_matrix('../Data/queen.csv', polygoncount)\nimport math\nimport numpy as np\nshapefile = '../Data/Parcels.shp'\n\n\narealist = init.readarealist(shapefile, polygoncount)\nlocs_list = init.normalizeloc(shapefile)\nlocs = torch.tensor(locs_list, dtype=torch.float32).unsqueeze(0)\nareas = torch.tensor(init.normalizearea(arealist), dtype=torch.float32).unsqueeze(0)\ninit_plan = torch.tensor(init.map_to_num(basiclanduse, landtype), dtype=torch.int64).unsqueeze(0)\nfixed_mask = torch.ones_like(init_plan, dtype=torch.bool)\nfixed_mask[(init_plan == 4) | (init_plan == 6)] = 0\nneighbourlist = torch.tensor(adj_matrix).unsqueeze(0)\ndistances = torch.tensor(init.calculate_distance_matrix(locs_list), dtype=torch.float32).unsqueeze(0)\ntd = TensorDict(\n    {\n        \"locs\": locs,\n        \"areas\": areas,\n        \"init_plan\": init_plan,\n        \"fixed_mask\": fixed_mask,\n        \"adjacency_list\": neighbourlist,\n        \"distances\": distances,\n    },\n    batch_size=1,\n)\n</pre> ''' neighbour to list ''' polygoncount = 221 neighbourlist = init.getneighbourlist('../Data/queen.csv', polygoncount) basiclanduse = init.readlanduselist('../Data/baseParcels.shp', polygoncount) landusePalette = {'Commercial': 'coral',                   'Residential': 'peachpuff',                   'Office': 'indianred',                   'Residential&amp;Commercial': 'lightsalmon',                   'Green Space': 'lightgreen',                   'Education': 'lightskyblue',                   'Hospital': 'royalblue',                   'SOHO': 'lightcoral'                   }  landtype = ['Commercial', 'Residential', 'Office', 'Residential&amp;Commercial', 'Green Space', 'Education', 'Hospital',             'SOHO'] adj_matrix = init.get_adjacency_matrix('../Data/queen.csv', polygoncount) import math import numpy as np shapefile = '../Data/Parcels.shp'   arealist = init.readarealist(shapefile, polygoncount) locs_list = init.normalizeloc(shapefile) locs = torch.tensor(locs_list, dtype=torch.float32).unsqueeze(0) areas = torch.tensor(init.normalizearea(arealist), dtype=torch.float32).unsqueeze(0) init_plan = torch.tensor(init.map_to_num(basiclanduse, landtype), dtype=torch.int64).unsqueeze(0) fixed_mask = torch.ones_like(init_plan, dtype=torch.bool) fixed_mask[(init_plan == 4) | (init_plan == 6)] = 0 neighbourlist = torch.tensor(adj_matrix).unsqueeze(0) distances = torch.tensor(init.calculate_distance_matrix(locs_list), dtype=torch.float32).unsqueeze(0) td = TensorDict(     {         \"locs\": locs,         \"areas\": areas,         \"init_plan\": init_plan,         \"fixed_mask\": fixed_mask,         \"adjacency_list\": neighbourlist,         \"distances\": distances,     },     batch_size=1, ) <pre>D:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\pyproj\\__init__.py:89: UserWarning: pyproj unable to set database path.\n  _pyproj_global_context_initialize()\n</pre> In\u00a0[3]: Copied! <pre>batch_size = 1024\nenv = landuseOptEnv(generator_params=dict(num_loc=50))\nemb_dim = 128\npolicy = AttentionModelPolicy(env_name=env.name, # this is actually not needed since we are initializing the embeddings!\n                              embed_dim=emb_dim,\n                              init_embedding=lopInitEmbedding(emb_dim),\n                              context_embedding=LOPContext(emb_dim),\n                              dynamic_embedding=StaticEmbedding(emb_dim)\n)\n# Model: default is AM with REINFORCE and greedy rollout baseline\nmodel = AttentionModel(env,\n                       baseline='rollout',\n                       policy=policy,\n                       )\n</pre> batch_size = 1024 env = landuseOptEnv(generator_params=dict(num_loc=50)) emb_dim = 128 policy = AttentionModelPolicy(env_name=env.name, # this is actually not needed since we are initializing the embeddings!                               embed_dim=emb_dim,                               init_embedding=lopInitEmbedding(emb_dim),                               context_embedding=LOPContext(emb_dim),                               dynamic_embedding=StaticEmbedding(emb_dim) ) # Model: default is AM with REINFORCE and greedy rollout baseline model = AttentionModel(env,                        baseline='rollout',                        policy=policy,                        ) <pre>D:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\nD:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n</pre> In\u00a0[\u00a0]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenv = landuseOptEnv(generator_params=dict(num_loc=50)).to(device)\nnew_model_checkpoint = AttentionModel.load_from_checkpoint(\"checkpoints/last.ckpt\", strict=False)\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") env = landuseOptEnv(generator_params=dict(num_loc=50)).to(device) new_model_checkpoint = AttentionModel.load_from_checkpoint(\"checkpoints/last.ckpt\", strict=False) <pre>D:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\lightning\\pytorch\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['baseline.baseline.policy.encoder.init_embedding.init_embed.weight', 'baseline.baseline.policy.encoder.init_embedding.init_embed.bias', 'baseline.baseline.policy.encoder.net.layers.0.0.module.Wqkv.weight', 'baseline.baseline.policy.encoder.net.layers.0.0.module.Wqkv.bias', 'baseline.baseline.policy.encoder.net.layers.0.0.module.out_proj.weight', 'baseline.baseline.policy.encoder.net.layers.0.0.module.out_proj.bias', 'baseline.baseline.policy.encoder.net.layers.0.1.normalizer.weight', 'baseline.baseline.policy.encoder.net.layers.0.1.normalizer.bias', 'baseline.baseline.policy.encoder.net.layers.0.1.normalizer.running_mean', 'baseline.baseline.policy.encoder.net.layers.0.1.normalizer.running_var', 'baseline.baseline.policy.encoder.net.layers.0.1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.net.layers.0.2.module.lins.0.weight', 'baseline.baseline.policy.encoder.net.layers.0.2.module.lins.0.bias', 'baseline.baseline.policy.encoder.net.layers.0.2.module.lins.1.weight', 'baseline.baseline.policy.encoder.net.layers.0.2.module.lins.1.bias', 'baseline.baseline.policy.encoder.net.layers.0.3.normalizer.weight', 'baseline.baseline.policy.encoder.net.layers.0.3.normalizer.bias', 'baseline.baseline.policy.encoder.net.layers.0.3.normalizer.running_mean', 'baseline.baseline.policy.encoder.net.layers.0.3.normalizer.running_var', 'baseline.baseline.policy.encoder.net.layers.0.3.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.net.layers.1.0.module.Wqkv.weight', 'baseline.baseline.policy.encoder.net.layers.1.0.module.Wqkv.bias', 'baseline.baseline.policy.encoder.net.layers.1.0.module.out_proj.weight', 'baseline.baseline.policy.encoder.net.layers.1.0.module.out_proj.bias', 'baseline.baseline.policy.encoder.net.layers.1.1.normalizer.weight', 'baseline.baseline.policy.encoder.net.layers.1.1.normalizer.bias', 'baseline.baseline.policy.encoder.net.layers.1.1.normalizer.running_mean', 'baseline.baseline.policy.encoder.net.layers.1.1.normalizer.running_var', 'baseline.baseline.policy.encoder.net.layers.1.1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.net.layers.1.2.module.lins.0.weight', 'baseline.baseline.policy.encoder.net.layers.1.2.module.lins.0.bias', 'baseline.baseline.policy.encoder.net.layers.1.2.module.lins.1.weight', 'baseline.baseline.policy.encoder.net.layers.1.2.module.lins.1.bias', 'baseline.baseline.policy.encoder.net.layers.1.3.normalizer.weight', 'baseline.baseline.policy.encoder.net.layers.1.3.normalizer.bias', 'baseline.baseline.policy.encoder.net.layers.1.3.normalizer.running_mean', 'baseline.baseline.policy.encoder.net.layers.1.3.normalizer.running_var', 'baseline.baseline.policy.encoder.net.layers.1.3.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.net.layers.2.0.module.Wqkv.weight', 'baseline.baseline.policy.encoder.net.layers.2.0.module.Wqkv.bias', 'baseline.baseline.policy.encoder.net.layers.2.0.module.out_proj.weight', 'baseline.baseline.policy.encoder.net.layers.2.0.module.out_proj.bias', 'baseline.baseline.policy.encoder.net.layers.2.1.normalizer.weight', 'baseline.baseline.policy.encoder.net.layers.2.1.normalizer.bias', 'baseline.baseline.policy.encoder.net.layers.2.1.normalizer.running_mean', 'baseline.baseline.policy.encoder.net.layers.2.1.normalizer.running_var', 'baseline.baseline.policy.encoder.net.layers.2.1.normalizer.num_batches_tracked', 'baseline.baseline.policy.encoder.net.layers.2.2.module.lins.0.weight', 'baseline.baseline.policy.encoder.net.layers.2.2.module.lins.0.bias', 'baseline.baseline.policy.encoder.net.layers.2.2.module.lins.1.weight', 'baseline.baseline.policy.encoder.net.layers.2.2.module.lins.1.bias', 'baseline.baseline.policy.encoder.net.layers.2.3.normalizer.weight', 'baseline.baseline.policy.encoder.net.layers.2.3.normalizer.bias', 'baseline.baseline.policy.encoder.net.layers.2.3.normalizer.running_mean', 'baseline.baseline.policy.encoder.net.layers.2.3.normalizer.running_var', 'baseline.baseline.policy.encoder.net.layers.2.3.normalizer.num_batches_tracked', 'baseline.baseline.policy.decoder.context_embedding.project_context.weight', 'baseline.baseline.policy.decoder.pointer.project_out.weight', 'baseline.baseline.policy.decoder.project_node_embeddings.weight', 'baseline.baseline.policy.decoder.project_fixed_context.weight']\nval_file not set. Generating dataset instead\ntest_file not set. Generating dataset instead\nE:\\temp\\DRLtest\\rl4co\\rl4co\\models\\nn\\attention.py:128: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n  out = self.sdpa_fn(\n</pre> <pre>reward: 1.001778483390808\ncompatibility: 7.656000000000001\naccessibility: -2.3844751102089616\nreward: 0.9676637053489685\ncompatibility: 7.435500000000002\naccessibility: -2.317522324069746\nreward: 1.192399501800537\ncompatibility: 7.642499999999997\naccessibility: -2.091311543256259\nreward: 0.656539797782898\ncompatibility: 7.511500000000001\naccessibility: -2.824922456238185\nreward: 1.1365175247192383\ncompatibility: 7.488500000000001\naccessibility: -2.0926344392396534\nreward: 0.7877736687660217\ncompatibility: 7.431000000000003\naccessibility: -2.5849466387006497\nreward: 0.7733287215232849\ncompatibility: 7.412500000000005\naccessibility: -2.5967033152843495\nreward: 1.1690138578414917\ncompatibility: 7.471000000000001\naccessibility: -2.034514864968714\nreward: 0.9809635877609253\ncompatibility: 7.414999999999997\naccessibility: -2.2865903231192357\nreward: 1.0408053398132324\ncompatibility: 7.5675\naccessibility: -2.278524154691778\nreward: 1.105639100074768\ncompatibility: 7.527\naccessibility: -2.1595770884181964\nreward: 1.209086537361145\ncompatibility: 7.657\naccessibility: -2.0740486964525746\nreward: 0.8134313225746155\ncompatibility: 7.576999999999999\naccessibility: -2.624674469429946\nreward: 0.8481356501579285\ncompatibility: 7.475000000000001\naccessibility: -2.5179750675910424\nreward: 1.1077402830123901\ncompatibility: 7.296499999999998\naccessibility: -2.032943138831256\nreward: 0.9326587915420532\ncompatibility: 7.406000000000002\naccessibility: -2.3542261408488607\nreward: 1.0801243782043457\ncompatibility: 7.6035\naccessibility: -2.23883134731676\nreward: 1.0416271686553955\ncompatibility: 7.684500000000002\naccessibility: -2.3399699891378756\nreward: 1.0141186714172363\ncompatibility: 7.550999999999999\naccessibility: -2.309714828082522\nreward: 1.0700836181640625\ncompatibility: 7.463500000000002\naccessibility: -2.1788924236454372\nreward: 0.8241402506828308\ncompatibility: 7.471499999999999\naccessibility: -2.5520931561804376\nreward: 0.7766275405883789\ncompatibility: 7.2495\naccessibility: -2.5044336716777513\nreward: 1.0228278636932373\ncompatibility: 7.511500000000003\naccessibility: -2.2754903348999354\nreward: 1.0928961038589478\ncompatibility: 7.475\naccessibility: -2.1508343762727344\nreward: 1.0931392908096313\ncompatibility: 7.721499999999997\naccessibility: -2.2825231901909593\nreward: 0.9478731751441956\ncompatibility: 7.466500000000001\naccessibility: -2.3638152456631745\nreward: 0.9688505530357361\ncompatibility: 7.1800000000000015\naccessibility: -2.1788669879007276\nreward: 0.7896503210067749\ncompatibility: 7.126999999999999\naccessibility: -2.419274501613957\nreward: 0.7941057085990906\ncompatibility: 7.489000000000003\naccessibility: -2.606520024056289\nreward: 0.8769984841346741\ncompatibility: 7.491000000000003\naccessibility: -2.4832522530490486\nreward: 0.9106881618499756\ncompatibility: 7.425999999999999\naccessibility: -2.397896345481382\nreward: 0.9250473380088806\ncompatibility: 7.479000000000003\naccessibility: -2.4047504547338336\nreward: 1.1685762405395508\ncompatibility: 7.4879999999999995\naccessibility: -2.044278530910989\nreward: 1.1347309350967407\ncompatibility: 7.689000000000002\naccessibility: -2.2027249589084614\nreward: 0.7790508270263672\ncompatibility: 7.512999999999998\naccessibility: -2.6419595176953905\nreward: 0.8685311079025269\ncompatibility: 7.423499999999999\naccessibility: -2.459792663190508\nreward: 0.8949558734893799\ncompatibility: 7.442500000000003\naccessibility: -2.430334003729473\nreward: 1.0783852338790894\ncompatibility: 7.7655\naccessibility: -2.328225709384857\nreward: 1.0599545240402222\ncompatibility: 7.632\naccessibility: -2.2843538531455985\nreward: 0.9574841260910034\ncompatibility: 7.474499999999999\naccessibility: -2.353684535123388\nreward: 0.8282057642936707\ncompatibility: 7.531500000000001\naccessibility: -2.5781378145852267\nreward: 0.9273480772972107\ncompatibility: 7.3229999999999995\naccessibility: -2.3177279099897548\nreward: 0.9740543365478516\ncompatibility: 7.6114999999999995\naccessibility: -2.402222063277529\nreward: 0.9118666648864746\ncompatibility: 7.3645000000000005\naccessibility: -2.363182126886473\nreward: 1.0574556589126587\ncompatibility: 7.781999999999997\naccessibility: -2.3684593470412456\nreward: 1.1828402280807495\ncompatibility: 7.724500000000003\naccessibility: -2.149579011623252\nreward: 0.7642995119094849\ncompatibility: 7.337500000000001\naccessibility: -2.5700685452946326\nreward: 0.9017908573150635\ncompatibility: 7.555\naccessibility: -2.4803494661603382\nreward: 0.5302156805992126\ncompatibility: 7.312999999999999\naccessibility: -2.908069332020391\nreward: 0.9885926842689514\ncompatibility: 7.502000000000002\naccessibility: -2.321753807123189\nreward: 1.0628124475479126\ncompatibility: 7.543499999999999\naccessibility: -2.23265627369933\nreward: 1.1040232181549072\ncompatibility: 7.360500000000001\naccessibility: -2.0728044804482426\nreward: 0.9121976494789124\ncompatibility: 7.537999999999999\naccessibility: -2.455632099282738\nreward: 0.7453547120094299\ncompatibility: 7.538500000000004\naccessibility: -2.706164320498094\nreward: 1.0941685438156128\ncompatibility: 7.752499999999999\naccessibility: -2.2975864640367063\nreward: 0.8117135763168335\ncompatibility: 7.603500000000001\naccessibility: -2.6414474850579523\nreward: 0.8649884462356567\ncompatibility: 7.4145\naccessibility: -2.46028520685363\nreward: 0.820690393447876\ncompatibility: 7.648499999999999\naccessibility: -2.6520894353980298\nreward: 1.0834072828292847\ncompatibility: 7.515500000000002\naccessibility: -2.1867640909176864\nreward: 0.7694265842437744\ncompatibility: 7.4384999999999994\naccessibility: -2.6164851257877024\nreward: 0.9127360582351685\ncompatibility: 7.676500000000001\naccessibility: -2.529020929831549\nreward: 0.7953671216964722\ncompatibility: 7.637499999999999\naccessibility: -2.684181477640028\nreward: 1.2501839399337769\ncompatibility: 7.6299999999999955\naccessibility: -1.9979384361191863\nreward: 0.7898356318473816\ncompatibility: 7.4440000000000035\naccessibility: -2.588817944382442\nreward: 1.2492214441299438\ncompatibility: 7.33\naccessibility: -1.83866781079887\nreward: 0.5000866055488586\ncompatibility: 7.349499999999998\naccessibility: -2.972816558472907\nreward: 0.8888819813728333\ncompatibility: 7.595500000000002\naccessibility: -2.5214091361024287\nreward: 1.1744660139083862\ncompatibility: 7.566999999999999\naccessibility: -2.0777652900701558\nreward: 0.9296368956565857\ncompatibility: 7.577999999999999\naccessibility: -2.4509018416164965\nreward: 0.9408108592033386\ncompatibility: 7.43\naccessibility: -2.3548550995541557\nreward: 1.1213568449020386\ncompatibility: 7.628499999999997\naccessibility: -2.1903754208000303\nreward: 0.911821186542511\ncompatibility: 7.588000000000001\naccessibility: -2.482982525568096\nreward: 0.9659362435340881\ncompatibility: 7.4525000000000015\naccessibility: -2.329220668418274\nreward: 0.6957194209098816\ncompatibility: 7.594000000000001\naccessibility: -2.8103494191007083\nreward: 0.9251560568809509\ncompatibility: 7.408499999999999\naccessibility: -2.3668195008408346\nreward: 0.8340142369270325\ncompatibility: 7.5165\naccessibility: -2.5613893325005392\nreward: 0.960189163684845\ncompatibility: 7.619500000000003\naccessibility: -2.4273055630758607\nreward: 0.7925750613212585\ncompatibility: 7.413499999999999\naccessibility: -2.5683695923310337\nreward: 1.0381966829299927\ncompatibility: 7.726000000000003\naccessibility: -2.3673477832444574\nreward: 1.0336073637008667\ncompatibility: 7.349000000000001\naccessibility: -2.172267449917469\nreward: 1.0094902515411377\ncompatibility: 7.525999999999999\naccessibility: -2.3032646326774304\nreward: 0.955254077911377\ncompatibility: 7.421500000000002\naccessibility: -2.3286367474331744\nreward: 1.0917056798934937\ncompatibility: 7.579000000000001\naccessibility: -2.208334295812085\nreward: 0.9145154356956482\ncompatibility: 7.546000000000005\naccessibility: -2.4564411072998467\nreward: 1.1163783073425293\ncompatibility: 7.6129999999999995\naccessibility: -2.1895397294383363\nreward: 0.879248321056366\ncompatibility: 7.5835\naccessibility: -2.5294310615066946\nreward: 1.1118254661560059\ncompatibility: 7.7325\naccessibility: -2.260386714182719\nreward: 1.0334514379501343\ncompatibility: 7.364999999999998\naccessibility: -2.181072776308036\nreward: 0.7877811789512634\ncompatibility: 7.527500000000001\naccessibility: -2.636631804394467\nreward: 0.8776072859764099\ncompatibility: 7.689\naccessibility: -2.588410498456903\nreward: 0.9270512461662292\ncompatibility: 7.4159999999999995\naccessibility: -2.367994550578821\nreward: 1.0672348737716675\ncompatibility: 7.662499999999999\naccessibility: -2.2897726054024936\nreward: 0.9295153617858887\ncompatibility: 7.316000000000002\naccessibility: -2.3107269717518872\nreward: 0.9225425124168396\ncompatibility: 7.409\naccessibility: -2.3710077029847914\nreward: 1.1423206329345703\ncompatibility: 7.559500000000002\naccessibility: -2.1219654352137525\nreward: 1.1479084491729736\ncompatibility: 7.501999999999998\naccessibility: -2.0827802374833517\nreward: 0.6461206674575806\ncompatibility: 7.6605\naccessibility: -2.920372535842735\nreward: 0.7555535435676575\ncompatibility: 7.363499999999999\naccessibility: -2.5971161363200057\nreward: 0.8212524056434631\ncompatibility: 7.5055\naccessibility: -2.5746392661783832\nreward: 0.8809229135513306\ncompatibility: 7.474500000000002\naccessibility: -2.4685263534382487\nreward: 1.0317350625991821\ncompatibility: 7.6419999999999995\naccessibility: -2.3320403481295795\nreward: 0.40065309405326843\ncompatibility: 7.2915\naccessibility: -3.0908953505631853\nreward: 0.9781711101531982\ncompatibility: 7.320000000000002\naccessibility: -2.2398861828295917\nreward: 1.203142762184143\ncompatibility: 7.563500000000001\naccessibility: -2.0328750915389944\nreward: 0.938530445098877\ncompatibility: 7.439000000000001\naccessibility: -2.363097229491582\nreward: 1.0815763473510742\ncompatibility: 7.566\naccessibility: -2.216564017916167\nreward: 1.1871098279953003\ncompatibility: 7.586500000000004\naccessibility: -2.0692460407763305\nreward: 0.8992687463760376\ncompatibility: 7.5745\naccessibility: -2.494579015153999\nreward: 1.0880333185195923\ncompatibility: 7.566000000000002\naccessibility: -2.2068786670095273\nreward: 0.8921138644218445\ncompatibility: 7.576500000000001\naccessibility: -2.5063828032254905\nreward: 0.9539502263069153\ncompatibility: 7.5489999999999995\naccessibility: -2.3988961148471795\nreward: 1.049641489982605\ncompatibility: 7.309000000000001\naccessibility: -2.1267877966073674\nreward: 0.9095847606658936\ncompatibility: 7.652500000000003\naccessibility: -2.520890690460573\nreward: 0.761814296245575\ncompatibility: 7.534000000000001\naccessibility: -2.6790642661626802\nreward: 1.0518240928649902\ncompatibility: 7.434000000000001\naccessibility: -2.1904780948880713\nreward: 1.0097460746765137\ncompatibility: 7.558\naccessibility: -2.3200236655126023\nreward: 0.7878096699714661\ncompatibility: 7.2775\naccessibility: -2.5026605298921534\nreward: 0.8394572734832764\ncompatibility: 7.700999999999997\naccessibility: -2.6520641066384725\nreward: 1.0878996849060059\ncompatibility: 7.644\naccessibility: -2.2488647350168347\nreward: 0.906559944152832\ncompatibility: 7.291\naccessibility: -2.331767196568406\nreward: 1.4072939157485962\ncompatibility: 7.7325\naccessibility: -1.8171840729212745\nreward: 0.8214316964149475\ncompatibility: 7.473499999999998\naccessibility: -2.5572274857081867\nreward: 0.9138803482055664\ncompatibility: 7.4945\naccessibility: -2.4298044400717673\nreward: 1.1746113300323486\ncompatibility: 7.591500000000001\naccessibility: -2.0906723433209686\nreward: 1.0121389627456665\ncompatibility: 7.436000000000003\naccessibility: -2.2510772770397534\nreward: 0.8802314400672913\ncompatibility: 7.664499999999999\naccessibility: -2.5713492605734314\nreward: 0.8667720556259155\ncompatibility: 7.352999999999996\naccessibility: -2.424663346319304\nreward: 0.8345529437065125\ncompatibility: 7.367999999999999\naccessibility: -2.4810277593763868\nreward: 0.8489677906036377\ncompatibility: 7.513999999999999\naccessibility: -2.537619738416409\nreward: 0.9376635551452637\ncompatibility: 7.4615\naccessibility: -2.376451070768727\nreward: 1.0669864416122437\ncompatibility: 7.521500000000002\naccessibility: -2.2146096396806505\nreward: 0.9265322685241699\ncompatibility: 7.406499999999998\naccessibility: -2.3636837095005094\nreward: 1.0928640365600586\ncompatibility: 7.695000000000001\naccessibility: -2.268739729768076\nreward: 0.8303058743476868\ncompatibility: 7.392499999999999\naccessibility: -2.500523366327743\nreward: 0.8894064426422119\ncompatibility: 7.529000000000001\naccessibility: -2.4849974849345147\nreward: 0.9735552072525024\ncompatibility: 7.4495\naccessibility: -2.3161850674878384\nreward: 0.9601202011108398\ncompatibility: 7.633000000000002\naccessibility: -2.4346410893649475\nreward: 0.9136755466461182\ncompatibility: 7.582499999999998\naccessibility: -2.4772545258958996\nreward: 0.9210554361343384\ncompatibility: 7.577000000000002\naccessibility: -2.4632382540519107\nreward: 0.9058069586753845\ncompatibility: 7.406999999999999\naccessibility: -2.3950395795922033\nreward: 0.5811807513237\ncompatibility: 7.434499999999999\naccessibility: -2.8967109777387723\nreward: 0.632584810256958\ncompatibility: 7.502999999999999\naccessibility: -2.8563013779152793\nreward: 0.8551084399223328\ncompatibility: 7.392500000000001\naccessibility: -2.4633195217160537\nreward: 1.1054797172546387\ncompatibility: 7.660500000000002\naccessibility: -2.231333912899362\nreward: 0.9118149876594543\ncompatibility: 7.650000000000001\naccessibility: -2.5162061122182324\nreward: 0.9646350145339966\ncompatibility: 7.585999999999999\naccessibility: -2.402690326951024\nreward: 0.5974956154823303\ncompatibility: 7.417499999999999\naccessibility: -2.8631315477611894\nreward: 1.1375648975372314\ncompatibility: 7.545\naccessibility: -2.121331237361206\nreward: 0.648078978061676\ncompatibility: 7.605500000000001\naccessibility: -2.887970802638638\nreward: 0.8264756202697754\ncompatibility: 7.717000000000001\naccessibility: -2.680107988793068\nreward: 1.079613447189331\ncompatibility: 7.445000000000001\naccessibility: -2.154686948168703\nreward: 1.092544674873352\ncompatibility: 7.531999999999996\naccessibility: -2.181897333489723\nreward: 0.9228272438049316\ncompatibility: 7.617499999999999\naccessibility: -2.482276995518794\nreward: 0.916108250617981\ncompatibility: 7.482999999999999\naccessibility: -2.4203019483786563\nreward: 0.9738043546676636\ncompatibility: 7.4910000000000005\naccessibility: -2.3380434530054868\nreward: 1.1270630359649658\ncompatibility: 7.4325\naccessibility: -2.0768160716711135\nreward: 0.7995203137397766\ncompatibility: 7.3969999999999985\naccessibility: -2.5491123570125263\nreward: 0.9969846606254578\ncompatibility: 7.5375\naccessibility: -2.328183716737362\nreward: 1.1202043294906616\ncompatibility: 7.6835\naccessibility: -2.2215685157122183\nreward: 1.3296083211898804\ncompatibility: 7.741999999999999\naccessibility: -1.9388017859621594\nreward: 1.0914422273635864\ncompatibility: 7.613999999999999\naccessibility: -2.227479516559926\nreward: 1.1239522695541382\ncompatibility: 7.435999999999999\naccessibility: -2.083357302716521\nreward: 0.8797440528869629\ncompatibility: 7.441999999999999\naccessibility: -2.452883927033407\nreward: 0.8188298344612122\ncompatibility: 7.185500000000002\naccessibility: -2.4068445723988474\nreward: 0.95331871509552\ncompatibility: 7.5745000000000005\naccessibility: -2.4135041129318946\nreward: 1.0816868543624878\ncompatibility: 7.7444999999999995\naccessibility: -2.3120232480194445\nreward: 1.10368013381958\ncompatibility: 7.575500000000002\naccessibility: -2.1884975832367095\nreward: 0.9143142104148865\ncompatibility: 7.409000000000004\naccessibility: -2.383350133704872\nreward: 1.0265589952468872\ncompatibility: 7.575500000000001\naccessibility: -2.3041793101920924\nreward: 0.8954216241836548\ncompatibility: 7.429499999999999\naccessibility: -2.4226710957249606\nreward: 0.954012930393219\ncompatibility: 7.379499999999999\naccessibility: -2.307998453114309\nreward: 1.1315643787384033\ncompatibility: 7.551000000000001\naccessibility: -2.133546294234328\nreward: 1.0633525848388672\ncompatibility: 7.318000000000001\naccessibility: -2.1110424639344902\nreward: 0.761650562286377\ncompatibility: 7.572499999999998\naccessibility: -2.699934864464805\nreward: 0.9475034475326538\ncompatibility: 7.234000000000001\naccessibility: -2.239816230402324\nreward: 0.7754859328269958\ncompatibility: 7.439499999999999\naccessibility: -2.6079318431790965\nreward: 0.77436763048172\ncompatibility: 7.434000000000001\naccessibility: -2.606662862699437\nreward: 1.058777928352356\ncompatibility: 7.4905\naccessibility: -2.210315191120287\nreward: 0.7586818337440491\ncompatibility: 7.187\naccessibility: -2.497870066634081\nreward: 1.15337073802948\ncompatibility: 7.532999999999998\naccessibility: -2.091193891045825\nreward: 1.0383957624435425\ncompatibility: 7.696999999999997\naccessibility: -2.351513491387465\nreward: 1.048829197883606\ncompatibility: 7.563000000000001\naccessibility: -2.2640775855730393\nreward: 0.9258956909179688\ncompatibility: 7.684499999999998\naccessibility: -2.5135671435256945\nreward: 1.1868791580200195\ncompatibility: 7.750000000000003\naccessibility: -2.1571811814675526\nreward: 1.0465985536575317\ncompatibility: 7.468\naccessibility: -2.2165307503523985\nreward: 1.2740939855575562\ncompatibility: 7.501\naccessibility: -1.892966150498816\nreward: 0.9218891859054565\ncompatibility: 7.507000000000002\naccessibility: -2.424487632972801\nreward: 0.9430405497550964\ncompatibility: 7.397499999999999\naccessibility: -2.334099861979069\nreward: 0.818311870098114\ncompatibility: 7.4625\naccessibility: -2.556014319362337\nreward: 0.8618655800819397\ncompatibility: 7.5554999999999986\naccessibility: -2.5405051751331262\nreward: 0.8219600915908813\ncompatibility: 7.4445\naccessibility: -2.54089919135851\nreward: 0.8712764978408813\ncompatibility: 7.449\naccessibility: -2.4693352329228686\nreward: 1.1008374691009521\ncompatibility: 7.6355\naccessibility: -2.224904456681233\nreward: 1.089822769165039\ncompatibility: 7.670499999999999\naccessibility: -2.260176513702333\nreward: 0.8499521613121033\ncompatibility: 7.555999999999999\naccessibility: -2.558643180943241\nreward: 1.006631851196289\ncompatibility: 7.319500000000001\naccessibility: -2.196927150291657\nreward: 0.960196316242218\ncompatibility: 7.411000000000001\naccessibility: -2.3155984105057343\nreward: 1.2729109525680542\ncompatibility: 7.754499999999999\naccessibility: -2.030544244837762\nreward: 0.7655513286590576\ncompatibility: 7.292499999999999\naccessibility: -2.544083688455664\nreward: 1.075646996498108\ncompatibility: 7.513999999999999\naccessibility: -2.19760100022094\nreward: 0.990250825881958\ncompatibility: 7.537499999999999\naccessibility: -2.3382844513067464\nreward: 1.0100433826446533\ncompatibility: 7.5115\naccessibility: -2.294666991303102\nreward: 0.6853643655776978\ncompatibility: 7.446499999999999\naccessibility: -2.746864141948368\nreward: 0.8709179759025574\ncompatibility: 7.5465\naccessibility: -2.5221052179175416\nreward: 1.0739694833755493\ncompatibility: 7.724000000000002\naccessibility: -2.3126172279723907\nreward: 1.0119564533233643\ncompatibility: 7.740500000000004\naccessibility: -2.4144760883215697\nreward: 0.7041195631027222\ncompatibility: 7.325\naccessibility: -2.653642127672937\nreward: 0.7753398418426514\ncompatibility: 7.349500000000001\naccessibility: -2.559936647102432\nreward: 1.0808194875717163\ncompatibility: 7.6865\naccessibility: -2.282252968582524\nreward: 0.9137715101242065\ncompatibility: 7.550999999999999\naccessibility: -2.460235600746363\nreward: 0.8098570704460144\ncompatibility: 7.3755000000000015\naccessibility: -2.5220893872581795\nreward: 0.9876771569252014\ncompatibility: 7.410000000000001\naccessibility: -2.2738413819398566\nreward: 0.8265029788017273\ncompatibility: 7.547499999999998\naccessibility: -2.589263419958056\nreward: 0.9028881192207336\ncompatibility: 7.3679999999999986\naccessibility: -2.3785249764331895\nreward: 0.9399164319038391\ncompatibility: 7.485500000000001\naccessibility: -2.3859289654443208\nreward: 0.8798376321792603\ncompatibility: 7.582499999999999\naccessibility: -2.5280114355224796\nreward: 0.9637414813041687\ncompatibility: 7.441999999999999\naccessibility: -2.3268877635177385\nreward: 1.0760953426361084\ncompatibility: 7.610000000000002\naccessibility: -2.2483569501050926\nreward: 0.9706252217292786\ncompatibility: 7.558000000000002\naccessibility: -2.3787050450383087\nreward: 0.6047696471214294\ncompatibility: 7.103500000000002\naccessibility: -2.6840062708647294\nreward: 0.9176619052886963\ncompatibility: 7.425000000000001\naccessibility: -2.3868999983366948\nreward: 0.5163203477859497\ncompatibility: 7.452999999999998\naccessibility: -3.0039123385341755\nreward: 0.8919745087623596\ncompatibility: 7.598000000000001\naccessibility: -2.5181096225271546\nreward: 0.9121308326721191\ncompatibility: 7.2810000000000015\naccessibility: -2.318053736217931\nreward: 0.6086833477020264\ncompatibility: 7.3485000000000005\naccessibility: -2.80938571822556\nreward: 0.9145582318305969\ncompatibility: 7.566000000000002\naccessibility: -2.4670912215623817\nreward: 1.129172444343567\ncompatibility: 7.646\naccessibility: -2.188027092437965\nreward: 1.055792212486267\ncompatibility: 7.4095\naccessibility: -2.171400921130103\nreward: 0.9951492547988892\ncompatibility: 7.335999999999999\naccessibility: -2.2229904111488272\nreward: 0.8955824971199036\ncompatibility: 7.427000000000001\naccessibility: -2.421090534738045\nreward: 1.0132536888122559\ncompatibility: 7.634499999999998\naccessibility: -2.3557445546022393\nreward: 1.0146682262420654\ncompatibility: 7.488500000000001\naccessibility: -2.275408347907938\nreward: 0.756419837474823\ncompatibility: 7.448\naccessibility: -2.6410845400290737\nreward: 1.1744576692581177\ncompatibility: 7.543500000000002\naccessibility: -2.0651885772568686\nreward: 0.9551787972450256\ncompatibility: 7.497999999999999\naccessibility: -2.3697318066156123\nreward: 0.8954256772994995\ncompatibility: 7.364\naccessibility: -2.3875758029533234\nreward: 1.0463817119598389\ncompatibility: 7.5865\naccessibility: -2.2803380815750556\nreward: 0.9457378387451172\ncompatibility: 7.303999999999999\naccessibility: -2.2799646295729246\nreward: 0.8885926604270935\ncompatibility: 7.243499999999999\naccessibility: -2.3332717611182314\nreward: 0.7557100057601929\ncompatibility: 7.499499999999999\naccessibility: -2.669738565353786\nreward: 1.0498480796813965\ncompatibility: 7.595999999999999\naccessibility: -2.280227864980996\nreward: 0.869454562664032\ncompatibility: 7.336500000000001\naccessibility: -2.41180030817299\nreward: 0.9801408648490906\ncompatibility: 7.588999999999999\naccessibility: -2.3810386881642085\nreward: 1.0638787746429443\ncompatibility: 7.7669999999999995\naccessibility: -2.3507889031396374\nreward: 0.9335610866546631\ncompatibility: 7.493000000000003\naccessibility: -2.399479824827309\nreward: 1.0045565366744995\ncompatibility: 7.444000000000002\naccessibility: -2.2667366827699156\nreward: 1.064476728439331\ncompatibility: 7.410999999999998\naccessibility: -2.1591777365411673\nreward: 0.7960091233253479\ncompatibility: 7.439\naccessibility: -2.5768791462818497\nreward: 1.002897024154663\ncompatibility: 7.4559999999999995\naccessibility: -2.275654521596762\nreward: 0.9864285588264465\ncompatibility: 7.417999999999999\naccessibility: -2.280000015980757\nreward: 0.9630652666091919\ncompatibility: 7.4445000000000014\naccessibility: -2.3292413845471867\nreward: 1.0911654233932495\ncompatibility: 7.444500000000001\naccessibility: -2.137091117519081\nreward: 0.8558354377746582\ncompatibility: 7.597500000000001\naccessibility: -2.572050396671183\nreward: 0.7944753170013428\ncompatibility: 7.414\naccessibility: -2.565787048841528\nreward: 1.0772149562835693\ncompatibility: 7.583499999999999\naccessibility: -2.2324810685315493\nreward: 0.934391975402832\ncompatibility: 7.395\naccessibility: -2.345733454630304\nreward: 0.8149511814117432\ncompatibility: 7.413000000000002\naccessibility: -2.534537525546145\nreward: 0.7728067636489868\ncompatibility: 7.6320000000000014\naccessibility: -2.7150755351767404\nreward: 0.7682453989982605\ncompatibility: 7.3585\naccessibility: -2.575399756032811\nreward: 0.5196527242660522\ncompatibility: 7.443499999999998\naccessibility: -2.9938244999825123\nreward: 1.075045108795166\ncompatibility: 7.463999999999999\naccessibility: -2.171718111401424\nreward: 0.9459537863731384\ncompatibility: 7.4019999999999975\naccessibility: -2.3321407209283818\nreward: 1.2017290592193604\ncompatibility: 7.616999999999999\naccessibility: -2.0636564759720697\nreward: 0.9122921228408813\ncompatibility: 7.331999999999998\naccessibility: -2.345133275704253\nreward: 0.8675678372383118\ncompatibility: 7.510000000000003\naccessibility: -2.507576786952534\nreward: 1.1298377513885498\ncompatibility: 7.460500000000001\naccessibility: -2.087654126741483\nreward: 1.0025094747543335\ncompatibility: 7.448500000000001\naccessibility: -2.2722178716480403\nreward: 1.0017043352127075\ncompatibility: 7.498000000000001\naccessibility: -2.299943533880838\nreward: 0.9958536624908447\ncompatibility: 7.308000000000002\naccessibility: -2.2069337912246536\nreward: 0.9439826607704163\ncompatibility: 7.388000000000002\naccessibility: -2.3275974040352887\nreward: 1.2497183084487915\ncompatibility: 7.554500000000001\naccessibility: -1.9581904277770779\nreward: 0.974017322063446\ncompatibility: 7.6285000000000025\naccessibility: -2.4113847439363716\nreward: 0.9208856821060181\ncompatibility: 7.560000000000001\naccessibility: -2.4543857649146745\nreward: 0.8904517292976379\ncompatibility: 7.2575\naccessibility: -2.337983095270202\nreward: 0.7598665356636047\ncompatibility: 7.491500000000003\naccessibility: -2.6592180206140945\nreward: 1.044801115989685\ncompatibility: 7.423499999999999\naccessibility: -2.1953876037650124\nreward: 1.3385480642318726\ncompatibility: 7.6825\naccessibility: -1.893517170905561\nreward: 0.9556769132614136\ncompatibility: 7.498500000000003\naccessibility: -2.3692525212143707\nreward: 0.9559814929962158\ncompatibility: 7.701999999999999\naccessibility: -2.4778134874672113\nreward: 0.9550273418426514\ncompatibility: 7.563499999999998\naccessibility: -2.4050483079469185\nreward: 0.8606763482093811\ncompatibility: 7.554500000000003\naccessibility: -2.541753351407687\nreward: 0.96169114112854\ncompatibility: 7.607500000000001\naccessibility: -2.418623998569128\nreward: 1.0925596952438354\ncompatibility: 7.588500000000003\naccessibility: -2.2121426128190564\nreward: 1.1380876302719116\ncompatibility: 7.4385\naccessibility: -2.0634935897319746\nreward: 0.8711363673210144\ncompatibility: 7.226000000000002\naccessibility: -2.3500811221796254\nreward: 0.9514411687850952\ncompatibility: 7.499000000000002\naccessibility: -2.3758740011003328\nreward: 0.9825770854949951\ncompatibility: 7.748499999999998\naccessibility: -2.462830819453314\nreward: 1.0914570093154907\ncompatibility: 7.486499999999998\naccessibility: -2.159153834851127\nreward: 1.189146637916565\ncompatibility: 7.499999999999997\naccessibility: -2.019851426216495\nreward: 0.8633822798728943\ncompatibility: 7.5755\naccessibility: -2.548944416820102\nreward: 1.1237797737121582\ncompatibility: 7.663500000000002\naccessibility: -2.2054910150485623\nreward: 0.8525012135505676\ncompatibility: 7.358\naccessibility: -2.4487482053224445\nreward: 0.8328734636306763\ncompatibility: 7.520000000000001\naccessibility: -2.564975496601637\nreward: 0.9557497501373291\ncompatibility: 7.540499999999997\naccessibility: -2.3916431891226266\nreward: 0.775433361530304\ncompatibility: 7.638000000000003\naccessibility: -2.71434994823866\nreward: 1.040482521057129\ncompatibility: 7.364\naccessibility: -2.169990463196839\nreward: 0.8790686130523682\ncompatibility: 7.393000000000002\naccessibility: -2.427647066297732\nreward: 0.9100837707519531\ncompatibility: 7.5195000000000025\naccessibility: -2.448892208220546\nreward: 1.0217669010162354\ncompatibility: 7.551\naccessibility: -2.298242441075113\nreward: 1.2295488119125366\ncompatibility: 7.583000000000002\naccessibility: -2.00371241475876\nreward: 0.8299901485443115\ncompatibility: 7.501\naccessibility: -2.5591219142551234\nreward: 0.9990898370742798\ncompatibility: 7.3935\naccessibility: -2.24788311323956\nreward: 0.8443748950958252\ncompatibility: 7.551500000000003\naccessibility: -2.5645983852996226\nreward: 0.8976679444313049\ncompatibility: 7.387999999999997\naccessibility: -2.397069544251315\nreward: 0.42651450634002686\ncompatibility: 7.168499999999999\naccessibility: -2.9862103924607517\nreward: 1.036684274673462\ncompatibility: 7.563999999999998\naccessibility: -2.2828306997843617\nreward: 0.9123637676239014\ncompatibility: 7.569000000000002\naccessibility: -2.4719901015770915\nreward: 1.2449088096618652\ncompatibility: 7.7010000000000005\naccessibility: -2.0438867505131726\nreward: 0.9397609233856201\ncompatibility: 7.638\naccessibility: -2.4678585732983707\nreward: 0.8456258177757263\ncompatibility: 7.611499999999999\naccessibility: -2.5948648481667784\nreward: 0.6047612428665161\ncompatibility: 7.429999999999999\naccessibility: -2.8589295832221273\nreward: 0.9686621427536011\ncompatibility: 7.4215\naccessibility: -2.3085246732902744\nreward: 0.803432822227478\ncompatibility: 7.416500000000001\naccessibility: -2.553690012490007\nreward: 1.117218255996704\ncompatibility: 7.699\naccessibility: -2.2343512711019837\nreward: 1.0195297002792358\ncompatibility: 7.487\naccessibility: -2.267312581750357\nreward: 0.9526544809341431\ncompatibility: 7.7105000000000015\naccessibility: -2.4873575662700302\nreward: 1.0917550325393677\ncompatibility: 7.381\naccessibility: -2.102188959791942\nreward: 1.0420243740081787\ncompatibility: 7.354000000000003\naccessibility: -2.1623204988422904\nreward: 0.7766374945640564\ncompatibility: 7.425999999999999\naccessibility: -2.5989723399041877\nreward: 0.664596676826477\ncompatibility: 7.570000000000004\naccessibility: -2.844176403364782\nreward: 1.0216007232666016\ncompatibility: 7.644499999999998\naccessibility: -2.3485811112027157\nreward: 1.0689749717712402\ncompatibility: 7.5275\naccessibility: -2.214841073288077\nreward: 0.9843782186508179\ncompatibility: 7.751000000000002\naccessibility: -2.4614683592338045\nreward: 1.0320630073547363\ncompatibility: 7.685500000000001\naccessibility: -2.3548519871968137\nreward: 0.7525460124015808\ncompatibility: 7.133499999999998\naccessibility: -2.4784131493005717\nreward: 0.9910058379173279\ncompatibility: 7.444\naccessibility: -2.2870626882596334\nreward: 0.8836497664451599\ncompatibility: 7.506000000000001\naccessibility: -2.4813110560256395\nreward: 1.2762932777404785\ncompatibility: 7.6\naccessibility: -1.9427028736596035\nreward: 0.9240401983261108\ncompatibility: 7.5095\naccessibility: -2.422600436189864\nreward: 0.9087702631950378\ncompatibility: 7.519000000000001\naccessibility: -2.450594620711054\nreward: 1.0043686628341675\ncompatibility: 7.493499999999999\naccessibility: -2.293536285178105\nreward: 1.014513611793518\ncompatibility: 7.7675000000000045\naccessibility: -2.425104650917757\nreward: 0.8798096179962158\ncompatibility: 7.584000000000002\naccessibility: -2.5288569626805275\nreward: 1.158995270729065\ncompatibility: 7.634000000000002\naccessibility: -2.136864257815572\nreward: 1.035738229751587\ncompatibility: 7.615500000000002\naccessibility: -2.311839042530072\nreward: 0.991884171962738\ncompatibility: 7.565000000000001\naccessibility: -2.3505665981021364\nreward: 0.981953501701355\ncompatibility: 7.5575\naccessibility: -2.3614447730222667\nreward: 0.7624066472053528\ncompatibility: 7.481499999999999\naccessibility: -2.650050715322424\nreward: 0.6488507390022278\ncompatibility: 7.329499999999997\naccessibility: -2.7389559963788264\nreward: 1.07636559009552\ncompatibility: 7.643999999999998\naccessibility: -2.2661659091833575\nreward: 0.4156722128391266\ncompatibility: 7.360500000000001\naccessibility: -3.1053309848413893\nreward: 0.8122175335884094\ncompatibility: 7.292499999999999\naccessibility: -2.4740843961593426\nreward: 1.06504487991333\ncompatibility: 7.614500000000003\naccessibility: -2.267343354460462\nreward: 0.9574921131134033\ncompatibility: 7.495500000000002\naccessibility: -2.364922557640663\nreward: 0.7964063882827759\ncompatibility: 7.258500000000001\naccessibility: -2.4795868597143462\nreward: 1.0756773948669434\ncompatibility: 7.612000000000002\naccessibility: -2.2500553057562853\nreward: 0.7222762107849121\ncompatibility: 7.448\naccessibility: -2.6922999336015665\nreward: 0.7077370285987854\ncompatibility: 7.5120000000000005\naccessibility: -2.7483944389845836\nreward: 0.9478280544281006\ncompatibility: 7.5440000000000005\naccessibility: -2.4054007816219696\nreward: 0.8806083798408508\ncompatibility: 7.483000000000003\naccessibility: -2.473551691538339\nreward: 0.9414815902709961\ncompatibility: 7.448499999999999\naccessibility: -2.3637597296329043\nreward: 0.9936344623565674\ncompatibility: 7.326500000000001\naccessibility: -2.2201733351578445\nreward: 0.768743634223938\ncompatibility: 7.5305\naccessibility: -2.6667952274667126\nreward: 1.084916353225708\ncompatibility: 7.576500000000003\naccessibility: -2.217178965859336\nreward: 1.0589483976364136\ncompatibility: 7.526499999999998\naccessibility: -2.2293452583699382\nreward: 0.845477819442749\ncompatibility: 7.4245\naccessibility: -2.494908271658688\nreward: 1.0304104089736938\ncompatibility: 7.302499999999998\naccessibility: -2.152152251404114\nreward: 1.175567626953125\ncompatibility: 7.643500000000001\naccessibility: -2.1170950137096614\nreward: 1.0059943199157715\ncompatibility: 7.505\naccessibility: -2.2972585558454957\nreward: 1.1288572549819946\ncompatibility: 7.562500000000003\naccessibility: -2.1437677062669094\nreward: 0.7809385657310486\ncompatibility: 7.507500000000005\naccessibility: -2.6361814620904314\nreward: 0.8031245470046997\ncompatibility: 7.490000000000001\naccessibility: -2.5935274619885655\nreward: 1.287399172782898\ncompatibility: 7.738500000000001\naccessibility: -2.00024054137721\nreward: 0.8779522180557251\ncompatibility: 7.348\naccessibility: -2.4052145095723168\nreward: 0.8331040143966675\ncompatibility: 7.415\naccessibility: -2.5083797034718316\nreward: 0.8870589733123779\ncompatibility: 7.496500000000001\naccessibility: -2.4711079768916457\nreward: 0.750084638595581\ncompatibility: 7.574500000000001\naccessibility: -2.718355141430659\nreward: 1.1755177974700928\ncompatibility: 7.7490000000000006\naccessibility: -2.17368761226046\nreward: 0.999779462814331\ncompatibility: 7.468999999999999\naccessibility: -2.287295082444122\nreward: 0.8901433348655701\ncompatibility: 7.285499999999998\naccessibility: -2.3534457561556534\nreward: 0.8036849498748779\ncompatibility: 7.156499999999999\naccessibility: -2.4140261740500453\nreward: 1.043023705482483\ncompatibility: 7.4155000000000015\naccessibility: -2.193767929078656\nreward: 1.145122766494751\ncompatibility: 7.615999999999999\naccessibility: -2.1480301345140984\nreward: 1.0515806674957275\ncompatibility: 7.553999999999999\naccessibility: -2.255128984294397\nreward: 1.0791290998458862\ncompatibility: 7.509999999999996\naccessibility: -2.1902348595835215\nreward: 0.6982077956199646\ncompatibility: 7.559000000000001\naccessibility: -2.7878668593002733\nreward: 0.9006807208061218\ncompatibility: 7.536000000000001\naccessibility: -2.4718360524793503\nreward: 1.3233609199523926\ncompatibility: 7.693500000000001\naccessibility: -1.9221907273415977\nreward: 0.9796209335327148\ncompatibility: 7.552\naccessibility: -2.3619971791757344\nreward: 0.8179451823234558\ncompatibility: 7.620000000000001\naccessibility: -2.640939393994956\nreward: 1.0454351902008057\ncompatibility: 7.4965\naccessibility: -2.2335436633097285\nreward: 1.058842420578003\ncompatibility: 7.553500000000001\naccessibility: -2.2439684431772196\nreward: 0.9873456358909607\ncompatibility: 7.611000000000002\naccessibility: -2.3820172855367585\nreward: 1.026894450187683\ncompatibility: 7.599999999999999\naccessibility: -2.3168012374647584\nreward: 0.9202159643173218\ncompatibility: 7.455999999999998\naccessibility: -2.3996760515671856\nreward: 0.9550642967224121\ncompatibility: 7.484\naccessibility: -2.362403557206015\nreward: 1.089941382408142\ncompatibility: 7.570500000000003\naccessibility: -2.2064272680008927\nreward: 0.8801316022872925\ncompatibility: 7.433500000000004\naccessibility: -2.447749024702496\nreward: 0.8137091398239136\ncompatibility: 7.4155\naccessibility: -2.537739837378835\nreward: 1.2202485799789429\ncompatibility: 7.680499999999999\naccessibility: -2.069895072301998\nreward: 1.1454623937606812\ncompatibility: 7.552500000000001\naccessibility: -2.1135028234213666\nreward: 1.197054386138916\ncompatibility: 7.685499999999999\naccessibility: -2.1073647808938296\nreward: 0.8787581920623779\ncompatibility: 7.5325000000000015\naccessibility: -2.5028448342252405\nreward: 1.0089973211288452\ncompatibility: 7.660500000000002\naccessibility: -2.3760575190213458\nreward: 0.8622485995292664\ncompatibility: 7.481999999999999\naccessibility: -2.5005556562381113\nreward: 1.0067325830459595\ncompatibility: 7.475000000000001\naccessibility: -2.280079663629172\nreward: 1.1148333549499512\ncompatibility: 7.379\naccessibility: -2.0664999767694443\nreward: 0.8168590068817139\ncompatibility: 7.536499999999998\naccessibility: -2.5978365042256946\nreward: 1.1594254970550537\ncompatibility: 7.548999999999996\naccessibility: -2.090683173351711\nreward: 0.9457809925079346\ncompatibility: 7.301\naccessibility: -2.278292827489142\nreward: 0.9224480986595154\ncompatibility: 7.359000000000001\naccessibility: -2.344363524417825\nreward: 0.9867517948150635\ncompatibility: 7.561500000000002\naccessibility: -2.3563901528114153\nreward: 1.1113076210021973\ncompatibility: 7.584499999999999\naccessibility: -2.181877924355854\nreward: 1.2149686813354492\ncompatibility: 7.6215\naccessibility: -2.046207666457195\nreward: 1.1101123094558716\ncompatibility: 7.641000000000003\naccessibility: -2.2139386107742323\nreward: 0.8869209885597229\ncompatibility: 7.3530000000000015\naccessibility: -2.394439944555454\nreward: 1.029075026512146\ncompatibility: 7.608500000000003\naccessibility: -2.3180838270178614\nreward: 0.8316882252693176\ncompatibility: 7.490000000000004\naccessibility: -2.550681937956151\nreward: 0.8178054094314575\ncompatibility: 7.505500000000001\naccessibility: -2.579809777442185\nreward: 1.118513822555542\ncompatibility: 7.488999999999999\naccessibility: -2.119907873874981\nreward: 0.9318647980690002\ncompatibility: 7.4289999999999985\naccessibility: -2.367738552677705\nreward: 0.8907032608985901\ncompatibility: 7.7655\naccessibility: -2.609748649022197\nreward: 1.1653672456741333\ncompatibility: 7.429999999999999\naccessibility: -2.0180204816731213\nreward: 1.1322801113128662\ncompatibility: 7.429500000000001\naccessibility: -2.067383384291455\nreward: 0.8261643052101135\ncompatibility: 7.485000000000001\naccessibility: -2.556289222183143\nreward: 1.0657278299331665\ncompatibility: 7.411500000000002\naccessibility: -2.157569024506215\nreward: 1.107681393623352\ncompatibility: 7.497000000000002\naccessibility: -2.1404421659265873\nreward: 1.0819215774536133\ncompatibility: 7.289999999999999\naccessibility: -2.0681891421877356\nreward: 0.8863273859024048\ncompatibility: 7.439000000000001\naccessibility: -2.4414017924319635\nreward: 0.8778015375137329\ncompatibility: 7.624500000000004\naccessibility: -2.553565518595503\nreward: 0.98314368724823\ncompatibility: 7.528999999999999\naccessibility: -2.344391599732126\nreward: 0.9390403628349304\ncompatibility: 7.565500000000001\naccessibility: -2.430100210575839\nreward: 1.038546085357666\ncompatibility: 7.5785\naccessibility: -2.287805793332501\nreward: 0.9498009085655212\ncompatibility: 7.606999999999998\naccessibility: -2.4361914916743967\nreward: 0.7074721455574036\ncompatibility: 7.513000000000003\naccessibility: -2.749327484556378\nreward: 0.9003051519393921\ncompatibility: 7.512000000000001\naccessibility: -2.4595422603779786\nreward: 1.1746289730072021\ncompatibility: 7.617000000000003\naccessibility: -2.1043064533616618\nreward: 0.9845064878463745\ncompatibility: 7.570000000000002\naccessibility: -2.3643116995857105\nreward: 1.119339108467102\ncompatibility: 7.530499999999998\naccessibility: -2.1409021243346693\nreward: 0.9619198441505432\ncompatibility: 7.569\naccessibility: -2.3976559191504645\nreward: 1.0395201444625854\ncompatibility: 7.6960000000000015\naccessibility: -2.349291214240656\nreward: 0.8599610328674316\ncompatibility: 7.489999999999998\naccessibility: -2.508272744166514\nreward: 1.1589750051498413\ncompatibility: 7.591000000000005\naccessibility: -2.1138588361605004\nreward: 1.1320841312408447\ncompatibility: 7.719000000000002\naccessibility: -2.2227667386757517\nreward: 0.8908628821372986\ncompatibility: 7.610500000000001\naccessibility: -2.526473577072469\nreward: 0.9646257758140564\ncompatibility: 7.552000000000001\naccessibility: -2.3844899473784698\nreward: 0.9982901215553284\ncompatibility: 7.650999999999999\naccessibility: -2.3870291395139724\nreward: 1.0270044803619385\ncompatibility: 7.505999999999999\naccessibility: -2.266279021081219\nreward: 0.9093557000160217\ncompatibility: 7.4055\naccessibility: -2.3889129173344896\nreward: 0.6419011354446411\ncompatibility: 7.5045\naccessibility: -2.843130473870479\nreward: 0.9668718576431274\ncompatibility: 7.413000000000003\naccessibility: -2.3066565106833576\nreward: 1.148585319519043\ncompatibility: 7.551\naccessibility: -2.1080149341301144\nreward: 1.0143284797668457\ncompatibility: 7.599500000000003\naccessibility: -2.335382349791993\nreward: 1.0816632509231567\ncompatibility: 7.627000000000002\naccessibility: -2.249112318659896\nreward: 0.9285591840744019\ncompatibility: 7.472500000000001\naccessibility: -2.396000524014729\nreward: 0.8191338777542114\ncompatibility: 7.5539999999999985\naccessibility: -2.6037991860652636\nreward: 0.9229142069816589\ncompatibility: 7.319\naccessibility: -2.322235833154028\nreward: 0.8364323377609253\ncompatibility: 7.501500000000001\naccessibility: -2.54972652584519\nreward: 0.8287879824638367\ncompatibility: 7.2429999999999986\naccessibility: -2.4227108699030993\nreward: 0.9887585639953613\ncompatibility: 7.653000000000003\naccessibility: -2.4023978458128123\nreward: 0.9262505173683167\ncompatibility: 7.452000000000002\naccessibility: -2.3884813785771177\nreward: 0.7108572125434875\ncompatibility: 7.413499999999999\naccessibility: -2.6909463060985077\nreward: 0.7491058111190796\ncompatibility: 7.2405\naccessibility: -2.5408948204019275\nreward: 0.968900740146637\ncompatibility: 7.583000000000001\naccessibility: -2.3946845975942885\nreward: 0.8649169206619263\ncompatibility: 7.431999999999999\naccessibility: -2.469767495020216\nreward: 1.0346612930297852\ncompatibility: 7.555499999999999\naccessibility: -2.281311648276822\nreward: 1.0205367803573608\ncompatibility: 7.4549999999999965\naccessibility: -2.2486591362308266\nreward: 1.120255947113037\ncompatibility: 7.687000000000003\naccessibility: -2.223365995289969\nreward: 0.4614788293838501\ncompatibility: 7.502999999999998\naccessibility: -3.1129603114052284\nreward: 1.2669728994369507\ncompatibility: 7.7745000000000015\naccessibility: -2.0501656461661177\nreward: 0.6623703241348267\ncompatibility: 7.426000000000004\naccessibility: -2.770373095428825\nreward: 1.0200340747833252\ncompatibility: 7.437\naccessibility: -2.2397702970399527\nreward: 1.069697380065918\ncompatibility: 7.7555000000000005\naccessibility: -2.335900307426874\nreward: 0.8271984457969666\ncompatibility: 7.4514999999999985\naccessibility: -2.536791620225321\nreward: 0.9046999216079712\ncompatibility: 7.5435000000000025\naccessibility: -2.469825075682351\nreward: 0.8250186443328857\ncompatibility: 7.199999999999999\naccessibility: -2.4053292031810356\nreward: 1.0689449310302734\ncompatibility: 7.853500000000004\naccessibility: -2.3895289478150934\nreward: 0.9048876762390137\ncompatibility: 7.486999999999999\naccessibility: -2.4392755857059982\nreward: 0.8845292329788208\ncompatibility: 7.313500000000001\naccessibility: -2.376866838039658\nreward: 1.0049232244491577\ncompatibility: 7.418500000000002\naccessibility: -2.2525258467417135\nreward: 0.8259577751159668\ncompatibility: 7.396500000000003\naccessibility: -2.509188322893738\nreward: 1.1007505655288696\ncompatibility: 7.605999999999999\naccessibility: -2.2092313223361875\nreward: 0.955817461013794\ncompatibility: 7.488000000000003\naccessibility: -2.363416666617598\nreward: 1.0267561674118042\ncompatibility: 7.390999999999999\naccessibility: -2.2050443361026586\nreward: 1.0048305988311768\ncompatibility: 7.4775\naccessibility: -2.284271873374496\nreward: 1.1140787601470947\ncompatibility: 7.627499999999999\naccessibility: -2.2007568874931898\nreward: 1.0480351448059082\ncompatibility: 7.337999999999997\naccessibility: -2.144733026986057\nreward: 0.8855502009391785\ncompatibility: 7.1560000000000015\naccessibility: -2.2909603687217883\nreward: 0.7121333479881287\ncompatibility: 7.385500000000002\naccessibility: -2.674032093231137\nreward: 1.2095842361450195\ncompatibility: 7.488\naccessibility: -1.9827665496372968\nreward: 0.8961699604988098\ncompatibility: 7.360499999999999\naccessibility: -2.384584343068748\nreward: 0.9347086548805237\ncompatibility: 7.721499999999999\naccessibility: -2.5201691631075556\nreward: 1.014350175857544\ncompatibility: 7.526500000000003\naccessibility: -2.2962425495013408\nreward: 1.0472755432128906\ncompatibility: 7.559\naccessibility: -2.264265307825768\nreward: 1.0282609462738037\ncompatibility: 7.492000000000001\naccessibility: -2.256894264873336\nreward: 1.1188687086105347\ncompatibility: 7.463000000000001\naccessibility: -2.105446997296303\nreward: 0.7260224223136902\ncompatibility: 7.395000000000003\naccessibility: -2.658287783002032\nreward: 0.8648145198822021\ncompatibility: 7.454\naccessibility: -2.4817068227057297\nreward: 1.0575408935546875\ncompatibility: 7.4655000000000005\naccessibility: -2.198777932904552\nreward: 0.9210762977600098\ncompatibility: 7.382\naccessibility: -2.3587426975963472\nreward: 1.0016127824783325\ncompatibility: 7.589999999999999\naccessibility: -2.349366493759873\nreward: 0.8473747968673706\ncompatibility: 7.350999999999996\naccessibility: -2.452687810125891\nreward: 0.9591662883758545\ncompatibility: 7.4445\naccessibility: -2.3350898089211483\nreward: 0.8705422878265381\ncompatibility: 7.4945\naccessibility: -2.4948115405316296\nreward: 0.9395514130592346\ncompatibility: 7.426500000000002\naccessibility: -2.3548693068512705\nreward: 0.9157249927520752\ncompatibility: 7.581500000000001\naccessibility: -2.473644675475753\nreward: 1.0181435346603394\ncompatibility: 7.515999999999999\naccessibility: -2.2849276300504395\nreward: 0.6812748312950134\ncompatibility: 7.203000000000002\naccessibility: -2.6225520823529944\nreward: 1.0481455326080322\ncompatibility: 7.4620000000000015\naccessibility: -2.2109959123028426\nreward: 0.9368632435798645\ncompatibility: 7.5299999999999985\naccessibility: -2.4143479866565145\nreward: 1.1400506496429443\ncompatibility: 7.5585\naccessibility: -2.1248348148885583\nreward: 0.9772819876670837\ncompatibility: 7.463499999999997\naccessibility: -2.3180948819075193\nreward: 0.8741462230682373\ncompatibility: 7.461499999999997\naccessibility: -2.471727120524136\nreward: 0.7894107699394226\ncompatibility: 7.331500000000001\naccessibility: -2.529187446823407\nreward: 1.0543369054794312\ncompatibility: 7.494499999999999\naccessibility: -2.2191196406286413\nreward: 0.8867138624191284\ncompatibility: 7.395000000000001\naccessibility: -2.4172506521101558\nreward: 0.7078286409378052\ncompatibility: 7.25\naccessibility: -2.6078999023086946\nreward: 0.7142175436019897\ncompatibility: 7.519500000000002\naccessibility: -2.7426915537739034\nreward: 0.9627459645271301\ncompatibility: 7.420000000000001\naccessibility: -2.3165953158386983\nreward: 0.9113875031471252\ncompatibility: 7.5775000000000015\naccessibility: -2.478008034852816\nreward: 1.0610731840133667\ncompatibility: 7.371500000000002\naccessibility: -2.1431222840611706\nreward: 1.0750503540039062\ncompatibility: 7.616000000000001\naccessibility: -2.253138666439911\nreward: 0.9050766229629517\ncompatibility: 7.371999999999999\naccessibility: -2.3773851088551017\nreward: 0.9608662128448486\ncompatibility: 7.548999999999999\naccessibility: -2.3885221119842774\nreward: 1.073889970779419\ncompatibility: 7.560000000000001\naccessibility: -2.2248793803194404\nreward: 1.0089561939239502\ncompatibility: 7.45\naccessibility: -2.263351410709362\nreward: 0.7529153227806091\ncompatibility: 7.355\naccessibility: -2.5965198419906126\nreward: 0.77068692445755\ncompatibility: 7.527\naccessibility: -2.6620053339480245\nreward: 0.9018740653991699\ncompatibility: 7.471000000000003\naccessibility: -2.4352246397983675\nreward: 0.7804529070854187\ncompatibility: 7.4384999999999994\naccessibility: -2.599945657853537\nreward: 0.9448547959327698\ncompatibility: 7.560500000000001\naccessibility: -2.4186999636580886\nreward: 1.1720013618469238\ncompatibility: 7.804000000000001\naccessibility: -2.2084264730839007\nreward: 0.9290601015090942\ncompatibility: 7.6095000000000015\naccessibility: -2.4686419566147295\nreward: 0.8136848211288452\ncompatibility: 7.431000000000001\naccessibility: -2.5460799175725723\nreward: 1.1389862298965454\ncompatibility: 7.662499999999999\naccessibility: -2.1821457239637807\nreward: 1.101900339126587\ncompatibility: 7.7925\naccessibility: -2.307417399473615\nreward: 0.9795511364936829\ncompatibility: 7.4994999999999985\naccessibility: -2.333976889582905\nreward: 1.2198926210403442\ncompatibility: 7.576000000000002\naccessibility: -2.0144467564223683\nreward: 1.0293949842453003\ncompatibility: 7.2520000000000024\naccessibility: -2.1266217877185642\nreward: 0.9553117156028748\ncompatibility: 7.2515\naccessibility: -2.2374788777926615\nreward: 1.082143783569336\ncompatibility: 7.622000000000001\naccessibility: -2.245712965035356\nreward: 1.0335849523544312\ncompatibility: 7.582000000000002\naccessibility: -2.2971224829801224\nreward: 0.8861823081970215\ncompatibility: 7.639500000000001\naccessibility: -2.5490301145244936\nreward: 0.9872170686721802\ncompatibility: 7.694\naccessibility: -2.426674400662499\nreward: 0.971757173538208\ncompatibility: 7.4095\naccessibility: -2.29745350657679\nreward: 1.0821728706359863\ncompatibility: 7.517499999999999\naccessibility: -2.1896870507951314\nreward: 1.0088564157485962\ncompatibility: 7.694000000000002\naccessibility: -2.3942153281563465\nreward: 0.8811406493186951\ncompatibility: 7.470999999999999\naccessibility: -2.466324769231984\nreward: 0.90668123960495\ncompatibility: 7.554000000000001\naccessibility: -2.4724781575817087\nreward: 1.003957986831665\ncompatibility: 7.380500000000001\naccessibility: -2.2336165837630357\nreward: 1.2126197814941406\ncompatibility: 7.584999999999997\naccessibility: -2.0301774948580187\nreward: 1.1638864278793335\ncompatibility: 7.428500000000002\naccessibility: -2.01943819327706\nreward: 0.9628766775131226\ncompatibility: 7.2730000000000015\naccessibility: -2.2376492622805726\nreward: 1.1152241230010986\ncompatibility: 7.641500000000002\naccessibility: -2.206538738931467\nreward: 0.8846603035926819\ncompatibility: 7.753000000000003\naccessibility: -2.6121166646066385\nreward: 0.8764477968215942\ncompatibility: 7.4205\naccessibility: -2.4463104324329215\nreward: 0.8815621137619019\ncompatibility: 7.540000000000004\naccessibility: -2.5026568081115816\nreward: 0.9687798023223877\ncompatibility: 7.679000000000001\naccessibility: -2.4462946112565085\nreward: 0.9310783743858337\ncompatibility: 7.473500000000001\naccessibility: -2.392757482184758\nreward: 0.9994990229606628\ncompatibility: 7.6945000000000014\naccessibility: -2.4085192865620324\nreward: 1.2921345233917236\ncompatibility: 7.5195000000000025\naccessibility: -1.875816066701494\nreward: 0.9063601493835449\ncompatibility: 7.405000000000002\naccessibility: -2.39313834645421\nreward: 0.8394305109977722\ncompatibility: 7.371000000000002\naccessibility: -2.4753184984919017\nreward: 0.9162399172782898\ncompatibility: 7.456\naccessibility: -2.405640160841237\nreward: 0.8615190982818604\ncompatibility: 7.484500000000004\naccessibility: -2.502989244530783\nreward: 1.0388833284378052\ncompatibility: 7.571000000000001\naccessibility: -2.283282099830407\nreward: 0.9252607226371765\ncompatibility: 7.457999999999998\naccessibility: -2.3931803316628537\nreward: 0.9780641794204712\ncompatibility: 7.444500000000001\naccessibility: -2.306743046078212\nreward: 1.1969026327133179\ncompatibility: 7.5085000000000015\naccessibility: -2.012771138573051\nreward: 1.0945762395858765\ncompatibility: 7.701500000000002\naccessibility: -2.269653575865461\nreward: 1.2365792989730835\ncompatibility: 7.584999999999999\naccessibility: -1.9942382738807403\nreward: 0.8081514239311218\ncompatibility: 7.1755\naccessibility: -2.417504980157586\nreward: 0.9934021234512329\ncompatibility: 7.6995\naccessibility: -2.420343235786638\nreward: 1.2375521659851074\ncompatibility: 7.655999999999999\naccessibility: -2.030814684963454\nreward: 1.0964670181274414\ncompatibility: 7.604000000000002\naccessibility: -2.2145852533061965\nreward: 0.7870914340019226\ncompatibility: 7.428000000000001\naccessibility: -2.584362846150471\nreward: 0.8003495931625366\ncompatibility: 7.391500000000001\naccessibility: -2.5449219988952425\nreward: 0.996150016784668\ncompatibility: 7.5465\naccessibility: -2.3342570753076517\nreward: 0.8850075006484985\ncompatibility: 7.486000000000001\naccessibility: -2.4685601762840386\nreward: 0.975987434387207\ncompatibility: 7.4030000000000005\naccessibility: -2.2876259584342824\nreward: 1.0397846698760986\ncompatibility: 7.4670000000000005\naccessibility: -2.2262159193810205\nreward: 0.7141242623329163\ncompatibility: 7.510499999999999\naccessibility: -2.7380100445229782\nreward: 1.0381464958190918\ncompatibility: 7.639500000000002\naccessibility: -2.3210837894539766\nreward: 0.8094689846038818\ncompatibility: 7.3405000000000005\naccessibility: -2.5039215556924606\nreward: 0.6932176947593689\ncompatibility: 7.5825\naccessibility: -2.8079413504715753\nreward: 1.1254942417144775\ncompatibility: 7.620999999999999\naccessibility: -2.1801514905370736\nreward: 1.1001988649368286\ncompatibility: 7.367999999999999\naccessibility: -2.082558772620245\nreward: 1.0682764053344727\ncompatibility: 7.410500000000001\naccessibility: -2.1532104060830233\nreward: 0.74993497133255\ncompatibility: 7.380500000000001\naccessibility: -2.6146510902794122\nreward: 1.0372034311294556\ncompatibility: 7.443999999999997\naccessibility: -2.2177663543405397\nreward: 0.9099257588386536\ncompatibility: 7.240500000000002\naccessibility: -2.299664930773448\nreward: 1.143552541732788\ncompatibility: 7.5960000000000045\naccessibility: -2.13967119787791\nreward: 0.6982269883155823\ncompatibility: 7.422\naccessibility: -2.7144452492853364\nreward: 0.7700085639953613\ncompatibility: 7.477000000000003\naccessibility: -2.6362371176522217\nreward: 1.0129451751708984\ncompatibility: 7.733499999999999\naccessibility: -2.409243035701169\nreward: 0.9739862084388733\ncompatibility: 7.7215000000000025\naccessibility: -2.461252870481008\nreward: 0.9186334609985352\ncompatibility: 7.3475\naccessibility: -2.3439247854388428\nreward: 0.9014002680778503\ncompatibility: 7.501500000000001\naccessibility: -2.4522745903211973\nreward: 0.805893063545227\ncompatibility: 7.508499999999999\naccessibility: -2.5992854267785166\nreward: 0.9926028251647949\ncompatibility: 7.623000000000001\naccessibility: -2.380560091406652\nreward: 1.085156798362732\ncompatibility: 7.6564999999999985\naccessibility: -2.259675597470772\nreward: 0.86590975522995\ncompatibility: 7.7354999999999965\naccessibility: -2.630867504626373\nreward: 1.0381609201431274\ncompatibility: 7.176500000000002\naccessibility: -2.0730265285418144\nreward: 0.9087069630622864\ncompatibility: 7.416999999999999\naccessibility: -2.3960467344903598\nreward: 1.1215654611587524\ncompatibility: 7.685500000000002\naccessibility: -2.2205982012151675\nreward: 0.6623306274414062\ncompatibility: 7.637999999999998\naccessibility: -2.8840040428381992\nreward: 1.0305880308151245\ncompatibility: 7.5755\naccessibility: -2.2981358227588617\nreward: 0.9673778414726257\ncompatibility: 7.457500000000001\naccessibility: -2.32973680505337\nreward: 0.949603796005249\ncompatibility: 7.383999999999999\naccessibility: -2.3170228758062335\nreward: 1.035990834236145\ncompatibility: 7.637000000000001\naccessibility: -2.3229780192787275\nreward: 1.0318593978881836\ncompatibility: 7.334499999999999\naccessibility: -2.167121578719978\nreward: 0.9033676385879517\ncompatibility: 7.5870000000000015\naccessibility: -2.495127127464499\nreward: 1.0035101175308228\ncompatibility: 7.480000000000002\naccessibility: -2.2875919275045944\nreward: 0.9538744688034058\ncompatibility: 7.483500000000004\naccessibility: -2.363920420317509\nreward: 0.9987494349479675\ncompatibility: 7.798000000000001\naccessibility: -2.465090171208658\nreward: 0.8644447922706604\ncompatibility: 7.4895000000000005\naccessibility: -2.5012792799821537\nreward: 0.8489841222763062\ncompatibility: 7.428999999999999\naccessibility: -2.49205955728077\nreward: 0.9992173314094543\ncompatibility: 7.530999999999998\naccessibility: -2.3213525533035027\nreward: 0.9545560479164124\ncompatibility: 7.5465\naccessibility: -2.396648055130175\nreward: 0.9254050850868225\ncompatibility: 7.5840000000000005\naccessibility: -2.4604637809372836\nreward: 1.1688175201416016\ncompatibility: 7.551500000000003\naccessibility: -2.077934353628472\nreward: 1.0011379718780518\ncompatibility: 7.626999999999999\naccessibility: -2.369900110033633\nreward: 1.3591039180755615\ncompatibility: 7.4885\naccessibility: -1.7587549134838216\nreward: 1.0411403179168701\ncompatibility: 7.372499999999999\naccessibility: -2.17355729868287\nreward: 0.9479445219039917\ncompatibility: 7.528500000000003\naccessibility: -2.396922496942689\nreward: 0.8285374045372009\ncompatibility: 7.6215\naccessibility: -2.6258546382730885\nreward: 1.0923244953155518\ncompatibility: 7.615000000000004\naccessibility: -2.2266917983932366\nreward: 1.2667495012283325\ncompatibility: 7.735499999999999\naccessibility: -2.0296078411992644\nreward: 0.592832088470459\ncompatibility: 7.329\naccessibility: -2.8227161354382377\nreward: 1.1325922012329102\ncompatibility: 7.716000000000004\naccessibility: -2.2203973783117794\nreward: 0.9994896650314331\ncompatibility: 7.443000000000004\naccessibility: -2.2738012122267\nreward: 0.9558396935462952\ncompatibility: 7.5394999999999985\naccessibility: -2.390972602309436\nreward: 0.8095654249191284\ncompatibility: 7.439999999999998\naccessibility: -2.5570804716979083\nreward: 1.1545056104660034\ncompatibility: 7.759500000000002\naccessibility: -2.2108308783738284\nreward: 0.7738745212554932\ncompatibility: 7.435500000000002\naccessibility: -2.6082061158256438\nreward: 0.9022121429443359\ncompatibility: 7.3629999999999995\naccessibility: -2.376860358124018\nreward: 0.9620260000228882\ncompatibility: 7.566000000000002\naccessibility: -2.3958895520305306\nreward: 0.8347582221031189\ncompatibility: 7.2935\naccessibility: -2.440809102375919\nreward: 1.0057199001312256\ncompatibility: 7.343000000000002\naccessibility: -2.2108844973036224\nreward: 1.017884373664856\ncompatibility: 7.6285000000000025\naccessibility: -2.3455842080323905\nreward: 0.8489678502082825\ncompatibility: 7.3900000000000015\naccessibility: -2.4711910632422693\nreward: 1.1335946321487427\ncompatibility: 7.511500000000002\naccessibility: -2.109340263370606\nreward: 0.6528273820877075\ncompatibility: 7.532499999999999\naccessibility: -2.841741029528139\nreward: 0.9732298851013184\ncompatibility: 7.310499999999999\naccessibility: -2.2422087632656034\nreward: 0.9680376052856445\ncompatibility: 7.656499999999999\naccessibility: -2.435354328231757\nreward: 1.0574913024902344\ncompatibility: 7.648000000000001\naccessibility: -2.296620129943004\nreward: 0.8722416758537292\ncompatibility: 7.355500000000002\naccessibility: -2.4177982262516764\nreward: 0.8857062458992004\ncompatibility: 7.546000000000001\naccessibility: -2.4996548802319762\nreward: 1.1812525987625122\ncompatibility: 7.667500000000001\naccessibility: -2.121424623432212\nreward: 1.0674246549606323\ncompatibility: 7.488499999999998\naccessibility: -2.1962736521089785\nreward: 0.907446026802063\ncompatibility: 7.615\naccessibility: -2.5040095550377606\nreward: 0.9778475761413574\ncompatibility: 7.4975\naccessibility: -2.3354607722739322\nreward: 0.933136522769928\ncompatibility: 7.667500000000001\naccessibility: -2.493598817230329\nreward: 0.9084235429763794\ncompatibility: 7.4300000000000015\naccessibility: -2.4034361384850778\nreward: 0.6867416501045227\ncompatibility: 7.3505\naccessibility: -2.6933696722424276\nreward: 0.7946598529815674\ncompatibility: 7.575999999999998\naccessibility: -2.6522959204119596\nreward: 1.1147830486297607\ncompatibility: 7.568999999999999\naccessibility: -2.168361190058559\nreward: 0.9171068072319031\ncompatibility: 7.464499999999999\naccessibility: -2.4088933261430165\nreward: 0.8906434178352356\ncompatibility: 7.493500000000001\naccessibility: -2.4641241239849103\nreward: 0.8220140933990479\ncompatibility: 7.299000000000001\naccessibility: -2.4628717490052674\nreward: 0.9532495737075806\ncompatibility: 7.6485\naccessibility: -2.453250633755231\nreward: 0.8268469572067261\ncompatibility: 7.537000000000002\naccessibility: -2.5831223948887176\nreward: 1.0736982822418213\ncompatibility: 7.557999999999998\naccessibility: -2.224095446271275\nreward: 1.0188194513320923\ncompatibility: 7.2764999999999995\naccessibility: -2.15561016871538\nreward: 0.9069530963897705\ncompatibility: 7.223499999999998\naccessibility: -2.2950168033645815\nreward: 0.8346405029296875\ncompatibility: 7.488499999999999\naccessibility: -2.5454499606459002\nreward: 1.065446376800537\ncompatibility: 7.572000000000004\naccessibility: -2.243973306360019\nreward: 1.1873849630355835\ncompatibility: 7.546999999999998\naccessibility: -2.0476726142699\nreward: 0.8896738290786743\ncompatibility: 7.418499999999999\naccessibility: -2.4253999640436437\nreward: 0.6935716867446899\ncompatibility: 7.335499999999997\naccessibility: -2.675088902405799\nreward: 1.2050573825836182\ncompatibility: 7.605000000000003\naccessibility: -2.052235279716926\nreward: 1.0421092510223389\ncompatibility: 7.701499999999999\naccessibility: -2.34835406301131\nreward: 0.921972930431366\ncompatibility: 7.583499999999999\naccessibility: -2.4653441941560885\nreward: 0.854989230632782\ncompatibility: 7.415999999999998\naccessibility: -2.476087550997246\nreward: 0.8479793667793274\ncompatibility: 7.6305\naccessibility: -2.601513116114556\nreward: 1.0112950801849365\ncompatibility: 7.569500000000004\naccessibility: -2.3238609201745266\nreward: 0.9036985635757446\ncompatibility: 7.615500000000001\naccessibility: -2.5098985583880364\nreward: 0.7952033877372742\ncompatibility: 7.392500000000001\naccessibility: -2.553177084178479\nreward: 1.1381452083587646\ncompatibility: 7.738999999999999\naccessibility: -2.2243894127238653\nreward: 1.1561331748962402\ncompatibility: 7.602999999999998\naccessibility: -2.124550234869317\nreward: 0.7865715622901917\ncompatibility: 7.4285000000000005\naccessibility: -2.585410476322472\nreward: 0.8806268572807312\ncompatibility: 7.5950000000000015\naccessibility: -2.533524039728314\nreward: 1.0600470304489136\ncompatibility: 7.634000000000002\naccessibility: -2.2852865754979415\nreward: 0.8296374678611755\ncompatibility: 7.240500000000001\naccessibility: -2.4200973769110004\nreward: 1.1060336828231812\ncompatibility: 7.723000000000001\naccessibility: -2.2639851147125127\nreward: 0.9747800230979919\ncompatibility: 7.5710000000000015\naccessibility: -2.379437100285651\nreward: 1.193292260169983\ncompatibility: 7.5080000000000044\naccessibility: -2.017918688011377\nreward: 1.024715781211853\ncompatibility: 7.437000000000001\naccessibility: -2.232747770493956\nreward: 0.853794515132904\ncompatibility: 7.370500000000002\naccessibility: -2.453504634964397\nreward: 0.8684505820274353\ncompatibility: 7.593500000000002\naccessibility: -2.550984842624924\nreward: 0.9677334427833557\ncompatibility: 7.477999999999998\naccessibility: -2.340185572807423\nreward: 1.1466000080108643\ncompatibility: 7.7945\naccessibility: -2.241439260614829\nreward: 1.0891801118850708\ncompatibility: 7.4095\naccessibility: -2.121319130232743\nreward: 1.035575032234192\ncompatibility: 7.546000000000003\naccessibility: -2.274851776296737\nreward: 1.1766607761383057\ncompatibility: 7.455499999999998\naccessibility: -2.0147409354856647\nreward: 1.0925524234771729\ncompatibility: 7.4045\naccessibility: -2.1135820448966856\nreward: 1.0982571840286255\ncompatibility: 7.647000000000003\naccessibility: -2.234935613863525\nreward: 0.6956577301025391\ncompatibility: 7.661500000000002\naccessibility: -2.846602663247735\nreward: 0.7149056196212769\ncompatibility: 7.415999999999998\naccessibility: -2.686212955313164\nreward: 0.8552353382110596\ncompatibility: 7.5045\naccessibility: -2.5231291263577966\nreward: 0.8002793192863464\ncompatibility: 7.481\naccessibility: -2.592973916250325\nreward: 0.9712858200073242\ncompatibility: 7.5525\naccessibility: -2.3747677142965884\nreward: 0.9094082713127136\ncompatibility: 7.6745\naccessibility: -2.5329411423019486\nreward: 0.8731138706207275\ncompatibility: 7.692000000000002\naccessibility: -2.5967577561580693\nreward: 0.9893863201141357\ncompatibility: 7.638999999999999\naccessibility: -2.3939561898355883\nreward: 1.01663339138031\ncompatibility: 7.685000000000001\naccessibility: -2.3777284446738083\nreward: 1.135743498802185\ncompatibility: 7.599499999999999\naccessibility: -2.153259782235434\nreward: 0.6914174556732178\ncompatibility: 7.4075000000000015\naccessibility: -2.7168917144206115\nreward: 1.2081025838851929\ncompatibility: 7.565\naccessibility: -2.026238917069904\nreward: 1.0444204807281494\ncompatibility: 7.770500000000001\naccessibility: -2.3818513386220577\nreward: 0.6991419792175293\ncompatibility: 7.442500000000001\naccessibility: -2.724054857235003\nreward: 1.0175310373306274\ncompatibility: 7.454500000000001\naccessibility: -2.25289982885896\nreward: 0.9288031458854675\ncompatibility: 7.557\naccessibility: -2.4409024676284603\nreward: 1.2601782083511353\ncompatibility: 7.648500000000001\naccessibility: -1.992857742685359\nreward: 1.1098368167877197\ncompatibility: 7.535500000000001\naccessibility: -2.1578340572292647\nreward: 0.9418221116065979\ncompatibility: 7.2780000000000005\naccessibility: -2.2719097160087096\nreward: 1.032533049583435\ncompatibility: 7.3355000000000015\naccessibility: -2.1666468435686346\nreward: 0.8779742121696472\ncompatibility: 7.386500000000002\naccessibility: -2.425806518348956\nreward: 0.954345703125\ncompatibility: 7.624499999999997\naccessibility: -2.4387492653985277\nreward: 1.0726841688156128\ncompatibility: 7.472999999999997\naccessibility: -2.1800808753376253\nreward: 0.8860915899276733\ncompatibility: 7.296499999999998\naccessibility: -2.3654161982360327\nreward: 0.9666270017623901\ncompatibility: 7.553000000000001\naccessibility: -2.382023741234041\nreward: 0.7577646970748901\ncompatibility: 7.627000000000001\naccessibility: -2.7349600667291547\nreward: 1.064778447151184\ncompatibility: 7.654\naccessibility: -2.2889036974381294\nreward: 0.9750986099243164\ncompatibility: 7.601499999999999\naccessibility: -2.395298542977544\nreward: 0.865000307559967\ncompatibility: 7.594499999999998\naccessibility: -2.5566959414109998\nreward: 0.7690144777297974\ncompatibility: 7.545000000000003\naccessibility: -2.6741568740274553\nreward: 0.8664090037345886\ncompatibility: 7.140999999999998\naccessibility: -2.311636458382265\nreward: 0.9994926452636719\ncompatibility: 7.464000000000001\naccessibility: -2.2850467086372093\nreward: 1.0310980081558228\ncompatibility: 7.567499999999999\naccessibility: -2.293085139357294\nreward: 0.9141600131988525\ncompatibility: 7.420000000000003\naccessibility: -2.389474261255706\nreward: 1.0978302955627441\ncompatibility: 7.740000000000002\naccessibility: -2.2853974966246646\nreward: 0.824474573135376\ncompatibility: 7.362500000000003\naccessibility: -2.4931988757327574\nreward: 0.9952970147132874\ncompatibility: 7.582999999999999\naccessibility: -2.3550901824916854\nreward: 1.0553934574127197\ncompatibility: 7.756500000000001\naccessibility: -2.357892042512305\nreward: 1.1053589582443237\ncompatibility: 7.621500000000002\naccessibility: -2.2106222695964877\nreward: 0.9430068731307983\ncompatibility: 7.624\naccessibility: -2.455489695646042\nreward: 0.9957383275032043\ncompatibility: 7.603500000000002\naccessibility: -2.3654103217502676\nreward: 1.1117092370986938\ncompatibility: 7.578500000000002\naccessibility: -2.1780611396315352\nreward: 0.9301705956459045\ncompatibility: 7.6135\naccessibility: -2.4691190673368415\nreward: 1.0994013547897339\ncompatibility: 7.5815\naccessibility: -2.1981300795241414\nreward: 0.890070378780365\ncompatibility: 7.518500000000001\naccessibility: -2.4783765955155714\nreward: 0.8995391726493835\ncompatibility: 7.426999999999998\naccessibility: -2.415155509433093\nreward: 1.0831118822097778\ncompatibility: 7.556\naccessibility: -2.2089035571886493\nreward: 1.0900154113769531\ncompatibility: 7.599500000000002\naccessibility: -2.221851965489657\nreward: 0.8165815472602844\ncompatibility: 7.278\naccessibility: -2.4597705597712323\nreward: 0.9640740156173706\ncompatibility: 7.387\naccessibility: -2.2969247049285118\nreward: 0.8827441930770874\ncompatibility: 7.4110000000000005\naccessibility: -2.431776541876282\nreward: 0.9230077266693115\ncompatibility: 7.464499999999998\naccessibility: -2.4000419780052664\nreward: 0.8076258897781372\ncompatibility: 7.651\naccessibility: -2.673025412573767\nreward: 0.7648690342903137\ncompatibility: 7.389999999999999\naccessibility: -2.5973393121106283\nreward: 0.892309844493866\ncompatibility: 7.5555\naccessibility: -2.494838826099297\nreward: 0.9516412019729614\ncompatibility: 7.282499999999999\naccessibility: -2.259591790444469\nreward: 1.0895016193389893\ncompatibility: 7.617999999999999\naccessibility: -2.2325332663280224\nreward: 0.7993631958961487\ncompatibility: 7.697500000000002\naccessibility: -2.710330239350348\nreward: 0.8452891707420349\ncompatibility: 7.313500000000001\naccessibility: -2.4357269723442614\nreward: 0.6766307353973389\ncompatibility: 7.442000000000002\naccessibility: -2.7575538583130346\nreward: 0.9804001450538635\ncompatibility: 7.422999999999998\naccessibility: -2.291721237173541\nreward: 1.166388988494873\ncompatibility: 7.5889999999999995\naccessibility: -2.101666554501917\nreward: 0.7962363362312317\ncompatibility: 7.011000000000003\naccessibility: -2.347252668592437\nreward: 1.023618221282959\ncompatibility: 7.589000000000002\naccessibility: -2.3158226287604653\nreward: 0.7623026371002197\ncompatibility: 7.509499999999998\naccessibility: -2.6652067737323164\nreward: 1.0115567445755005\ncompatibility: 7.429500000000002\naccessibility: -2.248468499983907\nreward: 1.169843316078186\ncompatibility: 7.662500000000001\naccessibility: -2.135860073182979\nreward: 1.2613379955291748\ncompatibility: 7.701500000000002\naccessibility: -2.019510788781947\nreward: 0.9034557938575745\ncompatibility: 7.6555\naccessibility: -2.5316913082739245\nreward: 0.9074103236198425\ncompatibility: 7.357500000000001\naccessibility: -2.366116689727713\nreward: 0.8051706552505493\ncompatibility: 7.4495\naccessibility: -2.5687619177148093\nreward: 0.7866300344467163\ncompatibility: 7.4365000000000006\naccessibility: -2.5896085201005343\nreward: 0.8918314576148987\ncompatibility: 7.3365\naccessibility: -2.3782349704925587\nreward: 0.9798247814178467\ncompatibility: 7.514999999999999\naccessibility: -2.3418699701042467\nreward: 1.1737297773361206\ncompatibility: 7.338000000000001\naccessibility: -1.956191044382365\nreward: 0.8550191521644592\ncompatibility: 7.434500000000001\naccessibility: -2.4859534019700074\nreward: 0.9491062760353088\ncompatibility: 7.476999999999999\naccessibility: -2.3675905815438663\nreward: 0.9579952955245972\ncompatibility: 7.4945\naccessibility: -2.3636320841250513\nreward: 0.6926198601722717\ncompatibility: 7.254\naccessibility: -2.632855963722579\nreward: 0.9515268802642822\ncompatibility: 7.4559999999999995\naccessibility: -2.352709702888907\nreward: 0.9756313562393188\ncompatibility: 7.557\naccessibility: -2.3706601218881396\nreward: 0.937390148639679\ncompatibility: 7.4769999999999985\naccessibility: -2.385164773090338\nreward: 1.0993412733078003\ncompatibility: 7.6514999999999995\naccessibility: -2.235720234452955\nreward: 0.6752986311912537\ncompatibility: 7.3050000000000015\naccessibility: -2.6861591822897366\nreward: 0.9268651604652405\ncompatibility: 7.641999999999997\naccessibility: -2.4893451007874585\nreward: 0.9684863686561584\ncompatibility: 7.450999999999998\naccessibility: -2.324591855310772\nreward: 0.8373215794563293\ncompatibility: 7.703999999999999\naccessibility: -2.6568747427548263\nreward: 0.9175609946250916\ncompatibility: 7.365\naccessibility: -2.35490847396762\nreward: 0.8330849409103394\ncompatibility: 7.516500000000001\naccessibility: -2.562783259547294\nreward: 1.0130313634872437\ncompatibility: 7.647000000000002\naccessibility: -2.362774384735034\nreward: 1.0572664737701416\ncompatibility: 7.423499999999999\naccessibility: -2.1766895477242163\nreward: 1.1711397171020508\ncompatibility: 7.652999999999999\naccessibility: -2.1288260949371183\nreward: 0.7577165961265564\ncompatibility: 7.716499999999999\naccessibility: -2.7829786576654065\nreward: 0.9613443613052368\ncompatibility: 7.809000000000002\naccessibility: -2.527090634234292\nreward: 1.2189310789108276\ncompatibility: 7.939000000000001\naccessibility: -2.2103533244377744\nreward: 0.9424561262130737\ncompatibility: 7.588000000000002\naccessibility: -2.4370301045216216\nreward: 0.8169129490852356\ncompatibility: 7.668499999999998\naccessibility: -2.6684698204670148\nreward: 0.7636107206344604\ncompatibility: 7.3775\naccessibility: -2.592530304557002\nreward: 0.8335266709327698\ncompatibility: 7.213999999999999\naccessibility: -2.400067158710323\nreward: 0.8302565217018127\ncompatibility: 7.4845000000000015\naccessibility: -2.5498830836436888\nreward: 1.0956473350524902\ncompatibility: 7.540000000000001\naccessibility: -2.1815289180153488\nreward: 1.0405508279800415\ncompatibility: 7.618\naccessibility: -2.3059595501346575\nreward: 0.9292776584625244\ncompatibility: 7.355500000000001\naccessibility: -2.332244224667956\nreward: 0.951801598072052\ncompatibility: 7.492499999999999\naccessibility: -2.3718511558163526\nreward: 1.2282414436340332\ncompatibility: 7.5555\naccessibility: -1.9909413324677465\nreward: 0.982399046421051\ncompatibility: 7.4735000000000005\naccessibility: -2.3157764316488842\nreward: 0.9422463774681091\ncompatibility: 7.367499999999999\naccessibility: -2.3192197243233386\nreward: 1.1919246912002563\ncompatibility: 7.623499999999999\naccessibility: -2.0818451652714045\nreward: 0.8804326057434082\ncompatibility: 7.369\naccessibility: -2.41274395223457\nreward: 0.9527055025100708\ncompatibility: 7.681000000000001\naccessibility: -2.4714774676259723\nreward: 0.655322790145874\ncompatibility: 7.178000000000001\naccessibility: -2.6480872661559776\nreward: 1.118395209312439\ncompatibility: 7.401499999999999\naccessibility: -2.0732107187722293\nreward: 1.0036425590515137\ncompatibility: 7.545500000000001\naccessibility: -2.322482509783359\nreward: 0.7163949012756348\ncompatibility: 7.5455\naccessibility: -2.7533540826445724\nreward: 0.8298989534378052\ncompatibility: 7.418000000000003\naccessibility: -2.514794429207841\nreward: 0.8792311549186707\ncompatibility: 7.3455\naccessibility: -2.4019568670078804\nreward: 1.0672649145126343\ncompatibility: 7.458000000000002\naccessibility: -2.180174095860675\nreward: 0.9811747074127197\ncompatibility: 7.484000000000002\naccessibility: -2.3232379575379007\nreward: 1.2099854946136475\ncompatibility: 7.550499999999999\naccessibility: -2.0156467651842442\nreward: 0.89108806848526\ncompatibility: 7.463500000000003\naccessibility: -2.4473857287685683\nreward: 1.1143349409103394\ncompatibility: 7.461000000000002\naccessibility: -2.111176142207319\nreward: 0.7876487970352173\ncompatibility: 7.4110000000000005\naccessibility: -2.57441968584107\nreward: 1.008420467376709\ncompatibility: 7.6910000000000025\naccessibility: -2.3932622363587166\nreward: 0.8174566626548767\ncompatibility: 7.406499999999999\naccessibility: -2.527297173851406\nreward: 0.8821266293525696\ncompatibility: 7.410000000000002\naccessibility: -2.432167155385384\nreward: 0.7030532360076904\ncompatibility: 7.5815\naccessibility: -2.792652309416758\nreward: 0.7862243056297302\ncompatibility: 7.495\naccessibility: -2.621556373535029\nreward: 1.1133075952529907\ncompatibility: 7.6114999999999995\naccessibility: -2.1933422649200986\nreward: 0.9611436724662781\ncompatibility: 7.5485000000000015\naccessibility: -2.387838021528729\nreward: 0.9387137293815613\ncompatibility: 7.457499999999999\naccessibility: -2.3727329777625332\nreward: 1.2911370992660522\ncompatibility: 7.468000000000002\naccessibility: -1.8497229050144819\nreward: 1.0754127502441406\ncompatibility: 7.307000000000001\naccessibility: -2.0870595299826333\nreward: 1.186582326889038\ncompatibility: 7.513999999999999\naccessibility: -2.0311980177908153\nreward: 0.9363059997558594\ncompatibility: 7.378500000000002\naccessibility: -2.3340231809838805\nreward: 1.264003038406372\ncompatibility: 7.4635\naccessibility: -1.8880133281830425\nreward: 0.785752534866333\ncompatibility: 7.645000000000003\naccessibility: -2.7026212166900776\nreward: 0.9102051854133606\ncompatibility: 7.376500000000002\naccessibility: -2.3721029038439188\nreward: 0.9098090529441833\ncompatibility: 7.361000000000001\naccessibility: -2.3643935440629766\nreward: 0.9749721884727478\ncompatibility: 7.4275\naccessibility: -2.302273881295325\nreward: 1.2185308933258057\ncompatibility: 7.6145\naccessibility: -2.0371144340523464\nreward: 1.04456627368927\ncompatibility: 7.549500000000003\naccessibility: -2.2632399427440957\nreward: 0.8666282296180725\ncompatibility: 7.451000000000005\naccessibility: -2.4773791031636168\nreward: 1.0665396451950073\ncompatibility: 7.449500000000003\naccessibility: -2.1767084634389895\nreward: 0.8208861351013184\ncompatibility: 7.521500000000002\naccessibility: -2.5837601069976017\nreward: 0.9520905613899231\ncompatibility: 7.480000000000001\naccessibility: -2.3647212715332175\nreward: 1.080337405204773\ncompatibility: 7.455000000000001\naccessibility: -2.1589581340453687\nreward: 0.8439722657203674\ncompatibility: 7.401999999999998\naccessibility: -2.485113026599204\nreward: 0.9534729719161987\ncompatibility: 7.459500000000003\naccessibility: -2.351665527912645\nreward: 0.9728501439094543\ncompatibility: 7.644000000000003\naccessibility: -2.42143906424178\nreward: 0.955350935459137\ncompatibility: 7.494500000000001\naccessibility: -2.3675986349741764\nreward: 1.2822564840316772\ncompatibility: 7.7255\naccessibility: -2.0009902781623112\nreward: 0.8363469243049622\ncompatibility: 7.314\naccessibility: -2.4494081713334226\nreward: 0.9370135068893433\ncompatibility: 7.379000000000002\naccessibility: -2.333229776393543\nreward: 0.9552566409111023\ncompatibility: 7.475500000000002\naccessibility: -2.35756149749847\nreward: 0.7657643556594849\ncompatibility: 7.436000000000003\naccessibility: -2.6206392128964353\nreward: 0.932106077671051\ncompatibility: 7.327000000000001\naccessibility: -2.312733710153095\nreward: 0.7406113147735596\ncompatibility: 7.584\naccessibility: -2.737654450608626\nreward: 1.123119592666626\ncompatibility: 7.536500000000003\naccessibility: -2.1384456613590084\nreward: 0.8721611499786377\ncompatibility: 7.309500000000002\naccessibility: -2.3932761023007805\nreward: 0.84221351146698\ncompatibility: 7.5055\naccessibility: -2.54319760271646\nreward: 1.079375982284546\ncompatibility: 7.3759999999999994\naccessibility: -2.118078952268995\nreward: 1.0576653480529785\ncompatibility: 7.437500000000003\naccessibility: -2.1835912158206296\nreward: 0.8937050104141235\ncompatibility: 7.3489999999999975\naccessibility: -2.382121088720966\nreward: 1.045042872428894\ncompatibility: 7.4905\naccessibility: -2.230917897505597\nreward: 1.1136484146118164\ncompatibility: 7.414499999999997\naccessibility: -2.087295163545763\nreward: 0.9117045402526855\ncompatibility: 7.476000000000002\naccessibility: -2.4231575105934757\nreward: 1.060488224029541\ncompatibility: 7.5295\naccessibility: -2.2286426505927026\nreward: 0.8872261643409729\ncompatibility: 7.602\naccessibility: -2.5273750282466305\nreward: 0.9510619640350342\ncompatibility: 7.518000000000002\naccessibility: -2.3866213410013284\nreward: 0.8429471254348755\ncompatibility: 7.354499999999999\naccessibility: -2.461204324239695\nreward: 0.8679472804069519\ncompatibility: 7.504000000000002\naccessibility: -2.5037933520611553\nreward: 0.9758746027946472\ncompatibility: 7.307\naccessibility: -2.2363666389102086\nreward: 0.9893139600753784\ncompatibility: 7.636999999999997\naccessibility: -2.3929933512667194\nreward: 0.9998020529747009\ncompatibility: 7.678500000000003\naccessibility: -2.3994933618749625\nreward: 1.1304231882095337\ncompatibility: 7.617499999999999\naccessibility: -2.1708831513305475\nreward: 0.7731286883354187\ncompatibility: 7.531999999999999\naccessibility: -2.6610212282789254\nreward: 0.8791015148162842\ncompatibility: 7.458499999999999\naccessibility: -2.4626870151959315\nreward: 0.7315970659255981\ncompatibility: 7.445499999999999\naccessibility: -2.676979395561146\nreward: 1.1634446382522583\ncompatibility: 7.593500000000001\naccessibility: -2.1084938438229073\nreward: 1.033658504486084\ncompatibility: 7.585999999999999\naccessibility: -2.299155100837782\nreward: 1.0389220714569092\ncompatibility: 7.3185\naccessibility: -2.147956099860597\nreward: 0.6885375380516052\ncompatibility: 7.328999999999999\naccessibility: -2.679157966651268\nreward: 1.1693509817123413\ncompatibility: 7.604000000000001\naccessibility: -2.1052593106557747\nreward: 0.8322372436523438\ncompatibility: 7.530500000000002\naccessibility: -2.57155481007283\nreward: 0.8276873826980591\ncompatibility: 7.330500000000001\naccessibility: -2.471236825246848\nreward: 0.6427332758903503\ncompatibility: 7.4625\naccessibility: -2.819382254895494\nreward: 0.8279420733451843\ncompatibility: 7.434\naccessibility: -2.526301199103232\nreward: 1.1155935525894165\ncompatibility: 7.411000000000001\naccessibility: -2.082502576608287\nreward: 0.9558237791061401\ncompatibility: 7.541500000000001\naccessibility: -2.392067897033582\nreward: 1.076453447341919\ncompatibility: 7.477000000000001\naccessibility: -2.1765698582458235\nreward: 0.7388517260551453\ncompatibility: 7.4510000000000005\naccessibility: -2.669043868830974\nreward: 0.6546830534934998\ncompatibility: 7.3225\naccessibility: -2.7264575746599298\nreward: 1.1309614181518555\ncompatibility: 7.566500000000002\naccessibility: -2.14275436457762\nreward: 1.141421914100647\ncompatibility: 7.694000000000001\naccessibility: -2.195367169392897\nreward: 0.6982166767120361\ncompatibility: 7.359500000000001\naccessibility: -2.6809785604352045\nreward: 1.1414333581924438\ncompatibility: 7.6145000000000005\naccessibility: -2.1527606512814197\nreward: 1.1260234117507935\ncompatibility: 7.2995\naccessibility: -2.007125649313286\nreward: 0.8558304905891418\ncompatibility: 7.401499999999999\naccessibility: -2.4670578536684618\nreward: 0.871747612953186\ncompatibility: 7.6480000000000015\naccessibility: -2.5752357536850394\nreward: 0.7152565717697144\ncompatibility: 7.212000000000002\naccessibility: -2.5764008381454513\nreward: 1.062241554260254\ncompatibility: 7.472000000000001\naccessibility: -2.195209108908113\nreward: 0.9550143480300903\ncompatibility: 7.493499999999999\naccessibility: -2.3675677720485164\nreward: 0.9668816328048706\ncompatibility: 7.571500000000002\naccessibility: -2.3915525908476702\nreward: 0.9904689788818359\ncompatibility: 7.3625\naccessibility: -2.2442072764421246\nreward: 0.9017671942710876\ncompatibility: 7.517\naccessibility: -2.460027783251846\nreward: 1.181379795074463\ncompatibility: 7.567000000000002\naccessibility: -2.067394658747681\nreward: 1.1518049240112305\ncompatibility: 7.437500000000001\naccessibility: -2.0423819775461447\nreward: 0.8877593874931335\ncompatibility: 7.671499999999998\naccessibility: -2.5638073881118313\nreward: 0.8528715372085571\ncompatibility: 7.431500000000001\naccessibility: -2.487567707476727\nreward: 0.7050554752349854\ncompatibility: 7.497999999999999\naccessibility: -2.7449167541326442\nreward: 0.9978732466697693\ncompatibility: 7.510000000000002\naccessibility: -2.3121186813251082\nreward: 0.8300447463989258\ncompatibility: 7.652000000000003\naccessibility: -2.639932903923012\nreward: 0.6923267841339111\ncompatibility: 7.4479999999999995\naccessibility: -2.737224131447349\nreward: 1.022681713104248\ncompatibility: 7.609000000000002\naccessibility: -2.3279417903362507\nreward: 1.149489164352417\ncompatibility: 7.498500000000002\naccessibility: -2.0785341480175505\nreward: 1.1955803632736206\ncompatibility: 7.626999999999999\naccessibility: -2.078236597087362\nreward: 0.9309537410736084\ncompatibility: 7.466999999999998\naccessibility: -2.3894622013237767\nreward: 1.0125752687454224\ncompatibility: 7.487500000000001\naccessibility: -2.2780121011464285\nreward: 0.7398262023925781\ncompatibility: 7.3020000000000005\naccessibility: -2.5877607316400324\nreward: 0.8796685338020325\ncompatibility: 7.212000000000001\naccessibility: -2.329782954666918\nreward: 1.16072678565979\ncompatibility: 7.453000000000003\naccessibility: -2.037302705319135\nreward: 0.7018598318099976\ncompatibility: 7.3175\naccessibility: -2.6530138663561122\nreward: 1.1392828226089478\ncompatibility: 7.497500000000003\naccessibility: -2.093307956554433\nreward: 1.0791865587234497\ncompatibility: 7.8005\naccessibility: -2.3457737856101297\nreward: 1.2168265581130981\ncompatibility: 7.64\naccessibility: -2.05333158889079\nreward: 0.849241316318512\ncompatibility: 7.559000000000001\naccessibility: -2.561316602151932\nreward: 1.1395833492279053\ncompatibility: 7.550999999999999\naccessibility: -2.1215177854274287\nreward: 1.104668378829956\ncompatibility: 7.231999999999999\naccessibility: -2.0029974800805728\nreward: 1.0293850898742676\ncompatibility: 7.380999999999999\naccessibility: -2.1957438299756573\nreward: 0.9862251877784729\ncompatibility: 7.5005\naccessibility: -2.3245015097131128\nreward: 0.9558119177818298\ncompatibility: 7.599499999999998\naccessibility: -2.4231571187327305\nreward: 1.0589311122894287\ncompatibility: 7.540500000000001\naccessibility: -2.23687117796334\nreward: 1.1481202840805054\ncompatibility: 7.569000000000001\naccessibility: -2.1183553171190597\nreward: 0.8867220282554626\ncompatibility: 7.5485000000000015\naccessibility: -2.4994705258764083\nreward: 1.0083973407745361\ncompatibility: 7.4300000000000015\naccessibility: -2.253475464009779\nreward: 1.0380340814590454\ncompatibility: 7.473000000000001\naccessibility: -2.2320561057904578\nreward: 1.0171481370925903\ncompatibility: 7.3969999999999985\naccessibility: -2.2226707244262123\nreward: 0.9042506814002991\ncompatibility: 7.441500000000002\naccessibility: -2.415856150585091\nreward: 0.7852278351783752\ncompatibility: 7.268\naccessibility: -2.501443917008432\nreward: 0.9377782344818115\ncompatibility: 7.332999999999999\naccessibility: -2.3074398180511304\nreward: 0.8481410145759583\ncompatibility: 7.559\naccessibility: -2.5629670231617134\nreward: 0.9715930223464966\ncompatibility: 7.453500000000002\naccessibility: -2.32127117248253\nreward: 1.1857810020446777\ncompatibility: 7.416500000000002\naccessibility: -1.9801677304613112\nreward: 1.1608036756515503\ncompatibility: 7.599500000000002\naccessibility: -2.1156694173518167\nreward: 0.7323930263519287\ncompatibility: 7.5764999999999985\naccessibility: -2.7459639908684856\nreward: 1.1184085607528687\ncompatibility: 7.744499999999999\naccessibility: -2.256940699607915\nreward: 0.8884714245796204\ncompatibility: 7.596\naccessibility: -2.522292904672455\nreward: 0.9033762216567993\ncompatibility: 7.618499999999999\naccessibility: -2.51198919745236\nreward: 0.847499668598175\ncompatibility: 7.4545\naccessibility: -2.507946911402515\nreward: 1.0587366819381714\ncompatibility: 7.559999999999999\naccessibility: -2.24760928600249\nreward: 1.1335550546646118\ncompatibility: 7.3025\naccessibility: -1.997435262875563\nreward: 1.0719565153121948\ncompatibility: 7.571000000000002\naccessibility: -2.2336724584255303\nreward: 0.8509373664855957\ncompatibility: 7.355\naccessibility: -2.4494868200175683\nreward: 1.0879391431808472\ncompatibility: 7.580500000000002\naccessibility: -2.2147877981972286\nreward: 0.8448264002799988\ncompatibility: 7.478500000000001\naccessibility: -2.5248139413134454\nreward: 0.747216522693634\ncompatibility: 7.454000000000001\naccessibility: -2.6581038238035655\nreward: 0.8658620715141296\ncompatibility: 7.566500000000002\naccessibility: -2.5404033075945986\nreward: 0.8818137049674988\ncompatibility: 7.340999999999999\naccessibility: -2.395672273067878\nreward: 1.1401511430740356\ncompatibility: 7.595\naccessibility: -2.144237590430922\nreward: 1.0184354782104492\ncompatibility: 7.622499999999999\naccessibility: -2.3415431841292507\nreward: 1.0356520414352417\ncompatibility: 7.353000000000001\naccessibility: -2.1713434153909055\nreward: 1.2389566898345947\ncompatibility: 7.711\naccessibility: -2.0581720735328317\nreward: 0.9008403420448303\ncompatibility: 7.333499999999999\naccessibility: -2.363114509917325\nreward: 1.0247513055801392\ncompatibility: 7.6179999999999986\naccessibility: -2.329658787719831\nreward: 0.7098754644393921\ncompatibility: 7.396\naccessibility: -2.683043953392134\nreward: 0.903819739818573\ncompatibility: 7.5020000000000024\naccessibility: -2.4489132189840372\nreward: 0.951011061668396\ncompatibility: 7.499999999999999\naccessibility: -2.377054865927405\nreward: 0.9358164072036743\ncompatibility: 7.535000000000004\naccessibility: -2.4185968485965463\nreward: 1.226806879043579\ncompatibility: 7.5020000000000024\naccessibility: -1.964432522117883\nreward: 1.152343511581421\ncompatibility: 7.727000000000001\naccessibility: -2.1966632982926138\nreward: 0.9398939609527588\ncompatibility: 7.412500000000002\naccessibility: -2.346855476072231\nreward: 0.9042700529098511\ncompatibility: 7.289000000000001\naccessibility: -2.334130675799854\nreward: 0.9786033630371094\ncompatibility: 7.278500000000001\naccessibility: -2.217005643724151\nreward: 0.8911924958229065\ncompatibility: 7.429499999999998\naccessibility: -2.4290148290357934\nreward: 0.8303332328796387\ncompatibility: 7.402\naccessibility: -2.5055716130620205\nreward: 0.9618621468544006\ncompatibility: 7.365500000000003\naccessibility: -2.2887246777032515\nreward: 1.0471216440200806\ncompatibility: 7.549499999999999\naccessibility: -2.259406812978562\nreward: 0.9805610775947571\ncompatibility: 7.647500000000001\naccessibility: -2.411747638197072\nreward: 0.8999181985855103\ncompatibility: 7.6750000000000025\naccessibility: -2.5474441553660405\nreward: 1.0432543754577637\ncompatibility: 7.5775000000000015\naccessibility: -2.2802077629116266\nreward: 0.7906832098960876\ncompatibility: 7.330500000000002\naccessibility: -2.526743080082354\nreward: 1.0049530267715454\ncompatibility: 7.7310000000000025\naccessibility: -2.419891879592839\nreward: 0.9766934514045715\ncompatibility: 7.423500000000001\naccessibility: -2.297549150199531\nreward: 0.867344856262207\ncompatibility: 7.343999999999999\naccessibility: -2.418982741670526\nreward: 0.9977253675460815\ncompatibility: 7.472500000000004\naccessibility: -2.292251237247825\nreward: 1.0306535959243774\ncompatibility: 7.491500000000001\naccessibility: -2.2530375081845864\nreward: 0.9997455477714539\ncompatibility: 7.546000000000001\naccessibility: -2.328595934497269\nreward: 0.9984527230262756\ncompatibility: 7.4659999999999975\naccessibility: -2.287678028739676\nreward: 1.0155301094055176\ncompatibility: 7.617000000000001\naccessibility: -2.3429547834861095\nreward: 0.7431556582450867\ncompatibility: 7.474500000000003\naccessibility: -2.67517723186593\nreward: 0.885027289390564\ncompatibility: 7.501500000000001\naccessibility: -2.476834056128124\nreward: 0.8911558389663696\ncompatibility: 7.296499999999996\naccessibility: -2.3578197855955345\nreward: 1.0418968200683594\ncompatibility: 7.5954999999999995\naccessibility: -2.291886920975794\nreward: 1.1155937910079956\ncompatibility: 7.529500000000004\naccessibility: -2.145984299541447\nreward: 1.27151620388031\ncompatibility: 7.403500000000003\naccessibility: -1.844600605478685\nreward: 0.7990696430206299\ncompatibility: 7.328000000000002\naccessibility: -2.512824070303567\nreward: 0.7664673924446106\ncompatibility: 7.203999999999999\naccessibility: -2.4952989226633946\nreward: 1.0390998125076294\ncompatibility: 7.893\naccessibility: -2.4554573609010135\nreward: 0.6470162272453308\ncompatibility: 7.139000000000001\naccessibility: -2.6396542172111266\nreward: 0.8182881474494934\ncompatibility: 7.1635\naccessibility: -2.3958713286027074\nreward: 0.7050287127494812\ncompatibility: 7.4275\naccessibility: -2.70718908612986\nreward: 1.1700592041015625\ncompatibility: 7.662500000000002\naccessibility: -2.1355361471270555\nreward: 1.13933527469635\ncompatibility: 7.604\naccessibility: -2.150282870223083\nreward: 0.967667818069458\ncompatibility: 7.5070000000000014\naccessibility: -2.3558197460536445\nreward: 0.8562865257263184\ncompatibility: 7.507000000000001\naccessibility: -2.522891649855059\nreward: 1.03174889087677\ncompatibility: 7.437499999999997\naccessibility: -2.2224660017588143\nreward: 1.0116161108016968\ncompatibility: 7.486500000000001\naccessibility: -2.2789150835907717\nreward: 0.9047926068305969\ncompatibility: 7.507500000000001\naccessibility: -2.4504003676921355\nreward: 1.098677396774292\ncompatibility: 7.2464999999999975\naccessibility: -2.0197518440062785\nreward: 0.957477867603302\ncompatibility: 7.4995\naccessibility: -2.36708675514899\nreward: 1.0438508987426758\ncompatibility: 7.606500000000002\naccessibility: -2.2948487365678867\nreward: 1.1095798015594482\ncompatibility: 7.366000000000001\naccessibility: -2.067416047268476\nreward: 0.8262434005737305\ncompatibility: 7.496500000000002\naccessibility: -2.5623313446986047\nreward: 1.0158101320266724\ncompatibility: 7.5335\naccessibility: -2.297802646068253\nreward: 1.1078529357910156\ncompatibility: 7.536000000000001\naccessibility: -2.1610777463107613\nreward: 0.9291034936904907\ncompatibility: 7.4689999999999985\naccessibility: -2.3933090555687326\nreward: 0.7323547601699829\ncompatibility: 7.559000000000001\naccessibility: -2.736646391049184\nreward: 0.9636759161949158\ncompatibility: 7.4225\naccessibility: -2.316539685155039\nreward: 0.8648039698600769\ncompatibility: 7.375999999999998\naccessibility: -2.439936874723936\nreward: 0.8819580674171448\ncompatibility: 7.303499999999998\naccessibility: -2.375366439038846\nreward: 0.8410151600837708\ncompatibility: 7.6495\naccessibility: -2.6221379843283925\nreward: 0.906528890132904\ncompatibility: 7.403500000000001\naccessibility: -2.3920816663088473\nreward: 0.8975029587745667\ncompatibility: 7.41\naccessibility: -2.409102687528497\nreward: 1.054016351699829\ncompatibility: 7.474500000000002\naccessibility: -2.208886258079649\nreward: 0.9409813284873962\ncompatibility: 7.6935\naccessibility: -2.495760166645647\nreward: 0.7873653769493103\ncompatibility: 7.446500000000001\naccessibility: -2.593862630344118\nreward: 1.0550346374511719\ncompatibility: 7.602999999999998\naccessibility: -2.276198093426832\nreward: 0.8230627775192261\ncompatibility: 7.551000000000003\naccessibility: -2.5962987352568785\nreward: 0.8601366281509399\ncompatibility: 7.446499999999999\naccessibility: -2.4847057896855045\nreward: 1.2339446544647217\ncompatibility: 7.5035000000000025\naccessibility: -1.9545295239646938\nreward: 0.9780141711235046\ncompatibility: 7.458999999999996\naccessibility: -2.3145859028159808\nreward: 0.811111330986023\ncompatibility: 7.303499999999999\naccessibility: -2.481636537242481\nreward: 0.6216827630996704\ncompatibility: 7.393500000000001\naccessibility: -2.8139937215859545\nreward: 1.062972068786621\ncompatibility: 7.580499999999998\naccessibility: -2.252238334160526\nreward: 1.1240798234939575\ncompatibility: 7.4675\naccessibility: -2.1000409669080624\nreward: 1.0669429302215576\ncompatibility: 7.5355\naccessibility: -2.222174911832764\nreward: 0.9326505064964294\ncompatibility: 7.519\naccessibility: -2.4147742249415414\nreward: 0.8239895105361938\ncompatibility: 7.488999999999999\naccessibility: -2.5616942716006212\nreward: 0.6833885312080383\ncompatibility: 7.377000000000002\naccessibility: -2.712595809721801\nreward: 0.970741331577301\ncompatibility: 7.630000000000001\naccessibility: -2.417102307660712\nreward: 0.8615087866783142\ncompatibility: 7.3355\naccessibility: -2.4231832168596936\nreward: 0.8765395879745483\ncompatibility: 7.454000000000002\naccessibility: -2.464119164946661\nreward: 0.5735588073730469\ncompatibility: 7.253000000000001\naccessibility: -2.8109117648068325\nreward: 0.944197952747345\ncompatibility: 7.370500000000001\naccessibility: -2.3178995194685195\nreward: 1.0141181945800781\ncompatibility: 7.242999999999999\naccessibility: -2.1447155535498954\nreward: 0.767348051071167\ncompatibility: 7.549500000000003\naccessibility: -2.6790671935203165\nreward: 1.0822473764419556\ncompatibility: 7.788500000000003\naccessibility: -2.3347539040782834\nreward: 1.1901525259017944\ncompatibility: 7.532999999999998\naccessibility: -2.0360211958697656\nreward: 1.0058109760284424\ncompatibility: 7.558499999999999\naccessibility: -2.3261942388515844\nreward: 0.9589635133743286\ncompatibility: 7.330999999999998\naccessibility: -2.274590420241357\nreward: 1.1153216361999512\ncompatibility: 7.558999999999998\naccessibility: -2.1621960890334533\nreward: 1.0237728357315063\ncompatibility: 7.651000000000002\naccessibility: -2.348805076764571\nreward: 0.7181209325790405\ncompatibility: 7.409000000000001\naccessibility: -2.677640014016836\nreward: 0.7919566631317139\ncompatibility: 7.433499999999998\naccessibility: -2.580011425420604\nreward: 0.7558904886245728\ncompatibility: 7.265999999999998\naccessibility: -2.5443785677574935\nreward: 1.1107219457626343\ncompatibility: 7.354999999999995\naccessibility: -2.059809902615402\nreward: 1.0265578031539917\ncompatibility: 7.406500000000001\naccessibility: -2.2136453673470577\nreward: 1.0388275384902954\ncompatibility: 7.528500000000001\naccessibility: -2.260598003926651\nreward: 0.8488040566444397\ncompatibility: 7.2855\naccessibility: -2.41545458597263\nreward: 1.0564734935760498\ncompatibility: 7.489499999999998\naccessibility: -2.21323626110024\nreward: 1.0362963676452637\ncompatibility: 7.715499999999999\naccessibility: -2.3645733444359314\nreward: 1.1338281631469727\ncompatibility: 7.570500000000002\naccessibility: -2.140596974293034\nreward: 0.8402838110923767\ncompatibility: 7.215000000000003\naccessibility: -2.3904671138410376\nreward: 0.8413317203521729\ncompatibility: 7.295999999999998\naccessibility: -2.432288109858176\nreward: 1.205138087272644\ncompatibility: 7.4590000000000005\naccessibility: -1.9739000862836509\nreward: 0.9075851440429688\ncompatibility: 7.414000000000001\naccessibility: -2.3961223102888294\nreward: 1.1720284223556519\ncompatibility: 7.3020000000000005\naccessibility: -1.9394573290107255\nreward: 0.6097213625907898\ncompatibility: 7.664000000000001\naccessibility: -2.9768465007429743\nreward: 0.8032892942428589\ncompatibility: 7.459\naccessibility: -2.5766731807953565\nreward: 0.5794169902801514\ncompatibility: 7.224000000000002\naccessibility: -2.7865887839210726\nreward: 0.7728671431541443\ncompatibility: 7.6564999999999985\naccessibility: -2.7281099715918646\nreward: 1.155481219291687\ncompatibility: 7.3785\naccessibility: -2.0052602854635193\nreward: 1.0650923252105713\ncompatibility: 7.430499999999999\naccessibility: -2.168700870821087\nreward: 0.8712195754051208\ncompatibility: 7.548000000000002\naccessibility: -2.5224563806502522\nreward: 0.9355630874633789\ncompatibility: 7.430499999999999\naccessibility: -2.362994649282668\nreward: 1.1788175106048584\ncompatibility: 7.388\naccessibility: -1.9753452385724897\nreward: 0.6746439337730408\ncompatibility: 7.546000000000001\naccessibility: -2.8162483972319654\nreward: 0.939682126045227\ncompatibility: 7.4655\naccessibility: -2.375566106234596\nreward: 0.8933328986167908\ncompatibility: 7.572499999999999\naccessibility: -2.502411343196203\nreward: 0.783737301826477\ncompatibility: 7.394000000000001\naccessibility: -2.5711797505272185\nreward: 0.9737647771835327\ncompatibility: 7.584500000000003\naccessibility: -2.3881921280526934\nreward: 0.8698254823684692\ncompatibility: 7.703499999999999\naccessibility: -2.6078511065303465\nreward: 1.131142020225525\ncompatibility: 7.521999999999999\naccessibility: -2.1186440913286053\nreward: 0.9864519238471985\ncompatibility: 7.301500000000002\naccessibility: -2.217554278825791\nreward: 0.8954601883888245\ncompatibility: 7.369000000000003\naccessibility: -2.3902025343655247\nreward: 0.8968918323516846\ncompatibility: 7.796500000000001\naccessibility: -2.6170729231324668\nreward: 0.8290541172027588\ncompatibility: 7.274000000000001\naccessibility: -2.438918854632214\nreward: 0.9159670472145081\ncompatibility: 7.485000000000002\naccessibility: -2.421585130494762\nreward: 0.9075444936752319\ncompatibility: 7.419500000000002\naccessibility: -2.3991296663815675\nreward: 0.5617222189903259\ncompatibility: 7.3125000000000036\naccessibility: -2.860541646057617\nreward: 0.8378066420555115\ncompatibility: 7.29\naccessibility: -2.434361456499129\nreward: 0.8686090111732483\ncompatibility: 7.657\naccessibility: -2.5847650653480385\nreward: 0.7403877973556519\ncompatibility: 7.400499999999997\naccessibility: -2.6396861681321955\nreward: 1.1031970977783203\ncompatibility: 7.6160000000000005\naccessibility: -2.2109185766481825\nreward: 0.973690390586853\ncompatibility: 7.4570000000000025\naccessibility: -2.3200001028702832\nreward: 0.9859386086463928\ncompatibility: 7.554000000000001\naccessibility: -2.353592094973424\nreward: 1.215643048286438\ncompatibility: 7.531000000000001\naccessibility: -1.9967140139259987\nreward: 0.4936930239200592\ncompatibility: 7.4045000000000005\naccessibility: -3.0118711912817266\nreward: 1.2297710180282593\ncompatibility: 7.696999999999999\naccessibility: -2.0644505916665405\nreward: 0.7280151844024658\ncompatibility: 7.458500000000002\naccessibility: -2.689316536851476\nreward: 0.7291039228439331\ncompatibility: 7.473000000000004\naccessibility: -2.695451290313225\nreward: 1.2318859100341797\ncompatibility: 7.717999999999998\naccessibility: -2.0725282943954686\nreward: 0.8618590831756592\ncompatibility: 7.3225\naccessibility: -2.415693501223768\nreward: 1.0748486518859863\ncompatibility: 7.386000000000003\naccessibility: -2.130226954757368\nreward: 0.865064263343811\ncompatibility: 7.150000000000002\naccessibility: -2.3184750762676187\nreward: 1.0162409543991089\ncompatibility: 7.620000000000004\naccessibility: -2.3434956813653596\nreward: 0.9245722889900208\ncompatibility: 7.588499999999998\naccessibility: -2.464123749374038\nreward: 0.9892796277999878\ncompatibility: 7.539500000000003\naccessibility: -2.340812669658725\nreward: 0.9841814637184143\ncompatibility: 7.644500000000001\naccessibility: -2.4047099080713106\nreward: 1.1494077444076538\ncompatibility: 7.605500000000001\naccessibility: -2.135977676450019\nreward: 0.6803361177444458\ncompatibility: 7.413\naccessibility: -2.736460150933933\nreward: 0.9240349531173706\ncompatibility: 7.464000000000002\naccessibility: -2.3982333273636884\nreward: 0.9318035840988159\ncompatibility: 7.515499999999999\naccessibility: -2.4141696285567833\nreward: 0.9174196720123291\ncompatibility: 7.255999999999998\naccessibility: -2.296727678102906\nreward: 0.7584887742996216\ncompatibility: 7.344500000000002\naccessibility: -2.58253473300252\nreward: 1.1241613626480103\ncompatibility: 7.570500000000003\naccessibility: -2.155097193053032\nreward: 1.002415418624878\ncompatibility: 7.5009999999999994\naccessibility: -2.300484047223404\nreward: 1.16953444480896\ncompatibility: 7.4380000000000015\naccessibility: -2.0160555479436217\nreward: 0.8260062336921692\ncompatibility: 7.795000000000001\naccessibility: -2.722597750495484\nreward: 0.989154577255249\ncompatibility: 7.2945\naccessibility: -2.209750301040471\nreward: 1.0513707399368286\ncompatibility: 7.347000000000003\naccessibility: -2.1445510230128106\nreward: 0.8831941485404968\ncompatibility: 7.224499999999999\naccessibility: -2.3311909186248023\nreward: 0.8192213773727417\ncompatibility: 7.358999999999997\naccessibility: -2.4992036792763366\nreward: 1.0528866052627563\ncompatibility: 7.509000000000002\naccessibility: -2.2290630124064545\nreward: 0.9071699976921082\ncompatibility: 7.593499999999999\naccessibility: -2.4929057226646933\nreward: 1.1540625095367432\ncompatibility: 7.531500000000001\naccessibility: -2.0893525945751668\nreward: 1.2832884788513184\ncompatibility: 7.682999999999997\naccessibility: -1.9766744028079613\nreward: 0.9439557790756226\ncompatibility: 7.640500000000003\naccessibility: -2.462905604454373\nreward: 0.9660260677337646\ncompatibility: 7.562500000000001\naccessibility: -2.3880144693956837\nreward: 0.7378387451171875\ncompatibility: 7.5375\naccessibility: -2.7169025565060037\nreward: 1.1883515119552612\ncompatibility: 7.484000000000001\naccessibility: -2.0124726763621905\nreward: 0.9122343063354492\ncompatibility: 7.715\naccessibility: -2.550398524272115\nreward: 1.0740034580230713\ncompatibility: 7.7215\naccessibility: -2.311227044354705\nreward: 0.8332439064979553\ncompatibility: 7.521500000000002\naccessibility: -2.5652234364764883\nreward: 1.1806988716125488\ncompatibility: 7.631500000000002\naccessibility: -2.1029696076750417\nreward: 1.108696699142456\ncompatibility: 7.526\naccessibility: -2.154454997110624\nreward: 0.8328985571861267\ncompatibility: 7.671500000000001\naccessibility: -2.6460985786685396\nreward: 1.0903410911560059\ncompatibility: 7.5230000000000015\naccessibility: -2.180381144204585\nreward: 1.0946372747421265\ncompatibility: 7.512\naccessibility: -2.1680440863904584\nreward: 0.9300816655158997\ncompatibility: 7.489500000000003\naccessibility: -2.4028239099658237\nreward: 0.9187189936637878\ncompatibility: 7.462499999999999\naccessibility: -2.4054036885207135\nreward: 1.1544218063354492\ncompatibility: 7.5169999999999995\naccessibility: -2.081045846589485\nreward: 0.7601902484893799\ncompatibility: 7.503000000000002\naccessibility: -2.664893159099873\nreward: 0.9733846187591553\ncompatibility: 7.530000000000004\naccessibility: -2.359565973070061\nreward: 0.8730475306510925\ncompatibility: 7.559500000000001\naccessibility: -2.5258750921841298\nreward: 1.073573350906372\ncompatibility: 7.598\naccessibility: -2.2457114380670244\nreward: 0.7642655968666077\ncompatibility: 7.489999999999998\naccessibility: -2.6518158714876225\nreward: 0.9433913826942444\ncompatibility: 7.322500000000002\naccessibility: -2.2933951134113704\nreward: 1.0075311660766602\ncompatibility: 7.348499999999999\naccessibility: -2.2111139255564227\nreward: 1.1138230562210083\ncompatibility: 7.6755\naccessibility: -2.226854624791551\nreward: 0.9142850041389465\ncompatibility: 7.440499999999999\naccessibility: -2.40026888059553\nreward: 0.7897031307220459\ncompatibility: 7.253000000000001\naccessibility: -2.4866952857153684\nreward: 1.0485613346099854\ncompatibility: 7.452000000000002\naccessibility: -2.2050152043627707\nreward: 1.0869673490524292\ncompatibility: 7.5760000000000005\naccessibility: -2.213834725971032\nreward: 1.0934998989105225\ncompatibility: 7.59\naccessibility: -2.2115357772821973\nreward: 1.017247200012207\ncompatibility: 7.592999999999998\naccessibility: -2.3275220930440184\nreward: 0.6846033930778503\ncompatibility: 7.221999999999997\naccessibility: -2.6277377657961756\nreward: 0.7497795224189758\ncompatibility: 7.285000000000002\naccessibility: -2.563723575846871\nreward: 1.2180249691009521\ncompatibility: 7.648000000000001\naccessibility: -2.055819667546383\nreward: 0.9605486989021301\ncompatibility: 7.557\naccessibility: -2.393284054177141\nreward: 0.8321595191955566\ncompatibility: 7.5045\naccessibility: -2.557742896081672\nreward: 1.0277442932128906\ncompatibility: 7.445\naccessibility: -2.2324906988969335\nreward: 0.7832728028297424\ncompatibility: 7.316500000000002\naccessibility: -2.5303586403486418\nreward: 0.8713170289993286\ncompatibility: 7.280499999999998\naccessibility: -2.3790066383852384\nreward: 0.862648069858551\ncompatibility: 7.513499999999999\naccessibility: -2.516831506167917\nreward: 0.9429936408996582\ncompatibility: 7.564\naccessibility: -2.4233667189230945\nreward: 1.3852853775024414\ncompatibility: 7.875000000000003\naccessibility: -1.9265361637211376\nreward: 1.0705265998840332\ncompatibility: 7.430999999999998\naccessibility: -2.1608172117387747\nreward: 0.9535665512084961\ncompatibility: 7.6555\naccessibility: -2.456525192385585\nreward: 0.8827094435691833\ncompatibility: 7.329500000000001\naccessibility: -2.3881680177743188\nreward: 0.9676125049591064\ncompatibility: 7.608499999999998\naccessibility: -2.4102777106514663\nreward: 0.9850842356681824\ncompatibility: 7.727000000000003\naccessibility: -2.4475521806618996\nreward: 0.802294135093689\ncompatibility: 7.3845\naccessibility: -2.5382552450150087\nreward: 0.6516446471214294\ncompatibility: 7.416999999999999\naccessibility: -2.781640175660994\nreward: 0.9734593629837036\ncompatibility: 7.361999999999999\naccessibility: -2.2694538169631744\nreward: 1.0579910278320312\ncompatibility: 7.335999999999999\naccessibility: -2.128727736266061\nreward: 1.0506744384765625\ncompatibility: 7.453\naccessibility: -2.2023811523190786\nreward: 1.1693308353424072\ncompatibility: 7.4159999999999995\naccessibility: -2.00457523509732\nreward: 0.8112022280693054\ncompatibility: 7.444999999999997\naccessibility: -2.557303827199844\nreward: 0.939770519733429\ncompatibility: 7.4365000000000006\naccessibility: -2.359897826608109\nreward: 1.0499805212020874\ncompatibility: 7.6575\naccessibility: -2.3129757292825515\nreward: 1.165687918663025\ncompatibility: 7.689000000000002\naccessibility: -2.1562895430267006\nreward: 0.8859249353408813\ncompatibility: 7.362500000000001\naccessibility: -2.4010232799575433\nreward: 0.8312761783599854\ncompatibility: 7.11\naccessibility: -2.347728597453945\nreward: 0.9389730095863342\ncompatibility: 7.5699999999999985\naccessibility: -2.432611938110214\nreward: 1.0504297018051147\ncompatibility: 7.390499999999999\naccessibility: -2.1692661457800853\nreward: 0.8632367253303528\ncompatibility: 7.430999999999998\naccessibility: -2.4717520696911195\nreward: 0.8270664811134338\ncompatibility: 7.463999999999999\naccessibility: -2.543686008200032\nreward: 0.9447351694107056\ncompatibility: 7.666\naccessibility: -2.4753972791275407\nreward: 1.0967668294906616\ncompatibility: 7.5974999999999975\naccessibility: -2.2106532431752\nreward: 0.8280515670776367\ncompatibility: 7.526500000000001\naccessibility: -2.5756904964643113\nreward: 0.6769835352897644\ncompatibility: 7.237\naccessibility: -2.6472032599578283\nreward: 0.9914897680282593\ncompatibility: 7.662500000000001\naccessibility: -2.4033903754862274\nreward: 0.9336890578269958\ncompatibility: 7.702000000000003\naccessibility: -2.5112521276352036\nreward: 0.9546089172363281\ncompatibility: 7.611500000000002\naccessibility: -2.431390238061563\nreward: 0.927153468132019\ncompatibility: 7.507999999999999\naccessibility: -2.4171269495429044\nreward: 0.8534186482429504\ncompatibility: 7.418000000000003\naccessibility: -2.479514925453172\nreward: 1.0147240161895752\ncompatibility: 7.423000000000001\naccessibility: -2.2402354166068146\nreward: 1.1736254692077637\ncompatibility: 7.641\naccessibility: -2.118668869720182\nreward: 1.1401643753051758\ncompatibility: 7.562499999999999\naccessibility: -2.1268070310260945\nreward: 0.9008470773696899\ncompatibility: 7.716000000000001\naccessibility: -2.5680150581610053\nreward: 0.6449334621429443\ncompatibility: 7.3245\naccessibility: -2.742153344228604\nreward: 0.7846254706382751\ncompatibility: 7.518000000000003\naccessibility: -2.6362760701564447\nreward: 0.9692586660385132\ncompatibility: 7.3395\naccessibility: -2.2637013081939985\nreward: 0.9636732935905457\ncompatibility: 7.276500000000003\naccessibility: -2.2383293253431895\nreward: 0.9708898067474365\ncompatibility: 7.558\naccessibility: -2.3783081036724925\nreward: 0.6394643783569336\ncompatibility: 7.587500000000001\naccessibility: -2.8912498495242156\nreward: 0.9794189929962158\ncompatibility: 7.629000000000002\naccessibility: -2.4035500675389714\nreward: 0.9812526702880859\ncompatibility: 7.5239999999999965\naccessibility: -2.3445495345882024\nreward: 0.8086957931518555\ncompatibility: 7.422\naccessibility: -2.5487419981703776\nreward: 0.8706253170967102\ncompatibility: 7.398500000000001\naccessibility: -2.443258455536217\nreward: 0.9836152195930481\ncompatibility: 7.713499999999998\naccessibility: -2.4425236079518045\nreward: 1.0665283203125\ncompatibility: 7.479500000000002\naccessibility: -2.1927967654192497\nreward: 1.3759403228759766\ncompatibility: 7.7490000000000006\naccessibility: -1.873053803017228\nreward: 0.7905175685882568\ncompatibility: 7.534000000000005\naccessibility: -2.636009335150551\nreward: 1.0815155506134033\ncompatibility: 7.221500000000001\naccessibility: -2.0321017264360175\nreward: 1.1212337017059326\ncompatibility: 7.529500000000002\naccessibility: -2.137524507483226\nreward: 1.0892088413238525\ncompatibility: 7.538\naccessibility: -2.1901152710681653\nreward: 0.7544518709182739\ncompatibility: 7.3075\naccessibility: -2.5687686438857007\nreward: 1.0148122310638428\ncompatibility: 7.514999999999999\naccessibility: -2.289388867191227\nreward: 0.9727516770362854\ncompatibility: 7.5550000000000015\naccessibility: -2.373908203299049\nreward: 0.9536799192428589\ncompatibility: 7.523999999999997\naccessibility: -2.3859086489869417\nreward: 0.97169429063797\ncompatibility: 7.582500000000002\naccessibility: -2.390226392274933\nreward: 1.0968730449676514\ncompatibility: 7.4905\naccessibility: -2.153172526027513\nreward: 1.1260790824890137\ncompatibility: 7.5795\naccessibility: -2.157042111284866\nreward: 1.1833381652832031\ncompatibility: 7.591000000000001\naccessibility: -2.0773141513473234\nreward: 0.8479537963867188\ncompatibility: 7.374999999999999\naccessibility: -2.4646764780790136\nreward: 0.8260435461997986\ncompatibility: 7.503000000000004\naccessibility: -2.566113263713954\nreward: 1.0558545589447021\ncompatibility: 7.613\naccessibility: -2.2803252820457476\nreward: 0.7390574812889099\ncompatibility: 7.635500000000002\naccessibility: -2.767574456481841\nreward: 1.0602236986160278\ncompatibility: 7.350000000000001\naccessibility: -2.132878694437824\nreward: 1.1016414165496826\ncompatibility: 7.629500000000003\naccessibility: -2.2204842580966266\nreward: 0.7558866143226624\ncompatibility: 7.302000000000001\naccessibility: -2.563670097156809\nreward: 0.8818009495735168\ncompatibility: 7.772500000000001\naccessibility: -2.626852106577541\nreward: 1.2146011590957642\ncompatibility: 7.5735\naccessibility: -2.0210446497163694\nreward: 1.1050519943237305\ncompatibility: 7.528\naccessibility: -2.1609935075038536\nreward: 0.7875741720199585\ncompatibility: 7.274500000000002\naccessibility: -2.50140657334203\nreward: 0.6825626492500305\ncompatibility: 7.382000000000001\naccessibility: -2.7165131649098346\nreward: 1.16193425655365\ncompatibility: 7.7395\naccessibility: -2.188973580581785\nreward: 1.1319972276687622\ncompatibility: 7.3934999999999995\naccessibility: -2.0485220671080064\nreward: 1.0352888107299805\ncompatibility: 7.494499999999999\naccessibility: -2.247691735216013\nreward: 1.0866336822509766\ncompatibility: 7.623499999999998\naccessibility: -2.239781700201823\nreward: 0.7153586149215698\ncompatibility: 7.205500000000001\naccessibility: -2.5727656895061233\nreward: 0.8881582617759705\ncompatibility: 7.5770000000000035\naccessibility: -2.512584010543497\nreward: 0.9755429029464722\ncompatibility: 7.379999999999999\naccessibility: -2.2759713586013897\nreward: 0.9421447515487671\ncompatibility: 7.434\naccessibility: -2.3549971190720704\nreward: 0.9754499793052673\ncompatibility: 7.5175\naccessibility: -2.3497714912474215\nreward: 1.0883774757385254\ncompatibility: 7.496500000000001\naccessibility: -2.169130170800875\nreward: 1.0322644710540771\ncompatibility: 7.7675000000000045\naccessibility: -2.3984783522667734\nreward: 1.124157428741455\ncompatibility: 7.461999999999996\naccessibility: -2.0969781177822346\nreward: 0.934270441532135\ncompatibility: 7.5685\naccessibility: -2.438862199498772\nreward: 1.102994680404663\ncompatibility: 7.555\naccessibility: -2.1785437745288907\nreward: 0.967536211013794\ncompatibility: 7.5169999999999995\naccessibility: -2.3613742130653215\nreward: 0.8132055997848511\ncompatibility: 7.393500000000001\naccessibility: -2.5267094452247725\nreward: 1.0867539644241333\ncompatibility: 7.616999999999999\naccessibility: -2.2361189818661877\nreward: 0.9489761590957642\ncompatibility: 7.648000000000003\naccessibility: -2.4593929414879536\nreward: 0.839431881904602\ncompatibility: 7.420499999999997\naccessibility: -2.5018343238636023\nreward: 1.2439861297607422\ncompatibility: 7.691500000000001\naccessibility: -2.0401815168378206\nreward: 0.8192718029022217\ncompatibility: 7.4185\naccessibility: -2.531002993835156\nreward: 0.8515915274620056\ncompatibility: 7.522500000000001\naccessibility: -2.5382377369983047\nreward: 1.262689232826233\ncompatibility: 7.729499999999999\naccessibility: -2.032483986174077\nreward: 0.8896938562393188\ncompatibility: 7.386\naccessibility: -2.4079592300179424\nreward: 0.7299074530601501\ncompatibility: 7.461500000000001\naccessibility: -2.68808520749184\nreward: 1.1637555360794067\ncompatibility: 7.5615000000000006\naccessibility: -2.0908844841108323\nreward: 0.986472487449646\ncompatibility: 7.531499999999999\naccessibility: -2.340737695678919\nreward: 1.0163276195526123\ncompatibility: 7.375000000000001\naccessibility: -2.2121157070315647\nreward: 0.8780336976051331\ncompatibility: 7.587500000000001\naccessibility: -2.5333959248641333\nreward: 0.5952643752098083\ncompatibility: 7.521500000000002\naccessibility: -2.922192746870837\nreward: 0.8715394735336304\ncompatibility: 7.491500000000002\naccessibility: -2.4917086514577784\nreward: 0.8487791419029236\ncompatibility: 7.4525000000000015\naccessibility: -2.5049563257608516\nreward: 1.009146809577942\ncompatibility: 7.4375000000000036\naccessibility: -2.25636902189447\nreward: 0.9697762131690979\ncompatibility: 7.555500000000002\naccessibility: -2.3786392830245333\nreward: 1.0079764127731323\ncompatibility: 7.610000000000005\naccessibility: -2.3505353811953182\nreward: 0.6281038522720337\ncompatibility: 7.6225\naccessibility: -2.927040654678209\nreward: 0.951441764831543\ncompatibility: 7.462999999999999\naccessibility: -2.3565873376013013\nreward: 0.9816961288452148\ncompatibility: 7.610000000000002\naccessibility: -2.3899557783505747\nreward: 0.9914346933364868\ncompatibility: 7.229999999999999\naccessibility: -2.1717765240207054\nreward: 1.1585012674331665\ncompatibility: 7.575500000000002\naccessibility: -2.1062658827519205\nreward: 0.7536792755126953\ncompatibility: 7.599999999999998\naccessibility: -2.7266239844335223\nreward: 0.9338053464889526\ncompatibility: 7.581499999999998\naccessibility: -2.446524145130935\nreward: 1.1586159467697144\ncompatibility: 7.547500000000001\naccessibility: -2.0910938592677413\nreward: 0.9233236312866211\ncompatibility: 7.513000000000003\naccessibility: -2.425550245555399\nreward: 1.3539522886276245\ncompatibility: 7.791000000000002\naccessibility: -1.9285358305873619\nreward: 0.9607795476913452\ncompatibility: 7.448500000000003\naccessibility: -2.3348128499652043\nreward: 0.7912878394126892\ncompatibility: 7.5489999999999995\naccessibility: -2.6428896267346245\nreward: 0.9483702182769775\ncompatibility: 7.444500000000001\naccessibility: -2.351283968550154\nreward: 1.1145557165145874\ncompatibility: 7.561999999999999\naccessibility: -2.164952097483498\nreward: 0.921896755695343\ncompatibility: 7.6975\naccessibility: -2.52652989482613\nreward: 0.9448831081390381\ncompatibility: 7.698000000000002\naccessibility: -2.492318175556083\nreward: 0.8126880526542664\ncompatibility: 7.539000000000001\naccessibility: -2.6054321859121936\nreward: 0.8651454448699951\ncompatibility: 7.375000000000001\naccessibility: -2.4388890051073133\nreward: 0.7880004644393921\ncompatibility: 7.0465000000000035\naccessibility: -2.3786242897621444\nreward: 1.0390310287475586\ncompatibility: 7.492000000000001\naccessibility: -2.2407392364735346\nreward: 1.0032376050949097\ncompatibility: 7.487000000000003\naccessibility: -2.2917508223435337\nreward: 0.931776225566864\ncompatibility: 7.457499999999999\naccessibility: -2.3831392633008877\nreward: 0.7846152782440186\ncompatibility: 7.176499999999999\naccessibility: -2.45334490544899\nreward: 0.9052031636238098\ncompatibility: 7.421000000000004\naccessibility: -2.4034452723693924\nreward: 0.6985129714012146\ncompatibility: 7.422999999999998\naccessibility: -2.7145519779553298\nreward: 1.0457123517990112\ncompatibility: 7.4479999999999995\naccessibility: -2.2071457876287237\nreward: 0.8823839426040649\ncompatibility: 7.387000000000003\naccessibility: -2.419459820789767\nreward: 1.023724913597107\ncompatibility: 7.600500000000002\naccessibility: -2.321823337573913\nreward: 0.9237892627716064\ncompatibility: 7.5504999999999995\naccessibility: -2.444941095808091\nreward: 0.7878930568695068\ncompatibility: 7.359000000000001\naccessibility: -2.546196135256989\nreward: 0.5611077547073364\ncompatibility: 7.4590000000000005\naccessibility: -2.939945528807744\nreward: 0.8940283060073853\ncompatibility: 7.224999999999998\naccessibility: -2.3152075627219286\nreward: 1.1193687915802002\ncompatibility: 7.648999999999998\naccessibility: -2.204339668936477\nreward: 0.9385815858840942\ncompatibility: 7.466500000000005\naccessibility: -2.3777526390032784\nreward: 1.0286800861358643\ncompatibility: 7.5845\naccessibility: -2.3058191280809064\nreward: 0.914802074432373\ncompatibility: 7.653\naccessibility: -2.513332566484007\nreward: 0.8707298636436462\ncompatibility: 7.3795\naccessibility: -2.4329230993577236\nreward: 0.8982589244842529\ncompatibility: 7.433000000000002\naccessibility: -2.420290209781769\nreward: 0.9118173718452454\ncompatibility: 7.4\naccessibility: -2.382273986269774\nreward: 0.9326372146606445\ncompatibility: 7.5900000000000025\naccessibility: -2.4528298861861084\nreward: 1.1308971643447876\ncompatibility: 7.518000000000001\naccessibility: -2.116868542816856\nreward: 0.6297682523727417\ncompatibility: 7.320999999999999\naccessibility: -2.763026203182799\nreward: 1.150708556175232\ncompatibility: 7.812500000000003\naccessibility: -2.244919384156068\nreward: 1.005651593208313\ncompatibility: 7.607\naccessibility: -2.352415527228476\nreward: 0.6826964616775513\ncompatibility: 7.384500000000002\naccessibility: -2.717651720824917\nreward: 1.050878643989563\ncompatibility: 7.517499999999999\naccessibility: -2.2366285434461677\nreward: 1.175555944442749\ncompatibility: 7.6095\naccessibility: -2.098898307115493\nreward: 1.0154011249542236\ncompatibility: 7.424499999999998\naccessibility: -2.2400233950968897\nreward: 0.7480683326721191\ncompatibility: 7.5630000000000015\naccessibility: -2.71521893937119\nreward: 0.9094139337539673\ncompatibility: 7.355\naccessibility: -2.3617719250204456\nreward: 1.2560313940048218\ncompatibility: 7.389500000000001\naccessibility: -1.8603278625815491\nreward: 0.9512828588485718\ncompatibility: 7.4594999999999985\naccessibility: -2.3549507276060933\nreward: 0.7449369430541992\ncompatibility: 7.2565\naccessibility: -2.55571955212417\nreward: 0.959064245223999\ncompatibility: 7.517499999999999\naccessibility: -2.3743500162727744\nreward: 1.0016440153121948\ncompatibility: 7.7435\naccessibility: -2.431551751427917\nreward: 1.139596700668335\ncompatibility: 7.673499999999998\naccessibility: -2.1871228940678336\nreward: 0.8599337339401245\ncompatibility: 7.4765\naccessibility: -2.501081580804245\nreward: 1.0060988664627075\ncompatibility: 7.461000000000002\naccessibility: -2.273530280081867\nreward: 0.931484580039978\ncompatibility: 7.583499999999999\naccessibility: -2.451076686702281\nreward: 1.0525474548339844\ncompatibility: 7.4755\naccessibility: -2.211625264269244\nreward: 0.776277482509613\ncompatibility: 7.567\naccessibility: -2.675048017285934\nreward: 0.8691217303276062\ncompatibility: 7.279000000000001\naccessibility: -2.381495999645782\nreward: 0.895145833492279\ncompatibility: 7.7315000000000005\naccessibility: -2.5848705661405926\nreward: 0.9787136316299438\ncompatibility: 7.502000000000002\naccessibility: -2.336572387024376\nreward: 0.9208540916442871\ncompatibility: 7.433999999999998\naccessibility: -2.3869331819066777\nreward: 0.8546372652053833\ncompatibility: 7.3770000000000024\naccessibility: -2.455722666943812\nreward: 0.8640539646148682\ncompatibility: 7.5475\naccessibility: -2.5329369459137308\nreward: 0.8364196419715881\ncompatibility: 7.259499999999999\naccessibility: -2.4201026684065665\nreward: 0.6927272081375122\ncompatibility: 7.365999999999999\naccessibility: -2.692694943703461\nreward: 0.8049730062484741\ncompatibility: 7.5654999999999974\naccessibility: -2.631201193007364\nreward: 0.9644673466682434\ncompatibility: 7.607000000000002\naccessibility: -2.414191834926674\nreward: 0.9387361407279968\ncompatibility: 7.515499999999999\naccessibility: -2.4037707829227086\nreward: 0.8266761898994446\ncompatibility: 7.758500000000002\naccessibility: -2.7020392970157654\nreward: 0.871034562587738\ncompatibility: 7.569500000000001\naccessibility: -2.5342517264700986\nreward: 0.9015967845916748\ncompatibility: 7.518000000000002\naccessibility: -2.4608191175547174\nreward: 0.8430116176605225\ncompatibility: 7.367499999999999\naccessibility: -2.468071830108026\nreward: 1.1728765964508057\ncompatibility: 7.7165\naccessibility: -2.1602386347843403\nreward: 1.0128341913223267\ncompatibility: 7.4689999999999985\naccessibility: -2.2677130419157416\nreward: 0.8612208366394043\ncompatibility: 7.551500000000003\naccessibility: -2.5393294473999255\nreward: 1.0573264360427856\ncompatibility: 7.640499999999999\naccessibility: -2.2928496493131587\nreward: 0.7654594779014587\ncompatibility: 7.627999999999998\naccessibility: -2.7239536674545866\nreward: 0.9156429767608643\ncompatibility: 7.380999999999998\naccessibility: -2.3663569299138594\nreward: 1.0026650428771973\ncompatibility: 7.2929999999999975\naccessibility: -2.1886809789817834\nreward: 1.019888997077942\ncompatibility: 7.461000000000001\naccessibility: -2.252845005446187\nreward: 0.8432942032814026\ncompatibility: 7.508500000000003\naccessibility: -2.5431837117363134\nreward: 0.844668984413147\ncompatibility: 7.5325\naccessibility: -2.553978631202786\nreward: 0.8916820287704468\ncompatibility: 7.457499999999998\naccessibility: -2.443280515761526\nreward: 0.9290370345115662\ncompatibility: 7.447\naccessibility: -2.3816230539211927\nreward: 0.8332459926605225\ncompatibility: 7.412499999999999\naccessibility: -2.5068274518813363\nreward: 1.2444809675216675\ncompatibility: 7.5615\naccessibility: -1.9697963345325975\nreward: 0.9666727781295776\ncompatibility: 7.371999999999999\naccessibility: -2.284990810799787\nreward: 1.059332251548767\ncompatibility: 7.716499999999999\naccessibility: -2.3305552057984875\nreward: 1.0556676387786865\ncompatibility: 7.669500000000003\naccessibility: -2.3108734981394154\nreward: 1.1096627712249756\ncompatibility: 7.520499999999998\naccessibility: -2.1500593528091567\nreward: 1.1788079738616943\ncompatibility: 7.485500000000003\naccessibility: -2.0275916579274442\nreward: 0.9679722785949707\ncompatibility: 7.577000000000003\naccessibility: -2.3928630146336776\nreward: 0.7777103185653687\ncompatibility: 7.4944999999999995\naccessibility: -2.6340595403922955\nreward: 0.8225845098495483\ncompatibility: 7.419500000000002\naccessibility: -2.526569669847215\nreward: 0.9207141399383545\ncompatibility: 7.583000000000001\naccessibility: -2.46696451774671\nreward: 0.6285144686698914\ncompatibility: 7.352\naccessibility: -2.781514007357756\nreward: 0.8649241328239441\ncompatibility: 7.4045000000000005\naccessibility: -2.455024534813594\nreward: 1.1463284492492676\ncompatibility: 7.6110000000000015\naccessibility: -2.1435430461792926\nreward: 0.7081333994865417\ncompatibility: 7.473000000000001\naccessibility: -2.7269070590217854\nreward: 0.844997227191925\ncompatibility: 7.587000000000001\naccessibility: -2.582682770941934\nreward: 0.7788131237030029\ncompatibility: 7.653000000000002\naccessibility: -2.717316014407559\nreward: 1.1975476741790771\ncompatibility: 7.723500000000002\naccessibility: -2.1269820906232253\nreward: 1.077821969985962\ncompatibility: 7.602000000000001\naccessibility: -2.241481360705003\nreward: 0.9690531492233276\ncompatibility: 7.554999999999999\naccessibility: -2.3794559728536493\nreward: 1.3188011646270752\ncompatibility: 7.829\naccessibility: -2.0016196288536574\nreward: 0.9665791392326355\ncompatibility: 7.739000000000003\naccessibility: -2.4817384038281403\nreward: 0.7381665706634521\ncompatibility: 7.2760000000000025\naccessibility: -2.576321576992318\nreward: 0.9309006929397583\ncompatibility: 7.349499999999999\naccessibility: -2.3265953637669865\nreward: 1.151311993598938\ncompatibility: 7.556999999999999\naccessibility: -2.1071391080295503\nreward: 1.0404393672943115\ncompatibility: 7.396000000000002\naccessibility: -2.1871980401884743\nreward: 1.0784579515457153\ncompatibility: 7.581499999999998\naccessibility: -2.2295451454099036\nreward: 0.9683055281639099\ncompatibility: 7.560000000000001\naccessibility: -2.383255968600121\nreward: 0.8102021813392639\ncompatibility: 7.394500000000001\naccessibility: -2.5317503371223102\nreward: 0.9038383364677429\ncompatibility: 7.497999999999999\naccessibility: -2.446742457487359\nreward: 0.9655919671058655\ncompatibility: 7.511500000000002\naccessibility: -2.3613441985404826\nreward: 0.9757381677627563\ncompatibility: 7.460499999999997\naccessibility: -2.3188034681526877\nreward: 0.7901171445846558\ncompatibility: 7.476500000000001\naccessibility: -2.6058064163663213\nreward: 0.9152474999427795\ncompatibility: 7.8210000000000015\naccessibility: -2.6026644449324268\nreward: 1.0433357954025269\ncompatibility: 7.531999999999997\naccessibility: -2.255710548051015\nreward: 1.0234413146972656\ncompatibility: 7.418499999999999\naccessibility: -2.2247488052984816\nreward: 0.8629128932952881\ncompatibility: 7.5524999999999975\naccessibility: -2.5373270944383037\nreward: 0.8074086308479309\ncompatibility: 7.344999999999999\naccessibility: -2.509422799024666\nreward: 0.7626370191574097\ncompatibility: 7.344499999999999\naccessibility: -2.5763123229294225\nreward: 0.8767346739768982\ncompatibility: 7.435499999999999\naccessibility: -2.453915886359889\nreward: 1.054052710533142\ncompatibility: 7.646000000000002\naccessibility: -2.3007067258332086\nreward: 0.6227647066116333\ncompatibility: 7.295000000000001\naccessibility: -2.7596029763966765\nreward: 0.8754693269729614\ncompatibility: 7.401499999999999\naccessibility: -2.437599553157329\nreward: 0.9547224044799805\ncompatibility: 7.430000000000002\naccessibility: -2.3339877926233177\nreward: 0.906943142414093\ncompatibility: 7.444000000000001\naccessibility: -2.413156705064135\nreward: 0.9667850732803345\ncompatibility: 7.307500000000001\naccessibility: -2.250268843543364\nreward: 1.0782201290130615\ncompatibility: 7.950500000000001\naccessibility: -2.427580438128444\nreward: 0.924126148223877\ncompatibility: 7.4385\naccessibility: -2.3844357776640432\nreward: 0.3384445309638977\ncompatibility: 7.280999999999999\naccessibility: -3.1785832258700415\nreward: 0.8533344864845276\ncompatibility: 7.2940000000000005\naccessibility: -2.4132125535977074\nreward: 0.8324729800224304\ncompatibility: 7.598500000000001\naccessibility: -2.607629783596313\nreward: 0.9722459316253662\ncompatibility: 7.620500000000001\naccessibility: -2.4097560653392125\nreward: 1.1098780632019043\ncompatibility: 7.517500000000001\naccessibility: -2.14812926597005\nreward: 0.9136846661567688\ncompatibility: 7.319000000000001\naccessibility: -2.336080179774968\nreward: 1.0163617134094238\ncompatibility: 7.4205000000000005\naccessibility: -2.236439498779674\nreward: 1.1534066200256348\ncompatibility: 7.817000000000002\naccessibility: -2.243282995202393\nreward: 1.0832953453063965\ncompatibility: 7.333499999999999\naccessibility: -2.0894319180034757\nreward: 0.9899040460586548\ncompatibility: 7.681499999999999\naccessibility: -2.415947521732358\nreward: 0.6339956521987915\ncompatibility: 7.5285\naccessibility: -2.8678458017468014\nreward: 0.8921804428100586\ncompatibility: 7.378999999999999\naccessibility: -2.4004793412354535\nreward: 1.0323383808135986\ncompatibility: 7.542000000000002\naccessibility: -2.277563920777259\nreward: 1.0327253341674805\ncompatibility: 7.581500000000002\naccessibility: -2.2981441059101706\nreward: 0.8282903432846069\ncompatibility: 7.340000000000001\naccessibility: -2.4754216303810317\nreward: 0.9276778101921082\ncompatibility: 7.332000000000001\naccessibility: -2.322054741920359\nreward: 1.0327775478363037\ncompatibility: 7.541999999999999\naccessibility: -2.2769051630560604\nreward: 0.9061146378517151\ncompatibility: 7.31\naccessibility: -2.3426137293260267\nreward: 0.8358513712882996\ncompatibility: 7.413000000000004\naccessibility: -2.503187273483004\nreward: 1.0096691846847534\ncompatibility: 7.543500000000002\naccessibility: -2.312371260458692\nreward: 0.821103036403656\ncompatibility: 7.543000000000002\naccessibility: -2.59495259780703\nreward: 0.710576057434082\ncompatibility: 7.4790000000000045\naccessibility: -2.7264573680295117\nreward: 0.7387146353721619\ncompatibility: 7.4449999999999985\naccessibility: -2.66603516618862\nreward: 0.9558582305908203\ncompatibility: 7.307500000000001\naccessibility: -2.266659079347562\nreward: 0.8631559610366821\ncompatibility: 7.523000000000003\naccessibility: -2.5211589324848442\nreward: 0.9780316352844238\ncompatibility: 7.588999999999999\naccessibility: -2.384202583509475\nreward: 0.8535345792770386\ncompatibility: 7.673999999999999\naccessibility: -2.6164838102074213\nreward: 0.9305976033210754\ncompatibility: 7.528000000000005\naccessibility: -2.4226750402598\nreward: 1.0574039220809937\ncompatibility: 7.2505000000000015\naccessibility: -2.083804761469815\nreward: 0.7533630132675171\ncompatibility: 7.3405\naccessibility: -2.5880804981157564\nreward: 1.1113749742507935\ncompatibility: 7.302500000000002\naccessibility: -2.0307054189429405\nreward: 0.7838460206985474\ncompatibility: 7.615500000000003\naccessibility: -2.689677372408079\nreward: 1.0054844617843628\ncompatibility: 7.566000000000002\naccessibility: -2.3307019521966845\nreward: 0.8219113945960999\ncompatibility: 7.525999999999998\naccessibility: -2.5846329093056246\nreward: 1.0569463968276978\ncompatibility: 7.364500000000001\naccessibility: -2.1455626217399604\nreward: 0.8994386792182922\ncompatibility: 7.3489999999999975\naccessibility: -2.3735205457578794\nreward: 0.9447426795959473\ncompatibility: 7.728000000000001\naccessibility: -2.508600268105977\nreward: 1.2493696212768555\ncompatibility: 7.694499999999998\naccessibility: -2.0337133517440233\nreward: 0.9636359810829163\ncompatibility: 7.331499999999998\naccessibility: -2.267849586259239\nreward: 0.9837438464164734\ncompatibility: 7.528000000000004\naccessibility: -2.3429556242802048\nreward: 0.7865093350410461\ncompatibility: 7.523500000000001\naccessibility: -2.636396721346359\nreward: 1.0696924924850464\ncompatibility: 7.486000000000001\naccessibility: -2.1915326820404486\nreward: 0.937507152557373\ncompatibility: 7.526499999999999\naccessibility: -2.411507108080605\nreward: 0.8927662372589111\ncompatibility: 7.553000000000002\naccessibility: -2.4928149163848468\nreward: 0.9777358770370483\ncompatibility: 7.5144999999999955\naccessibility: -2.3447354488902623\nreward: 0.9230546355247498\ncompatibility: 7.397\naccessibility: -2.363810894040534\nreward: 1.07002592086792\ncompatibility: 7.5105\naccessibility: -2.204157462272813\nreward: 1.1687651872634888\ncompatibility: 7.545000000000003\naccessibility: -2.0745308571929733\nreward: 0.8531993627548218\ncompatibility: 7.524000000000002\naccessibility: -2.536629500054475\nreward: 0.9511018395423889\ncompatibility: 7.4719999999999995\naccessibility: -2.3619187093623006\nreward: 0.9672390222549438\ncompatibility: 7.387500000000001\naccessibility: -2.292445075142155\nreward: 0.7356449365615845\ncompatibility: 7.509500000000002\naccessibility: -2.705193353893751\nreward: 1.0185823440551758\ncompatibility: 7.3385\naccessibility: -2.18917999727918\nreward: 0.7869137525558472\ncompatibility: 7.498500000000002\naccessibility: -2.622397238332214\nreward: 1.0228890180587769\ncompatibility: 7.545000000000001\naccessibility: -2.2933449992695984\nreward: 1.0495628118515015\ncompatibility: 7.571000000000001\naccessibility: -2.2672628600776488\nreward: 0.8476232290267944\ncompatibility: 7.425000000000001\naccessibility: -2.4919580452420957\nreward: 1.09670889377594\ncompatibility: 7.7425\naccessibility: -2.2884187457247447\nreward: 1.0333480834960938\ncompatibility: 7.658500000000001\naccessibility: -2.3384600059247913\nreward: 0.9592315554618835\ncompatibility: 7.628000000000002\naccessibility: -2.433295557617208\nreward: 1.1528019905090332\ncompatibility: 7.705000000000002\naccessibility: -2.1841898353406806\nreward: 1.2319117784500122\ncompatibility: 7.599499999999999\naccessibility: -2.0090073655794853\nreward: 0.946439266204834\ncompatibility: 7.35\naccessibility: -2.3035554178494215\nreward: 1.103747010231018\ncompatibility: 7.5365\naccessibility: -2.1675044822444516\nreward: 0.8518032431602478\ncompatibility: 7.5355\naccessibility: -2.5448844578960315\nreward: 0.8333185315132141\ncompatibility: 7.441\naccessibility: -2.5219864792464413\nreward: 1.0300755500793457\ncompatibility: 7.457000000000002\naccessibility: -2.235422369172937\nreward: 1.1517213582992554\ncompatibility: 7.529500000000002\naccessibility: -2.0917930257473394\nreward: 0.7961810827255249\ncompatibility: 7.610000000000002\naccessibility: -2.6682284108608267\nreward: 0.9162200689315796\ncompatibility: 7.704999999999999\naccessibility: -2.5390627857047234\nreward: 1.0127962827682495\ncompatibility: 7.785500000000002\naccessibility: -2.437323373672463\nreward: 0.9538173675537109\ncompatibility: 7.453\naccessibility: -2.347666771049874\nreward: 0.8446305990219116\ncompatibility: 7.514500000000001\naccessibility: -2.544393398663866\nreward: 1.019091010093689\ncompatibility: 7.625\naccessibility: -2.3418991707723817\nreward: 0.997869610786438\ncompatibility: 7.5329999999999995\naccessibility: -2.324445571256268\nreward: 0.9500383138656616\ncompatibility: 7.453999999999997\naccessibility: -2.3538710671786496\nreward: 1.0240232944488525\ncompatibility: 7.6085\naccessibility: -2.3256615726674967\nreward: 0.8855868577957153\ncompatibility: 7.584\naccessibility: -2.5201911455403065\nreward: 0.936257004737854\ncompatibility: 7.638500000000001\naccessibility: -2.4733823933448744\nreward: 0.9082434773445129\ncompatibility: 7.553999999999999\naccessibility: -2.470134825317183\nreward: 0.8638705611228943\ncompatibility: 7.5225\naccessibility: -2.519819140028573\nreward: 0.9450052380561829\ncompatibility: 7.6385000000000005\naccessibility: -2.4602599833797383\nreward: 1.081498622894287\ncompatibility: 7.674000000000002\naccessibility: -2.2745377294747517\nreward: 0.9861125946044922\ncompatibility: 7.416000000000001\naccessibility: -2.2794025477197644\nreward: 0.7134125232696533\ncompatibility: 7.199000000000001\naccessibility: -2.5722026344743005\nreward: 0.9957054257392883\ncompatibility: 7.639000000000003\naccessibility: -2.3844776150213125\nreward: 0.8643638491630554\ncompatibility: 7.289999999999999\naccessibility: -2.3945256866820763\nreward: 1.036481261253357\ncompatibility: 7.579499999999999\naccessibility: -2.2914388877671232\nreward: 0.7301927208900452\ncompatibility: 7.266\naccessibility: -2.5829252168729107\nreward: 0.9744398593902588\ncompatibility: 7.3565\naccessibility: -2.2650366043251076\nreward: 0.8610398769378662\ncompatibility: 7.4670000000000005\naccessibility: -2.4943330125436805\nreward: 0.8577677607536316\ncompatibility: 7.454\naccessibility: -2.492276939621417\nreward: 0.9441198706626892\ncompatibility: 7.481\naccessibility: -2.3772130850301902\nreward: 1.1088759899139404\ncompatibility: 7.425500000000003\naccessibility: -2.100346743380694\nreward: 0.7973474860191345\ncompatibility: 7.532999999999999\naccessibility: -2.625228749478791\nreward: 1.0527700185775757\ncompatibility: 7.8370000000000015\naccessibility: -2.4049521552476567\nreward: 1.1593598127365112\ncompatibility: 7.4585\naccessibility: -2.042299490368501\nreward: 0.7512226700782776\ncompatibility: 7.529000000000001\naccessibility: -2.692273093445261\nreward: 1.211850643157959\ncompatibility: 7.566500000000001\naccessibility: -2.021420487758995\nreward: 0.9811056852340698\ncompatibility: 7.474500000000001\naccessibility: -2.318252212963984\nreward: 0.7646554708480835\ncompatibility: 7.322499999999999\naccessibility: -2.5614989181698737\nreward: 0.939090371131897\ncompatibility: 7.559\naccessibility: -2.42654297895311\nreward: 0.79917973279953\ncompatibility: 7.3294999999999995\naccessibility: -2.5134625337855443\nreward: 1.0368757247924805\ncompatibility: 7.685500000000003\naccessibility: -2.3476328827853488\nreward: 1.2102020978927612\ncompatibility: 7.597499999999999\naccessibility: -2.0405004990072424\nreward: 1.0231009721755981\ncompatibility: 7.579000000000001\naccessibility: -2.311241380624305\nreward: 1.152566909790039\ncompatibility: 7.661500000000001\naccessibility: -2.161238965347366\nreward: 1.0536963939666748\ncompatibility: 7.612\naccessibility: -2.2830267893615126\nreward: 1.179075837135315\ncompatibility: 7.524499999999997\naccessibility: -2.0480827392584766\nreward: 1.1864337921142578\ncompatibility: 7.738000000000002\naccessibility: -2.151420731074633\nreward: 0.7666475772857666\ncompatibility: 7.3585\naccessibility: -2.5777965259897537\nreward: 0.9826068878173828\ncompatibility: 7.391\naccessibility: -2.2712682157515975\nreward: 0.9426468014717102\ncompatibility: 7.683\naccessibility: -2.4876369197612296\nreward: 0.8688406944274902\ncompatibility: 7.610499999999999\naccessibility: -2.5595068164685473\nreward: 0.820094883441925\ncompatibility: 7.552\naccessibility: -2.601286276789299\nreward: 1.3086771965026855\ncompatibility: 7.692500000000003\naccessibility: -1.943680643160501\nreward: 0.8041195869445801\ncompatibility: 7.2414999999999985\naccessibility: -2.458909909242681\nreward: 0.916423499584198\ncompatibility: 7.490500000000001\naccessibility: -2.4238469279248283\nreward: 0.9914950132369995\ncompatibility: 7.36\naccessibility: -2.2413289251228887\nreward: 0.8074905276298523\ncompatibility: 7.388000000000001\naccessibility: -2.5323356743941767\nreward: 1.055962085723877\ncompatibility: 7.394499999999999\naccessibility: -2.1631103932057134\nreward: 0.8383951187133789\ncompatibility: 7.536499999999998\naccessibility: -2.5655323441248217\nreward: 0.879493772983551\ncompatibility: 7.469000000000001\naccessibility: -2.4677236024755453\nreward: 1.1063339710235596\ncompatibility: 7.618\naccessibility: -2.207284743600388\nreward: 0.9567121863365173\ncompatibility: 7.650500000000001\naccessibility: -2.4491281600098294\nreward: 0.754082977771759\ncompatibility: 7.411999999999999\naccessibility: -2.6253041456115773\nreward: 1.0477643013000488\ncompatibility: 7.3580000000000005\naccessibility: -2.1558535722134784\nreward: 0.76068514585495\ncompatibility: 7.3345\naccessibility: -2.5738829814601134\nreward: 1.0385910272598267\ncompatibility: 7.4375\naccessibility: -2.21220277280241\nreward: 0.7920872569084167\ncompatibility: 7.525000000000002\naccessibility: -2.6288333612125383\nreward: 0.9820055365562439\ncompatibility: 7.6785000000000005\naccessibility: -2.426188119651658\nreward: 0.926760196685791\ncompatibility: 7.616500000000001\naccessibility: -2.475841825438557\nreward: 1.1036266088485718\ncompatibility: 7.664500000000001\naccessibility: -2.2362565604423073\nreward: 0.8545371294021606\ncompatibility: 7.571000000000001\naccessibility: -2.559801485011932\nreward: 1.0232266187667847\ncompatibility: 7.573499999999999\naccessibility: -2.3081064334880743\nreward: 1.0447382926940918\ncompatibility: 7.5360000000000005\naccessibility: -2.2557496219525897\nreward: 0.8506911993026733\ncompatibility: 7.337000000000001\naccessibility: -2.440213235645264\nreward: 1.0434738397598267\ncompatibility: 7.5169999999999995\naccessibility: -2.247467770434937\nreward: 1.0238559246063232\ncompatibility: 7.815500000000002\naccessibility: -2.436805371587408\nreward: 1.0765949487686157\ncompatibility: 7.394500000000001\naccessibility: -2.132161204403765\nreward: 0.7819889187812805\ncompatibility: 7.337999999999998\naccessibility: -2.5438023593627825\nreward: 0.7775624394416809\ncompatibility: 7.406500000000001\naccessibility: -2.587138491669969\nreward: 0.9429349899291992\ncompatibility: 7.353000000000001\naccessibility: -2.3104189021738426\nreward: 1.0235443115234375\ncompatibility: 7.592500000000001\naccessibility: -2.3178086024339923\nreward: 1.2117226123809814\ncompatibility: 7.612000000000003\naccessibility: -2.0459875141902693\nreward: 1.069744348526001\ncompatibility: 7.429000000000002\naccessibility: -2.1609192035943856\nreward: 1.155036449432373\ncompatibility: 7.725\naccessibility: -2.1915525207229893\nreward: 1.2440056800842285\ncompatibility: 7.671500000000002\naccessibility: -2.0294378288330464\nreward: 0.9741055965423584\ncompatibility: 7.337500000000002\naccessibility: -2.255359423699067\nreward: 1.0199137926101685\ncompatibility: 7.6365\naccessibility: -2.3468256837617614\nreward: 1.3600873947143555\ncompatibility: 7.8205000000000044\naccessibility: -1.9351366889703927\nreward: 0.8571562170982361\ncompatibility: 7.396000000000003\naccessibility: -2.4621228528591717\nreward: 0.7978243827819824\ncompatibility: 7.421500000000002\naccessibility: -2.5647812971509616\nreward: 0.8524436354637146\ncompatibility: 7.439499999999998\naccessibility: -2.492495301184842\nreward: 1.0360667705535889\ncompatibility: 7.702500000000002\naccessibility: -2.3579534893070835\nreward: 0.8798433542251587\ncompatibility: 7.303\naccessibility: -2.3782707101055593\nreward: 1.0632966756820679\ncompatibility: 7.793000000000002\naccessibility: -2.3655906697729296\nreward: 0.8494241237640381\ncompatibility: 7.4145\naccessibility: -2.4836317069333473\nreward: 1.0176031589508057\ncompatibility: 7.372999999999999\naccessibility: -2.209130957916012\nreward: 1.0620617866516113\ncompatibility: 7.541999999999998\naccessibility: -2.2329788120396183\nreward: 0.8545165061950684\ncompatibility: 7.323999999999999\naccessibility: -2.4275109708511433\nreward: 0.9786467552185059\ncompatibility: 7.683499999999998\naccessibility: -2.433904844096075\nreward: 1.027174949645996\ncompatibility: 7.494500000000003\naccessibility: -2.2598625902088676\nreward: 1.052211880683899\ncompatibility: 7.513999999999999\naccessibility: -2.232753541173342\nreward: 1.203677773475647\ncompatibility: 7.687000000000003\naccessibility: -2.098233403704622\nreward: 0.9959147572517395\ncompatibility: 7.4335\naccessibility: -2.274074263689565\nreward: 1.0126259326934814\ncompatibility: 7.528499999999999\naccessibility: -2.29990039418348\nreward: 1.1208759546279907\ncompatibility: 7.601500000000001\naccessibility: -2.176632429090658\nreward: 0.7973324060440063\ncompatibility: 7.5105\naccessibility: -2.613197834778192\nreward: 0.6947500109672546\ncompatibility: 7.295999999999999\naccessibility: -2.6521606882007807\nreward: 0.9250393509864807\ncompatibility: 7.507000000000001\naccessibility: -2.4197624435906064\nreward: 0.8586320281028748\ncompatibility: 7.314500000000001\naccessibility: -2.4162483766333143\nreward: 1.0043494701385498\ncompatibility: 7.5225\naccessibility: -2.3091008647652456\nreward: 0.9464633464813232\ncompatibility: 7.722500000000004\naccessibility: -2.503072865051494\nreward: 0.984398603439331\ncompatibility: 7.507500000000002\naccessibility: -2.3309913923053305\nreward: 0.9621383547782898\ncompatibility: 7.644500000000001\naccessibility: -2.4377745976206837\nreward: 0.7936780452728271\ncompatibility: 7.340999999999999\naccessibility: -2.527875775554628\nreward: 0.9870567321777344\ncompatibility: 7.5920000000000005\naccessibility: -2.372272088792964\nreward: 0.7620528340339661\ncompatibility: 7.474999999999999\naccessibility: -2.6470992838570924\nreward: 1.0424985885620117\ncompatibility: 7.684000000000002\naccessibility: -2.3383950423698865\nreward: 0.69415682554245\ncompatibility: 7.418499999999999\naccessibility: -2.718675513953529\nreward: 1.0422831773757935\ncompatibility: 7.216\naccessibility: -2.088003770892557\nreward: 0.988105058670044\ncompatibility: 7.212999999999999\naccessibility: -2.167663879768744\nreward: 1.0090718269348145\ncompatibility: 7.573\naccessibility: -2.329070826766864\nreward: 0.9572452902793884\ncompatibility: 7.518\naccessibility: -2.37734639127064\nreward: 0.649270236492157\ncompatibility: 7.2025000000000015\naccessibility: -2.670291094136358\nreward: 0.9000911712646484\ncompatibility: 7.608999999999997\naccessibility: -2.511827530437347\nreward: 0.9044948220252991\ncompatibility: 7.279499999999998\naccessibility: -2.3287041529963415\nreward: 0.9967228770256042\ncompatibility: 7.513000000000002\naccessibility: -2.3154514124035797\nreward: 1.0556421279907227\ncompatibility: 7.4510000000000005\naccessibility: -2.193858294815741\nreward: 0.970187783241272\ncompatibility: 7.440499999999998\naccessibility: -2.316414735080218\nreward: 0.9552279710769653\ncompatibility: 7.63\naccessibility: -2.44037233738292\nreward: 0.9589917659759521\ncompatibility: 7.338000000000002\naccessibility: -2.278298094485344\nreward: 1.3142794370651245\ncompatibility: 7.724\naccessibility: -1.9521523463948802\nreward: 0.8358237147331238\ncompatibility: 7.305500000000002\naccessibility: -2.4456394229297422\nreward: 0.94879150390625\ncompatibility: 7.449000000000001\naccessibility: -2.3530627786308584\nreward: 1.0055043697357178\ncompatibility: 7.3420000000000005\naccessibility: -2.210671986325559\nreward: 0.7418345212936401\ncompatibility: 7.169499999999999\naccessibility: -2.513766118906002\nreward: 1.028818130493164\ncompatibility: 7.536000000000002\naccessibility: -2.279629883417609\nreward: 0.9432520866394043\ncompatibility: 7.567\naccessibility: -2.424586169276453\nreward: 0.9138349890708923\ncompatibility: 7.636500000000001\naccessibility: -2.5059439378136634\nreward: 0.8978947997093201\ncompatibility: 7.456\naccessibility: -2.433157759861407\nreward: 0.6320123672485352\ncompatibility: 7.350499999999999\naccessibility: -2.7754636025281183\nreward: 1.087618112564087\ncompatibility: 7.548999999999999\naccessibility: -2.198394175475655\nreward: 0.86503666639328\ncompatibility: 7.471999999999998\naccessibility: -2.4910164679839277\nreward: 1.0000234842300415\ncompatibility: 7.365\naccessibility: -2.2312147937739577\nreward: 0.8718963861465454\ncompatibility: 7.3694999999999995\naccessibility: -2.425816110196083\nreward: 0.934914767742157\ncompatibility: 7.449999999999999\naccessibility: -2.3744135684120318\nreward: 1.027850866317749\ncompatibility: 7.650999999999998\naccessibility: -2.3426879032759325\nreward: 1.0146442651748657\ncompatibility: 7.487999999999997\naccessibility: -2.2751764370723184\nreward: 1.130393385887146\ncompatibility: 7.3980000000000015\naccessibility: -2.0533385236347925\nreward: 1.0436307191848755\ncompatibility: 7.5405\naccessibility: -2.2598218185789336\nreward: 1.1681417226791382\ncompatibility: 7.6465\naccessibility: -2.129840935630017\nreward: 0.9398819208145142\ncompatibility: 7.397500000000003\naccessibility: -2.3388377943113046\nreward: 1.0633128881454468\ncompatibility: 7.62\naccessibility: -2.2728877220516748\nreward: 1.0166500806808472\ncompatibility: 7.469999999999999\naccessibility: -2.2625249653037938\nreward: 1.1714403629302979\ncompatibility: 7.612499999999999\naccessibility: -2.106678781986201\nreward: 1.0098745822906494\ncompatibility: 7.411500000000003\naccessibility: -2.2413489106799767\nreward: 0.8455051183700562\ncompatibility: 7.445499999999998\naccessibility: -2.5061173042879945\nreward: 0.9828771352767944\ncompatibility: 7.415499999999997\naccessibility: -2.283987895934706\nreward: 1.0797961950302124\ncompatibility: 7.655500000000005\naccessibility: -2.2671807776142248\nreward: 0.8739237189292908\ncompatibility: 7.465500000000004\naccessibility: -2.4742037367669507\nreward: 1.0265470743179321\ncompatibility: 7.646500000000004\naccessibility: -2.342233028912196\nreward: 0.9745419025421143\ncompatibility: 7.379\naccessibility: -2.2769371766180373\nreward: 1.0086714029312134\ncompatibility: 7.352499999999999\naccessibility: -2.211546551721149\nreward: 0.8783597946166992\ncompatibility: 7.382999999999999\naccessibility: -2.4233531959205115\nreward: 0.8566800355911255\ncompatibility: 7.613\naccessibility: -2.5790870760731366\nreward: 0.7231115698814392\ncompatibility: 7.354499999999999\naccessibility: -2.6409576773319556\nreward: 0.8451240658760071\ncompatibility: 7.422000000000002\naccessibility: -2.494099625723366\nreward: 1.1570909023284912\ncompatibility: 7.673000000000002\naccessibility: -2.160613716010809\nreward: 0.8712843656539917\ncompatibility: 7.408500000000001\naccessibility: -2.4476270581735933\nreward: 0.9657375812530518\ncompatibility: 7.385499999999999\naccessibility: -2.2936257929445922\nreward: 1.0106449127197266\ncompatibility: 7.5714999999999995\naccessibility: -2.325907638105226\nreward: 1.170539140701294\ncompatibility: 7.658\naccessibility: -2.132405561052639\nreward: 0.8222557902336121\ncompatibility: 7.483999999999996\naccessibility: -2.5616163227309166\nreward: 1.0942623615264893\ncompatibility: 7.584999999999997\naccessibility: -2.207713576083629\nreward: 0.8976888656616211\ncompatibility: 7.192000000000002\naccessibility: -2.2920380917254284\nreward: 0.854468584060669\ncompatibility: 7.238000000000005\naccessibility: -2.381511392697866\nreward: 0.907952070236206\ncompatibility: 7.474\naccessibility: -2.427714765850757\nreward: 1.0442745685577393\ncompatibility: 7.602\naccessibility: -2.291802403529112\nreward: 1.061257243156433\ncompatibility: 7.603\naccessibility: -2.2668641885387855\nreward: 0.8987224698066711\ncompatibility: 7.372000000000001\naccessibility: -2.386916332040287\nreward: 0.9071952104568481\ncompatibility: 7.596500000000001\naccessibility: -2.4944750333380368\nreward: 0.6145608425140381\ncompatibility: 7.471999999999998\naccessibility: -2.8667301669620127\nreward: 0.9198744297027588\ncompatibility: 7.649\naccessibility: -2.50358121780023\nreward: 1.2984294891357422\ncompatibility: 7.786000000000001\naccessibility: -2.009141495800363\nreward: 1.0219979286193848\ncompatibility: 7.434499999999997\naccessibility: -2.2354852684058946\nreward: 1.1422775983810425\ncompatibility: 7.714500000000004\naccessibility: -2.2050658226710222\nreward: 0.8593841791152954\ncompatibility: 7.526999999999999\naccessibility: -2.528959419403966\nreward: 1.2182331085205078\ncompatibility: 7.607999999999999\naccessibility: -2.034078943721059\nreward: 0.8700578212738037\ncompatibility: 7.32\naccessibility: -2.402056111214114\nreward: 1.0741170644760132\ncompatibility: 7.573\naccessibility: -2.2315030103087943\nreward: 0.9039292335510254\ncompatibility: 7.594499999999999\naccessibility: -2.4983025684261486\nreward: 1.2387028932571411\ncompatibility: 7.672999999999998\naccessibility: -2.0381955766651867\nreward: 1.034746766090393\ncompatibility: 7.403000000000002\naccessibility: -2.199487026596226\nreward: 0.9586330056190491\ncompatibility: 7.619500000000001\naccessibility: -2.4296398190218653\nreward: 0.8400624990463257\ncompatibility: 7.571500000000002\naccessibility: -2.5817812374882756\nreward: 0.8850288391113281\ncompatibility: 7.561499999999999\naccessibility: -2.508974599792066\nreward: 1.2323380708694458\ncompatibility: 7.554500000000002\naccessibility: -1.9842607160170083\nreward: 0.7078921794891357\ncompatibility: 7.364499999999998\naccessibility: -2.669143914476393\nreward: 0.9679743647575378\ncompatibility: 7.324500000000001\naccessibility: -2.257592031727849\nreward: 1.001456379890442\ncompatibility: 7.687500000000003\naccessibility: -2.4018332598580847\nreward: 1.124332308769226\ncompatibility: 7.416000000000001\naccessibility: -2.0720728777375204\nreward: 1.1889644861221313\ncompatibility: 7.482\naccessibility: -2.0104818483806937\nreward: 0.8838219046592712\ncompatibility: 7.373500000000002\naccessibility: -2.4100706822657223\nreward: 1.1568914651870728\ncompatibility: 7.666000000000001\naccessibility: -2.157162810180509\nreward: 0.9248536229133606\ncompatibility: 7.707500000000002\naccessibility: -2.527451707152162\nreward: 0.8377557992935181\ncompatibility: 7.288\naccessibility: -2.4333663274180495\nreward: 0.9477445483207703\ncompatibility: 7.367499999999999\naccessibility: -2.310972485495183\nreward: 0.9158512949943542\ncompatibility: 7.267500000000002\naccessibility: -2.30524088864885\nreward: 0.849062442779541\ncompatibility: 7.297500000000001\naccessibility: -2.421495612213022\nreward: 0.9542776942253113\ncompatibility: 7.594000000000002\naccessibility: -2.422512071234791\nreward: 0.966568112373352\ncompatibility: 7.0775\naccessibility: -2.1273799867361163\nreward: 0.7645829319953918\ncompatibility: 7.536500000000005\naccessibility: -2.676250585970827\nreward: 1.0690268278121948\ncompatibility: 7.4775\naccessibility: -2.1879775927574037\nreward: 0.9799410700798035\ncompatibility: 7.646\naccessibility: -2.411874115699613\nreward: 1.1390762329101562\ncompatibility: 7.678000000000002\naccessibility: -2.1903141559481707\nreward: 0.8532377481460571\ncompatibility: 7.412999999999998\naccessibility: -2.477107678772059\nreward: 1.1844518184661865\ncompatibility: 7.779000000000002\naccessibility: -2.1763579792230856\nreward: 0.8277438282966614\ncompatibility: 7.308\naccessibility: -2.4590985816409665\nreward: 0.8406120538711548\ncompatibility: 7.609499999999998\naccessibility: -2.601314092886262\nreward: 1.0245397090911865\ncompatibility: 7.480000000000001\naccessibility: -2.256047616977984\nreward: 1.0985430479049683\ncompatibility: 7.5475\naccessibility: -2.181203373836679\nreward: 1.095777153968811\ncompatibility: 7.641999999999999\naccessibility: -2.235977118283207\nreward: 1.128035306930542\ncompatibility: 7.699000000000001\naccessibility: -2.218125644651962\nreward: 1.1641132831573486\ncompatibility: 7.537000000000003\naccessibility: -2.0772229273303715\nreward: 1.0977673530578613\ncompatibility: 7.506\naccessibility: -2.160134599548524\nreward: 0.9362804293632507\ncompatibility: 7.5\naccessibility: -2.3991508208254553\nreward: 1.0704689025878906\ncompatibility: 7.690500000000004\naccessibility: -2.2999215830541506\nreward: 0.8531072735786438\ncompatibility: 7.507999999999997\naccessibility: -2.5281962718855375\nreward: 1.0748291015625\ncompatibility: 7.5284999999999975\naccessibility: -2.2065956559485382\nreward: 1.1232671737670898\ncompatibility: 7.446999999999998\naccessibility: -2.0902778353435427\nreward: 0.7017140984535217\ncompatibility: 7.44\naccessibility: -2.718857416913614\nreward: 1.048198938369751\ncompatibility: 7.6305000000000005\naccessibility: -2.301183683665999\nreward: 1.103508472442627\ncompatibility: 7.634999999999998\naccessibility: -2.220630112869409\nreward: 1.006635308265686\ncompatibility: 7.467500000000002\naccessibility: -2.2762077963731264\nreward: 0.9760770201683044\ncompatibility: 7.587000000000001\naccessibility: -2.3860630233383464\nreward: 0.9465511441230774\ncompatibility: 7.450999999999999\naccessibility: -2.357494738970826\nreward: 1.0891581773757935\ncompatibility: 7.419\naccessibility: -2.126441235061188\nreward: 1.0685346126556396\ncompatibility: 7.4645\naccessibility: -2.1817516758545574\nreward: 0.6882080435752869\ncompatibility: 7.381500000000001\naccessibility: -2.7077772106907654\nreward: 1.0918288230895996\ncompatibility: 7.5815\naccessibility: -2.209488975220004\nreward: 0.8691579699516296\ncompatibility: 7.3655\naccessibility: -2.427780938874265\nreward: 1.1570932865142822\ncompatibility: 7.419500000000002\naccessibility: -2.0248065547769474\nreward: 1.0197129249572754\ncompatibility: 7.608\naccessibility: -2.331859255648366\nreward: 0.9077099561691284\ncompatibility: 7.6145000000000005\naccessibility: -2.5033458057280766\nreward: 0.8462550044059753\ncompatibility: 7.423500000000001\naccessibility: -2.493206815703487\nreward: 0.8846980929374695\ncompatibility: 7.864\naccessibility: -2.6715243087522262\nreward: 1.1806704998016357\ncompatibility: 7.523499999999997\naccessibility: -2.0451549339574524\nreward: 1.1041793823242188\ncompatibility: 7.673500000000001\naccessibility: -2.2402487111586056\nreward: 0.9638849496841431\ncompatibility: 7.341000000000001\naccessibility: -2.272565404736511\nreward: 1.0460456609725952\ncompatibility: 7.482499999999999\naccessibility: -2.225128015023622\nreward: 0.9846425652503967\ncompatibility: 7.489000000000002\naccessibility: -2.320714744348155\nreward: 0.8714364767074585\ncompatibility: 7.4795000000000025\naccessibility: -2.4854345735290027\nreward: 0.9815219044685364\ncompatibility: 7.456\naccessibility: -2.3077171666394887\nreward: 0.8900189995765686\ncompatibility: 7.569500000000001\naccessibility: -2.5057750683630964\nreward: 0.8543815016746521\ncompatibility: 7.4235000000000015\naccessibility: -2.481017026664768\nreward: 1.012103796005249\ncompatibility: 7.614\naccessibility: -2.346487156044595\nreward: 1.021185040473938\ncompatibility: 7.521999999999999\naccessibility: -2.283579598632632\nreward: 0.9572283625602722\ncompatibility: 7.620000000000001\naccessibility: -2.432014640285583\nreward: 0.7753306031227112\ncompatibility: 7.693499999999999\naccessibility: -2.7442362311239714\nreward: 1.1101124286651611\ncompatibility: 7.702999999999999\naccessibility: -2.2471527948707664\nreward: 0.8388940691947937\ncompatibility: 7.305499999999999\naccessibility: -2.4410338742846003\nreward: 0.9988561272621155\ncompatibility: 7.496000000000001\naccessibility: -2.3031444056258263\nreward: 0.9706028699874878\ncompatibility: 7.5665\naccessibility: -2.3832921309618027\nreward: 1.162989854812622\ncompatibility: 7.547\naccessibility: -2.084265157432079\nreward: 0.8364430665969849\ncompatibility: 7.430500000000002\naccessibility: -2.511674656926218\nreward: 0.8601699471473694\ncompatibility: 7.261499999999999\naccessibility: -2.385548685891001\nreward: 1.011414885520935\ncompatibility: 7.542999999999998\naccessibility: -2.3094848791216895\nreward: 0.890470027923584\ncompatibility: 7.581499999999998\naccessibility: -2.511527062187821\nreward: 0.8862044215202332\ncompatibility: 7.475000000000002\naccessibility: -2.460871904000133\nreward: 1.1587575674057007\ncompatibility: 7.6080000000000005\naccessibility: -2.1232922879556178\nreward: 0.7604953646659851\ncompatibility: 7.2895\naccessibility: -2.5500605116482378\nreward: 0.819414496421814\ncompatibility: 7.461500000000002\naccessibility: -2.553824653190591\nreward: 1.1891562938690186\ncompatibility: 7.382500000000001\naccessibility: -1.9568905211854113\nreward: 0.982195258140564\ncompatibility: 7.536500000000001\naccessibility: -2.3498321335155534\nreward: 1.1077711582183838\ncompatibility: 7.703000000000003\naccessibility: -2.2506647299584066\nreward: 0.9303659796714783\ncompatibility: 7.453000000000003\naccessibility: -2.382843857343762\nreward: 0.9014002084732056\ncompatibility: 7.537999999999998\naccessibility: -2.471828258582531\nreward: 0.9282368421554565\ncompatibility: 7.5475\naccessibility: -2.4366625564039506\nreward: 1.146103024482727\ncompatibility: 7.509000000000002\naccessibility: -2.089238338233976\nreward: 1.0725412368774414\ncompatibility: 7.592500000000002\naccessibility: -2.2443131946676824\nreward: 0.7594452500343323\ncompatibility: 7.449000000000001\naccessibility: -2.637082121488973\nreward: 0.7516977787017822\ncompatibility: 7.252000000000001\naccessibility: -2.5431675742773967\nreward: 1.3047423362731934\ncompatibility: 8.024000000000004\naccessibility: -2.1271722243918076\nreward: 0.8237167000770569\ncompatibility: 7.488999999999999\naccessibility: -2.562103525086817\nreward: 0.9043470025062561\ncompatibility: 7.693999999999999\naccessibility: -2.550979474517164\nreward: 0.85580974817276\ncompatibility: 7.294499999999999\naccessibility: -2.4097675432673924\nreward: 1.2294520139694214\ncompatibility: 7.6125\naccessibility: -2.0196612718756377\nreward: 0.8853323459625244\ncompatibility: 7.5455\naccessibility: -2.499947908944205\nreward: 0.9956110119819641\ncompatibility: 7.749500000000001\naccessibility: -2.443815635817474\nreward: 1.0143500566482544\ncompatibility: 7.834000000000003\naccessibility: -2.4609749670841228\nreward: 1.0346606969833374\ncompatibility: 7.724999999999999\naccessibility: -2.3721161301381044\nreward: 0.9074541330337524\ncompatibility: 7.653999999999997\naccessibility: -2.5248902262113835\nreward: 0.8413069844245911\ncompatibility: 7.388000000000001\naccessibility: -2.481610923347494\nreward: 1.1519855260849\ncompatibility: 7.351000000000002\naccessibility: -1.9957717445279872\nreward: 0.78309565782547\ncompatibility: 7.271000000000002\naccessibility: -2.5062493877254726\nreward: 0.9637897610664368\ncompatibility: 7.373000000000003\naccessibility: -2.289851035713311\nreward: 1.003470540046692\ncompatibility: 7.507999999999998\naccessibility: -2.3026514220258107\nreward: 0.4170990586280823\ncompatibility: 7.3195000000000014\naccessibility: -3.081226426488405\nreward: 0.8128481507301331\ncompatibility: 7.423000000000003\naccessibility: -2.543049195400366\nreward: 1.2287079095840454\ncompatibility: 7.388999999999999\naccessibility: -1.901045314436071\nreward: 1.109027624130249\ncompatibility: 7.6485\naccessibility: -2.219583626571705\nreward: 0.8203081488609314\ncompatibility: 7.406499999999999\naccessibility: -2.5230199562706486\nreward: 1.0542579889297485\ncompatibility: 7.5520000000000005\naccessibility: -2.250041563758184\nreward: 1.086647868156433\ncompatibility: 7.633000000000003\naccessibility: -2.24484964116463\nreward: 0.9282609224319458\ncompatibility: 7.626999999999999\naccessibility: -2.47921576451795\nreward: 0.724908173084259\ncompatibility: 7.411000000000001\naccessibility: -2.6685305564717767\nreward: 1.1136717796325684\ncompatibility: 7.532\naccessibility: -2.1502065889673228\nreward: 0.982495903968811\ncompatibility: 7.273000000000001\naccessibility: -2.2082204613593275\nreward: 1.2786333560943604\ncompatibility: 7.726499999999999\naccessibility: -2.0069606562220024\nreward: 1.2444701194763184\ncompatibility: 7.730000000000002\naccessibility: -2.0600806073837017\nreward: 0.7574208378791809\ncompatibility: 7.456000000000001\naccessibility: -2.643868769416579\nreward: 1.040363073348999\ncompatibility: 7.565\naccessibility: -2.2778481938730217\nreward: 1.1434928178787231\ncompatibility: 7.4304999999999986\naccessibility: -2.0511001381664946\nreward: 1.084511399269104\ncompatibility: 7.607\naccessibility: -2.2341257199081626\nreward: 0.6279421448707581\ncompatibility: 7.317500000000001\naccessibility: -2.763890392171932\nreward: 1.1462002992630005\ncompatibility: 7.595\naccessibility: -2.1351637783928075\nreward: 0.8052719831466675\ncompatibility: 7.499500000000001\naccessibility: -2.5953956239731983\nreward: 1.110064148902893\ncompatibility: 7.721\naccessibility: -2.256868134947335\nreward: 0.8934236764907837\ncompatibility: 7.495000000000001\naccessibility: -2.460757366487189\nreward: 0.7815082669258118\ncompatibility: 7.456499999999998\naccessibility: -2.608005490076621\nreward: 0.9995362162590027\ncompatibility: 7.4639999999999995\naccessibility: -2.2849813862794104\nreward: 1.0971627235412598\ncompatibility: 7.5925\naccessibility: -2.207380841792685\nreward: 0.8831886053085327\ncompatibility: 7.495000000000002\naccessibility: -2.4761099430862865\nreward: 0.9271487593650818\ncompatibility: 7.3825\naccessibility: -2.3499018250539763\nreward: 1.2032012939453125\ncompatibility: 7.686\naccessibility: -2.0984123559072034\nreward: 0.8368496298789978\ncompatibility: 7.544499999999999\naccessibility: -2.572136289298264\nreward: 0.9153181314468384\ncompatibility: 7.498500000000004\naccessibility: -2.429790699541221\nreward: 1.0108143091201782\ncompatibility: 7.6385000000000005\naccessibility: -2.361546358826915\nreward: 0.9210859537124634\ncompatibility: 7.352000000000001\naccessibility: -2.342656752471093\nreward: 0.7928289175033569\ncompatibility: 7.411999999999999\naccessibility: -2.5671852030087687\nreward: 0.8281724452972412\ncompatibility: 7.757000000000004\naccessibility: -2.6989913246544397\nreward: 0.8534082770347595\ncompatibility: 7.488499999999999\naccessibility: -2.5172982725027877\nreward: 1.0590484142303467\ncompatibility: 7.4205000000000005\naccessibility: -2.1724095196468607\nreward: 0.6892760992050171\ncompatibility: 7.5280000000000005\naccessibility: -2.784657285448097\nreward: 0.9079641699790955\ncompatibility: 7.351500000000001\naccessibility: -2.362071628754306\nreward: 0.884819746017456\ncompatibility: 7.620500000000001\naccessibility: -2.5408953848950913\nreward: 0.8386113047599792\ncompatibility: 7.593999999999999\naccessibility: -2.596011613401341\nreward: 0.8249055743217468\ncompatibility: 7.577000000000002\naccessibility: -2.60746302765341\nreward: 1.1417042016983032\ncompatibility: 7.533999999999997\naccessibility: -2.1092294746268596\nreward: 0.9349378347396851\ncompatibility: 7.540500000000001\naccessibility: -2.4228610701051494\nreward: 0.9293752312660217\ncompatibility: 7.5855\naccessibility: -2.455312108410508\nreward: 1.0110599994659424\ncompatibility: 7.6400000000000015\naccessibility: -2.361981438322045\nreward: 0.8739210367202759\ncompatibility: 7.326000000000004\naccessibility: -2.39947560594505\nreward: 0.669829249382019\ncompatibility: 7.538000000000002\naccessibility: -2.8191847366897083\nreward: 0.9234602451324463\ncompatibility: 7.422999999999999\naccessibility: -2.3771310643545047\nreward: 1.09884774684906\ncompatibility: 7.6370000000000005\naccessibility: -2.2286927331674904\nreward: 0.7296410202980042\ncompatibility: 7.351\naccessibility: -2.629288434224498\nreward: 1.234029769897461\ncompatibility: 7.7555\naccessibility: -2.0894017098308493\nreward: 1.0647006034851074\ncompatibility: 7.7005\naccessibility: -2.313931163224365\nreward: 1.0502461194992065\ncompatibility: 7.499000000000002\naccessibility: -2.22766647278066\nreward: 0.8390301465988159\ncompatibility: 7.329999999999999\naccessibility: -2.453954744791957\nreward: 0.7637540102005005\ncompatibility: 7.368500000000003\naccessibility: -2.5874940156317394\nreward: 1.1968070268630981\ncompatibility: 7.727499999999999\naccessibility: -2.1302358379445048\nreward: 0.8361250758171082\ncompatibility: 7.6185\naccessibility: -2.6128659855706533\nreward: 0.9942035675048828\ncompatibility: 7.581500000000001\naccessibility: -2.3559268022711377\nreward: 0.9679858088493347\ncompatibility: 7.424499999999999\naccessibility: -2.311146287251141\nreward: 1.111469030380249\ncompatibility: 7.586499999999998\naccessibility: -2.182707134230692\nreward: 0.8202412724494934\ncompatibility: 7.380999999999997\naccessibility: -2.509459537748193\nreward: 0.8514444828033447\ncompatibility: 7.379499999999999\naccessibility: -2.4618511369338836\nreward: 0.9202194213867188\ncompatibility: 7.510999999999999\naccessibility: -2.4291351102831964\nreward: 0.968929648399353\ncompatibility: 7.461500000000002\naccessibility: -2.3295519580057453\nreward: 0.8764049410820007\ncompatibility: 7.495500000000001\naccessibility: -2.4865533176253933\nreward: 1.0143101215362549\ncompatibility: 7.715500000000003\naccessibility: -2.3975527244258155\nreward: 0.9877637028694153\ncompatibility: 7.405\naccessibility: -2.2710330216194605\nreward: 0.9132946133613586\ncompatibility: 7.570999999999999\naccessibility: -2.4716652335256972\nreward: 1.1369506120681763\ncompatibility: 7.792999999999999\naccessibility: -2.2551098071363236\nreward: 0.9863088130950928\ncompatibility: 7.315999999999998\naccessibility: -2.225536750653598\nreward: 0.8705911636352539\ncompatibility: 7.361000000000001\naccessibility: -2.4232204205249\nreward: 0.9487267136573792\ncompatibility: 7.534999999999999\naccessibility: -2.3992313280943534\nreward: 0.9257513880729675\ncompatibility: 7.622500000000002\naccessibility: -2.4805693358886467\nreward: 0.7200007438659668\ncompatibility: 7.281999999999999\naccessibility: -2.6067846007508675\nreward: 1.1392390727996826\ncompatibility: 7.591\naccessibility: -2.143462789326405\nreward: 0.829687237739563\ncompatibility: 7.530000000000001\naccessibility: -2.575112040994776\nreward: 0.8433337807655334\ncompatibility: 7.379500000000002\naccessibility: -2.4740171636631025\nreward: 1.1458909511566162\ncompatibility: 7.811\naccessibility: -2.25134212804524\nreward: 0.8115290999412537\ncompatibility: 7.316500000000001\naccessibility: -2.4879742150267834\nreward: 0.9742026925086975\ncompatibility: 7.431000000000004\naccessibility: -2.3053030990988814\nreward: 1.139266848564148\ncompatibility: 7.703500000000002\naccessibility: -2.2036889395562214\nreward: 1.030055284500122\ncompatibility: 7.605500000000003\naccessibility: -2.3150064358675984\nreward: 0.9171490669250488\ncompatibility: 7.449\naccessibility: -2.4005264165356968\nreward: 0.9216297268867493\ncompatibility: 7.403500000000001\naccessibility: -2.369430406051015\nreward: 0.8848484754562378\ncompatibility: 7.510999999999999\naccessibility: -2.482191551882922\nreward: 0.963387668132782\ncompatibility: 7.859000000000002\naccessibility: -2.550811370355012\nreward: 0.6934993267059326\ncompatibility: 7.471999999999998\naccessibility: -2.7483223945451503\nreward: 1.0101070404052734\ncompatibility: 7.518500000000004\naccessibility: -2.298321510927471\nreward: 0.9973952174186707\ncompatibility: 7.391999999999999\naccessibility: -2.2496214813090045\nreward: 0.9754372835159302\ncompatibility: 7.445\naccessibility: -2.310951189692878\nreward: 1.0126259326934814\ncompatibility: 7.565499999999999\naccessibility: -2.319721859845778\nreward: 0.7478904128074646\ncompatibility: 7.378000000000001\naccessibility: -2.6163786652136096\nreward: 1.210889220237732\ncompatibility: 7.500500000000001\naccessibility: -1.9875054488939448\nreward: 1.17665696144104\ncompatibility: 7.643\naccessibility: -2.1151931884327384\nreward: 0.7800953984260559\ncompatibility: 7.2215\naccessibility: -2.484231929961112\nreward: 0.6982443332672119\ncompatibility: 7.55\naccessibility: -2.7829906611075654\nreward: 0.9868708848953247\ncompatibility: 7.655\naccessibility: -2.4063008289705152\nreward: 0.8138740658760071\ncompatibility: 7.650500000000002\naccessibility: -2.663385286756968\nreward: 0.7496129274368286\ncompatibility: 7.5065000000000035\naccessibility: -2.6826341506185862\nreward: 0.8866260051727295\ncompatibility: 7.4975\naccessibility: -2.4722931416418668\nreward: 0.7729228734970093\ncompatibility: 7.444000000000002\naccessibility: -2.614187081136407\nreward: 0.9505261182785034\ncompatibility: 7.6270000000000024\naccessibility: -2.445817932824575\nreward: 1.026097297668457\ncompatibility: 7.497999999999999\naccessibility: -2.2633539704146113\nreward: 1.1364086866378784\ncompatibility: 7.658999999999999\naccessibility: -2.1841369369305594\nreward: 0.9733092188835144\ncompatibility: 7.449000000000001\naccessibility: -2.316286128883462\nreward: 0.7757607102394104\ncompatibility: 7.404000000000001\naccessibility: -2.5885018097585895\nreward: 0.9644461870193481\ncompatibility: 7.5935000000000015\naccessibility: -2.406991443962584\nreward: 0.8749145269393921\ncompatibility: 7.504000000000001\naccessibility: -2.4933424899633323\nreward: 1.0292365550994873\ncompatibility: 7.5020000000000024\naccessibility: -2.2607880769779785\nreward: 1.0966954231262207\ncompatibility: 7.339000000000002\naccessibility: -2.072278266796984\nreward: 0.8079148530960083\ncompatibility: 7.400000000000001\naccessibility: -2.538127760508521\nreward: 0.9227705001831055\ncompatibility: 7.549000000000001\naccessibility: -2.445665645600801\nreward: 0.9734434485435486\ncompatibility: 7.464000000000001\naccessibility: -2.3241205118581365\nreward: 1.0324093103408813\ncompatibility: 7.459\naccessibility: -2.2329932376747603\nreward: 0.9237926006317139\ncompatibility: 7.552000000000001\naccessibility: -2.4457397075239977\nreward: 0.9633700847625732\ncompatibility: 7.751999999999997\naccessibility: -2.493516317736648\nreward: 0.7341372966766357\ncompatibility: 7.563\naccessibility: -2.7361155240540587\nreward: 0.5702599287033081\ncompatibility: 7.512000000000004\naccessibility: -2.9546101494060633\nreward: 0.6440939903259277\ncompatibility: 7.540500000000003\naccessibility: -2.8591269027556208\nreward: 0.7996696829795837\ncompatibility: 7.448999999999999\naccessibility: -2.5767455096416803\nreward: 0.9098179340362549\ncompatibility: 7.4985\naccessibility: -2.4380409484419503\nreward: 0.8020423054695129\ncompatibility: 7.578000000000002\naccessibility: -2.642293659119232\nreward: 0.877747118473053\ncompatibility: 7.542000000000001\naccessibility: -2.509450789513034\nreward: 1.0038858652114868\ncompatibility: 7.493000000000002\naccessibility: -2.293992605736042\nreward: 0.7684224247932434\ncompatibility: 7.380499999999999\naccessibility: -2.5869199405157985\nreward: 0.8111928701400757\ncompatibility: 7.543999999999998\naccessibility: -2.6103535568640472\nreward: 0.9382928013801575\ncompatibility: 7.540999999999997\naccessibility: -2.418096485348667\nreward: 1.0140068531036377\ncompatibility: 7.765499999999999\naccessibility: -2.4247933546928047\nreward: 0.6257743239402771\ncompatibility: 7.557500000000004\naccessibility: -2.895713537550256\nreward: 1.2265459299087524\ncompatibility: 7.559499999999999\naccessibility: -1.9956275847544365\nreward: 0.9004945755004883\ncompatibility: 7.520000000000001\naccessibility: -2.463543890527711\nreward: 1.0050647258758545\ncompatibility: 7.479000000000003\naccessibility: -2.2847243924536844\nreward: 0.7766546607017517\ncompatibility: 7.506500000000002\naccessibility: -2.642071560779521\nreward: 1.0819733142852783\ncompatibility: 7.629000000000002\naccessibility: -2.249718646405204\nreward: 0.9496055841445923\ncompatibility: 7.416\naccessibility: -2.3341630647302356\nreward: 1.02554452419281\ncompatibility: 7.6585\naccessibility: -2.350165327132682\nreward: 0.9493295550346375\ncompatibility: 7.2905\naccessibility: -2.267344953264049\nreward: 0.9430636167526245\ncompatibility: 7.549000000000001\naccessibility: -2.4152259722402216\nreward: 1.1038709878921509\ncompatibility: 7.604499999999998\naccessibility: -2.2037470327831414\nreward: 0.7754710912704468\ncompatibility: 7.159999999999999\naccessibility: -2.458221963837478\nreward: 1.0024844408035278\ncompatibility: 7.251499999999998\naccessibility: -2.16671973288116\nreward: 0.7294201850891113\ncompatibility: 7.417999999999999\naccessibility: -2.6655126103417914\nreward: 0.94742751121521\ncompatibility: 7.539000000000001\naccessibility: -2.403323034150061\nreward: 0.9146379232406616\ncompatibility: 7.601999999999999\naccessibility: -2.4862573850871046\nreward: 0.7796640396118164\ncompatibility: 7.333000000000001\naccessibility: -2.544611071694814\nreward: 0.927150309085846\ncompatibility: 7.343999999999999\naccessibility: -2.3292744955056026\nreward: 0.825920581817627\ncompatibility: 7.218000000000001\naccessibility: -2.4136190893064136\nreward: 0.9669350981712341\ncompatibility: 7.540500000000001\naccessibility: -2.374865166586046\nreward: 0.8700557351112366\ncompatibility: 7.5405000000000015\naccessibility: -2.520184229307291\nreward: 1.0436490774154663\ncompatibility: 7.544500000000002\naccessibility: -2.261937095535254\nreward: 1.1130656003952026\ncompatibility: 7.458499999999998\naccessibility: -2.111740823132843\nreward: 0.9682319760322571\ncompatibility: 7.685000000000002\naccessibility: -2.45033062375178\nreward: 0.7905651330947876\ncompatibility: 7.637500000000004\naccessibility: -2.691384435057132\nreward: 0.9404124021530151\ncompatibility: 7.411500000000003\naccessibility: -2.345542139696589\nreward: 1.0338436365127563\ncompatibility: 7.278499999999999\naccessibility: -2.1341452664358997\nreward: 1.0988129377365112\ncompatibility: 7.429000000000001\naccessibility: -2.1173162391497318\nreward: 1.0923752784729004\ncompatibility: 7.415000000000003\naccessibility: -2.119472819907311\nreward: 1.1055762767791748\ncompatibility: 7.5235\naccessibility: -2.15779621829944\nreward: 0.773685872554779\ncompatibility: 7.297999999999998\naccessibility: -2.5348283025914666\nreward: 0.8626837134361267\ncompatibility: 7.3025\naccessibility: -2.403742298004042\nreward: 0.9775981307029724\ncompatibility: 7.551500000000001\naccessibility: -2.36476348261805\nreward: 1.0800889730453491\ncompatibility: 7.514499999999998\naccessibility: -2.191205763703075\nreward: 0.9926849007606506\ncompatibility: 7.447999999999999\naccessibility: -2.2866869508947736\nreward: 1.2681033611297607\ncompatibility: 7.804999999999998\naccessibility: -2.0648091723444226\nreward: 1.0870858430862427\ncompatibility: 7.565499999999999\naccessibility: -2.208031880731634\nreward: 0.9133385419845581\ncompatibility: 7.408\naccessibility: -2.3842778937596023\nreward: 0.9613454341888428\ncompatibility: 7.564500000000002\naccessibility: -2.396106873597157\nreward: 0.8493962287902832\ncompatibility: 7.3295\naccessibility: -2.4381378440122323\nreward: 0.8750490546226501\ncompatibility: 7.462000000000001\naccessibility: -2.4706407163073414\nreward: 0.9249610304832458\ncompatibility: 7.2975\naccessibility: -2.307647697366396\nreward: 1.0011005401611328\ncompatibility: 7.2\naccessibility: -2.141206247678449\nreward: 0.8363015651702881\ncompatibility: 7.247500000000002\naccessibility: -2.413851181639256\nreward: 0.9011126160621643\ncompatibility: 7.377000000000001\naccessibility: -2.3860096236633854\nreward: 1.2473578453063965\ncompatibility: 7.616999999999999\naccessibility: -1.9952131974825735\nreward: 0.8163313865661621\ncompatibility: 7.364499999999998\naccessibility: -2.506485070691111\nreward: 0.75245600938797\ncompatibility: 7.467\naccessibility: -2.6572088198556743\nreward: 1.1569006443023682\ncompatibility: 7.576999999999999\naccessibility: -2.1094704487697484\nreward: 0.889545738697052\ncompatibility: 7.476499999999999\naccessibility: -2.4566635431883315\nreward: 0.8462337255477905\ncompatibility: 7.4145\naccessibility: -2.4884173054782783\nreward: 1.1608966588974\ncompatibility: 7.394000000000001\naccessibility: -2.0054407024599996\nreward: 0.9818350076675415\ncompatibility: 7.433000000000002\naccessibility: -2.294926046137551\nreward: 1.0115545988082886\ncompatibility: 7.6250000000000036\naccessibility: -2.3532038018399515\nreward: 1.189839243888855\ncompatibility: 7.711000000000004\naccessibility: -2.1318483295148405\nreward: 1.0222214460372925\ncompatibility: 7.4595\naccessibility: -2.2485428912095937\nreward: 1.1822764873504639\ncompatibility: 7.609999999999998\naccessibility: -2.0890853544465355\nreward: 1.115464210510254\ncompatibility: 7.625500000000001\naccessibility: -2.197607204489361\nreward: 0.8675443530082703\ncompatibility: 7.1865000000000006\naccessibility: -2.3343084554533395\nreward: 1.1197052001953125\ncompatibility: 7.705000000000004\naccessibility: -2.2338351454386465\nreward: 0.7549733519554138\ncompatibility: 7.368499999999999\naccessibility: -2.6006649386229954\nreward: 0.9095069169998169\ncompatibility: 7.2375\naccessibility: -2.2986860630979433\nreward: 0.886605978012085\ncompatibility: 7.424000000000001\naccessibility: -2.4329481480174957\nreward: 1.0252662897109985\ncompatibility: 7.501000000000002\naccessibility: -2.2662076932357924\nreward: 1.03459632396698\ncompatibility: 7.5980000000000025\naccessibility: -2.3041768698839027\nreward: 0.8652800917625427\ncompatibility: 7.304499999999999\naccessibility: -2.400919116649996\nreward: 1.1493321657180786\ncompatibility: 7.619000000000002\naccessibility: -2.1433231477234287\nreward: 1.0517767667770386\ncompatibility: 7.348000000000002\naccessibility: -2.1444776515301287\nreward: 0.9013106822967529\ncompatibility: 7.343499999999998\naccessibility: -2.3677660804300835\nreward: 0.6376086473464966\ncompatibility: 7.428000000000002\naccessibility: -2.808586998314857\nreward: 0.7221021056175232\ncompatibility: 7.512\naccessibility: -2.7268468367370877\nreward: 0.9003067016601562\ncompatibility: 7.607999999999997\naccessibility: -2.5109685181333212\nreward: 0.881804347038269\ncompatibility: 7.314500000000002\naccessibility: -2.3814899325139764\nreward: 1.1047217845916748\ncompatibility: 7.466\naccessibility: -2.1282744431465\nreward: 0.7621455788612366\ncompatibility: 7.464500000000003\naccessibility: -2.6413352160622083\nreward: 0.9728643298149109\ncompatibility: 7.46\naccessibility: -2.322846357145475\nreward: 1.0089962482452393\ncompatibility: 7.490999999999999\naccessibility: -2.285255675619969\nreward: 0.8226863145828247\ncompatibility: 7.391499999999997\naccessibility: -2.5114169696791273\nreward: 0.9459977746009827\ncompatibility: 7.6695\naccessibility: -2.475378312772897\nreward: 1.1255592107772827\ncompatibility: 7.5245000000000015\naccessibility: -2.1283576819669636\nreward: 1.1356587409973145\ncompatibility: 7.558500000000001\naccessibility: -2.131422582621923\nreward: 0.9235267043113708\ncompatibility: 7.376499999999998\naccessibility: -2.352120663240477\nreward: 1.1139602661132812\ncompatibility: 7.3925\naccessibility: -2.075041670861399\nreward: 0.8966034650802612\ncompatibility: 7.399000000000002\naccessibility: -2.404559075335055\nreward: 0.9295305013656616\ncompatibility: 7.328000000000002\naccessibility: -2.317132852024571\nreward: 0.8682538866996765\ncompatibility: 7.644500000000002\naccessibility: -2.578601313199562\nreward: 0.9841678142547607\ncompatibility: 7.565499999999999\naccessibility: -2.362408984234321\nreward: 0.7529460787773132\ncompatibility: 7.3904999999999985\naccessibility: -2.615491606304522\nreward: 0.8904704451560974\ncompatibility: 7.497499999999999\naccessibility: -2.4665265069569946\nreward: 0.9552634358406067\ncompatibility: 7.607500000000003\naccessibility: -2.4282655686994965\nreward: 0.8224667310714722\ncompatibility: 7.391499999999997\naccessibility: -2.511746366237423\nreward: 0.7605695724487305\ncompatibility: 7.399500000000001\naccessibility: -2.608877768638395\nreward: 0.6278563737869263\ncompatibility: 7.476500000000003\naccessibility: -2.849197591739846\nreward: 1.066930890083313\ncompatibility: 7.632500000000001\naccessibility: -2.27415718775436\nreward: 0.8647060394287109\ncompatibility: 7.206000000000001\naccessibility: -2.349012408621749\nreward: 1.1645779609680176\ncompatibility: 7.467000000000001\naccessibility: -2.0390259947650655\nreward: 1.2176016569137573\ncompatibility: 7.6004999999999985\naccessibility: -2.031008261893518\nreward: 0.8788428902626038\ncompatibility: 7.5815\naccessibility: -2.528967805606535\nreward: 1.084299087524414\ncompatibility: 7.460500000000001\naccessibility: -2.1559620206192314\nreward: 0.9915382862091064\ncompatibility: 7.509500000000002\naccessibility: -2.321353241757165\nreward: 0.8538100123405457\ncompatibility: 7.4559999999999995\naccessibility: -2.4992849673185034\nreward: 0.8632122874259949\ncompatibility: 7.387500000000001\naccessibility: -2.4484851639433574\nreward: 0.9732323884963989\ncompatibility: 7.566000000000005\naccessibility: -2.3790799901359447\nreward: 0.6031925082206726\ncompatibility: 7.102500000000001\naccessibility: -2.6858362535877416\nreward: 1.1344242095947266\ncompatibility: 7.370999999999999\naccessibility: -2.0328279792694666\nreward: 0.9143322706222534\ncompatibility: 7.4265\naccessibility: -2.392698010473789\nreward: 0.8367010951042175\ncompatibility: 7.428000000000003\naccessibility: -2.509948401025722\nreward: 1.154443383216858\ncompatibility: 7.348000000000001\naccessibility: -1.9904778340408178\nreward: 0.9526465535163879\ncompatibility: 7.4224999999999985\naccessibility: -2.3330837125449944\nreward: 0.9866493940353394\ncompatibility: 7.5255000000000045\naccessibility: -2.3372580557554716\nreward: 1.1100833415985107\ncompatibility: 7.554499999999999\naccessibility: -2.1676428646499457\nreward: 0.8640403747558594\ncompatibility: 7.399000000000002\naccessibility: -2.453403699940192\nreward: 1.0069973468780518\ncompatibility: 7.417000000000002\naccessibility: -2.248611058855861\nreward: 1.2113455533981323\ncompatibility: 7.763500000000001\naccessibility: -2.1277138386433645\nreward: 0.9374465942382812\ncompatibility: 7.593\naccessibility: -2.4472229750410923\nreward: 0.8374961614608765\ncompatibility: 7.5245000000000015\naccessibility: -2.5604521433227214\nreward: 0.8536777496337891\ncompatibility: 7.5685\naccessibility: -2.5597512445119484\nreward: 0.7406730651855469\ncompatibility: 7.4135\naccessibility: -2.6462225211241535\nreward: 1.0821462869644165\ncompatibility: 7.5985000000000005\naccessibility: -2.2331198206060545\nreward: 1.0063273906707764\ncompatibility: 7.4864999999999995\naccessibility: -2.286848235376837\nreward: 1.0724029541015625\ncompatibility: 7.5355\naccessibility: -2.2139847858433006\nreward: 0.9117441177368164\ncompatibility: 7.3294999999999995\naccessibility: -2.3446159953232817\nreward: 1.0404653549194336\ncompatibility: 7.564999999999999\naccessibility: -2.277694882701399\nreward: 0.8388152122497559\ncompatibility: 7.160000000000001\naccessibility: -2.3632057692851514\nreward: 0.6424867510795593\ncompatibility: 7.346499999999997\naccessibility: -2.75760915967043\nreward: 0.7119810581207275\ncompatibility: 7.3305\naccessibility: -2.6447962302997317\nreward: 1.0894228219985962\ncompatibility: 7.3705\naccessibility: -2.1000621273570492\nreward: 0.7861821055412292\ncompatibility: 7.340999999999999\naccessibility: -2.5391197056511197\nreward: 1.0304300785064697\ncompatibility: 7.466500000000001\naccessibility: -2.2399799684032504\nreward: 0.9493002891540527\ncompatibility: 7.487000000000003\naccessibility: -2.3726567106478984\nreward: 1.043391466140747\ncompatibility: 7.405\naccessibility: -2.1875913704141783\nreward: 0.8416580557823181\ncompatibility: 7.716500000000001\naccessibility: -2.6570665132353444\nreward: 1.143479347229004\ncompatibility: 7.704500000000001\naccessibility: -2.197905925221364\nreward: 0.9988868236541748\ncompatibility: 7.388500000000002\naccessibility: -2.245509063819069\nreward: 1.0148237943649292\ncompatibility: 7.729000000000001\naccessibility: -2.404014270602231\nreward: 0.7919642925262451\ncompatibility: 7.434\naccessibility: -2.580267808883468\nreward: 1.1169387102127075\ncompatibility: 7.704500000000001\naccessibility: -2.2377170209529655\nreward: 0.8488357663154602\ncompatibility: 7.5305\naccessibility: -2.546657074398874\nreward: 0.6761963367462158\ncompatibility: 7.5065\naccessibility: -2.7927590476272117\nreward: 0.9009360074996948\ncompatibility: 7.554499999999999\naccessibility: -2.4813638328020176\nreward: 1.1294645071029663\ncompatibility: 7.459000000000002\naccessibility: -2.087410454801599\nreward: 0.9657304286956787\ncompatibility: 7.446499999999999\naccessibility: -2.326315108862844\nreward: 0.9020713567733765\ncompatibility: 7.435000000000001\naccessibility: -2.415642975482229\nreward: 0.9530537724494934\ncompatibility: 7.4695\naccessibility: -2.3576514716692847\nreward: 0.7305606007575989\ncompatibility: 7.363499999999999\naccessibility: -2.6346055508898925\nreward: 1.0714445114135742\ncompatibility: 7.569999999999998\naccessibility: -2.2339047150473808\nreward: 1.1186864376068115\ncompatibility: 7.590000000000001\naccessibility: -2.173756058507282\nreward: 1.0454533100128174\ncompatibility: 7.5115\naccessibility: -2.2415521872933457\nreward: 1.2800886631011963\ncompatibility: 7.570000000000002\naccessibility: -1.9209384907148814\nreward: 1.2636311054229736\ncompatibility: 7.653499999999999\naccessibility: -1.9903569240215977\nreward: 0.984874427318573\ncompatibility: 7.441000000000002\naccessibility: -2.2946526799478084\nreward: 1.2648324966430664\ncompatibility: 7.861999999999999\naccessibility: -2.1002512990742375\nreward: 0.9967656135559082\ncompatibility: 7.308499999999999\naccessibility: -2.205833737058818\nreward: 0.757232129573822\ncompatibility: 7.377000000000001\naccessibility: -2.601830346725194\nreward: 0.9700251817703247\ncompatibility: 7.5275000000000025\naccessibility: -2.3632658306512573\nreward: 1.2969210147857666\ncompatibility: 7.586\naccessibility: -1.9042613888406905\nreward: 1.0648764371871948\ncompatibility: 7.5905\naccessibility: -2.2547388524309415\nreward: 0.7274463772773743\ncompatibility: 7.292000000000001\naccessibility: -2.600973309765852\nreward: 1.1857081651687622\ncompatibility: 7.493500000000001\naccessibility: -2.0215269889498266\nreward: 0.9641333818435669\ncompatibility: 7.534500000000001\naccessibility: -2.3758534907125286\nreward: 0.9743927717208862\ncompatibility: 7.61\naccessibility: -2.4009108341353818\nreward: 1.0830318927764893\ncompatibility: 7.520000000000001\naccessibility: -2.189737917470366\nreward: 0.9228844046592712\ncompatibility: 7.45\naccessibility: -2.392459094764095\nreward: 1.0622020959854126\ncompatibility: 7.6705\naccessibility: -2.301607556766512\nreward: 1.1491862535476685\ncompatibility: 7.531499999999998\naccessibility: -2.096667093716742\nreward: 0.6328086853027344\ncompatibility: 7.470000000000002\naccessibility: -2.838286975137159\nreward: 0.9578211903572083\ncompatibility: 7.621000000000002\naccessibility: -2.431661051050543\nreward: 1.1031174659729004\ncompatibility: 7.335500000000001\naccessibility: -2.0607702909582555\nreward: 0.9621394276618958\ncompatibility: 7.511500000000001\naccessibility: -2.3665229620406123\nreward: 0.8170539736747742\ncompatibility: 7.443499999999999\naccessibility: -2.5477225705678483\nreward: 0.7928708791732788\ncompatibility: 7.5520000000000005\naccessibility: -2.642122277411158\nreward: 1.0099538564682007\ncompatibility: 7.586999999999996\naccessibility: -2.3352477699449787\nreward: 0.9515154361724854\ncompatibility: 7.280500000000001\naccessibility: -2.2587090230176035\nreward: 0.8136866092681885\ncompatibility: 7.413500000000002\naccessibility: -2.5367022142431392\nreward: 0.8601549863815308\ncompatibility: 7.3355\naccessibility: -2.4252139691486216\nreward: 0.8024742603302002\ncompatibility: 7.364500000000005\naccessibility: -2.527270784421245\nreward: 1.15736722946167\ncompatibility: 7.502000000000002\naccessibility: -2.0685919739165692\nreward: 0.8420225977897644\ncompatibility: 7.323499999999998\naccessibility: -2.445983944501043\nreward: 1.1234104633331299\ncompatibility: 7.431000000000001\naccessibility: -2.081491488207452\nreward: 0.936733603477478\ncompatibility: 7.616000000000001\naccessibility: -2.4606138543711795\nreward: 0.9360832571983337\ncompatibility: 7.462999999999997\naccessibility: -2.3796251459536673\nreward: 0.7032853960990906\ncompatibility: 7.411999999999998\naccessibility: -2.7015004340016473\nreward: 1.0971641540527344\ncompatibility: 7.511000000000001\naccessibility: -2.1637179907825765\nreward: 1.0140633583068848\ncompatibility: 7.350999999999999\naccessibility: -2.202655037316906\nreward: 1.007874608039856\ncompatibility: 7.683000000000002\naccessibility: -2.389795290728573\nreward: 1.0985028743743896\ncompatibility: 7.578500000000003\naccessibility: -2.1978707475562875\nreward: 0.8472866415977478\ncompatibility: 7.350000000000001\naccessibility: -2.452284313438491\nreward: 0.8406181335449219\ncompatibility: 7.530000000000004\naccessibility: -2.5587156294971054\nreward: 1.012961745262146\ncompatibility: 7.400000000000001\naccessibility: -2.2305574594724598\nreward: 0.756459653377533\ncompatibility: 7.440500000000002\naccessibility: -2.637006953441948\nreward: 1.0420151948928833\ncompatibility: 7.491500000000002\naccessibility: -2.23599507038154\nreward: 0.9227595925331116\ncompatibility: 7.506499999999999\naccessibility: -2.4229141905634295\nreward: 0.9088456630706787\ncompatibility: 7.658000000000004\naccessibility: -2.5249458340117177\nreward: 1.0814836025238037\ncompatibility: 7.456999999999999\naccessibility: -2.158310257583846\nreward: 1.0470894575119019\ncompatibility: 7.763000000000001\naccessibility: -2.373830097471726\nreward: 1.0503216981887817\ncompatibility: 7.492000000000002\naccessibility: -2.2238031363832658\nreward: 1.0008270740509033\ncompatibility: 7.2494999999999985\naccessibility: -2.1681343947143183\nreward: 0.8125198483467102\ncompatibility: 7.431500000000001\naccessibility: -2.548095183777814\nreward: 0.689127504825592\ncompatibility: 7.341499999999999\naccessibility: -2.684969453077317\nreward: 1.1681169271469116\ncompatibility: 7.644\naccessibility: -2.128538934100939\nreward: 1.032192349433899\ncompatibility: 7.664499999999999\naccessibility: -2.3434079746798013\nreward: 1.170701026916504\ncompatibility: 7.7695000000000025\naccessibility: -2.1918948256049875\nreward: 0.8688486218452454\ncompatibility: 7.497500000000001\naccessibility: -2.498959213598831\nreward: 1.1362144947052002\ncompatibility: 7.788500000000003\naccessibility: -2.2538032769374743\nreward: 1.0238076448440552\ncompatibility: 7.424500000000002\naccessibility: -2.2274135378869273\nreward: 1.0762361288070679\ncompatibility: 7.549500000000002\naccessibility: -2.215735004714056\nreward: 0.8233940601348877\ncompatibility: 7.527500000000001\naccessibility: -2.583212464292133\nreward: 0.8721257448196411\ncompatibility: 7.566500000000001\naccessibility: -2.53100780701993\nreward: 0.7145999073982239\ncompatibility: 7.309\naccessibility: -2.6293501176231096\nreward: 0.9731383323669434\ncompatibility: 7.330500000000001\naccessibility: -2.253060360994769\nreward: 1.0146121978759766\ncompatibility: 7.591000000000001\naccessibility: -2.330403065065971\nreward: 0.9088923931121826\ncompatibility: 7.4025\naccessibility: -2.3880006681563666\nreward: 0.8525833487510681\ncompatibility: 7.468499999999999\naccessibility: -2.5078214058429356\nreward: 0.7494825124740601\ncompatibility: 7.421\naccessibility: -2.6370262365854638\nreward: 1.0787714719772339\ncompatibility: 7.6514999999999995\naccessibility: -2.266574990693199\nreward: 0.9098843932151794\ncompatibility: 7.526\naccessibility: -2.4526733745519893\nreward: 0.8186208009719849\ncompatibility: 7.504499999999999\naccessibility: -2.578050954418478\nreward: 1.107182502746582\ncompatibility: 7.747500000000002\naccessibility: -2.2753869386602803\nreward: 0.767486572265625\ncompatibility: 7.485000000000001\naccessibility: -2.644305893651974\nreward: 0.9026046395301819\ncompatibility: 7.564500000000001\naccessibility: -2.484218054376909\nreward: 0.9475311636924744\ncompatibility: 7.321000000000001\naccessibility: -2.286381862126453\nreward: 1.0481735467910767\ncompatibility: 7.602000000000001\naccessibility: -2.2859539508854536\nreward: 0.9140526056289673\ncompatibility: 7.4970000000000026\naccessibility: -2.430885334823425\nreward: 1.0576645135879517\ncompatibility: 7.329\naccessibility: -2.1254675140145007\nreward: 1.1618237495422363\ncompatibility: 7.695499999999998\naccessibility: -2.1655679714855314\nreward: 0.9283837080001831\ncompatibility: 7.5695000000000014\naccessibility: -2.4482280104309426\nreward: 1.0621882677078247\ncompatibility: 7.753500000000003\naccessibility: -2.346092669345847\nreward: 0.6691363453865051\ncompatibility: 7.4384999999999994\naccessibility: -2.766920440265668\nreward: 1.1127679347991943\ncompatibility: 7.5865\naccessibility: -2.180758859613562\nreward: 0.9979238510131836\ncompatibility: 7.3765\naccessibility: -2.240524964379477\nreward: 1.0355862379074097\ncompatibility: 7.5005000000000015\naccessibility: -2.250459999092771\nreward: 0.8838682174682617\ncompatibility: 7.412500000000001\naccessibility: -2.430894071431162\nreward: 1.054260492324829\ncompatibility: 7.688500000000004\naccessibility: -2.323162918727597\nreward: 1.0302616357803345\ncompatibility: 7.714000000000002\naccessibility: -2.3728217565084173\nreward: 1.1528480052947998\ncompatibility: 7.6525000000000025\naccessibility: -2.1559957867773134\nreward: 0.8662779927253723\ncompatibility: 7.423000000000003\naccessibility: -2.462904420238257\nreward: 1.1362937688827515\ncompatibility: 7.5029999999999974\naccessibility: -2.100737848537559\nreward: 0.9778711199760437\ncompatibility: 7.4995\naccessibility: -2.3364968924224883\nreward: 1.0821298360824585\ncompatibility: 7.437999999999999\naccessibility: -2.1471624251413988\nreward: 0.830644965171814\ncompatibility: 7.412499999999998\naccessibility: -2.510728967238984\nreward: 0.7523676753044128\ncompatibility: 7.430999999999999\naccessibility: -2.6380556308918486\nreward: 1.0613689422607422\ncompatibility: 7.690000000000001\naccessibility: -2.3133038171387685\nreward: 0.7135316133499146\ncompatibility: 7.4544999999999995\naccessibility: -2.70889901439189\nreward: 0.924298882484436\ncompatibility: 7.222\naccessibility: -2.2681945656107336\nreward: 1.0871301889419556\ncompatibility: 7.459500000000002\naccessibility: -2.1511797302311915\nreward: 0.9160351753234863\ncompatibility: 7.522499999999999\naccessibility: -2.4415722684213987\nreward: 1.0234766006469727\ncompatibility: 7.6339999999999995\naccessibility: -2.340142231832364\nreward: 0.970294713973999\ncompatibility: 7.3505\naccessibility: -2.268040090448541\nreward: 1.057313084602356\ncompatibility: 7.589999999999999\naccessibility: -2.265816139837689\nreward: 0.8349666595458984\ncompatibility: 7.3655\naccessibility: -2.479067875143916\nreward: 0.9715400338172913\ncompatibility: 7.333499999999998\naccessibility: -2.257064985901151\nreward: 0.6813167333602905\ncompatibility: 7.315500000000001\naccessibility: -2.6827570732694275\nreward: 0.993611752986908\ncompatibility: 7.462000000000001\naccessibility: -2.2927966249071763\nreward: 1.0026499032974243\ncompatibility: 7.5395\naccessibility: -2.320757206036945\nreward: 0.8062037825584412\ncompatibility: 7.396\naccessibility: -2.5385514721578737\nreward: 0.6928741931915283\ncompatibility: 7.371999999999998\naccessibility: -2.695688709952955\nreward: 0.995220422744751\ncompatibility: 7.3614999999999995\naccessibility: -2.2365443521592403\nreward: 0.8435376882553101\ncompatibility: 7.4045000000000005\naccessibility: -2.487104200821554\nreward: 0.906958818435669\ncompatibility: 7.449000000000002\naccessibility: -2.415811809141879\nreward: 1.1306676864624023\ncompatibility: 7.599500000000001\naccessibility: -2.160873487134012\nreward: 0.899961531162262\ncompatibility: 7.589500000000002\naccessibility: -2.501575562284273\nreward: 0.8101939558982849\ncompatibility: 7.249\naccessibility: -2.4538162180047993\nreward: 0.939660906791687\ncompatibility: 7.638499999999998\naccessibility: -2.468276514104127\nreward: 1.1719688177108765\ncompatibility: 7.705000000000001\naccessibility: -2.1554396753261047\nreward: 1.1693689823150635\ncompatibility: 7.429500000000002\naccessibility: -2.0117501872483263\nreward: 0.7254288792610168\ncompatibility: 7.285999999999999\naccessibility: -2.6007852116274903\nreward: 0.8470068573951721\ncompatibility: 7.467000000000001\naccessibility: -2.5153825715616698\nreward: 1.0481765270233154\ncompatibility: 7.419500000000002\naccessibility: -2.1881816059767303\nreward: 0.9452692866325378\ncompatibility: 7.53\naccessibility: -2.4017389387375583\nreward: 1.1372359991073608\ncompatibility: 7.3214999999999995\naccessibility: -2.0020925039605073\nreward: 0.9288723468780518\ncompatibility: 7.589999999999998\naccessibility: -2.4584771888155355\nreward: 1.1024229526519775\ncompatibility: 7.688500000000002\naccessibility: -2.250919208547899\nreward: 0.8827922344207764\ncompatibility: 7.4475000000000025\naccessibility: -2.4512581006576815\nreward: 1.044113039970398\ncompatibility: 7.6355\naccessibility: -2.3099912218905536\nreward: 0.8534665703773499\ncompatibility: 7.353\naccessibility: -2.444621542005085\nreward: 0.6860524415969849\ncompatibility: 7.697499999999999\naccessibility: -2.8802962951398134\nreward: 0.986899197101593\ncompatibility: 7.235499999999999\naccessibility: -2.1815261925094234\nreward: 0.7859066724777222\ncompatibility: 7.444500000000001\naccessibility: -2.5949792866388792\nreward: 1.1594041585922241\ncompatibility: 7.5785\naccessibility: -2.106518838485437\nreward: 1.055497407913208\ncompatibility: 7.582000000000001\naccessibility: -2.2642538295160937\nreward: 1.0261270999908447\ncompatibility: 7.419499999999997\naccessibility: -2.2212557872247403\nreward: 0.9020846486091614\ncompatibility: 7.474499999999998\naccessibility: -2.436783728566329\nreward: 0.6379938125610352\ncompatibility: 7.3535\naccessibility: -2.76809856261444\nreward: 0.8108738660812378\ncompatibility: 7.287500000000001\naccessibility: -2.4734213029939043\nreward: 0.8304039239883423\ncompatibility: 7.360500000000002\naccessibility: -2.483233373597052\nreward: 0.946273148059845\ncompatibility: 7.519\naccessibility: -2.3943402927822777\nreward: 0.9270744919776917\ncompatibility: 7.3915\naccessibility: -2.354834683443042\nreward: 1.2119262218475342\ncompatibility: 7.566000000000002\naccessibility: -2.021039232593534\nreward: 0.7916317582130432\ncompatibility: 7.524000000000001\naccessibility: -2.6289809490671874\nreward: 0.9315828084945679\ncompatibility: 7.506500000000002\naccessibility: -2.4096794013297935\nreward: 1.08406400680542\ncompatibility: 7.559000000000001\naccessibility: -2.2090825403181134\nreward: 0.7243813872337341\ncompatibility: 7.458000000000003\naccessibility: -2.694499344616201\nreward: 0.8794195652008057\ncompatibility: 7.510999999999999\naccessibility: -2.490334949840913\nreward: 0.9064760208129883\ncompatibility: 7.2185000000000015\naccessibility: -2.2930537893050476\nreward: 1.078187108039856\ncompatibility: 7.5775000000000015\naccessibility: -2.2278086702389963\nreward: 0.9662391543388367\ncompatibility: 7.457499999999999\naccessibility: -2.3314448483857353\nreward: 1.1922688484191895\ncompatibility: 7.473500000000001\naccessibility: -2.000971747157214\nreward: 0.9536218643188477\ncompatibility: 7.622499999999999\naccessibility: -2.4387636761880116\nreward: 1.0641053915023804\ncompatibility: 7.462500000000001\naccessibility: -2.1873240438781987\nreward: 0.799117386341095\ncompatibility: 7.3370000000000015\naccessibility: -2.517573902286914\nreward: 0.841604471206665\ncompatibility: 7.590500000000002\naccessibility: -2.589646888060115\nreward: 0.7849581837654114\ncompatibility: 7.445999999999997\naccessibility: -2.5972056098808496\nreward: 1.082637906074524\ncompatibility: 7.664499999999999\naccessibility: -2.267739611481071\nreward: 0.725030779838562\ncompatibility: 7.6160000000000005\naccessibility: -2.7781681095705157\nreward: 0.8498808145523071\ncompatibility: 7.400000000000001\naccessibility: -2.4751787405495937\nreward: 1.2795778512954712\ncompatibility: 7.847500000000001\naccessibility: -2.0703654000131957\nreward: 0.8303903341293335\ncompatibility: 7.425\naccessibility: -2.517807326422704\nreward: 1.1485880613327026\ncompatibility: 7.412500000000001\naccessibility: -2.033814270763823\nreward: 0.8068867325782776\ncompatibility: 7.368\naccessibility: -2.5225270129777106\nreward: 0.6931265592575073\ncompatibility: 7.5074999999999985\naccessibility: -2.767899449287274\nreward: 0.8148613572120667\ncompatibility: 7.598999999999998\naccessibility: -2.6343151389202917\nreward: 0.9437984228134155\ncompatibility: 7.6819999999999995\naccessibility: -2.4853737836346457\nreward: 1.0792092084884644\ncompatibility: 7.5575\naccessibility: -2.215561131455587\nreward: 0.9870262145996094\ncompatibility: 7.5035\naccessibility: -2.324907125247169\nreward: 0.8114363551139832\ncompatibility: 7.565499999999999\naccessibility: -2.62150618142091\nreward: 1.083986759185791\ncompatibility: 7.503999999999999\naccessibility: -2.179734077704523\nreward: 0.9122626781463623\ncompatibility: 7.541999999999999\naccessibility: -2.4576774172990556\nreward: 0.5345052480697632\ncompatibility: 7.544499999999999\naccessibility: -3.025652821631359\nreward: 1.0736943483352661\ncompatibility: 7.270999999999999\naccessibility: -2.0703512760006264\nreward: 1.003783941268921\ncompatibility: 7.526499999999999\naccessibility: -2.312091998296351\nreward: 1.0387812852859497\ncompatibility: 7.458499999999998\naccessibility: -2.2231672917494216\nreward: 0.7707549333572388\ncompatibility: 7.472\naccessibility: -2.6324390619923923\nreward: 0.8679484724998474\ncompatibility: 7.252000000000001\naccessibility: -2.368791558435954\nreward: 0.6758157014846802\ncompatibility: 7.168500000000002\naccessibility: -2.612258626312725\nreward: 1.086186408996582\ncompatibility: 7.560500000000001\naccessibility: -2.206702539868998\nreward: 0.8159915804862976\ncompatibility: 7.5005\naccessibility: -2.579851958011588\nreward: 0.988505482673645\ncompatibility: 7.537500000000002\naccessibility: -2.3409025205507743\nreward: 0.9753545522689819\ncompatibility: 7.516499999999999\naccessibility: -2.3493788793478045\nreward: 1.0806039571762085\ncompatibility: 7.474000000000001\naccessibility: -2.1687369326790233\nreward: 0.9316174387931824\ncompatibility: 7.516499999999999\naccessibility: -2.414984526509492\nreward: 0.9927076697349548\ncompatibility: 7.546500000000001\naccessibility: -2.339420598470407\nreward: 1.0072275400161743\ncompatibility: 7.562000000000001\naccessibility: -2.3259444694342535\nreward: 1.210908055305481\ncompatibility: 7.624999999999997\naccessibility: -2.0541735837321977\nreward: 0.862199604511261\ncompatibility: 7.362499999999999\naccessibility: -2.436611298740267\nreward: 0.9470316767692566\ncompatibility: 7.394\naccessibility: -2.326238191680851\nreward: 1.2139676809310913\ncompatibility: 7.507000000000001\naccessibility: -1.9863699601411589\nreward: 1.071187973022461\ncompatibility: 7.656999999999999\naccessibility: -2.2808965682888855\nreward: 0.8632186651229858\ncompatibility: 7.292999999999999\naccessibility: -2.397850571587301\nreward: 0.753576934337616\ncompatibility: 7.504\naccessibility: -2.6753488820857365\nreward: 0.922285258769989\ncompatibility: 7.581\naccessibility: -2.463536392321407\nreward: 0.8335189819335938\ncompatibility: 7.435499999999999\naccessibility: -2.518739400143088\nreward: 1.0324375629425049\ncompatibility: 7.685999999999999\naccessibility: -2.354557919574527\nreward: 1.024346947669983\ncompatibility: 7.666500000000001\naccessibility: -2.3562475222359764\nreward: 0.7752845287322998\ncompatibility: 7.502999999999999\naccessibility: -2.642251812289436\nreward: 0.7158036231994629\ncompatibility: 7.312\naccessibility: -2.6291517330167684\nreward: 1.179534912109375\ncompatibility: 7.498000000000002\naccessibility: -2.033197648853367\nreward: 0.9787876009941101\ncompatibility: 7.3395\naccessibility: -2.2494079008854984\nreward: 0.7943094968795776\ncompatibility: 7.393999999999998\naccessibility: -2.5553214437422946\nreward: 1.0177034139633179\ncompatibility: 7.438500000000001\naccessibility: -2.244069943484909\nreward: 1.0384066104888916\ncompatibility: 7.6195\naccessibility: -2.309979442331187\nreward: 0.7271870970726013\ncompatibility: 7.363500000000001\naccessibility: -2.6396657610570324\nreward: 1.0464787483215332\ncompatibility: 7.475000000000001\naccessibility: -2.2204605067662055\nreward: 1.02029550075531\ncompatibility: 7.467000000000001\naccessibility: -2.255449664733736\nreward: 1.0179249048233032\ncompatibility: 7.528\naccessibility: -2.2916840923195143\nreward: 0.9253087043762207\ncompatibility: 7.538000000000003\naccessibility: -2.4359655519977363\nreward: 0.9807829260826111\ncompatibility: 7.416999999999999\naccessibility: -2.2879327581432007\nreward: 0.8759677410125732\ncompatibility: 7.212\naccessibility: -2.3353340876150614\nreward: 1.0603593587875366\ncompatibility: 7.3614999999999995\naccessibility: -2.1388359290235868\nreward: 0.9555209279060364\ncompatibility: 7.495999999999999\naccessibility: -2.36814721569504\nreward: 0.7710704207420349\ncompatibility: 7.35\naccessibility: -2.5666086389134897\nreward: 1.0604355335235596\ncompatibility: 7.4525000000000015\naccessibility: -2.18747166312997\nreward: 1.100217580795288\ncompatibility: 7.655000000000001\naccessibility: -2.236280699673257\nreward: 1.1141777038574219\ncompatibility: 7.495500000000001\naccessibility: -2.129894070801053\nreward: 1.0973368883132935\ncompatibility: 7.609\naccessibility: -2.2159589535219\nreward: 1.133744239807129\ncompatibility: 7.393000000000002\naccessibility: -2.0456336730082727\nreward: 0.9694662094116211\ncompatibility: 7.495500000000002\naccessibility: -2.3469614345295167\nreward: 0.7498639225959778\ncompatibility: 7.360499999999998\naccessibility: -2.60404337031714\nreward: 0.8997662663459778\ncompatibility: 7.625\naccessibility: -2.520886298272883\nreward: 0.9713335037231445\ncompatibility: 7.424500000000002\naccessibility: -2.3061247241866614\nreward: 0.8770845532417297\ncompatibility: 7.299500000000003\naccessibility: -2.3805338430858853\nreward: 0.8106207847595215\ncompatibility: 7.339000000000001\naccessibility: -2.501390266360856\nreward: 0.9499036073684692\ncompatibility: 7.680500000000002\naccessibility: -2.4754124166911278\nreward: 1.1061607599258423\ncompatibility: 7.7725000000000035\naccessibility: -2.290312505617666\nreward: 0.6941267251968384\ncompatibility: 7.296\naccessibility: -2.6530956090083326\nreward: 1.0040650367736816\ncompatibility: 7.548500000000001\naccessibility: -2.323456048468181\nreward: 0.9889569282531738\ncompatibility: 7.677499999999998\naccessibility: -2.415225356893158\nreward: 0.9063973426818848\ncompatibility: 7.395499999999999\naccessibility: -2.3879932532053183\nreward: 0.8677428364753723\ncompatibility: 7.526000000000002\naccessibility: -2.515885760188832\nreward: 0.879978358745575\ncompatibility: 7.468500000000001\naccessibility: -2.466728899511844\nreward: 0.800138533115387\ncompatibility: 7.509000000000001\naccessibility: -2.6081850785666383\nreward: 0.6390178799629211\ncompatibility: 7.5735\naccessibility: -2.8844196408149103\nreward: 1.0668156147003174\ncompatibility: 7.553\naccessibility: -2.2317408788795357\nreward: 0.9902932643890381\ncompatibility: 7.4375\naccessibility: -2.284649384370366\nreward: 0.8347284197807312\ncompatibility: 7.418000000000003\naccessibility: -2.507550205455086\nreward: 0.6757606267929077\ncompatibility: 7.598000000000004\naccessibility: -2.842430492938162\nreward: 0.9973580241203308\ncompatibility: 7.5554999999999986\naccessibility: -2.337266514096758\nreward: 0.6579753160476685\ncompatibility: 7.3195000000000014\naccessibility: -2.719912058787662\nreward: 0.8051184415817261\ncompatibility: 7.567\naccessibility: -2.6317866597562136\nreward: 0.8781757354736328\ncompatibility: 7.5569999999999995\naccessibility: -2.5168435552648503\nreward: 0.8050999045372009\ncompatibility: 7.361999999999999\naccessibility: -2.521993039807861\nreward: 0.8182725310325623\ncompatibility: 7.553\naccessibility: -2.6045554686735093\nreward: 1.0341906547546387\ncompatibility: 7.443\naccessibility: -2.221749755895836\nreward: 0.7328900694847107\ncompatibility: 7.631499999999999\naccessibility: -2.7746827113211108\nreward: 1.0930348634719849\ncompatibility: 7.3389999999999995\naccessibility: -2.0777690938690254\nreward: 0.822904646396637\ncompatibility: 7.6775\naccessibility: -2.6643037704612587\nreward: 0.9520847797393799\ncompatibility: 7.484500000000001\naccessibility: -2.367140700741137\nreward: 0.8281853199005127\ncompatibility: 7.499999999999997\naccessibility: -2.561293405521556\nreward: 1.0466982126235962\ncompatibility: 7.413000000000002\naccessibility: -2.1869170285497863\nreward: 0.8384304642677307\ncompatibility: 7.417999999999999\naccessibility: -2.501997171813748\nreward: 1.067413330078125\ncompatibility: 7.489500000000003\naccessibility: -2.196826345020873\nreward: 1.16850745677948\ncompatibility: 7.7535\naccessibility: -2.1866137347972834\nreward: 1.1608792543411255\ncompatibility: 7.511999999999997\naccessibility: -2.0686811340190534\nreward: 1.0659987926483154\ncompatibility: 7.5275000000000025\naccessibility: -2.2193053553289355\nreward: 0.7591837048530579\ncompatibility: 7.403999999999999\naccessibility: -2.6133672642890575\nreward: 1.167850136756897\ncompatibility: 7.6404999999999985\naccessibility: -2.127064123795195\nreward: 0.8100246787071228\ncompatibility: 7.398500000000001\naccessibility: -2.5341593781352056\nreward: 0.64193195104599\ncompatibility: 7.513000000000001\naccessibility: -2.847637762768387\nreward: 0.8306368589401245\ncompatibility: 7.395999999999997\naccessibility: -2.5019018647712477\nreward: 0.6446359157562256\ncompatibility: 7.3705\naccessibility: -2.767242590772859\nreward: 0.8421597480773926\ncompatibility: 7.246500000000001\naccessibility: -2.4045281971587746\nreward: 1.0812090635299683\ncompatibility: 7.475999999999998\naccessibility: -2.1689007789838177\nreward: 0.9493185877799988\ncompatibility: 7.487999999999999\naccessibility: -2.3731650053789277\nreward: 1.0989328622817993\ncompatibility: 7.533\naccessibility: -2.172850766542831\nreward: 1.1865218877792358\ncompatibility: 7.286000000000001\naccessibility: -1.9091457602313473\nreward: 1.2290042638778687\ncompatibility: 7.6080000000000005\naccessibility: -2.017922207513094\nreward: 1.0006945133209229\ncompatibility: 7.405500000000001\naccessibility: -2.251904594151489\nreward: 0.9497776627540588\ncompatibility: 7.7055\naccessibility: -2.488994192384861\nreward: 0.9775341153144836\ncompatibility: 7.5\naccessibility: -2.337270263277315\nreward: 1.1147702932357788\ncompatibility: 7.638000000000002\naccessibility: -2.2053445083991225\nreward: 0.8744654655456543\ncompatibility: 7.529500000000001\naccessibility: -2.5076768151210778\nreward: 1.0626355409622192\ncompatibility: 7.559000000000001\naccessibility: -2.241225347397746\nreward: 1.0555886030197144\ncompatibility: 7.635999999999999\naccessibility: -2.293045676328677\nreward: 0.8226395845413208\ncompatibility: 7.4899999999999975\naccessibility: -2.5642549009346465\nreward: 0.9172139763832092\ncompatibility: 7.311999999999997\naccessibility: -2.327036184176902\nreward: 1.1152966022491455\ncompatibility: 7.3595000000000015\naccessibility: -2.0553586527256105\nreward: 1.0736969709396362\ncompatibility: 7.432500000000004\naccessibility: -2.156865317873504\nreward: 0.8742191791534424\ncompatibility: 7.621499999999998\naccessibility: -2.5573319761621973\nreward: 1.0144778490066528\ncompatibility: 7.587\naccessibility: -2.328461736752718\nreward: 0.9459190368652344\ncompatibility: 7.448\naccessibility: -2.3568357424853232\nreward: 0.9202354550361633\ncompatibility: 7.788\naccessibility: -2.5775039975175167\nreward: 0.9885869026184082\ncompatibility: 7.672999999999999\naccessibility: -2.413369650954252\nreward: 0.7895238995552063\ncompatibility: 7.5470000000000015\naccessibility: -2.644464175351555\nreward: 0.7723486423492432\ncompatibility: 7.314499999999998\naccessibility: -2.5456734257680735\nreward: 0.7743613719940186\ncompatibility: 7.431500000000001\naccessibility: -2.6053329387685302\nreward: 0.761532723903656\ncompatibility: 7.5165\naccessibility: -2.670111614511439\nreward: 0.7278103828430176\ncompatibility: 7.587500000000001\naccessibility: -2.758730822678759\nreward: 0.8459500670433044\ncompatibility: 7.490500000000002\naccessibility: -2.5295570846502438\nreward: 0.9716904759407043\ncompatibility: 7.369999999999999\naccessibility: -2.276392884189061\nreward: 1.233682632446289\ncompatibility: 7.755500000000003\naccessibility: -2.089922488543228\nreward: 1.2000055313110352\ncompatibility: 7.805\naccessibility: -2.16695595240631\nreward: 0.9350435733795166\ncompatibility: 7.3375\naccessibility: -2.3139524921817585\nreward: 1.014121413230896\ncompatibility: 7.72\naccessibility: -2.4002463697354117\nreward: 0.8064008951187134\ncompatibility: 7.5625\naccessibility: -2.627452201228896\nreward: 1.0369130373001099\ncompatibility: 7.564500000000001\naccessibility: -2.282755451050336\nreward: 0.9060545563697815\ncompatibility: 7.509000000000001\naccessibility: -2.4493110629338526\nreward: 1.154375433921814\ncompatibility: 7.650000000000002\naccessibility: -2.1523654300030275\nreward: 0.6605491638183594\ncompatibility: 7.4385\naccessibility: -2.7798012720171554\nreward: 0.7924622297286987\ncompatibility: 7.702499999999998\naccessibility: -2.723360254937966\nreward: 0.9318495988845825\ncompatibility: 7.534000000000001\naccessibility: -2.4240113107967556\nreward: 0.900183916091919\ncompatibility: 7.646500000000003\naccessibility: -2.5317777139370734\nreward: 0.8695482015609741\ncompatibility: 7.5104999999999995\naccessibility: -2.5048741363831932\nreward: 1.2041919231414795\ncompatibility: 7.587999999999998\naccessibility: -2.044426386702669\nreward: 0.7777138948440552\ncompatibility: 7.525000000000001\naccessibility: -2.650393466899901\nreward: 0.983837366104126\ncompatibility: 7.475499999999999\naccessibility: -2.3146903623650776\nreward: 0.8423292636871338\ncompatibility: 7.392500000000001\naccessibility: -2.482488217876995\nreward: 1.0591084957122803\ncompatibility: 7.610999999999999\naccessibility: -2.2743729869452416\nreward: 1.146153450012207\ncompatibility: 7.672000000000002\naccessibility: -2.176484053815022\nreward: 0.9501000046730042\ncompatibility: 7.509499999999999\naccessibility: -2.3835106653527993\nreward: 1.027349829673767\ncompatibility: 7.5335\naccessibility: -2.2804931583456565\nreward: 0.973271906375885\ncompatibility: 7.498000000000003\naccessibility: -2.3425921144403206\nreward: 0.8827406764030457\ncompatibility: 7.5009999999999994\naccessibility: -2.479996149360278\nreward: 1.0721992254257202\ncompatibility: 7.442500000000001\naccessibility: -2.164468949156409\nreward: 0.9908573031425476\ncompatibility: 7.46\naccessibility: -2.29585689501931\nreward: 1.0421669483184814\ncompatibility: 7.331000000000001\naccessibility: -2.1497853749567417\nreward: 1.098697543144226\ncompatibility: 7.507499999999998\naccessibility: -2.159542910466209\nreward: 1.0138928890228271\ncompatibility: 7.370999999999999\naccessibility: -2.213625022834022\nreward: 0.9301879405975342\ncompatibility: 7.417000000000001\naccessibility: -2.3638252081858293\nreward: 0.9716499447822571\ncompatibility: 7.407\naccessibility: -2.2962751119722267\nreward: 0.8193615674972534\ncompatibility: 7.505500000000001\naccessibility: -2.5774755428179197\nreward: 1.1437444686889648\ncompatibility: 7.523499999999998\naccessibility: -2.100544001915976\nreward: 0.9213454723358154\ncompatibility: 7.594000000000005\naccessibility: -2.471910343275399\nreward: 0.8487361669540405\ncompatibility: 7.6560000000000015\naccessibility: -2.6140385861813473\nreward: 1.2147040367126465\ncompatibility: 7.689500000000002\naccessibility: -2.083033187323562\nreward: 0.8918745517730713\ncompatibility: 7.551500000000001\naccessibility: -2.4933488939109107\nreward: 0.9674833416938782\ncompatibility: 7.297000000000001\naccessibility: -2.2435964175134107\nreward: 1.0233924388885498\ncompatibility: 7.316500000000001\naccessibility: -2.1701792667399515\nreward: 1.014089584350586\ncompatibility: 7.646\naccessibility: -2.36065129915559\nreward: 0.9841336607933044\ncompatibility: 7.5245\naccessibility: -2.340495979672546\nreward: 0.9759159088134766\ncompatibility: 7.5\naccessibility: -2.3396975468931274\nreward: 0.9510783553123474\ncompatibility: 7.396499999999998\naccessibility: -2.321507495181461\nreward: 0.8901084661483765\ncompatibility: 7.5889999999999995\naccessibility: -2.5160873209951586\nreward: 1.1369961500167847\ncompatibility: 7.355500000000002\naccessibility: -2.0206665103312904\nreward: 1.0681158304214478\ncompatibility: 7.578500000000003\naccessibility: -2.243451248747086\nreward: 1.2308011054992676\ncompatibility: 7.5615000000000006\naccessibility: -1.9903162549839106\nreward: 0.8294069766998291\ncompatibility: 7.483999999999998\naccessibility: -2.5508895507158433\nreward: 1.1198784112930298\ncompatibility: 7.618000000000002\naccessibility: -2.1869680898626562\nreward: 0.985488772392273\ncompatibility: 7.3835000000000015\naccessibility: -2.2629275292923614\nreward: 1.0045735836029053\ncompatibility: 7.176499999999996\naccessibility: -2.123407470837096\nreward: 0.750731348991394\ncompatibility: 7.2330000000000005\naccessibility: -2.534438728761032\nreward: 0.8640552759170532\ncompatibility: 7.350499999999999\naccessibility: -2.4273992223993277\nreward: 0.9061480760574341\ncompatibility: 7.678499999999999\naccessibility: -2.5399743440461235\nreward: 0.8476107716560364\ncompatibility: 7.391999999999997\naccessibility: -2.4742981105380473\nreward: 1.1360993385314941\ncompatibility: 7.435499999999999\naccessibility: -2.0648689195627234\nreward: 1.114949345588684\ncompatibility: 7.643000000000001\naccessibility: -2.207754615389752\nreward: 1.0618476867675781\ncompatibility: 7.656499999999999\naccessibility: -2.294639175520637\nreward: 0.8561275601387024\ncompatibility: 7.4675\naccessibility: -2.5019694138324\nreward: 0.8008655905723572\ncompatibility: 7.511499999999997\naccessibility: -2.6084337524991654\nreward: 0.764004111289978\ncompatibility: 7.237999999999999\naccessibility: -2.517208160476558\nreward: 0.8270776271820068\ncompatibility: 7.56\naccessibility: -2.5950978803304743\nreward: 0.9242817759513855\ncompatibility: 7.509999999999998\naccessibility: -2.422505895708321\nreward: 0.88908851146698\ncompatibility: 7.554000000000002\naccessibility: -2.498867236877306\nreward: 0.9471681118011475\ncompatibility: 7.571999999999999\naccessibility: -2.4213907010210236\nreward: 0.7548790574073792\ncompatibility: 7.381000000000001\naccessibility: -2.6075028836834413\nreward: 0.9633418917655945\ncompatibility: 7.5850000000000035\naccessibility: -2.4040943368795253\nreward: 0.784085214138031\ncompatibility: 7.635000000000003\naccessibility: -2.699765037028\nreward: 0.8730270266532898\ncompatibility: 7.3125000000000036\naccessibility: -2.3935844472549768\nreward: 1.1377328634262085\ncompatibility: 7.571500000000003\naccessibility: -2.135275723274566\nreward: 0.7114089727401733\ncompatibility: 7.326499999999999\naccessibility: -2.643511518596521\nreward: 0.9566562175750732\ncompatibility: 7.428000000000001\naccessibility: -2.3300156905369658\nreward: 0.9675345420837402\ncompatibility: 7.526000000000002\naccessibility: -2.3661981813166664\nreward: 1.0989797115325928\ncompatibility: 7.623000000000001\naccessibility: -2.2209947773216303\nreward: 1.2654927968978882\ncompatibility: 7.732499999999999\naccessibility: -2.029885823696346\nreward: 1.0397282838821411\ncompatibility: 7.427\naccessibility: -2.204871793831414\nreward: 0.8932971954345703\ncompatibility: 7.2615\naccessibility: -2.335857766957852\nreward: 1.0894919633865356\ncompatibility: 7.633000000000003\naccessibility: -2.2405834340421573\nreward: 0.9450327157974243\ncompatibility: 7.389500000000002\naccessibility: -2.3268259204271606\nreward: 1.0122617483139038\ncompatibility: 7.532\naccessibility: -2.3023217204852786\nreward: 1.0124520063400269\ncompatibility: 7.578499999999999\naccessibility: -2.3269469281132076\nreward: 1.1070234775543213\ncompatibility: 7.702999999999999\naccessibility: -2.2517862829555266\nreward: 0.9246192574501038\ncompatibility: 7.316\naccessibility: -2.318071139641619\nreward: 1.0073007345199585\ncompatibility: 7.367500000000002\naccessibility: -2.2216380952080006\nreward: 1.0196629762649536\ncompatibility: 7.393500000000002\naccessibility: -2.2170234372922533\nreward: 1.0063139200210571\ncompatibility: 7.618000000000004\naccessibility: -2.3573147606824216\nreward: 0.611673891544342\ncompatibility: 7.496000000000001\naccessibility: -2.883917750239406\nreward: 0.7672730088233948\ncompatibility: 7.441999999999999\naccessibility: -2.6215905046349923\nreward: 0.71954345703125\ncompatibility: 7.678\naccessibility: -2.819613357753719\nreward: 1.2617061138153076\ncompatibility: 7.602\naccessibility: -1.9656550297056805\nreward: 1.006765365600586\ncompatibility: 7.357000000000002\naccessibility: -2.216816226636637\nreward: 0.8948385715484619\ncompatibility: 7.414\naccessibility: -2.415242128794624\nreward: 0.9111806750297546\ncompatibility: 7.2284999999999995\naccessibility: -2.2913539598609445\nreward: 0.8465293049812317\ncompatibility: 7.479500000000001\naccessibility: -2.5227952984842923\nreward: 0.8395559787750244\ncompatibility: 7.444500000000001\naccessibility: -2.5145052751075143\nreward: 0.8147138953208923\ncompatibility: 7.4575\naccessibility: -2.5587327010954937\nreward: 0.7522510886192322\ncompatibility: 7.3585\naccessibility: -2.5993912423917758\nreward: 0.9730192422866821\ncompatibility: 7.502000000000001\naccessibility: -2.3451140380387816\nreward: 1.0944688320159912\ncompatibility: 7.504000000000001\naccessibility: -2.1640111118071474\nreward: 1.0620704889297485\ncompatibility: 7.504500000000002\naccessibility: -2.2128763562968885\nreward: 0.707175612449646\ncompatibility: 7.649000000000002\naccessibility: -2.8226294282038857\nreward: 1.009891390800476\ncompatibility: 7.5504999999999995\naccessibility: -2.315787885129426\nreward: 0.7535194158554077\ncompatibility: 7.273\naccessibility: -2.551685135226612\nreward: 1.1851245164871216\ncompatibility: 7.5195000000000025\naccessibility: -2.0363310173745353\nreward: 0.8527527451515198\ncompatibility: 7.385500000000002\naccessibility: -2.4631030484735734\nreward: 0.9853370189666748\ncompatibility: 7.550000000000001\naccessibility: -2.352351649158113\nreward: 1.022054672241211\ncompatibility: 7.588000000000001\naccessibility: -2.317632269956926\nreward: 1.0247728824615479\ncompatibility: 7.647\naccessibility: -2.345162060990237\nreward: 0.9145005941390991\ncompatibility: 7.377999999999998\naccessibility: -2.366463357301729\nreward: 1.072191834449768\ncompatibility: 7.706500000000001\naccessibility: -2.305908691618085\nreward: 0.8110553622245789\ncompatibility: 7.1930000000000005\naccessibility: -2.4225241403575195\nreward: 0.9826562404632568\ncompatibility: 7.714000000000002\naccessibility: -2.444229958330741\nreward: 1.0226197242736816\ncompatibility: 7.434000000000002\naccessibility: -2.234284754055775\nreward: 1.245327115058899\ncompatibility: 7.760500000000002\naccessibility: -2.075134387623033\nreward: 0.9106447696685791\ncompatibility: 7.493499999999999\naccessibility: -2.434122166406022\nreward: 1.033059000968933\ncompatibility: 7.488500000000003\naccessibility: -2.247822281620319\nreward: 1.2869724035263062\ncompatibility: 7.689499999999999\naccessibility: -1.974630688422952\nreward: 0.8092674016952515\ncompatibility: 7.562\naccessibility: -2.622884630860738\nreward: 0.7652043104171753\ncompatibility: 7.422500000000002\naccessibility: -2.6142470956156996\nreward: 0.9035841822624207\ncompatibility: 7.586000000000001\naccessibility: -2.4942665658202134\nreward: 0.9262789487838745\ncompatibility: 7.435499999999999\naccessibility: -2.379599409299227\nreward: 0.8458874821662903\ncompatibility: 7.402500000000001\naccessibility: -2.482508059598766\nreward: 0.7616057991981506\ncompatibility: 7.4395000000000024\naccessibility: -2.628752032065534\nreward: 0.8669748902320862\ncompatibility: 7.593\naccessibility: -2.552930549000356\nreward: 1.1070162057876587\ncompatibility: 7.5965\naccessibility: -2.194743493571667\nreward: 1.014146089553833\ncompatibility: 7.601500000000002\naccessibility: -2.336727380269135\nreward: 0.9299116730690002\ncompatibility: 7.4215\naccessibility: -2.3666503869510658\nreward: 0.656955361366272\ncompatibility: 7.555000000000002\naccessibility: -2.847602634002921\nreward: 0.940255343914032\ncompatibility: 7.4895000000000005\naccessibility: -2.3875634138893758\nreward: 1.1123279333114624\ncompatibility: 7.668999999999998\naccessibility: -2.2256151814642653\nreward: 1.0019737482070923\ncompatibility: 7.699500000000004\naccessibility: -2.407485789136641\nreward: 1.4173669815063477\ncompatibility: 7.664500000000001\naccessibility: -1.7656459438300403\nreward: 1.072526216506958\ncompatibility: 7.490499999999998\naccessibility: -2.1896928315780846\nreward: 0.9445773363113403\ncompatibility: 7.4239999999999995\naccessibility: -2.345991095290187\nreward: 0.6555525660514832\ncompatibility: 7.6549999999999985\naccessibility: -2.903278301106061\nreward: 0.7468647956848145\ncompatibility: 7.4545\naccessibility: -2.658899198787683\nreward: 0.8763821721076965\ncompatibility: 7.170999999999999\naccessibility: -2.3127481732131185\nreward: 0.9620673060417175\ncompatibility: 7.7090000000000005\naccessibility: -2.4724347245627714\nreward: 1.1198824644088745\ncompatibility: 7.6370000000000005\naccessibility: -2.197140598860554\nreward: 1.1699295043945312\ncompatibility: 7.4285000000000005\naccessibility: -2.01037354457972\nreward: 1.198943018913269\ncompatibility: 7.655000000000001\naccessibility: -2.0881926173760856\nreward: 1.0724042654037476\ncompatibility: 7.797000000000001\naccessibility: -2.3540721416422183\nreward: 0.9556198716163635\ncompatibility: 7.511000000000003\naccessibility: -2.376034453724695\nreward: 0.8632164001464844\ncompatibility: 7.464499999999999\naccessibility: -2.4897289737561596\nreward: 1.146378517150879\ncompatibility: 7.506500000000001\naccessibility: -2.0874857922784997\nreward: 0.7178001403808594\ncompatibility: 7.576999999999999\naccessibility: -2.7681212470887284\nreward: 0.7536723017692566\ncompatibility: 7.4079999999999995\naccessibility: -2.6237772787015388\nreward: 1.0344029664993286\ncompatibility: 7.4449999999999985\naccessibility: -2.2225026816239364\nreward: 0.9936432838439941\ncompatibility: 7.426\naccessibility: -2.273463682310938\nreward: 1.0006448030471802\ncompatibility: 7.518000000000002\naccessibility: -2.3122469999560624\nreward: 1.1418442726135254\ncompatibility: 7.247500000000003\naccessibility: -1.9555371531043977\nreward: 0.6345455646514893\ncompatibility: 7.538000000000002\naccessibility: -2.8721102350834937\nreward: 0.9703760147094727\ncompatibility: 7.455499999999996\naccessibility: -2.324168082358474\nreward: 1.180437445640564\ncompatibility: 7.534499999999997\naccessibility: -2.051397387298123\nreward: 0.9710959196090698\ncompatibility: 7.334999999999999\naccessibility: -2.2585346755404556\nreward: 0.9861882328987122\ncompatibility: 7.4949999999999966\naccessibility: -2.321610519355757\nreward: 0.9715927243232727\ncompatibility: 7.661500000000001\naccessibility: -2.432700178082267\nreward: 0.6653826236724854\ncompatibility: 7.459499999999999\naccessibility: -2.783801047174423\nreward: 1.0540357828140259\ncompatibility: 7.5735\naccessibility: -2.2618928280002617\nreward: 0.8454886674880981\ncompatibility: 7.379500000000002\naccessibility: -2.4707848915657404\nreward: 0.9104631543159485\ncompatibility: 7.609000000000001\naccessibility: -2.4962695107368487\nreward: 0.8268341422080994\ncompatibility: 7.264499999999998\naccessibility: -2.437159511237579\nreward: 0.6977795958518982\ncompatibility: 7.456500000000002\naccessibility: -2.7335984972635696\nreward: 1.0285654067993164\ncompatibility: 7.631\naccessibility: -2.330901924316744\nreward: 1.095896601676941\ncompatibility: 7.843999999999999\naccessibility: -2.344012215387737\nreward: 0.8867694735527039\ncompatibility: 7.718999999999999\naccessibility: -2.590738684820111\nreward: 1.0637505054473877\ncompatibility: 7.383499999999998\naccessibility: -2.1455348911542265\nreward: 1.128676414489746\ncompatibility: 7.389499999999998\naccessibility: -2.0513603645093834\nreward: 0.7406635284423828\ncompatibility: 7.4445\naccessibility: -2.6628439851126897\nreward: 0.6716395020484924\ncompatibility: 7.337000000000004\naccessibility: -2.708790713402177\nreward: 1.0433728694915771\ncompatibility: 7.650500000000002\naccessibility: -2.3191372101792043\nreward: 1.0891224145889282\ncompatibility: 7.381\naccessibility: -2.106137736291837\nreward: 0.9206844568252563\ncompatibility: 7.544999999999999\naccessibility: -2.446651892030029\nreward: 0.9948176741600037\ncompatibility: 7.469000000000003\naccessibility: -2.2947377872312926\nreward: 1.0868362188339233\ncompatibility: 7.565999999999999\naccessibility: -2.2086741851764438\nreward: 1.1131525039672852\ncompatibility: 7.620000000000001\naccessibility: -2.198128399357531\nreward: 1.0428622961044312\ncompatibility: 7.4655000000000005\naccessibility: -2.2207957524661293\nreward: 0.8221101760864258\ncompatibility: 7.530000000000002\naccessibility: -2.586477572783597\nreward: 1.0089869499206543\ncompatibility: 7.438999999999998\naccessibility: -2.2574124863414142\nreward: 0.9857614040374756\ncompatibility: 7.174500000000003\naccessibility: -2.150554331598142\nreward: 1.1790682077407837\ncompatibility: 7.756000000000005\naccessibility: -2.172112014897504\nreward: 1.2028878927230835\ncompatibility: 7.6445000000000025\naccessibility: -2.0766503925557576\nreward: 1.1019501686096191\ncompatibility: 7.745500000000002\naccessibility: -2.282164081677709\nreward: 1.2320597171783447\ncompatibility: 7.428999999999999\naccessibility: -1.917446094896148\nreward: 1.0837823152542114\ncompatibility: 7.719999999999998\naccessibility: -2.2957550801775035\nreward: 0.9170064926147461\ncompatibility: 7.480999999999999\naccessibility: -2.417883093133592\nreward: 0.9948024153709412\ncompatibility: 7.1495\naccessibility: -2.123599925587385\nreward: 0.9772913455963135\ncompatibility: 7.566999999999998\naccessibility: -2.3735272712689537\nreward: 0.8478739261627197\ncompatibility: 7.411999999999999\naccessibility: -2.4846176525064196\nreward: 0.6686577796936035\ncompatibility: 7.481999999999995\naccessibility: -2.7909419130130764\nreward: 0.9705389738082886\ncompatibility: 7.644500000000001\naccessibility: -2.4251736683547125\nreward: 0.815163254737854\ncompatibility: 7.332999999999998\naccessibility: -2.491362247795813\nreward: 0.9615673422813416\ncompatibility: 7.621500000000002\naccessibility: -2.4263096568903357\nreward: 0.8971465229988098\ncompatibility: 7.519999999999999\naccessibility: -2.4685659098465105\nreward: 1.0387681722640991\ncompatibility: 7.5095\naccessibility: -2.2505084278209844\nreward: 0.9766887426376343\ncompatibility: 7.560499999999999\naccessibility: -2.3709490243264866\nreward: 0.7222267985343933\ncompatibility: 7.487000000000002\naccessibility: -2.713266973237105\nreward: 0.9584473371505737\ncompatibility: 7.4285000000000005\naccessibility: -2.3275968566808434\nreward: 0.9460912346839905\ncompatibility: 7.579499999999999\naccessibility: -2.427023889389974\nreward: 0.9102148413658142\ncompatibility: 7.397499999999997\naccessibility: -2.3833384421360098\nreward: 1.0552510023117065\ncompatibility: 7.286500000000001\naccessibility: -2.106319974448041\nreward: 1.0403261184692383\ncompatibility: 7.426500000000001\naccessibility: -2.2037073181953177\nreward: 0.9952533841133118\ncompatibility: 7.470999999999998\naccessibility: -2.2951556111246565\nreward: 0.7860214114189148\ncompatibility: 7.504499999999999\naccessibility: -2.62695005873555\nreward: 1.0234735012054443\ncompatibility: 7.306500000000001\naccessibility: -2.1647005295728174\nreward: 0.9458898901939392\ncompatibility: 7.4655000000000005\naccessibility: -2.366254442643361\nreward: 1.1592016220092773\ncompatibility: 7.758500000000003\naccessibility: -2.2032511203585785\nreward: 1.0204979181289673\ncompatibility: 7.742999999999999\naccessibility: -2.403003110214033\nreward: 0.9725177884101868\ncompatibility: 7.491999999999999\naccessibility: -2.3405089943418873\nreward: 1.136448860168457\ncompatibility: 7.457999999999998\naccessibility: -2.076398086696565\nreward: 1.0519570112228394\ncompatibility: 7.541\naccessibility: -2.247600183793528\nreward: 0.7671223282814026\ncompatibility: 7.372999999999999\naccessibility: -2.5848522071495657\nreward: 0.7106555700302124\ncompatibility: 7.332999999999999\naccessibility: -2.648123743186166\nreward: 0.9550873637199402\ncompatibility: 7.397499999999999\naccessibility: -2.31602968606904\nreward: 0.5199176669120789\ncompatibility: 7.279000000000001\naccessibility: -2.905302037275894\nreward: 0.8379855751991272\ncompatibility: 7.434500000000001\naccessibility: -2.511503761613762\nreward: 0.7012051939964294\ncompatibility: 7.666999999999997\naccessibility: -2.841227896363722\nreward: 1.0013530254364014\ncompatibility: 7.529000000000002\naccessibility: -2.3170775440807887\nreward: 0.9292639493942261\ncompatibility: 7.324000000000004\naccessibility: -2.3153897932417125\nreward: 1.130307674407959\ncompatibility: 7.541000000000001\naccessibility: -2.1300742839587974\nreward: 0.744481086730957\ncompatibility: 7.380500000000001\naccessibility: -2.622831969306323\nreward: 1.2032318115234375\ncompatibility: 7.479000000000002\naccessibility: -1.9874736501305112\nreward: 1.2134060859680176\ncompatibility: 7.731999999999998\naccessibility: -2.1077479879695\nreward: 1.0358384847640991\ncompatibility: 7.372500000000001\naccessibility: -2.181510041918224\nreward: 0.6049904227256775\ncompatibility: 7.3835000000000015\naccessibility: -2.833675069103616\nreward: 0.9993078708648682\ncompatibility: 7.3595\naccessibility: -2.2293417526924046\nreward: 0.9548373222351074\ncompatibility: 7.498000000000001\naccessibility: -2.3702440324025877\nreward: 0.7974358201026917\ncompatibility: 7.620500000000002\naccessibility: -2.671971243503366\nreward: 0.8749094605445862\ncompatibility: 7.516\naccessibility: -2.499778657335816\nreward: 0.8709151148796082\ncompatibility: 7.4190000000000005\naccessibility: -2.453805918606609\nreward: 1.0475455522537231\ncompatibility: 7.693500000000001\naccessibility: -2.335913874236728\nreward: 1.1087802648544312\ncompatibility: 7.497000000000001\naccessibility: -2.1387939346775333\nreward: 0.9991783499717712\ncompatibility: 7.401\naccessibility: -2.251768199267585\nreward: 1.0770069360733032\ncompatibility: 7.644\naccessibility: -2.265203854985706\nreward: 1.0899219512939453\ncompatibility: 7.698\naccessibility: -2.2747598890222243\nreward: 0.881747305393219\ncompatibility: 7.301\naccessibility: -2.3743433263380322\nreward: 1.0395137071609497\ncompatibility: 7.354500000000001\naccessibility: -2.166354415262191\nreward: 0.829992413520813\ncompatibility: 7.378000000000001\naccessibility: -2.4932256746823067\nreward: 0.9788386821746826\ncompatibility: 7.663000000000002\naccessibility: -2.4226348542444045\nreward: 1.2264585494995117\ncompatibility: 7.71\naccessibility: -2.0763836066516568\nreward: 1.2247905731201172\ncompatibility: 7.683000000000003\naccessibility: -2.06442135656295\nreward: 0.8476112484931946\ncompatibility: 7.477000000000001\naccessibility: -2.519833130613107\nreward: 0.8904498815536499\ncompatibility: 7.409999999999999\naccessibility: -2.4196823336531637\nreward: 1.0644193887710571\ncompatibility: 7.608500000000003\naccessibility: -2.2650672832726833\nreward: 1.1743683815002441\ncompatibility: 7.521000000000002\naccessibility: -2.053268809055034\nreward: 0.7591405510902405\ncompatibility: 7.4970000000000026\naccessibility: -2.6632534671345347\nreward: 1.1065142154693604\ncompatibility: 7.701499999999999\naccessibility: -2.251746523144274\nreward: 1.1309056282043457\ncompatibility: 7.5794999999999995\naccessibility: -2.1498022422526697\nreward: 0.7876055240631104\ncompatibility: 7.384499999999999\naccessibility: -2.560288183864211\nreward: 0.8799051642417908\ncompatibility: 7.5585\naccessibility: -2.5150529862269675\nreward: 0.9213213324546814\ncompatibility: 7.311499999999999\naccessibility: -2.320607319043722\nreward: 1.1200664043426514\ncompatibility: 7.661500000000002\naccessibility: -2.209989643375065\nreward: 0.8523533940315247\ncompatibility: 7.605999999999998\naccessibility: -2.5818270537122525\nreward: 0.7772584557533264\ncompatibility: 7.662500000000001\naccessibility: -2.7247372901495837\nreward: 1.0131112337112427\ncompatibility: 7.239000000000002\naccessibility: -2.1440831985261353\nreward: 0.7762280106544495\ncompatibility: 7.375999999999998\naccessibility: -2.572800874880233\nreward: 1.120207667350769\ncompatibility: 7.3839999999999995\naccessibility: -2.0611170680632114\nreward: 1.1613264083862305\ncompatibility: 7.506500000000001\naccessibility: -2.065063885267624\nreward: 0.8849087357521057\ncompatibility: 7.598499999999999\naccessibility: -2.528976137513541\nreward: 1.107092022895813\ncompatibility: 7.6675\naccessibility: -2.2326655792050665\nreward: 1.2544753551483154\ncompatibility: 7.548500000000002\naccessibility: -1.9478406048952976\nreward: 0.8233460187911987\ncompatibility: 7.390500000000002\naccessibility: -2.509891671188016\nreward: 0.36054012179374695\ncompatibility: 7.4415\naccessibility: -3.2314219379053175\nreward: 1.091427206993103\ncompatibility: 7.753500000000003\naccessibility: -2.30223418368849\nreward: 0.898604154586792\ncompatibility: 7.562500000000003\naccessibility: -2.489147383930572\nreward: 0.960161566734314\ncompatibility: 7.705\naccessibility: -2.4731504988884643\nreward: 0.8823691606521606\ncompatibility: 7.490500000000002\naccessibility: -2.474928443033973\nreward: 1.0729506015777588\ncompatibility: 7.468500000000001\naccessibility: -2.1772704544954564\nreward: 1.2178887128829956\ncompatibility: 7.654500000000001\naccessibility: -2.059506187958096\nreward: 1.0425211191177368\ncompatibility: 7.437999999999999\naccessibility: -2.2065754336007433\nreward: 0.7498046159744263\ncompatibility: 7.293000000000002\naccessibility: -2.5679716745166874\nreward: 0.695408284664154\ncompatibility: 7.496999999999999\naccessibility: -2.758851878086609\nreward: 0.6421288847923279\ncompatibility: 7.364999999999998\naccessibility: -2.7680566338781953\nreward: 0.713749349117279\ncompatibility: 7.343000000000002\naccessibility: -2.6488402866817795\nreward: 0.9720063805580139\ncompatibility: 7.358\naccessibility: -2.2694904102783284\nreward: 1.072624921798706\ncompatibility: 7.4765\naccessibility: -2.182044721044759\nreward: 1.0723905563354492\ncompatibility: 7.620500000000003\naccessibility: -2.259539235834027\nreward: 0.923015296459198\ncompatibility: 7.498999999999999\naccessibility: -2.4185127779192594\nreward: 0.9686843752861023\ncompatibility: 7.4650000000000025\naccessibility: -2.331794825709117\nreward: 0.9493582844734192\ncompatibility: 7.412000000000002\naccessibility: -2.3323911823352415\nreward: 0.7873442769050598\ncompatibility: 7.423999999999999\naccessibility: -2.5818407196986985\nreward: 1.041298747062683\ncompatibility: 7.8115000000000006\naccessibility: -2.4084983812746144\nreward: 0.6191455125808716\ncompatibility: 7.3625000000000025\naccessibility: -2.801192445339324\nreward: 0.8581708669662476\ncompatibility: 7.300000000000001\naccessibility: -2.409172245408831\nreward: 1.0131068229675293\ncompatibility: 7.401\naccessibility: -2.230875477100583\nreward: 0.8454551696777344\ncompatibility: 7.2684999999999995\naccessibility: -2.4113707789136845\nreward: 1.2291131019592285\ncompatibility: 7.679499999999999\naccessibility: -2.056062499855411\nreward: 0.8050966262817383\ncompatibility: 7.440000000000003\naccessibility: -2.563783649100817\nreward: 1.069056510925293\ncompatibility: 7.514500000000003\naccessibility: -2.207754524907555\nreward: 0.8705647587776184\ncompatibility: 7.585500000000001\naccessibility: -2.5435278509426817\nreward: 0.8905991911888123\ncompatibility: 7.578499999999999\naccessibility: -2.509726171536883\nreward: 1.0628191232681274\ncompatibility: 7.5969999999999995\naccessibility: -2.2613070642362145\nreward: 0.7956426739692688\ncompatibility: 7.269000000000001\naccessibility: -2.4863573960649483\nreward: 1.064284086227417\ncompatibility: 7.398500000000001\naccessibility: -2.1527703088579684\nreward: 0.6929183602333069\ncompatibility: 7.4129999999999985\naccessibility: -2.7175867604949446\nreward: 0.955446720123291\ncompatibility: 7.354999999999999\naccessibility: -2.2927227779693795\nreward: 0.9800522923469543\ncompatibility: 7.386\naccessibility: -2.2724215767236107\nreward: 0.6499155163764954\ncompatibility: 7.356499999999999\naccessibility: -2.751823157135033\nreward: 0.5984123349189758\ncompatibility: 7.557500000000001\naccessibility: -2.936756530600838\nreward: 1.0696067810058594\ncompatibility: 7.700500000000002\naccessibility: -2.306571909441869\nreward: 1.0744274854660034\ncompatibility: 7.447499999999999\naccessibility: -2.163805200915284\nreward: 0.9322302937507629\ncompatibility: 7.393500000000001\naccessibility: -2.348172399624394\nreward: 1.0001188516616821\ncompatibility: 7.532\naccessibility: -2.3205360293108734\nreward: 1.0300416946411133\ncompatibility: 7.3759999999999994\naccessibility: -2.1920803879013073\nreward: 0.9381710290908813\ncompatibility: 7.286999999999999\naccessibility: -2.282207719024074\nreward: 1.121543526649475\ncompatibility: 7.714000000000001\naccessibility: -2.2358990208444434\nreward: 1.306979775428772\ncompatibility: 7.5695\naccessibility: -1.8803338279427544\nreward: 0.6160629987716675\ncompatibility: 7.355\naccessibility: -2.8017983848179657\nreward: 1.031986951828003\ncompatibility: 7.674499999999998\naccessibility: -2.349073185404648\nreward: 0.7778021097183228\ncompatibility: 7.477500000000003\naccessibility: -2.624814649452385\nreward: 0.7858901023864746\ncompatibility: 7.2535\naccessibility: -2.4926827315542437\nreward: 0.9358208775520325\ncompatibility: 7.442499999999999\naccessibility: -2.369036528924144\nreward: 1.4042303562164307\ncompatibility: 7.798000000000003\naccessibility: -1.8568687524105996\nreward: 0.8738835453987122\ncompatibility: 7.4195\naccessibility: -2.4496210665792297\nreward: 1.1165746450424194\ncompatibility: 7.632\naccessibility: -2.199423833318452\nreward: 1.1960411071777344\ncompatibility: 7.6419999999999995\naccessibility: -2.0855812737097676\nreward: 1.1267166137695312\ncompatibility: 7.555499999999999\naccessibility: -2.1432287098291254\nreward: 1.05485200881958\ncompatibility: 7.689999999999998\naccessibility: -2.323079059847235\nreward: 0.9689437747001648\ncompatibility: 7.445499999999999\naccessibility: -2.3209593306498926\nreward: 1.1727336645126343\ncompatibility: 7.485000000000001\naccessibility: -2.03643520193083\nreward: 1.0756456851959229\ncompatibility: 7.5764999999999985\naccessibility: -2.2310850333980774\nreward: 0.9276819825172424\ncompatibility: 7.449\naccessibility: -2.384727032982986\nreward: 0.68752121925354\ncompatibility: 7.371500000000001\naccessibility: -2.7034503017763054\nreward: 1.2555049657821655\ncompatibility: 7.618999999999999\naccessibility: -1.984064066362712\nreward: 0.8305736780166626\ncompatibility: 7.505\naccessibility: -2.5603894515235806\nreward: 1.2425031661987305\ncompatibility: 7.560999999999999\naccessibility: -1.9724952062408283\nreward: 0.9633597731590271\ncompatibility: 7.3765\naccessibility: -2.2923710274995415\nreward: 1.1309376955032349\ncompatibility: 7.7345000000000015\naccessibility: -2.232789850877592\nreward: 0.8160281181335449\ncompatibility: 7.494\naccessibility: -2.576314957180842\nreward: 0.6057006120681763\ncompatibility: 7.44\naccessibility: -2.862877669508643\nreward: 0.8738200664520264\ncompatibility: 7.534000000000001\naccessibility: -2.511055618594582\nreward: 0.8307087421417236\ncompatibility: 7.2885\naccessibility: -2.4442047085290795\nreward: 1.1626209020614624\ncompatibility: 7.7170000000000005\naccessibility: -2.1758900737675333\nreward: 0.8068614602088928\ncompatibility: 7.415500000000001\naccessibility: -2.5480113650494416\nreward: 0.7496371865272522\ncompatibility: 7.333999999999998\naccessibility: -2.5901870995992606\nreward: 1.0673432350158691\ncompatibility: 7.752499999999998\naccessibility: -2.337824361810866\nreward: 0.8955479264259338\ncompatibility: 7.439500000000001\naccessibility: -2.427838854083423\nreward: 0.9539943933486938\ncompatibility: 7.4650000000000025\naccessibility: -2.3538298457952544\nreward: 1.1521281003952026\ncompatibility: 7.362000000000002\naccessibility: -2.001450721473788\nreward: 0.9565844535827637\ncompatibility: 7.757999999999998\naccessibility: -2.506908993971628\nreward: 0.9749065637588501\ncompatibility: 7.505000000000001\naccessibility: -2.3438901980280504\nreward: 1.0621871948242188\ncompatibility: 7.517500000000001\naccessibility: -2.219665563464299\nreward: 1.2077481746673584\ncompatibility: 7.605499999999999\naccessibility: -2.048467092840828\nreward: 1.069118618965149\ncompatibility: 7.5405000000000015\naccessibility: -2.2215899914999535\nreward: 0.6447481513023376\ncompatibility: 7.354000000000001\naccessibility: -2.758234899438867\nreward: 1.0922106504440308\ncompatibility: 7.5445\naccessibility: -2.189094816479669\nreward: 1.0227535963058472\ncompatibility: 7.451000000000001\naccessibility: -2.243191043278392\nreward: 1.1559220552444458\ncompatibility: 7.628000000000001\naccessibility: -2.1382597630447515\nreward: 0.960394024848938\ncompatibility: 7.437\naccessibility: -2.329230412841398\nreward: 0.8637779355049133\ncompatibility: 7.259000000000003\naccessibility: -2.3787974138227908\nreward: 1.0254861116409302\ncompatibility: 7.5895\naccessibility: -2.3132887200658097\nreward: 0.9124333262443542\ncompatibility: 7.506500000000001\naccessibility: -2.4384036211256856\nreward: 0.6427286267280579\ncompatibility: 7.4464999999999995\naccessibility: -2.810817781338019\nreward: 0.8758523464202881\ncompatibility: 7.350999999999999\naccessibility: -2.4099714578171043\nreward: 1.0580872297286987\ncompatibility: 7.33\naccessibility: -2.125369090100946\nreward: 0.8159595131874084\ncompatibility: 7.555999999999999\naccessibility: -2.60963216083435\nreward: 0.9654052257537842\ncompatibility: 7.445000000000004\naccessibility: -2.325999279131856\nreward: 0.7393580675125122\ncompatibility: 7.4275\naccessibility: -2.6556950317168675\nreward: 0.8261555433273315\ncompatibility: 7.399500000000002\naccessibility: -2.5104988467108145\nreward: 0.9595401287078857\ncompatibility: 7.382500000000001\naccessibility: -2.301314848843425\nreward: 0.9458364248275757\ncompatibility: 7.379499999999999\naccessibility: -2.32026318521348\nreward: 0.7865015864372253\ncompatibility: 7.376500000000001\naccessibility: -2.5576583487566102\nreward: 1.135758399963379\ncompatibility: 7.695999999999998\naccessibility: -2.2049337764922026\nreward: 0.8428593873977661\ncompatibility: 7.549000000000003\naccessibility: -2.5655323639225895\nreward: 0.9557448625564575\ncompatibility: 7.454\naccessibility: -2.3453112338033577\nreward: 0.7452964186668396\ncompatibility: 7.526000000000002\naccessibility: -2.6995553918571726\nreward: 1.0771660804748535\ncompatibility: 7.574\naccessibility: -2.22746525426109\nreward: 0.6127362251281738\ncompatibility: 7.437500000000001\naccessibility: -2.8509849624705774\nreward: 0.7893628478050232\ncompatibility: 7.3875\naccessibility: -2.5592593180754997\nreward: 0.9093740582466125\ncompatibility: 7.3995000000000015\naccessibility: -2.385671098997629\nreward: 0.7692880034446716\ncompatibility: 7.259500000000002\naccessibility: -2.520800125320682\nreward: 1.1452769041061401\ncompatibility: 7.620500000000003\naccessibility: -2.150209562868083\nreward: 0.9174456596374512\ncompatibility: 7.296499999999997\naccessibility: -2.3183851113386242\nreward: 1.2329307794570923\ncompatibility: 7.775499999999999\naccessibility: -2.101764622851036\nreward: 0.9932916760444641\ncompatibility: 7.5055000000000005\naccessibility: -2.316580330407067\nreward: 1.0363662242889404\ncompatibility: 7.546000000000004\naccessibility: -2.273665008332752\nreward: 1.191735029220581\ncompatibility: 7.749000000000002\naccessibility: -2.1493617952688595\nreward: 1.0233887434005737\ncompatibility: 7.553499999999999\naccessibility: -2.2971490225937665\nreward: 0.8969034552574158\ncompatibility: 7.430000000000002\naccessibility: -2.420716261744701\nreward: 0.9666520953178406\ncompatibility: 7.785500000000001\naccessibility: -2.5065396964396136\nreward: 1.0233118534088135\ncompatibility: 7.6370000000000005\naccessibility: -2.341996571860256\nreward: 0.8433623313903809\ncompatibility: 7.523\naccessibility: -2.55084936251176\nreward: 0.9864023923873901\ncompatibility: 7.499499999999999\naccessibility: -2.3236999439670587\nreward: 0.9126200079917908\ncompatibility: 7.7150000000000025\naccessibility: -2.5498199613596286\nreward: 1.1243278980255127\ncompatibility: 7.498000000000003\naccessibility: -2.116008121387637\nreward: 0.8069870471954346\ncompatibility: 7.520500000000001\naccessibility: -2.604073022616077\nreward: 0.8478701710700989\ncompatibility: 7.511500000000001\naccessibility: -2.5379269061429985\nreward: 0.8416736125946045\ncompatibility: 7.403999999999999\naccessibility: -2.489632435304704\nreward: 1.2106213569641113\ncompatibility: 7.577499999999999\naccessibility: -2.0291572183486797\nreward: 0.9681575298309326\ncompatibility: 7.434499999999998\naccessibility: -2.3162458533291232\nreward: 0.9708530902862549\ncompatibility: 7.6575000000000015\naccessibility: -2.43166677208243\nreward: 0.6064141988754272\ncompatibility: 7.310500000000003\naccessibility: -2.7924323135654\nreward: 1.156635046005249\ncompatibility: 7.552\naccessibility: -2.096476071204192\nreward: 0.8847096562385559\ncompatibility: 7.7494999999999985\naccessibility: -2.6101676255646664\nreward: 1.1427690982818604\ncompatibility: 7.662000000000002\naccessibility: -2.1762034313120497\nreward: 1.1159013509750366\ncompatibility: 7.468500000000002\naccessibility: -2.1128444606258165\nreward: 0.9410842657089233\ncompatibility: 7.423499999999997\naccessibility: -2.350962844625233\nreward: 1.103281021118164\ncompatibility: 7.494999999999997\naccessibility: -2.1459713661309765\nreward: 0.8318231701850891\ncompatibility: 7.389500000000003\naccessibility: -2.4966402072905316\nreward: 0.9474254250526428\ncompatibility: 7.511500000000003\naccessibility: -2.3885939887413405\nreward: 0.8123275637626648\ncompatibility: 7.206500000000002\naccessibility: -2.4278479836907754\nreward: 0.9762009382247925\ncompatibility: 7.3980000000000015\naccessibility: -2.284627159848965\nreward: 0.7623871564865112\ncompatibility: 7.439500000000002\naccessibility: -2.6275799961877744\nreward: 1.049500823020935\ncompatibility: 7.574000000000002\naccessibility: -2.2689630783830026\nreward: 0.9313390851020813\ncompatibility: 7.405999999999999\naccessibility: -2.356205656142449\nreward: 1.2120938301086426\ncompatibility: 7.745000000000004\naccessibility: -2.116680651904883\nreward: 0.8756126761436462\ncompatibility: 7.475\naccessibility: -2.4767595859432747\nreward: 0.942625105381012\ncompatibility: 7.577999999999999\naccessibility: -2.431419526164154\nreward: 0.940566897392273\ncompatibility: 7.369500000000001\naccessibility: -2.322810339448816\nreward: 1.0719330310821533\ncompatibility: 7.7780000000000005\naccessibility: -2.344600449815494\nreward: 0.9744545817375183\ncompatibility: 7.631\naccessibility: -2.4120681113798685\nreward: 0.968647301197052\ncompatibility: 7.644500000000001\naccessibility: -2.428011225563823\nreward: 0.8559871315956116\ncompatibility: 7.3870000000000005\naccessibility: -2.45905500162745\nreward: 1.2139372825622559\ncompatibility: 7.678000000000001\naccessibility: -2.0780226386379113\nreward: 0.9057344198226929\ncompatibility: 7.485499999999999\naccessibility: -2.4372019780872645\nreward: 0.7831944227218628\ncompatibility: 7.442\naccessibility: -2.5977083458070784\nreward: 1.1252679824829102\ncompatibility: 7.313499999999998\naccessibility: -2.015758808430915\nreward: 0.9863004684448242\ncompatibility: 7.436999999999997\naccessibility: -2.2903707489984515\nreward: 0.871209979057312\ncompatibility: 7.5725\naccessibility: -2.5355957708123746\nreward: 0.8129828572273254\ncompatibility: 7.434000000000003\naccessibility: -2.5487400109327343\nreward: 0.9698079228401184\ncompatibility: 7.403\naccessibility: -2.296895214034525\nreward: 1.0353665351867676\ncompatibility: 7.389000000000001\naccessibility: -2.191057388385446\nreward: 1.1472467184066772\ncompatibility: 7.633000000000001\naccessibility: -2.153951303036305\nreward: 0.5944242477416992\ncompatibility: 7.253999999999999\naccessibility: -2.780149312924755\nreward: 0.9858085513114929\ncompatibility: 7.687499999999999\naccessibility: -2.425305066547822\nreward: 0.9643151760101318\ncompatibility: 7.635\naccessibility: -2.429420133682493\nreward: 1.0131014585494995\ncompatibility: 7.558\naccessibility: -2.314990728505886\nreward: 1.1926394701004028\ncompatibility: 7.480999999999999\naccessibility: -2.0044336121453634\nreward: 0.8032210469245911\ncompatibility: 7.2395\naccessibility: -2.4591862892273606\nreward: 1.1100744009017944\ncompatibility: 7.540999999999997\naccessibility: -2.1604240731704354\nreward: 0.8639686703681946\ncompatibility: 7.7315000000000005\naccessibility: -2.631636255647081\nreward: 1.334823727607727\ncompatibility: 7.4990000000000006\naccessibility: -1.8008000484464297\nreward: 0.8486020565032959\ncompatibility: 7.559500000000001\naccessibility: -2.5625433115087377\nreward: 1.1486365795135498\ncompatibility: 7.642500000000002\naccessibility: -2.156955826071378\nreward: 0.9298090934753418\ncompatibility: 7.402499999999998\naccessibility: -2.3566256687280047\nreward: 0.7903212904930115\ncompatibility: 7.5075\naccessibility: -2.6221073691333854\nreward: 0.932276725769043\ncompatibility: 7.364000000000003\naccessibility: -2.3322992177803314\nreward: 0.6878742575645447\ncompatibility: 7.3020000000000005\naccessibility: -2.6656886329768663\nreward: 0.8492788672447205\ncompatibility: 7.527500000000001\naccessibility: -2.5443852744847777\nreward: 1.270810842514038\ncompatibility: 7.593\naccessibility: -1.94717659609154\nreward: 0.9343867301940918\ncompatibility: 7.4\naccessibility: -2.3484198734241835\nreward: 0.8594192862510681\ncompatibility: 7.4385\naccessibility: -2.4814960513651387\nreward: 0.7793779969215393\ncompatibility: 7.621000000000001\naccessibility: -2.699325878762465\nreward: 1.094649076461792\ncompatibility: 7.712500000000002\naccessibility: -2.2754371199458356\nreward: 0.91009920835495\ncompatibility: 7.5985000000000005\naccessibility: -2.4911904632369346\nreward: 0.9370325207710266\ncompatibility: 7.2135\naccessibility: -2.2445405318394998\nreward: 1.1951594352722168\ncompatibility: 7.727999999999998\naccessibility: -2.132975051479961\nreward: 1.0559806823730469\ncompatibility: 7.357500000000001\naccessibility: -2.1432610427824907\nreward: 1.1496232748031616\ncompatibility: 7.697500000000002\naccessibility: -2.1849401319808113\nreward: 0.8273972272872925\ncompatibility: 7.3065\naccessibility: -2.4588148837774124\nreward: 0.7937257885932922\ncompatibility: 7.185500000000002\naccessibility: -2.444500568185998\nreward: 1.0909613370895386\ncompatibility: 7.411999999999999\naccessibility: -2.119986497424668\nreward: 1.1198619604110718\ncompatibility: 7.672500000000002\naccessibility: -2.216189247879011\nreward: 0.7035591006278992\ncompatibility: 7.430999999999999\naccessibility: -2.7112684488743337\nreward: 0.715831458568573\ncompatibility: 7.4670000000000005\naccessibility: -2.7121456261245105\nreward: 0.7880091667175293\ncompatibility: 7.525500000000001\naccessibility: -2.6352184072533786\nreward: 0.8164154291152954\ncompatibility: 7.215499999999999\naccessibility: -2.4265375753254275\nreward: 1.0131773948669434\ncompatibility: 7.417000000000002\naccessibility: -2.2393411260921097\nreward: 0.9441757798194885\ncompatibility: 7.4675\naccessibility: -2.369897038691589\nreward: 0.9537511467933655\ncompatibility: 7.429500000000001\naccessibility: -2.335176825113586\nreward: 0.9670754671096802\ncompatibility: 7.4719999999999995\naccessibility: -2.3379582434678947\nreward: 0.9446481466293335\ncompatibility: 7.491499999999998\naccessibility: -2.3820456509516728\nreward: 1.049333930015564\ncompatibility: 7.5915\naccessibility: -2.2785884374217398\nreward: 0.9355102777481079\ncompatibility: 7.4849999999999985\naccessibility: -2.3922702802363984\nreward: 0.9884645938873291\ncompatibility: 7.553999999999999\naccessibility: -2.3498031498309517\nreward: 1.147617220878601\ncompatibility: 7.700500000000003\naccessibility: -2.189556242050591\nreward: 0.9689511060714722\ncompatibility: 7.410499999999998\naccessibility: -2.3021983672030637\nreward: 1.0671730041503906\ncompatibility: 7.435999999999999\naccessibility: -2.168526247695703\nreward: 0.8597528338432312\ncompatibility: 7.458500000000003\naccessibility: -2.491710016880242\nreward: 1.1796307563781738\ncompatibility: 7.569\naccessibility: -2.0710895069991833\nreward: 0.9355141520500183\ncompatibility: 7.413499999999999\naccessibility: -2.3539609241215773\nreward: 0.9883760213851929\ncompatibility: 7.513499999999998\naccessibility: -2.328239523858179\nreward: 1.2197716236114502\ncompatibility: 7.489000000000001\naccessibility: -1.968021212017474\nreward: 1.0427546501159668\ncompatibility: 7.5550000000000015\naccessibility: -2.268903730616994\nreward: 1.104315996170044\ncompatibility: 7.629999999999998\naccessibility: -2.216740219581926\nreward: 0.894280731678009\ncompatibility: 7.386499999999999\naccessibility: -2.401346802574066\nreward: 0.9841961860656738\ncompatibility: 7.463500000000002\naccessibility: -2.3077235904175053\nreward: 0.8607704639434814\ncompatibility: 7.517500000000001\naccessibility: -2.5217907185204567\nreward: 0.9769105315208435\ncompatibility: 7.527500000000004\naccessibility: -2.352937745069539\nreward: 0.7351067662239075\ncompatibility: 7.307999999999997\naccessibility: -2.5980541068415413\nreward: 1.3271366357803345\ncompatibility: 7.758999999999998\naccessibility: -1.9516164840000767\nreward: 1.0482450723648071\ncompatibility: 7.540499999999998\naccessibility: -2.2529002619114755\nreward: 0.9686284065246582\ncompatibility: 7.6255\naccessibility: -2.4178609658293144\nreward: 1.0772120952606201\ncompatibility: 7.400500000000001\naccessibility: -2.134449690923443\nreward: 0.9231277108192444\ncompatibility: 7.523000000000001\naccessibility: -2.431201273246363\nreward: 0.9486339688301086\ncompatibility: 7.493000000000002\naccessibility: -2.3768704968396928\nreward: 0.8865711092948914\ncompatibility: 7.491499999999999\naccessibility: -2.4691612100094904\nreward: 1.0383437871932983\ncompatibility: 7.435500000000001\naccessibility: -2.21150225375068\nreward: 0.8826427459716797\ncompatibility: 7.393500000000001\naccessibility: -2.422553716435157\nreward: 0.8374729156494141\ncompatibility: 7.380000000000001\naccessibility: -2.4830763382028627\nreward: 1.143182396888733\ncompatibility: 7.7135\naccessibility: -2.2031727923469973\nreward: 0.9176741242408752\ncompatibility: 7.432499999999999\naccessibility: -2.3908995428442994\nreward: 0.9676445126533508\ncompatibility: 7.5150000000000015\naccessibility: -2.3601404047883783\nreward: 0.850454568862915\ncompatibility: 7.6960000000000015\naccessibility: -2.63288959369189\nreward: 0.9247918128967285\ncompatibility: 7.452999999999999\naccessibility: -2.3912051730799893\nreward: 1.0898067951202393\ncompatibility: 7.601000000000003\naccessibility: -2.2229683087643672\nreward: 0.897234320640564\ncompatibility: 7.530500000000003\naccessibility: -2.474059241060769\nreward: 0.9135423898696899\ncompatibility: 7.501999999999999\naccessibility: -2.43432927046539\nreward: 0.9406347870826721\ncompatibility: 7.417499999999999\naccessibility: -2.348422843622172\nreward: 0.806305468082428\ncompatibility: 7.401999999999999\naccessibility: -2.5416132346439673\nreward: 1.0919886827468872\ncompatibility: 7.583500000000002\naccessibility: -2.210320565436314\nreward: 1.0526816844940186\ncompatibility: 7.541999999999997\naccessibility: -2.247048921982234\nreward: 0.9175530076026917\ncompatibility: 7.474500000000001\naccessibility: -2.413581194711038\nreward: 0.9976925849914551\ncompatibility: 7.3245\naccessibility: -2.213014710449277\nreward: 1.2058614492416382\ncompatibility: 7.7840000000000025\naccessibility: -2.1469221835236283\nreward: 1.1372045278549194\ncompatibility: 7.615999999999999\naccessibility: -2.1599074548396127\nreward: 0.8005218505859375\ncompatibility: 7.5245\naccessibility: -2.615913614092114\nreward: 1.239050030708313\ncompatibility: 7.525000000000001\naccessibility: -1.9583892352004184\nreward: 1.0511232614517212\ncompatibility: 7.5085000000000015\naccessibility: -2.231440185427912\nreward: 1.0175045728683472\ncompatibility: 7.617500000000003\naccessibility: -2.3402610347961335\nreward: 0.6118043661117554\ncompatibility: 7.5215000000000005\naccessibility: -2.8973827371222014\nreward: 0.9973196983337402\ncompatibility: 7.406999999999999\naccessibility: -2.257770416194294\nreward: 0.950701892375946\ncompatibility: 7.485500000000002\naccessibility: -2.3697507460964395\nreward: 0.8372077941894531\ncompatibility: 7.691000000000002\naccessibility: -2.650081198552191\nreward: 0.974013090133667\ncompatibility: 7.624\naccessibility: -2.4089803468083315\nreward: 1.0438393354415894\ncompatibility: 7.537999999999999\naccessibility: -2.2581695438048817\nreward: 0.751723051071167\ncompatibility: 7.464999999999999\naccessibility: -2.657236870592544\nreward: 1.0355899333953857\ncompatibility: 7.463500000000003\naccessibility: -2.2306329149618604\nreward: 0.9369539618492126\ncompatibility: 7.496499999999997\naccessibility: -2.396265514582858\nreward: 0.8170557618141174\ncompatibility: 7.375500000000001\naccessibility: -2.5112913261089562\nreward: 1.087118148803711\ncompatibility: 7.601499999999998\naccessibility: -2.2272691618931684\nreward: 0.9782915115356445\ncompatibility: 7.3050000000000015\naccessibility: -2.2316698436623774\nreward: 1.194300889968872\ncompatibility: 7.6240000000000006\naccessibility: -2.078548739411045\nreward: 1.0131008625030518\ncompatibility: 7.344500000000002\naccessibility: -2.20061648587655\nreward: 1.0673542022705078\ncompatibility: 7.491000000000001\naccessibility: -2.1977187847115354\nreward: 0.8798084855079651\ncompatibility: 7.627500000000001\naccessibility: -2.552162265155644\nreward: 0.7809151411056519\ncompatibility: 7.423000000000002\naccessibility: -2.590948714732769\nreward: 1.1042524576187134\ncompatibility: 7.495500000000001\naccessibility: -2.144782076889147\nreward: 0.7255206108093262\ncompatibility: 7.541000000000001\naccessibility: -2.7372547614363683\nreward: 0.8340969085693359\ncompatibility: 7.346999999999998\naccessibility: -2.470461745188219\nreward: 0.938377320766449\ncompatibility: 7.393000000000002\naccessibility: -2.3386839742138616\nreward: 0.9488067030906677\ncompatibility: 7.642\naccessibility: -2.456432781525238\nreward: 0.7531728148460388\ncompatibility: 7.418499999999999\naccessibility: -2.630151532363715\nreward: 1.1917608976364136\ncompatibility: 7.572500000000002\naccessibility: -2.054769382725823\nreward: 1.3482028245925903\ncompatibility: 7.861499999999998\naccessibility: -1.9749279763036198\nreward: 0.6233804225921631\ncompatibility: 7.364000000000002\naccessibility: -2.795643668929716\nreward: 0.9069781303405762\ncompatibility: 7.314500000000002\naccessibility: -2.343729188452885\nreward: 0.9682706594467163\ncompatibility: 7.580999999999998\naccessibility: -2.394558337575811\nreward: 0.8991194367408752\ncompatibility: 7.486500000000001\naccessibility: -2.4476601301837047\nreward: 1.1850329637527466\ncompatibility: 7.889499999999999\naccessibility: -2.2346827227879493\nreward: 0.98062664270401\ncompatibility: 7.4335\naccessibility: -2.2970064239366113\nreward: 0.8707146644592285\ncompatibility: 7.442999999999999\naccessibility: -2.4669636800709167\nreward: 0.881565511226654\ncompatibility: 7.4840000000000035\naccessibility: -2.472651760870211\nreward: 0.9163723587989807\ncompatibility: 7.6945000000000014\naccessibility: -2.5332092774866313\nreward: 0.6201531887054443\ncompatibility: 7.162000000000002\naccessibility: -2.692270185409196\nreward: 1.0173112154006958\ncompatibility: 7.5735\naccessibility: -2.316979672584667\nreward: 0.8214500546455383\ncompatibility: 7.445000000000001\naccessibility: -2.5419320475974576\nreward: 1.2945702075958252\ncompatibility: 7.9700000000000015\naccessibility: -2.113501855150035\nreward: 0.9310502409934998\ncompatibility: 7.6945\naccessibility: -2.5111924667712877\nreward: 1.03046452999115\ncompatibility: 7.545999999999999\naccessibility: -2.282517484692483\nreward: 0.7342453598976135\ncompatibility: 7.311500000000002\naccessibility: -2.6012212581210146\nreward: 0.8475694060325623\ncompatibility: 7.303499999999998\naccessibility: -2.4269494564402008\nreward: 0.7005318403244019\ncompatibility: 7.3085\naccessibility: -2.6501843587334606\nreward: 0.9208648800849915\ncompatibility: 7.266499999999997\naccessibility: -2.297184863217822\nreward: 1.191994071006775\ncompatibility: 7.6635000000000035\naccessibility: -2.103169672923139\nreward: 0.8178287744522095\ncompatibility: 7.664\naccessibility: -2.6646853891110958\nreward: 0.8510727286338806\ncompatibility: 7.386999999999996\naccessibility: -2.466426665093951\nreward: 0.8412404656410217\ncompatibility: 7.507500000000002\naccessibility: -2.5457285563996646\nreward: 0.9656916260719299\ncompatibility: 7.437499999999999\naccessibility: -2.321551828375755\nreward: 0.8161537647247314\ncompatibility: 7.4935\naccessibility: -2.5758586169104287\nreward: 0.9126451015472412\ncompatibility: 7.542000000000001\naccessibility: -2.457103763955099\nreward: 0.8564450144767761\ncompatibility: 7.2885\naccessibility: -2.4056003087173616\nreward: 0.8933802843093872\ncompatibility: 7.582000000000001\naccessibility: -2.507429606923127\nreward: 1.1204262971878052\ncompatibility: 7.634000000000002\naccessibility: -2.1947176393334713\nreward: 1.3038880825042725\ncompatibility: 7.4544999999999995\naccessibility: -1.8233643629755678\nreward: 0.8864487409591675\ncompatibility: 7.347999999999998\naccessibility: -2.392469723856344\nreward: 0.5015798211097717\ncompatibility: 7.394000000000002\naccessibility: -2.9944159918072435\nreward: 1.0389302968978882\ncompatibility: 7.4875\naccessibility: -2.238479586128978\nreward: 0.8050647974014282\ncompatibility: 7.4135\naccessibility: -2.5496349205708784\nreward: 1.0043710470199585\ncompatibility: 7.430499999999999\naccessibility: -2.2597827626214757\nreward: 0.8343861699104309\ncompatibility: 7.6125\naccessibility: -2.6122600744223243\nreward: 1.088079571723938\ncompatibility: 7.5175\naccessibility: -2.1808269974321624\nreward: 0.7784438133239746\ncompatibility: 7.1370000000000005\naccessibility: -2.4414414055994214\nreward: 0.968359649181366\ncompatibility: 7.447499999999996\naccessibility: -2.3229069497541674\nreward: 0.9337341785430908\ncompatibility: 7.3305\naccessibility: -2.312166593138518\nreward: 0.7231547832489014\ncompatibility: 7.4175\naccessibility: -2.674642854170779\nreward: 1.103394627571106\ncompatibility: 7.622500000000001\naccessibility: -2.2141044332897906\nreward: 1.1235885620117188\ncompatibility: 7.576999999999999\naccessibility: -2.1594385587492746\nreward: 1.0850591659545898\ncompatibility: 7.618\naccessibility: -2.239196915902183\nreward: 0.91818767786026\ncompatibility: 7.5445\naccessibility: -2.4501292280664115\nreward: 0.8859037160873413\ncompatibility: 7.490500000000001\naccessibility: -2.4696265854850408\nreward: 1.1403261423110962\ncompatibility: 7.561499999999999\naccessibility: -2.1260286152671544\nreward: 0.7843810319900513\ncompatibility: 7.582999999999999\naccessibility: -2.6714641614289523\nreward: 0.8616862297058105\ncompatibility: 7.452499999999998\naccessibility: -2.485595656047907\nreward: 0.9744763970375061\ncompatibility: 7.431000000000001\naccessibility: -2.30489251360504\nreward: 1.0603677034378052\ncompatibility: 7.413000000000003\naccessibility: -2.166412819310183\nreward: 0.9011304378509521\ncompatibility: 7.379\naccessibility: -2.3870543644781703\nreward: 1.1196318864822388\ncompatibility: 7.72\naccessibility: -2.241980673413624\nreward: 1.041724443435669\ncompatibility: 7.423499999999998\naccessibility: -2.20000265236978\nreward: 1.187342643737793\ncompatibility: 7.671500000000002\naccessibility: -2.114432495576513\nreward: 0.891586184501648\ncompatibility: 7.662000000000002\naccessibility: -2.5529778785405797\nreward: 1.0815315246582031\ncompatibility: 7.407500000000002\naccessibility: -2.131720497795584\nreward: 0.8547210693359375\ncompatibility: 7.459500000000002\naccessibility: -2.4997933599281583\nreward: 0.7497663497924805\ncompatibility: 7.56\naccessibility: -2.711064800812353\nreward: 1.0290354490280151\ncompatibility: 7.646500000000002\naccessibility: -2.338500470781312\nreward: 0.9354141354560852\ncompatibility: 7.748000000000001\naccessibility: -2.5333073900897944\nreward: 0.886012852191925\ncompatibility: 7.4645\naccessibility: -2.4555342704011682\nreward: 0.9349581003189087\ncompatibility: 7.576500000000001\naccessibility: -2.442116451874295\nreward: 0.8146196007728577\ncompatibility: 7.500500000000004\naccessibility: -2.5819099116867594\nreward: 0.933493971824646\ncompatibility: 7.422\naccessibility: -2.3615447287740388\nreward: 0.8723698258399963\ncompatibility: 7.377499999999999\naccessibility: -2.4293916728145253\nreward: 0.988913357257843\ncompatibility: 7.298499999999998\naccessibility: -2.212254979641501\nreward: 0.9789647459983826\ncompatibility: 7.6404999999999985\naccessibility: -2.4103921348491197\nreward: 1.0743550062179565\ncompatibility: 7.5470000000000015\naccessibility: -2.2172174890326737\nreward: 1.2035210132598877\ncompatibility: 7.594499999999999\naccessibility: -2.048914835175063\nreward: 0.8118401765823364\ncompatibility: 7.734\naccessibility: -2.7111683246321263\nreward: 0.8081921339035034\ncompatibility: 7.467\naccessibility: -2.57360461954641\nreward: 0.873627245426178\ncompatibility: 7.354499999999999\naccessibility: -2.4151841694573424\nreward: 0.9549238085746765\ncompatibility: 7.542500000000001\naccessibility: -2.393953549238249\nreward: 0.6635618805885315\ncompatibility: 7.419499999999997\naccessibility: -2.7651036258659376\nreward: 1.040637731552124\ncompatibility: 7.546500000000001\naccessibility: -2.267525587927309\nreward: 0.7910982370376587\ncompatibility: 7.278500000000001\naccessibility: -2.4982633670589416\nreward: 0.8088893294334412\ncompatibility: 7.612500000000001\naccessibility: -2.650505316537421\nreward: 1.0151965618133545\ncompatibility: 7.706999999999999\naccessibility: -2.3916694124679374\nreward: 0.724373459815979\ncompatibility: 7.241500000000002\naccessibility: -2.5785290805507417\nreward: 0.9344120025634766\ncompatibility: 7.442500000000003\naccessibility: -2.3711498221470686\nreward: 0.7722097635269165\ncompatibility: 7.527000000000003\naccessibility: -2.6597210756512872\nreward: 0.9484203457832336\ncompatibility: 7.521\naccessibility: -2.392190936039656\nreward: 1.1455812454223633\ncompatibility: 7.525499999999999\naccessibility: -2.0988601979438903\nreward: 1.0689362287521362\ncompatibility: 7.582000000000002\naccessibility: -2.244095673907595\nreward: 1.047932744026184\ncompatibility: 7.43\naccessibility: -2.194172336344617\nreward: 0.8518458008766174\ncompatibility: 7.643000000000001\naccessibility: -2.602409871564139\nreward: 0.9071976542472839\ncompatibility: 7.566500000000002\naccessibility: -2.4783999668700893\nreward: 0.715884804725647\ncompatibility: 7.4975\naccessibility: -2.728404897639867\nreward: 0.7193572521209717\ncompatibility: 7.389500000000002\naccessibility: -2.665339155727751\nreward: 0.685664176940918\ncompatibility: 7.502999999999999\naccessibility: -2.776682276438916\nreward: 0.9446070194244385\ncompatibility: 7.320499999999998\naccessibility: -2.2905002058017625\nreward: 1.156381368637085\ncompatibility: 7.824000000000003\naccessibility: -2.242570859931182\nreward: 1.1485068798065186\ncompatibility: 7.564499999999999\naccessibility: -2.1153647520781855\nreward: 0.7931939959526062\ncompatibility: 7.7775\naccessibility: -2.7624411196270087\nreward: 1.032626986503601\ncompatibility: 7.793000000000001\naccessibility: -2.4115952525836595\nreward: 1.0431901216506958\ncompatibility: 7.496000000000002\naccessibility: -2.236643400864577\nreward: 0.8982836604118347\ncompatibility: 7.6800000000000015\naccessibility: -2.5525745122066836\nreward: 1.1731537580490112\ncompatibility: 7.568500000000001\naccessibility: -2.0805372343470685\nreward: 0.994922935962677\ncompatibility: 7.538000000000002\naccessibility: -2.331544191196541\nreward: 0.8933934569358826\ncompatibility: 7.696500000000002\naccessibility: -2.5687491367159088\nreward: 0.8471347093582153\ncompatibility: 7.505\naccessibility: -2.53554791447323\nreward: 1.1783041954040527\ncompatibility: 7.576000000000001\naccessibility: -2.0768294701692525\nreward: 0.5596472024917603\ncompatibility: 7.269499999999999\naccessibility: -2.840618468141928\nreward: 0.960107684135437\ncompatibility: 7.4055\naccessibility: -2.312784885126544\nreward: 1.329255223274231\ncompatibility: 7.6015\naccessibility: -1.8640636607910102\nreward: 0.9091150760650635\ncompatibility: 7.4060000000000015\naccessibility: -2.389541631631218\nreward: 1.0349726676940918\ncompatibility: 7.334499999999999\naccessibility: -2.162451629851969\nreward: 0.6415456533432007\ncompatibility: 7.209999999999999\naccessibility: -2.6858957806328374\nreward: 0.8995566964149475\ncompatibility: 7.3825\naccessibility: -2.3912900000212867\nreward: 1.114660620689392\ncompatibility: 7.643000000000001\naccessibility: -2.2081877173213416\nreward: 1.077768325805664\ncompatibility: 7.659500000000001\naccessibility: -2.272365401583949\nreward: 1.0370539426803589\ncompatibility: 7.518499999999998\naccessibility: -2.25790129238726\nreward: 0.7890108823776245\ncompatibility: 7.376000000000001\naccessibility: -2.553626489066147\nreward: 0.9312666654586792\ncompatibility: 7.364500000000001\naccessibility: -2.334082160358677\nreward: 0.7889342308044434\ncompatibility: 7.459000000000002\naccessibility: -2.598205816740997\nreward: 0.9446295499801636\ncompatibility: 7.386000000000001\naccessibility: -2.3255557096929205\nreward: 1.1572920083999634\ncompatibility: 7.5535000000000005\naccessibility: -2.096294176525493\nreward: 0.7179503440856934\ncompatibility: 7.461500000000001\naccessibility: -2.70602088458568\nreward: 0.9945458769798279\ncompatibility: 7.5105\naccessibility: -2.3173776052962327\nreward: 1.0970170497894287\ncompatibility: 7.259499999999998\naccessibility: -2.0292065882812484\nreward: 0.9653382301330566\ncompatibility: 7.358499999999998\naccessibility: -2.279760480515013\nreward: 0.784098744392395\ncompatibility: 7.541500000000003\naccessibility: -2.649655487354297\nreward: 0.9878846406936646\ncompatibility: 7.294000000000003\naccessibility: -2.2113873303156053\nreward: 0.9805206656455994\ncompatibility: 7.484000000000001\naccessibility: -2.3242189835438647\nreward: 1.0530691146850586\ncompatibility: 7.6240000000000006\naccessibility: -2.2903964163425257\nreward: 0.9642431735992432\ncompatibility: 7.428000000000002\naccessibility: -2.318635266264291\nreward: 0.8488087058067322\ncompatibility: 7.4895\naccessibility: -2.524733370957448\nreward: 1.0797219276428223\ncompatibility: 7.775\naccessibility: -2.3313099609834844\nreward: 0.7064074277877808\ncompatibility: 7.514499999999999\naccessibility: -2.7517281293033835\nreward: 1.0416134595870972\ncompatibility: 7.545500000000001\naccessibility: -2.2655262815722974\nreward: 0.9265767931938171\ncompatibility: 7.567000000000002\naccessibility: -2.449599054956142\nreward: 0.9634687304496765\ncompatibility: 7.577999999999999\naccessibility: -2.400154059601264\nreward: 0.9570457935333252\ncompatibility: 7.141500000000002\naccessibility: -2.175949202899817\nreward: 0.9410145878791809\ncompatibility: 7.608000000000001\naccessibility: -2.449906699105416\nreward: 0.8851205110549927\ncompatibility: 7.3365\naccessibility: -2.388301338828543\nreward: 1.225940465927124\ncompatibility: 7.485499999999997\naccessibility: -1.9568928546478424\nreward: 1.0875091552734375\ncompatibility: 7.659000000000001\naccessibility: -2.2574863551384556\nreward: 1.0660450458526611\ncompatibility: 7.460000000000001\naccessibility: -2.183075201224435\nreward: 1.2424358129501343\ncompatibility: 7.607000000000002\naccessibility: -1.9972390824999144\nreward: 0.8009204864501953\ncompatibility: 7.549500000000003\naccessibility: -2.6287085802716934\nreward: 0.7879062294960022\ncompatibility: 7.385500000000002\naccessibility: -2.560372809723882\nreward: 1.0344960689544678\ncompatibility: 7.621\naccessibility: -2.3166488360079924\nreward: 0.8436912894248962\ncompatibility: 7.457999999999999\naccessibility: -2.515534455111964\nreward: 1.074052095413208\ncompatibility: 7.640499999999999\naccessibility: -2.2677612105871425\nreward: 0.7019908428192139\ncompatibility: 7.564\naccessibility: -2.784870864996935\nreward: 1.3207569122314453\ncompatibility: 7.6930000000000005\naccessibility: -1.925828971908097\nreward: 0.7491462230682373\ncompatibility: 7.389500000000003\naccessibility: -2.620655639444582\nreward: 0.9470184445381165\ncompatibility: 7.5325000000000015\naccessibility: -2.4004544734257833\nreward: 0.7995347380638123\ncompatibility: 7.351999999999999\naccessibility: -2.524983563692729\nreward: 0.9265574812889099\ncompatibility: 7.376999999999999\naccessibility: -2.3478423703649596\nreward: 1.02302086353302\ncompatibility: 7.647500000000001\naccessibility: -2.3480579411949973\nreward: 1.1289515495300293\ncompatibility: 7.596\naccessibility: -2.1615727378929415\nreward: 0.5651754140853882\ncompatibility: 7.090000000000001\naccessibility: -2.7361654393454087\nreward: 0.7014578580856323\ncompatibility: 7.298999999999999\naccessibility: -2.643706108489625\nreward: 0.8596972227096558\ncompatibility: 7.399499999999997\naccessibility: -2.4601863124350594\nreward: 0.9810965061187744\ncompatibility: 7.655\naccessibility: -2.41496235761441\nreward: 1.0018655061721802\ncompatibility: 7.669999999999998\naccessibility: -2.3918446032217537\nreward: 1.1092159748077393\ncompatibility: 7.558000000000001\naccessibility: -2.1708189061066245\nreward: 0.9933972358703613\ncompatibility: 7.663999999999998\naccessibility: -2.4013327485818223\nreward: 0.9202715158462524\ncompatibility: 7.476500000000002\naccessibility: -2.4105748881019218\nreward: 1.00869619846344\ncompatibility: 7.5680000000000005\naccessibility: -2.326955664527308\nreward: 0.979865550994873\ncompatibility: 7.3530000000000015\naccessibility: -2.2550231251232526\nreward: 0.8274819850921631\ncompatibility: 7.423499999999999\naccessibility: -2.5213663085412654\nreward: 1.1613318920135498\ncompatibility: 7.749000000000004\naccessibility: -2.1949665208641678\nreward: 0.7117772698402405\ncompatibility: 7.211500000000001\naccessibility: -2.5813519649202643\nreward: 1.1571673154830933\ncompatibility: 7.6065\naccessibility: -2.124874012464218\nreward: 1.0010180473327637\ncompatibility: 7.376000000000001\naccessibility: -2.235615748578277\nreward: 0.8611165285110474\ncompatibility: 7.303000000000002\naccessibility: -2.406360940158692\nreward: 1.0323046445846558\ncompatibility: 7.5760000000000005\naccessibility: -2.2958287692558823\nreward: 1.113683819770813\ncompatibility: 7.5035000000000025\naccessibility: -2.134920663475901\nreward: 0.7980630397796631\ncompatibility: 7.396999999999999\naccessibility: -2.551298272271105\nreward: 1.123827338218689\ncompatibility: 7.549000000000004\naccessibility: -2.144080476829509\nreward: 1.1225250959396362\ncompatibility: 7.423000000000002\naccessibility: -2.078533873101399\nreward: 0.9304092526435852\ncompatibility: 7.402499999999999\naccessibility: -2.3557254309531106\nreward: 0.6976112723350525\ncompatibility: 7.273499999999999\naccessibility: -2.6358152563922914\nreward: 0.7281633615493774\ncompatibility: 7.392\naccessibility: -2.653469204682321\nreward: 0.9028112888336182\ncompatibility: 7.428000000000001\naccessibility: -2.410783097957023\nreward: 0.9566613435745239\ncompatibility: 7.481500000000004\naccessibility: -2.358668702558173\nreward: 0.7372435331344604\ncompatibility: 7.460000000000003\naccessibility: -2.676277545911568\nreward: 0.9279006719589233\ncompatibility: 7.612999999999999\naccessibility: -2.4722561196891997\nreward: 0.9395914077758789\ncompatibility: 7.49\naccessibility: -2.3888271596091104\nreward: 0.8359282612800598\ncompatibility: 7.156500000000002\naccessibility: -2.365661145204852\nreward: 1.1148158311843872\ncompatibility: 7.462000000000002\naccessibility: -2.110990469069361\nreward: 1.0747764110565186\ncompatibility: 7.399500000000001\naccessibility: -2.1375675402060628\nreward: 1.0562061071395874\ncompatibility: 7.572\naccessibility: -2.2578336721670533\nreward: 1.0020004510879517\ncompatibility: 7.460000000000001\naccessibility: -2.279142197389726\nreward: 1.021984338760376\ncompatibility: 7.2655\naccessibility: -2.1449698444538736\nreward: 0.9318743944168091\ncompatibility: 7.5845\naccessibility: -2.4510277357990566\nreward: 0.8012575507164001\ncompatibility: 7.664\naccessibility: -2.6895422233934707\nreward: 1.421756386756897\ncompatibility: 7.674000000000003\naccessibility: -1.764151084984593\nreward: 1.1344538927078247\ncompatibility: 7.482000000000003\naccessibility: -2.0922476526664924\nreward: 1.018442153930664\ncompatibility: 7.286000000000002\naccessibility: -2.1612653138176734\nreward: 0.9956607818603516\ncompatibility: 7.5005\naccessibility: -2.310348115988508\nreward: 0.782113790512085\ncompatibility: 7.1465000000000005\naccessibility: -2.4410257030944464\nreward: 1.2299705743789673\ncompatibility: 7.591500000000002\naccessibility: -2.0076333984457615\nreward: 1.1508641242980957\ncompatibility: 7.592500000000002\naccessibility: -2.1268289011738197\nreward: 0.9791364669799805\ncompatibility: 7.695999999999999\naccessibility: -2.439866725386655\nreward: 0.7448421716690063\ncompatibility: 7.3715\naccessibility: -2.617468850746224\nreward: 0.8710967302322388\ncompatibility: 7.622999999999999\naccessibility: -2.562819218358891\nreward: 0.609424889087677\ncompatibility: 7.153999999999999\naccessibility: -2.70407694538803\nreward: 0.9566992521286011\ncompatibility: 7.730500000000001\naccessibility: -2.4920047000151633\nreward: 1.1750863790512085\ncompatibility: 7.559499999999999\naccessibility: -2.072816931297193\nreward: 0.7489483952522278\ncompatibility: 7.324500000000003\naccessibility: -2.5861309756944\nreward: 0.6928616166114807\ncompatibility: 7.490999999999999\naccessibility: -2.759457540536526\nreward: 0.9076939821243286\ncompatibility: 7.442000000000002\naccessibility: -2.4109590709293465\nreward: 0.884535551071167\ncompatibility: 7.5475\naccessibility: -2.502214514658785\nreward: 1.0193514823913574\ncompatibility: 7.355500000000002\naccessibility: -2.197133577828082\nreward: 0.8340997695922852\ncompatibility: 7.513499999999998\naccessibility: -2.559653958072066\nreward: 1.044045090675354\ncompatibility: 7.3340000000000005\naccessibility: -2.148575206775595\nreward: 0.8528380393981934\ncompatibility: 7.293000000000001\naccessibility: -2.4134214975967483\nreward: 0.9942417144775391\ncompatibility: 7.3805000000000005\naccessibility: -2.248190964931585\nreward: 1.1591033935546875\ncompatibility: 7.588000000000001\naccessibility: -2.1120591521278476\nreward: 1.0962352752685547\ncompatibility: 7.3599999999999985\naccessibility: -2.084218597216778\nreward: 1.236048698425293\ncompatibility: 7.4235000000000015\naccessibility: -1.9085162958500774\nreward: 0.9056324362754822\ncompatibility: 7.401500000000001\naccessibility: -2.3923549121038925\nreward: 1.009464979171753\ncompatibility: 7.463499999999998\naccessibility: -2.2698203841383613\n</pre> In\u00a0[\u00a0]: Copied! <pre>policy_new = new_model_checkpoint.policy.to(device)\n#env = new_model_checkpoint.env.to(device)\ntd_init = env.reset(batch_size=[1]).to(device)\nprint(td_init[0])\nprint(td_init[0].get(\"locs\"))\nprint(td_init[0].get(\"areas\"))\nprint(td_init[0].get(\"init_plan\"))\nprint(td_init[0].get(\"adjacency_list\"))\nprint(td_init[0].get(\"distances\").tolist())\nprint(init.map_to_strings(td_init[0].get(\"init_plan\").tolist(), landtype))\nout = policy_new(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\n\n# Plotting\nprint(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\")\nfor td, actions in zip(td_init, out['actions'].cpu()):\n    env.render(td, actions)\n</pre> policy_new = new_model_checkpoint.policy.to(device) #env = new_model_checkpoint.env.to(device) td_init = env.reset(batch_size=[1]).to(device) print(td_init[0]) print(td_init[0].get(\"locs\")) print(td_init[0].get(\"areas\")) print(td_init[0].get(\"init_plan\")) print(td_init[0].get(\"adjacency_list\")) print(td_init[0].get(\"distances\").tolist()) print(init.map_to_strings(td_init[0].get(\"init_plan\").tolist(), landtype)) out = policy_new(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)  # Plotting print(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\") for td, actions in zip(td_init, out['actions'].cpu()):     env.render(td, actions) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tests/luop50testreal/","title":"Luop50testreal","text":"In\u00a0[27]: Copied! <pre>from rl4co.envs import landuseOptEnv\nimport torch\nfrom rl4co.models import AttentionModel, AttentionModelPolicy\nfrom rl4co.models.nn.env_embeddings.context import LOPContext\nfrom rl4co.models.nn.env_embeddings.dynamic import StaticEmbedding\nfrom rl4co.models.nn.env_embeddings.init import lopInitEmbedding\nfrom rl4co.envs.urbanplan.cityplan import init\nimport torch\nfrom tensordict.tensordict import TensorDict\n</pre> from rl4co.envs import landuseOptEnv import torch from rl4co.models import AttentionModel, AttentionModelPolicy from rl4co.models.nn.env_embeddings.context import LOPContext from rl4co.models.nn.env_embeddings.dynamic import StaticEmbedding from rl4co.models.nn.env_embeddings.init import lopInitEmbedding from rl4co.envs.urbanplan.cityplan import init import torch from tensordict.tensordict import TensorDict In\u00a0[28]: Copied! <pre>''' neighbour to list '''\npolygoncount = 221\nneighbourlist = init.getneighbourlist('../Data/queen.csv', polygoncount)\nbasiclanduse = init.readlanduselist('../Data/baseParcels.shp', polygoncount)\nlandusePalette = {'Commercial': 'coral',\n                  'Residential': 'peachpuff',\n                  'Office': 'indianred',\n                  'Residential&amp;Commercial': 'lightsalmon',\n                  'Green Space': 'lightgreen',\n                  'Education': 'lightskyblue',\n                  'Hospital': 'royalblue',\n                  'SOHO': 'lightcoral'\n                  }\n\nlandtype = ['Commercial', 'Residential', 'Office', 'Residential&amp;Commercial', 'Green Space', 'Education', 'Hospital',\n            'SOHO']\nadj_matrix = init.get_adjacency_matrix('../Data/queen.csv', polygoncount)\nimport math\nimport numpy as np\nshapefile = '../Data/Parcels.shp'\n\n\narealist = init.readarealist(shapefile, polygoncount)\nlocs_list = init.normalizeloc(shapefile)\nlocs = torch.tensor(locs_list, dtype=torch.float32).unsqueeze(0)\nareas = torch.tensor(init.normalizearea(arealist), dtype=torch.float32).unsqueeze(0)\ninit_plan = torch.tensor(init.map_to_num(basiclanduse, landtype), dtype=torch.int64).unsqueeze(0)\nfixed_mask = torch.ones_like(init_plan, dtype=torch.bool)\nfixed_mask[(init_plan == 4) | (init_plan == 6)] = 0\nneighbourlist = torch.tensor(adj_matrix).unsqueeze(0)\ndistances = torch.tensor(init.calculate_distance_matrix(locs_list), dtype=torch.float32).unsqueeze(0)\ntd = TensorDict(\n    {\n        \"locs\": locs,\n        \"areas\": areas,\n        \"init_plan\": init_plan,\n        \"fixed_mask\": fixed_mask,\n        \"adjacency_list\": neighbourlist,\n        \"distances\": distances,\n    },\n    batch_size=1,\n)\n</pre> ''' neighbour to list ''' polygoncount = 221 neighbourlist = init.getneighbourlist('../Data/queen.csv', polygoncount) basiclanduse = init.readlanduselist('../Data/baseParcels.shp', polygoncount) landusePalette = {'Commercial': 'coral',                   'Residential': 'peachpuff',                   'Office': 'indianred',                   'Residential&amp;Commercial': 'lightsalmon',                   'Green Space': 'lightgreen',                   'Education': 'lightskyblue',                   'Hospital': 'royalblue',                   'SOHO': 'lightcoral'                   }  landtype = ['Commercial', 'Residential', 'Office', 'Residential&amp;Commercial', 'Green Space', 'Education', 'Hospital',             'SOHO'] adj_matrix = init.get_adjacency_matrix('../Data/queen.csv', polygoncount) import math import numpy as np shapefile = '../Data/Parcels.shp'   arealist = init.readarealist(shapefile, polygoncount) locs_list = init.normalizeloc(shapefile) locs = torch.tensor(locs_list, dtype=torch.float32).unsqueeze(0) areas = torch.tensor(init.normalizearea(arealist), dtype=torch.float32).unsqueeze(0) init_plan = torch.tensor(init.map_to_num(basiclanduse, landtype), dtype=torch.int64).unsqueeze(0) fixed_mask = torch.ones_like(init_plan, dtype=torch.bool) fixed_mask[(init_plan == 4) | (init_plan == 6)] = 0 neighbourlist = torch.tensor(adj_matrix).unsqueeze(0) distances = torch.tensor(init.calculate_distance_matrix(locs_list), dtype=torch.float32).unsqueeze(0) td = TensorDict(     {         \"locs\": locs,         \"areas\": areas,         \"init_plan\": init_plan,         \"fixed_mask\": fixed_mask,         \"adjacency_list\": neighbourlist,         \"distances\": distances,     },     batch_size=1, ) In\u00a0[29]: Copied! <pre>batch_size = 1024\nenv = landuseOptEnv(generator_params=dict(num_loc=50))\nemb_dim = 128\npolicy = AttentionModelPolicy(env_name=env.name, # this is actually not needed since we are initializing the embeddings!\n                              embed_dim=emb_dim,\n                              init_embedding=lopInitEmbedding(emb_dim),\n                              context_embedding=LOPContext(emb_dim),\n                              dynamic_embedding=StaticEmbedding(emb_dim)\n)\n# Model: default is AM with REINFORCE and greedy rollout baseline\nmodel = AttentionModel(env,\n                       baseline='rollout',\n                       policy=policy,\n                       )\n</pre> batch_size = 1024 env = landuseOptEnv(generator_params=dict(num_loc=50)) emb_dim = 128 policy = AttentionModelPolicy(env_name=env.name, # this is actually not needed since we are initializing the embeddings!                               embed_dim=emb_dim,                               init_embedding=lopInitEmbedding(emb_dim),                               context_embedding=LOPContext(emb_dim),                               dynamic_embedding=StaticEmbedding(emb_dim) ) # Model: default is AM with REINFORCE and greedy rollout baseline model = AttentionModel(env,                        baseline='rollout',                        policy=policy,                        ) In\u00a0[30]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenv = landuseOptEnv(generator_params=dict(num_loc=221)).to(device)\nnew_model_checkpoint = AttentionModel.load_from_checkpoint(\"checkpoints100/epoch_epoch=000.ckpt\", strict=False, load_baseline=False)\npolicy_new = new_model_checkpoint.policy.to(device)\n#env = new_model_checkpoint.env.to(device)\ntd_init = env.reset(td = td, batch_size=[1]).to(device)\n# print(td_init[0])\n# print(td_init[0].get(\"locs\"))\n# print(td_init[0].get(\"areas\"))\n# print(td_init[0].get(\"init_plan\"))\n# print(td_init[0].get(\"adjacency_list\"))\n# print(td_init[0].get(\"distances\").tolist())\n# print(init.map_to_strings(td_init[0].get(\"init_plan\").tolist(), landtype))\nimport time\nstart_time = time.time() \nout = policy_new(td_init.clone(), env, phase=\"test\", decode_type=\"sampling\", return_actions=True)\nend_time = time.time()  # \u83b7\u53d6\u51fd\u6570\u8fd0\u884c\u540e\u7684\u65f6\u95f4\uff08\u79d2\uff09\nelapsed_time = end_time - start_time  # \u8ba1\u7b97\u8fd0\u884c\u65f6\u95f4\nprint(f\"Time: {elapsed_time}\")\n# Plotting\nprint(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\")\nfor td, actions in zip(td_init, out['actions'].cpu()):\n    env.render(td, actions)\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") env = landuseOptEnv(generator_params=dict(num_loc=221)).to(device) new_model_checkpoint = AttentionModel.load_from_checkpoint(\"checkpoints100/epoch_epoch=000.ckpt\", strict=False, load_baseline=False) policy_new = new_model_checkpoint.policy.to(device) #env = new_model_checkpoint.env.to(device) td_init = env.reset(td = td, batch_size=[1]).to(device) # print(td_init[0]) # print(td_init[0].get(\"locs\")) # print(td_init[0].get(\"areas\")) # print(td_init[0].get(\"init_plan\")) # print(td_init[0].get(\"adjacency_list\")) # print(td_init[0].get(\"distances\").tolist()) # print(init.map_to_strings(td_init[0].get(\"init_plan\").tolist(), landtype)) import time start_time = time.time()  out = policy_new(td_init.clone(), env, phase=\"test\", decode_type=\"sampling\", return_actions=True) end_time = time.time()  # \u83b7\u53d6\u51fd\u6570\u8fd0\u884c\u540e\u7684\u65f6\u95f4\uff08\u79d2\uff09 elapsed_time = end_time - start_time  # \u8ba1\u7b97\u8fd0\u884c\u65f6\u95f4 print(f\"Time: {elapsed_time}\") # Plotting print(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\") for td, actions in zip(td_init, out['actions'].cpu()):     env.render(td, actions) <pre>tensor(1.5303) 7.911144353029169 -1.7283626685098388\nTime: 3.653977155685425\nTour lengths: ['-1.53']\n</pre> In\u00a0[30]: Copied! <pre>\n</pre>"}]}