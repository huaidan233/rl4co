{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:44.872517Z",
     "start_time": "2024-08-26T12:44:32.134939600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "logger = WandbLogger(project=\"rl4co\", name=\"luop-am\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:52.506613400Z",
     "start_time": "2024-08-26T12:44:44.873514200Z"
    }
   },
   "id": "bc2bdd1d56c33b51"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from rl4co.envs import landuseOptEnv\n",
    "import torch\n",
    "from rl4co.models import AttentionModel, AttentionModelPolicy\n",
    "from rl4co.models.nn.env_embeddings.context import LOPContext\n",
    "from rl4co.models.nn.env_embeddings.dynamic import StaticEmbedding\n",
    "from rl4co.models.nn.env_embeddings.init import lopInitEmbedding\n",
    "from rl4co.utils import RL4COTrainer\n",
    "from rl4co.utils.decoding import random_policy, rollout"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:54.567024Z",
     "start_time": "2024-08-26T12:44:52.508609100Z"
    }
   },
   "id": "6073c2ce4624951b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n",
      "D:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "env = landuseOptEnv(generator_params=dict(num_loc=50))\n",
    "emb_dim = 128\n",
    "policy = AttentionModelPolicy(env_name=env.name, # this is actually not needed since we are initializing the embeddings!\n",
    "                              embed_dim=emb_dim,\n",
    "                              init_embedding=lopInitEmbedding(emb_dim),\n",
    "                              context_embedding=LOPContext(emb_dim),\n",
    "                              dynamic_embedding=StaticEmbedding(emb_dim)\n",
    ")\n",
    "# Model: default is AM with REINFORCE and greedy rollout baseline\n",
    "model = AttentionModel(env,\n",
    "                       baseline='rollout',\n",
    "                       policy=policy,\n",
    "                       batch_size=512,\n",
    "                       val_batch_size= 1024,\n",
    "                       test_batch_size= 1024,\n",
    "                       train_data_size=640_000, # really small size for demo\n",
    "                       val_data_size=10_000,\n",
    "                       test_data_size=10_000,\n",
    "                       optimizer_kwargs={\n",
    "                           \"lr\":1e-4,\n",
    "                           \"weight_decay\": 1e-6,\n",
    "                       },\n",
    "                       lr_scheduler=\"MultiStepLR\",\n",
    "                       lr_scheduler_kwargs={\n",
    "                            \"milestones\": [80, 95],\n",
    "                            \"gamma\":0.1,\n",
    "                       },\n",
    "                       policy_kwargs={  # we can specify the decode types using the policy_kwargs\n",
    "                           \"train_decode_type\": \"sampling\",\n",
    "                           \"val_decode_type\": \"greedy\",\n",
    "                           \"test_decode_type\": \"greedy\",\n",
    "                       }\n",
    "                       )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-26T12:44:54.703230Z",
     "start_time": "2024-08-26T12:44:54.574006600Z"
    }
   },
   "id": "34c7711928e9cde3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\temp\\DRLtest\\rl4co\\rl4co\\models\\nn\\attention.py:128: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  out = self.sdpa_fn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Logits contain NaNs",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m td_init \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset(batch_size\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m3\u001B[39m])\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      3\u001B[0m policy \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m----> 4\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtd_init\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mphase\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgreedy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_actions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m actions_untrained \u001B[38;5;241m=\u001B[39m out[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mactions\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[0;32m      7\u001B[0m rewards_untrained \u001B[38;5;241m=\u001B[39m out[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreward\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mE:\\temp\\DRLtest\\rl4co\\rl4co\\models\\common\\constructive\\base.py:231\u001B[0m, in \u001B[0;36mConstructivePolicy.forward\u001B[1;34m(self, td, env, phase, calc_reward, return_actions, return_entropy, return_hidden, return_init_embeds, return_sum_log_likelihood, actions, max_steps, **decoding_kwargs)\u001B[0m\n\u001B[0;32m    229\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m td[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdone\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mall():\n\u001B[1;32m--> 231\u001B[0m     logits, mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_starts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    232\u001B[0m     td \u001B[38;5;241m=\u001B[39m decode_strategy\u001B[38;5;241m.\u001B[39mstep(\n\u001B[0;32m    233\u001B[0m         logits,\n\u001B[0;32m    234\u001B[0m         mask,\n\u001B[0;32m    235\u001B[0m         td,\n\u001B[0;32m    236\u001B[0m         action\u001B[38;5;241m=\u001B[39mactions[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, step] \u001B[38;5;28;01mif\u001B[39;00m actions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    237\u001B[0m     )\n\u001B[0;32m    238\u001B[0m     td \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(td)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mE:\\temp\\DRLtest\\rl4co\\rl4co\\models\\zoo\\am\\decoder.py:191\u001B[0m, in \u001B[0;36mAttentionModelDecoder.forward\u001B[1;34m(self, td, cached, num_starts)\u001B[0m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;66;03m# Compute logits\u001B[39;00m\n\u001B[0;32m    190\u001B[0m mask \u001B[38;5;241m=\u001B[39m td[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maction_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m--> 191\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpointer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mglimpse_q\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mglimpse_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mglimpse_v\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[38;5;66;03m# Now we need to reshape the logits and mask to [B*S,N,...] is num_starts > 1 without dynamic embeddings\u001B[39;00m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# note that rearranging order is important here\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_starts \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m has_dyn_emb_multi_start:\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\rl4co\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mE:\\temp\\DRLtest\\rl4co\\rl4co\\models\\nn\\attention.py:285\u001B[0m, in \u001B[0;36mPointerAttention.forward\u001B[1;34m(self, query, key, value, logit_key, attn_mask)\u001B[0m\n\u001B[0;32m    280\u001B[0m logits \u001B[38;5;241m=\u001B[39m (torch\u001B[38;5;241m.\u001B[39mbmm(glimpse, logit_key\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)))\u001B[38;5;241m.\u001B[39msqueeze(\n\u001B[0;32m    281\u001B[0m     \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m\n\u001B[0;32m    282\u001B[0m ) \u001B[38;5;241m/\u001B[39m math\u001B[38;5;241m.\u001B[39msqrt(glimpse\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_nan:\n\u001B[1;32m--> 285\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misnan(logits)\u001B[38;5;241m.\u001B[39many(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLogits contain NaNs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m logits\n",
      "\u001B[1;31mAssertionError\u001B[0m: Logits contain NaNs"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "td_init = env.reset(batch_size=[3]).to(device)\n",
    "policy = model.policy.to(device)\n",
    "out = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\n",
    "\n",
    "actions_untrained = out['actions'].cpu().detach()\n",
    "rewards_untrained = out['reward'].cpu().detach()\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"Problem {i+1} | Cost: {-rewards_untrained[i]:.3f}\")\n",
    "    env.render(td_init[i], actions_untrained[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-26T12:45:01.113492900Z",
     "start_time": "2024-08-26T12:44:54.698242900Z"
    }
   },
   "id": "d4061c784cefe2cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint, RichModelSummary\n",
    "# Checkpointing callback: save models when validation reward improves\n",
    "checkpoint_callback = ModelCheckpoint(  dirpath=\"checkpoints\", # save to checkpoints/\n",
    "                                        filename=\"epoch_{epoch:03d}\",  # save as epoch_XXX.ckpt\n",
    "                                        save_top_k=1, # save only the best model\n",
    "                                        save_last=True, # save the last model\n",
    "                                        monitor=\"val/reward\", # monitor validation reward\n",
    "                                        mode=\"max\") # maximize validation reward\n",
    "\n",
    "# Print model summary\n",
    "rich_model_summary = RichModelSummary(max_depth=3)\n",
    "# Callbacks list\n",
    "callbacks = [checkpoint_callback, rich_model_summary]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-26T12:45:01.117482500Z",
     "start_time": "2024-08-26T12:45:01.116485400Z"
    }
   },
   "id": "e5b79ef4586f2253"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from rl4co.utils.trainer import RL4COTrainer\n",
    "\n",
    "trainer = RL4COTrainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    callbacks=callbacks,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-26T12:45:01.122471500Z"
    }
   },
   "id": "31990eac52a022ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-26T12:45:01.127456600Z"
    }
   },
   "id": "13764d9d2965244d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Greedy rollouts over trained model (same states as previous plot)\n",
    "policy = model.policy.to(device)\n",
    "out = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\n",
    "\n",
    "# Plotting\n",
    "print(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\")\n",
    "for td, actions in zip(td_init, out['actions'].cpu()):\n",
    "    env.render(td, actions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-26T12:45:01.130448300Z"
    }
   },
   "id": "385bb46f4341aba6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-26T12:45:01.133440300Z"
    }
   },
   "id": "ad5e0e5829400683"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
